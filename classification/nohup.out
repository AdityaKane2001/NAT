--2023-11-28 08:14:46--  https://people.eecs.berkeley.edu/~hendrycks/imagenet-o.tar
Resolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190
Connecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 177617920 (169M) [application/x-tar]
Saving to: ‘imagenet-o.tar’

     0K .......... .......... .......... .......... ..........  0% 1.64M 1m43s
    50K .......... .......... .......... .......... ..........  0% 4.10M 72s
   100K .......... .......... .......... .......... ..........  0% 21.7M 51s
   150K .......... .......... .......... .......... ..........  0% 4.30M 48s
   200K .......... .......... .......... .......... ..........  0% 23.1M 40s
   250K .......... .......... .......... .......... ..........  0% 77.0M 33s
   300K .......... .......... .......... .......... ..........  0% 97.5M 29s
   350K .......... .......... .......... .......... ..........  0% 46.6M 26s
   400K .......... .......... .......... .......... ..........  0% 5.02M 27s
   450K .......... .......... .......... .......... ..........  0% 23.1M 25s
   500K .......... .......... .......... .......... ..........  0%  108M 23s
   550K .......... .......... .......... .......... ..........  0% 86.9M 21s
   600K .......... .......... .......... .......... ..........  0%  124M 19s
   650K .......... .......... .......... .......... ..........  0% 93.3M 18s
   700K .......... .......... .......... .......... ..........  0%  101M 17s
   750K .......... .......... .......... .......... ..........  0%  126M 16s
   800K .......... .......... .......... .......... ..........  0% 56.1M 15s
   850K .......... .......... .......... .......... ..........  0% 6.04M 16s
   900K .......... .......... .......... .......... ..........  0% 79.8M 15s
   950K .......... .......... .......... .......... ..........  0% 79.7M 15s
  1000K .......... .......... .......... .......... ..........  0% 38.8M 14s
  1050K .......... .......... .......... .......... ..........  0% 67.7M 14s
  1100K .......... .......... .......... .......... ..........  0%  211M 13s
  1150K .......... .......... .......... .......... ..........  0%  124M 12s
  1200K .......... .......... .......... .......... ..........  0%  213M 12s
  1250K .......... .......... .......... .......... ..........  0%  189M 12s
  1300K .......... .......... .......... .......... ..........  0%  215M 11s
  1350K .......... .......... .......... .......... ..........  0%  215M 11s
  1400K .......... .......... .......... .......... ..........  0%  212M 10s
  1450K .......... .......... .......... .......... ..........  0%  134M 10s
  1500K .......... .......... .......... .......... ..........  0%  210M 10s
  1550K .......... .......... .......... .......... ..........  0%  215M 10s
  1600K .......... .......... .......... .......... ..........  0%  193M 9s
  1650K .......... .......... .......... .......... ..........  0%  201M 9s
  1700K .......... .......... .......... .......... ..........  1% 80.2M 9s
  1750K .......... .......... .......... .......... ..........  1% 7.33M 9s
  1800K .......... .......... .......... .......... ..........  1%  218M 9s
  1850K .......... .......... .......... .......... ..........  1%  136M 9s
  1900K .......... .......... .......... .......... ..........  1%  200M 9s
  1950K .......... .......... .......... .......... ..........  1% 87.3M 8s
  2000K .......... .......... .......... .......... ..........  1% 49.3M 8s
  2050K .......... .......... .......... .......... ..........  1%  217M 8s
  2100K .......... .......... .......... .......... ..........  1% 90.7M 8s
  2150K .......... .......... .......... .......... ..........  1% 46.9M 8s
  2200K .......... .......... .......... .......... ..........  1%  257M 8s
  2250K .......... .......... .......... .......... ..........  1%  269M 8s
  2300K .......... .......... .......... .......... ..........  1%  259M 7s
  2350K .......... .......... .......... .......... ..........  1%  153M 7s
  2400K .......... .......... .......... .......... ..........  1%  119M 7s
  2450K .......... .......... .......... .......... ..........  1%  246M 7s
  2500K .......... .......... .......... .......... ..........  1%  264M 7s
  2550K .......... .......... .......... .......... ..........  1%  227M 7s
  2600K .......... .......... .......... .......... ..........  1%  266M 7s
  2650K .......... .......... .......... .......... ..........  1%  256M 7s
  2700K .......... .......... .......... .......... ..........  1%  234M 6s
  2750K .......... .......... .......... .......... ..........  1%  158M 6s
  2800K .......... .......... .......... .......... ..........  1%  260M 6s
  2850K .......... .......... .......... .......... ..........  1%  265M 6s
  2900K .......... .......... .......... .......... ..........  1%  211M 6s
  2950K .......... .......... .......... .......... ..........  1%  263M 6s
  3000K .......... .......... .......... .......... ..........  1%  202M 6s
  3050K .......... .......... .......... .......... ..........  1%  242M 6s
  3100K .......... .......... .......... .......... ..........  1%  260M 6s
  3150K .......... .......... .......... .......... ..........  1%  261M 6s
  3200K .......... .......... .......... .......... ..........  1%  242M 6s
  3250K .......... .......... .......... .......... ..........  1%  221M 5s
  3300K .......... .......... .......... .......... ..........  1%  242M 5s
  3350K .......... .......... .......... .......... ..........  1%  271M 5s
  3400K .......... .......... .......... .......... ..........  1%  240M 5s
  3450K .......... .......... .......... .......... ..........  2%  235M 5s
  3500K .......... .......... .......... .......... ..........  2% 12.3M 5s
  3550K .......... .......... .......... .......... ..........  2%  251M 5s
  3600K .......... .......... .......... .......... ..........  2% 95.0M 5s
  3650K .......... .......... .......... .......... ..........  2% 89.6M 5s
  3700K .......... .......... .......... .......... ..........  2% 55.5M 5s
  3750K .......... .......... .......... .......... ..........  2%  123M 5s
  3800K .......... .......... .......... .......... ..........  2%  112M 5s
  3850K .......... .......... .......... .......... ..........  2%  148M 5s
  3900K .......... .......... .......... .......... ..........  2%  193M 5s
  3950K .......... .......... .......... .......... ..........  2%  232M 5s
  4000K .......... .......... .......... .......... ..........  2%  187M 5s
  4050K .......... .......... .......... .......... ..........  2%  222M 5s
  4100K .......... .......... .......... .......... ..........  2%  189M 5s
  4150K .......... .......... .......... .......... ..........  2%  186M 5s
  4200K .......... .......... .......... .......... ..........  2%  201M 5s
  4250K .......... .......... .......... .......... ..........  2%  173M 5s
  4300K .......... .......... .......... .......... ..........  2%  187M 5s
  4350K .......... .......... .......... .......... ..........  2%  172M 4s
  4400K .......... .......... .......... .......... ..........  2%  209M 4s
  4450K .......... .......... .......... .......... ..........  2%  202M 4s
  4500K .......... .......... .......... .......... ..........  2%  183M 4s
  4550K .......... .......... .......... .......... ..........  2%  161M 4s
  4600K .......... .......... .......... .......... ..........  2%  193M 4s
  4650K .......... .......... .......... .......... ..........  2%  195M 4s
  4700K .......... .......... .......... .......... ..........  2%  153M 4s
  4750K .......... .......... .......... .......... ..........  2%  218M 4s
  4800K .......... .......... .......... .......... ..........  2%  163M 4s
  4850K .......... .......... .......... .......... ..........  2%  172M 4s
  4900K .......... .......... .......... .......... ..........  2%  197M 4s
  4950K .......... .......... .......... .......... ..........  2%  183M 4s
  5000K .......... .......... .......... .......... ..........  2%  111M 4s
  5050K .......... .......... .......... .......... ..........  2% 66.7M 4s
  5100K .......... .......... .......... .......... ..........  2% 85.7M 4s
  5150K .......... .......... .......... .......... ..........  2% 64.7M 4s
  5200K .......... .......... .......... .......... ..........  3% 23.2M 4s
  5250K .......... .......... .......... .......... ..........  3% 64.9M 4s
  5300K .......... .......... .......... .......... ..........  3%  210M 4s
  5350K .......... .......... .......... .......... ..........  3% 37.8M 4s
  5400K .......... .......... .......... .......... ..........  3%  178M 4s
  5450K .......... .......... .......... .......... ..........  3% 30.3M 4s
  5500K .......... .......... .......... .......... ..........  3% 26.8M 4s
  5550K .......... .......... .......... .......... ..........  3% 27.5M 4s
  5600K .......... .......... .......... .......... ..........  3% 47.3M 4s
  5650K .......... .......... .......... .......... ..........  3% 53.5M 4s
  5700K .......... .......... .......... .......... ..........  3% 40.8M 4s
  5750K .......... .......... .......... .......... ..........  3% 73.6M 4s
  5800K .......... .......... .......... .......... ..........  3% 50.7M 4s
  5850K .......... .......... .......... .......... ..........  3% 75.1M 4s
  5900K .......... .......... .......... .......... ..........  3% 50.6M 4s
  5950K .......... .......... .......... .......... ..........  3% 61.3M 4s
  6000K .......... .......... .......... .......... ..........  3% 63.1M 4s
  6050K .......... .......... .......... .......... ..........  3% 61.6M 4s
  6100K .......... .......... .......... .......... ..........  3% 76.1M 4s
  6150K .......... .......... .......... .......... ..........  3% 49.0M 4s
  6200K .......... .......... .......... .......... ..........  3% 46.7M 4s
  6250K .......... .......... .......... .......... ..........  3% 59.3M 4s
  6300K .......... .......... .......... .......... ..........  3% 90.5M 4s
  6350K .......... .......... .......... .......... ..........  3% 72.8M 4s
  6400K .......... .......... .......... .......... ..........  3% 30.7M 4s
  6450K .......... .......... .......... .......... ..........  3% 73.7M 4s
  6500K .......... .......... .......... .......... ..........  3% 83.6M 4s
  6550K .......... .......... .......... .......... ..........  3% 46.3M 4s
  6600K .......... .......... .......... .......... ..........  3% 67.2M 4s
  6650K .......... .......... .......... .......... ..........  3% 54.5M 4s
  6700K .......... .......... .......... .......... ..........  3% 73.2M 4s
  6750K .......... .......... .......... .......... ..........  3% 77.1M 4s
  6800K .......... .......... .......... .......... ..........  3% 75.2M 4s
  6850K .......... .......... .......... .......... ..........  3% 62.2M 4s
  6900K .......... .......... .......... .......... ..........  4% 51.8M 4s
  6950K .......... .......... .......... .......... ..........  4% 60.1M 4s
  7000K .......... .......... .......... .......... ..........  4% 81.9M 4s
  7050K .......... .......... .......... .......... ..........  4% 74.6M 4s
  7100K .......... .......... .......... .......... ..........  4% 55.5M 4s
  7150K .......... .......... .......... .......... ..........  4% 59.3M 4s
  7200K .......... .......... .......... .......... ..........  4% 62.2M 4s
  7250K .......... .......... .......... .......... ..........  4% 51.4M 4s
  7300K .......... .......... .......... .......... ..........  4% 70.3M 4s
  7350K .......... .......... .......... .......... ..........  4% 61.3M 4s
  7400K .......... .......... .......... .......... ..........  4% 66.9M 4s
  7450K .......... .......... .......... .......... ..........  4% 56.2M 4s
  7500K .......... .......... .......... .......... ..........  4% 43.2M 4s
  7550K .......... .......... .......... .......... ..........  4% 78.7M 4s
  7600K .......... .......... .......... .......... ..........  4% 50.3M 4s
  7650K .......... .......... .......... .......... ..........  4% 51.9M 4s
  7700K .......... .......... .......... .......... ..........  4% 63.6M 4s
  7750K .......... .......... .......... .......... ..........  4% 46.5M 4s
  7800K .......... .......... .......... .......... ..........  4% 51.6M 4s
  7850K .......... .......... .......... .......... ..........  4% 56.2M 4s
  7900K .......... .......... .......... .......... ..........  4% 52.3M 4s
  7950K .......... .......... .......... .......... ..........  4% 59.7M 4s
  8000K .......... .......... .......... .......... ..........  4% 48.2M 4s
  8050K .......... .......... .......... .......... ..........  4% 82.5M 4s
  8100K .......... .......... .......... .......... ..........  4% 65.6M 4s
  8150K .......... .......... .......... .......... ..........  4% 52.3M 4s
  8200K .......... .......... .......... .......... ..........  4% 47.7M 4s
  8250K .......... .......... .......... .......... ..........  4% 60.8M 4s
  8300K .......... .......... .......... .......... ..........  4% 42.6M 4s
  8350K .......... .......... .......... .......... ..........  4% 41.0M 4s
  8400K .......... .......... .......... .......... ..........  4% 57.1M 4s
  8450K .......... .......... .......... .......... ..........  4% 76.6M 4s
  8500K .......... .......... .......... .......... ..........  4% 69.7M 4s
  8550K .......... .......... .......... .......... ..........  4% 65.5M 4s
  8600K .......... .......... .......... .......... ..........  4% 79.9M 3s
  8650K .......... .......... .......... .......... ..........  5% 59.1M 3s
  8700K .......... .......... .......... .......... ..........  5% 55.3M 3s
  8750K .......... .......... .......... .......... ..........  5% 76.1M 3s
  8800K .......... .......... .......... .......... ..........  5% 64.7M 3s
  8850K .......... .......... .......... .......... ..........  5% 60.4M 3s
  8900K .......... .......... .......... .......... ..........  5% 75.1M 3s
  8950K .......... .......... .......... .......... ..........  5% 75.7M 3s
  9000K .......... .......... .......... .......... ..........  5% 62.5M 3s
  9050K .......... .......... .......... .......... ..........  5% 67.3M 3s
  9100K .......... .......... .......... .......... ..........  5% 58.9M 3s
  9150K .......... .......... .......... .......... ..........  5% 78.0M 3s
  9200K .......... .......... .......... .......... ..........  5% 68.5M 3s
  9250K .......... .......... .......... .......... ..........  5% 85.2M 3s
  9300K .......... .......... .......... .......... ..........  5% 68.6M 3s
  9350K .......... .......... .......... .......... ..........  5% 69.8M 3s
  9400K .......... .......... .......... .......... ..........  5% 57.1M 3s
  9450K .......... .......... .......... .......... ..........  5% 47.8M 3s
  9500K .......... .......... .......... .......... ..........  5% 64.2M 3s
  9550K .......... .......... .......... .......... ..........  5% 67.6M 3s
  9600K .......... .......... .......... .......... ..........  5% 61.1M 3s
  9650K .......... .......... .......... .......... ..........  5% 59.1M 3s
  9700K .......... .......... .......... .......... ..........  5% 61.1M 3s
  9750K .......... .......... .......... .......... ..........  5% 59.1M 3s
  9800K .......... .......... .......... .......... ..........  5% 30.4M 3s
  9850K .......... .......... .......... .......... ..........  5% 28.5M 3s
  9900K .......... .......... .......... .......... ..........  5% 28.3M 3s
  9950K .......... .......... .......... .......... ..........  5% 31.3M 3s
 10000K .......... .......... .......... .......... ..........  5% 24.3M 3s
 10050K .......... .......... .......... .......... ..........  5% 35.8M 3s
 10100K .......... .......... .......... .......... ..........  5% 55.0M 3s
 10150K .......... .......... .......... .......... ..........  5% 78.4M 3s
 10200K .......... .......... .......... .......... ..........  5% 44.0M 3s
 10250K .......... .......... .......... .......... ..........  5% 68.4M 3s
 10300K .......... .......... .......... .......... ..........  5% 62.1M 3s
 10350K .......... .......... .......... .......... ..........  5% 76.9M 3s
 10400K .......... .......... .......... .......... ..........  6% 76.3M 3s
 10450K .......... .......... .......... .......... ..........  6% 82.4M 3s
 10500K .......... .......... .......... .......... ..........  6% 57.8M 3s
 10550K .......... .......... .......... .......... ..........  6% 51.1M 3s
 10600K .......... .......... .......... .......... ..........  6% 89.9M 3s
 10650K .......... .......... .......... .......... ..........  6% 53.3M 3s
 10700K .......... .......... .......... .......... ..........  6% 62.0M 3s
 10750K .......... .......... .......... .......... ..........  6% 93.9M 3s
 10800K .......... .......... .......... .......... ..........  6% 63.0M 3s
 10850K .......... .......... .......... .......... ..........  6% 96.4M 3s
 10900K .......... .......... .......... .......... ..........  6% 66.8M 3s
 10950K .......... .......... .......... .......... ..........  6% 79.3M 3s
 11000K .......... .......... .......... .......... ..........  6% 62.8M 3s
 11050K .......... .......... .......... .......... ..........  6% 63.4M 3s
 11100K .......... .......... .......... .......... ..........  6% 69.8M 3s
 11150K .......... .......... .......... .......... ..........  6% 57.0M 3s
 11200K .......... .......... .......... .......... ..........  6%  100M 3s
 11250K .......... .......... .......... .......... ..........  6% 43.6M 3s
 11300K .......... .......... .......... .......... ..........  6% 60.1M 3s
 11350K .......... .......... .......... .......... ..........  6% 48.3M 3s
 11400K .......... .......... .......... .......... ..........  6% 66.0M 3s
 11450K .......... .......... .......... .......... ..........  6% 65.5M 3s
 11500K .......... .......... .......... .......... ..........  6% 54.2M 3s
 11550K .......... .......... .......... .......... ..........  6% 72.4M 3s
 11600K .......... .......... .......... .......... ..........  6% 65.2M 3s
 11650K .......... .......... .......... .......... ..........  6% 53.0M 3s
 11700K .......... .......... .......... .......... ..........  6% 61.9M 3s
 11750K .......... .......... .......... .......... ..........  6% 46.9M 3s
 11800K .......... .......... .......... .......... ..........  6% 41.4M 3s
 11850K .......... .......... .......... .......... ..........  6% 53.3M 3s
 11900K .......... .......... .......... .......... ..........  6% 68.1M 3s
 11950K .......... .......... .......... .......... ..........  6% 69.2M 3s
 12000K .......... .......... .......... .......... ..........  6% 50.9M 3s
 12050K .......... .......... .......... .......... ..........  6% 53.4M 3s
 12100K .......... .......... .......... .......... ..........  7% 59.9M 3s
 12150K .......... .......... .......... .......... ..........  7% 70.3M 3s
 12200K .......... .......... .......... .......... ..........  7% 86.2M 3s
 12250K .......... .......... .......... .......... ..........  7% 62.8M 3s
 12300K .......... .......... .......... .......... ..........  7% 69.1M 3s
 12350K .......... .......... .......... .......... ..........  7% 54.7M 3s
 12400K .......... .......... .......... .......... ..........  7% 53.3M 3s
 12450K .......... .......... .......... .......... ..........  7% 70.6M 3s
 12500K .......... .......... .......... .......... ..........  7% 60.5M 3s
 12550K .......... .......... .......... .......... ..........  7% 63.0M 3s
 12600K .......... .......... .......... .......... ..........  7% 71.1M 3s
 12650K .......... .......... .......... .......... ..........  7% 68.4M 3s
 12700K .......... .......... .......... .......... ..........  7% 71.1M 3s
 12750K .......... .......... .......... .......... ..........  7% 54.0M 3s
 12800K .......... .......... .......... .......... ..........  7% 73.9M 3s
 12850K .......... .......... .......... .......... ..........  7% 58.5M 3s
 12900K .......... .......... .......... .......... ..........  7% 63.8M 3s
 12950K .......... .......... .......... .......... ..........  7% 70.7M 3s
 13000K .......... .......... .......... .......... ..........  7% 71.7M 3s
 13050K .......... .......... .......... .......... ..........  7% 92.6M 3s
 13100K .......... .......... .......... .......... ..........  7% 56.7M 3s
 13150K .......... .......... .......... .......... ..........  7% 49.6M 3s
 13200K .......... .......... .......... .......... ..........  7% 58.1M 3s
 13250K .......... .......... .......... .......... ..........  7% 69.9M 3s
 13300K .......... .......... .......... .......... ..........  7% 64.2M 3s
 13350K .......... .......... .......... .......... ..........  7% 60.4M 3s
 13400K .......... .......... .......... .......... ..........  7% 71.3M 3s
 13450K .......... .......... .......... .......... ..........  7% 75.9M 3s
 13500K .......... .......... .......... .......... ..........  7% 51.1M 3s
 13550K .......... .......... .......... .......... ..........  7% 52.9M 3s
 13600K .......... .......... .......... .......... ..........  7% 77.9M 3s
 13650K .......... .......... .......... .......... ..........  7% 67.9M 3s
 13700K .......... .......... .......... .......... ..........  7% 81.9M 3s
 13750K .......... .......... .......... .......... ..........  7% 68.6M 3s
 13800K .......... .......... .......... .......... ..........  7% 39.6M 3s
 13850K .......... .......... .......... .......... ..........  8% 54.1M 3s
 13900K .......... .......... .......... .......... ..........  8% 56.3M 3s
 13950K .......... .......... .......... .......... ..........  8% 61.3M 3s
 14000K .......... .......... .......... .......... ..........  8% 54.3M 3s
 14050K .......... .......... .......... .......... ..........  8% 73.5M 3s
 14100K .......... .......... .......... .......... ..........  8% 46.8M 3s
 14150K .......... .......... .......... .......... ..........  8% 59.8M 3s
 14200K .......... .......... .......... .......... ..........  8% 37.6M 3s
 14250K .......... .......... .......... .......... ..........  8% 50.6M 3s
 14300K .......... .......... .......... .......... ..........  8% 32.4M 3s
 14350K .......... .......... .......... .......... ..........  8% 24.7M 3s
 14400K .......... .......... .......... .......... ..........  8% 23.2M 3s
 14450K .......... .......... .......... .......... ..........  8% 19.3M 3s
 14500K .......... .......... .......... .......... ..........  8% 15.1M 3s
 14550K .......... .......... .......... .......... ..........  8% 16.2M 3s
 14600K .......... .......... .......... .......... ..........  8% 34.6M 3s
 14650K .......... .......... .......... .......... ..........  8% 37.5M 3s
 14700K .......... .......... .......... .......... ..........  8% 37.6M 3s
 14750K .......... .......... .......... .......... ..........  8% 32.4M 3s
 14800K .......... .......... .......... .......... ..........  8% 40.3M 3s
 14850K .......... .......... .......... .......... ..........  8% 55.0M 3s
 14900K .......... .......... .......... .......... ..........  8% 57.8M 3s
 14950K .......... .......... .......... .......... ..........  8% 79.4M 3s
 15000K .......... .......... .......... .......... ..........  8% 72.3M 3s
 15050K .......... .......... .......... .......... ..........  8% 87.9M 3s
 15100K .......... .......... .......... .......... ..........  8% 70.7M 3s
 15150K .......... .......... .......... .......... ..........  8% 71.2M 3s
 15200K .......... .......... .......... .......... ..........  8% 68.4M 3s
 15250K .......... .......... .......... .......... ..........  8% 83.7M 3s
 15300K .......... .......... .......... .......... ..........  8% 49.9M 3s
 15350K .......... .......... .......... .......... ..........  8% 81.2M 3s
 15400K .......... .......... .......... .......... ..........  8% 74.8M 3s
 15450K .......... .......... .......... .......... ..........  8% 76.0M 3s
 15500K .......... .......... .......... .......... ..........  8% 75.7M 3s
 15550K .......... .......... .......... .......... ..........  8% 80.6M 3s
 15600K .......... .......... .......... .......... ..........  9% 60.7M 3s
 15650K .......... .......... .......... .......... ..........  9% 53.6M 3s
 15700K .......... .......... .......... .......... ..........  9% 71.5M 3s
 15750K .......... .......... .......... .......... ..........  9% 62.3M 3s
 15800K .......... .......... .......... .......... ..........  9% 71.2M 3s
 15850K .......... .......... .......... .......... ..........  9% 69.7M 3s
 15900K .......... .......... .......... .......... ..........  9% 49.7M 3s
 15950K .......... .......... .......... .......... ..........  9% 54.6M 3s
 16000K .......... .......... .......... .......... ..........  9% 44.6M 3s
 16050K .......... .......... .......... .......... ..........  9% 57.5M 3s
 16100K .......... .......... .......... .......... ..........  9% 54.9M 3s
 16150K .......... .......... .......... .......... ..........  9% 54.8M 3s
 16200K .......... .......... .......... .......... ..........  9% 59.5M 3s
 16250K .......... .......... .......... .......... ..........  9% 54.8M 3s
 16300K .......... .......... .......... .......... ..........  9% 40.5M 3s
 16350K .......... .......... .......... .......... ..........  9% 53.2M 3s
 16400K .......... .......... .......... .......... ..........  9% 48.5M 3s
 16450K .......... .......... .......... .......... ..........  9% 66.9M 3s
 16500K .......... .......... .......... .......... ..........  9% 57.2M 3s
 16550K .......... .......... .......... .......... ..........  9% 47.9M 3s
 16600K .......... .......... .......... .......... ..........  9% 63.1M 3s
 16650K .......... .......... .......... .......... ..........  9% 50.7M 3s
 16700K .......... .......... .......... .......... ..........  9% 47.2M 3s
 16750K .......... .......... .......... .......... ..........  9% 41.5M 3s
 16800K .......... .......... .......... .......... ..........  9% 56.7M 3s
 16850K .......... .......... .......... .......... ..........  9% 32.1M 3s
 16900K .......... .......... .......... .......... ..........  9% 63.3M 3s
 16950K .......... .......... .......... .......... ..........  9% 54.6M 3s
 17000K .......... .......... .......... .......... ..........  9% 71.7M 3s
 17050K .......... .......... .......... .......... ..........  9% 60.7M 3s
 17100K .......... .......... .......... .......... ..........  9% 50.2M 3s
 17150K .......... .......... .......... .......... ..........  9% 77.8M 3s
 17200K .......... .......... .......... .......... ..........  9% 55.0M 3s
 17250K .......... .......... .......... .......... ..........  9% 39.6M 3s
 17300K .......... .......... .......... .......... .......... 10% 80.6M 3s
 17350K .......... .......... .......... .......... .......... 10% 58.6M 3s
 17400K .......... .......... .......... .......... .......... 10% 61.5M 3s
 17450K .......... .......... .......... .......... .......... 10% 51.6M 3s
 17500K .......... .......... .......... .......... .......... 10% 79.8M 3s
 17550K .......... .......... .......... .......... .......... 10% 46.1M 3s
 17600K .......... .......... .......... .......... .......... 10% 68.2M 3s
 17650K .......... .......... .......... .......... .......... 10% 65.0M 3s
 17700K .......... .......... .......... .......... .......... 10% 74.2M 3s
 17750K .......... .......... .......... .......... .......... 10% 61.4M 3s
 17800K .......... .......... .......... .......... .......... 10% 57.9M 3s
 17850K .......... .......... .......... .......... .......... 10% 45.4M 3s
 17900K .......... .......... .......... .......... .......... 10% 57.5M 3s
 17950K .......... .......... .......... .......... .......... 10% 70.8M 3s
 18000K .......... .......... .......... .......... .......... 10% 65.8M 3s
 18050K .......... .......... .......... .......... .......... 10% 62.4M 3s
 18100K .......... .......... .......... .......... .......... 10% 61.1M 3s
 18150K .......... .......... .......... .......... .......... 10% 56.4M 3s
 18200K .......... .......... .......... .......... .......... 10% 51.5M 3s
 18250K .......... .......... .......... .......... .......... 10% 43.0M 3s
 18300K .......... .......... .......... .......... .......... 10% 27.5M 3s
 18350K .......... .......... .......... .......... .......... 10% 25.5M 3s
 18400K .......... .......... .......... .......... .......... 10% 32.6M 3s
 18450K .......... .......... .......... .......... .......... 10% 27.8M 3s
 18500K .......... .......... .......... .......... .......... 10% 31.2M 3s
 18550K .......... .......... .......... .......... .......... 10% 40.7M 3s
 18600K .......... .......... .......... .......... .......... 10% 62.3M 3s
 18650K .......... .......... .......... .......... .......... 10% 57.6M 3s
 18700K .......... .......... .......... .......... .......... 10% 56.1M 3s
 18750K .......... .......... .......... .......... .......... 10% 61.4M 3s
 18800K .......... .......... .......... .......... .......... 10% 56.6M 3s
 18850K .......... .......... .......... .......... .......... 10% 83.7M 3s
 18900K .......... .......... .......... .......... .......... 10% 57.8M 3s
 18950K .......... .......... .......... .......... .......... 10% 74.5M 3s
 19000K .......... .......... .......... .......... .......... 10% 54.7M 3s
 19050K .......... .......... .......... .......... .......... 11% 67.6M 3s
 19100K .......... .......... .......... .......... .......... 11% 67.3M 3s
 19150K .......... .......... .......... .......... .......... 11% 76.3M 3s
 19200K .......... .......... .......... .......... .......... 11% 69.2M 3s
 19250K .......... .......... .......... .......... .......... 11% 82.9M 3s
 19300K .......... .......... .......... .......... .......... 11% 64.3M 3s
 19350K .......... .......... .......... .......... .......... 11% 63.9M 3s
 19400K .......... .......... .......... .......... .......... 11% 49.9M 3s
 19450K .......... .......... .......... .......... .......... 11% 25.5M 3s
 19500K .......... .......... .......... .......... .......... 11% 34.2M 3s
 19550K .......... .......... .......... .......... .......... 11% 32.8M 3s
 19600K .......... .......... .......... .......... .......... 11% 32.6M 3s
 19650K .......... .......... .......... .......... .......... 11% 44.9M 3s
 19700K .......... .......... .......... .......... .......... 11% 48.6M 3s
 19750K .......... .......... .......... .......... .......... 11% 70.1M 3s
 19800K .......... .......... .......... .......... .......... 11% 53.6M 3s
 19850K .......... .......... .......... .......... .......... 11% 44.8M 3s
 19900K .......... .......... .......... .......... .......... 11% 50.2M 3s
 19950K .......... .......... .......... .......... .......... 11% 49.5M 3s
 20000K .......... .......... .......... .......... .......... 11% 51.9M 3s
 20050K .......... .......... .......... .......... .......... 11% 79.0M 3s
 20100K .......... .......... .......... .......... .......... 11% 72.0M 3s
 20150K .......... .......... .......... .......... .......... 11% 65.0M 3s
 20200K .......... .......... .......... .......... .......... 11% 61.0M 3s
 20250K .......... .......... .......... .......... .......... 11% 86.1M 3s
 20300K .......... .......... .......... .......... .......... 11% 65.5M 3s
 20350K .......... .......... .......... .......... .......... 11% 74.0M 3s
 20400K .......... .......... .......... .......... .......... 11% 84.0M 3s
 20450K .......... .......... .......... .......... .......... 11% 68.7M 3s
 20500K .......... .......... .......... .......... .......... 11% 73.3M 3s
 20550K .......... .......... .......... .......... .......... 11% 58.0M 3s
 20600K .......... .......... .......... .......... .......... 11% 64.4M 3s
 20650K .......... .......... .......... .......... .......... 11% 54.6M 3s
 20700K .......... .......... .......... .......... .......... 11% 63.4M 3s
 20750K .......... .......... .......... .......... .......... 11% 52.4M 3s
 20800K .......... .......... .......... .......... .......... 12% 38.4M 3s
 20850K .......... .......... .......... .......... .......... 12% 71.1M 3s
 20900K .......... .......... .......... .......... .......... 12% 46.6M 3s
 20950K .......... .......... .......... .......... .......... 12% 76.5M 3s
 21000K .......... .......... .......... .......... .......... 12% 61.8M 3s
 21050K .......... .......... .......... .......... .......... 12% 56.1M 3s
 21100K .......... .......... .......... .......... .......... 12% 40.4M 3s
 21150K .......... .......... .......... .......... .......... 12% 65.6M 3s
 21200K .......... .......... .......... .......... .......... 12% 46.4M 3s
 21250K .......... .......... .......... .......... .......... 12% 53.6M 3s
 21300K .......... .......... .......... .......... .......... 12% 63.8M 3s
 21350K .......... .......... .......... .......... .......... 12% 57.5M 3s
 21400K .......... .......... .......... .......... .......... 12% 56.0M 3s
 21450K .......... .......... .......... .......... .......... 12% 45.2M 3s
 21500K .......... .......... .......... .......... .......... 12% 70.5M 3s
 21550K .......... .......... .......... .......... .......... 12% 67.0M 3s
 21600K .......... .......... .......... .......... .......... 12% 63.4M 3s
 21650K .......... .......... .......... .......... .......... 12% 50.2M 3s
 21700K .......... .......... .......... .......... .......... 12% 66.7M 3s
 21750K .......... .......... .......... .......... .......... 12% 59.6M 3s
 21800K .......... .......... .......... .......... .......... 12% 70.4M 3s
 21850K .......... .......... .......... .......... .......... 12% 46.0M 3s
 21900K .......... .......... .......... .......... .......... 12% 70.4M 3s
 21950K .......... .......... .......... .......... .......... 12% 55.7M 3s
 22000K .......... .......... .......... .......... .......... 12% 82.7M 3s
 22050K .......... .......... .......... .......... .......... 12% 51.7M 3s
 22100K .......... .......... .......... .......... .......... 12% 60.4M 3s
 22150K .......... .......... .......... .......... .......... 12% 56.6M 3s
 22200K .......... .......... .......... .......... .......... 12% 47.0M 3s
 22250K .......... .......... .......... .......... .......... 12% 52.7M 3s
 22300K .......... .......... .......... .......... .......... 12% 45.3M 3s
 22350K .......... .......... .......... .......... .......... 12% 53.2M 3s
 22400K .......... .......... .......... .......... .......... 12% 49.8M 3s
 22450K .......... .......... .......... .......... .......... 12% 55.4M 3s
 22500K .......... .......... .......... .......... .......... 13% 59.8M 3s
 22550K .......... .......... .......... .......... .......... 13% 20.2M 3s
 22600K .......... .......... .......... .......... .......... 13% 24.1M 3s
 22650K .......... .......... .......... .......... .......... 13% 23.0M 3s
 22700K .......... .......... .......... .......... .......... 13% 27.3M 3s
 22750K .......... .......... .......... .......... .......... 13% 28.0M 3s
 22800K .......... .......... .......... .......... .......... 13% 50.8M 3s
 22850K .......... .......... .......... .......... .......... 13% 66.0M 3s
 22900K .......... .......... .......... .......... .......... 13% 61.3M 3s
 22950K .......... .......... .......... .......... .......... 13% 55.2M 3s
 23000K .......... .......... .......... .......... .......... 13% 76.8M 3s
 23050K .......... .......... .......... .......... .......... 13% 79.2M 3s
 23100K .......... .......... .......... .......... .......... 13% 82.5M 3s
 23150K .......... .......... .......... .......... .......... 13% 80.1M 3s
 23200K .......... .......... .......... .......... .......... 13% 63.3M 3s
 23250K .......... .......... .......... .......... .......... 13% 75.1M 3s
 23300K .......... .......... .......... .......... .......... 13% 47.8M 3s
 23350K .......... .......... .......... .......... .......... 13% 77.1M 3s
 23400K .......... .......... .......... .......... .......... 13%  107M 3s
 23450K .......... .......... .......... .......... .......... 13% 77.5M 3s
 23500K .......... .......... .......... .......... .......... 13% 79.7M 3s
 23550K .......... .......... .......... .......... .......... 13% 63.4M 3s
 23600K .......... .......... .......... .......... .......... 13% 77.0M 3s
 23650K .......... .......... .......... .......... .......... 13% 59.4M 3s
 23700K .......... .......... .......... .......... .......... 13% 75.8M 3s
 23750K .......... .......... .......... .......... .......... 13% 65.6M 3s
 23800K .......... .......... .......... .......... .......... 13% 65.9M 3s
 23850K .......... .......... .......... .......... .......... 13% 62.5M 3s
 23900K .......... .......... .......... .......... .......... 13% 59.6M 3s
 23950K .......... .......... .......... .......... .......... 13% 80.1M 3s
 24000K .......... .......... .......... .......... .......... 13% 43.4M 3s
 24050K .......... .......... .......... .......... .......... 13% 48.4M 3s
 24100K .......... .......... .......... .......... .......... 13% 48.6M 3s
 24150K .......... .......... .......... .......... .......... 13% 92.6M 3s
 24200K .......... .......... .......... .......... .......... 13% 64.6M 3s
 24250K .......... .......... .......... .......... .......... 14% 55.9M 3s
 24300K .......... .......... .......... .......... .......... 14% 76.1M 3s
 24350K .......... .......... .......... .......... .......... 14% 59.7M 3s
 24400K .......... .......... .......... .......... .......... 14% 49.4M 3s
 24450K .......... .......... .......... .......... .......... 14% 73.8M 3s
 24500K .......... .......... .......... .......... .......... 14% 58.7M 3s
 24550K .......... .......... .......... .......... .......... 14% 70.9M 3s
 24600K .......... .......... .......... .......... .......... 14% 60.9M 3s
 24650K .......... .......... .......... .......... .......... 14% 33.7M 3s
 24700K .......... .......... .......... .......... .......... 14% 20.4M 3s
 24750K .......... .......... .......... .......... .......... 14% 60.3M 3s
 24800K .......... .......... .......... .......... .......... 14% 58.6M 3s
 24850K .......... .......... .......... .......... .......... 14% 48.3M 3s
 24900K .......... .......... .......... .......... .......... 14% 57.9M 3s
 24950K .......... .......... .......... .......... .......... 14% 73.5M 3s
 25000K .......... .......... .......... .......... .......... 14% 60.8M 3s
 25050K .......... .......... .......... .......... .......... 14% 70.0M 3s
 25100K .......... .......... .......... .......... .......... 14% 45.0M 3s
 25150K .......... .......... .......... .......... .......... 14% 64.6M 3s
 25200K .......... .......... .......... .......... .......... 14% 38.8M 3s
 25250K .......... .......... .......... .......... .......... 14% 66.9M 3s
 25300K .......... .......... .......... .......... .......... 14% 63.7M 3s
 25350K .......... .......... .......... .......... .......... 14% 58.4M 3s
 25400K .......... .......... .......... .......... .......... 14% 79.9M 3s
 25450K .......... .......... .......... .......... .......... 14% 80.8M 3s
 25500K .......... .......... .......... .......... .......... 14% 65.0M 3s
 25550K .......... .......... .......... .......... .......... 14% 61.1M 3s
 25600K .......... .......... .......... .......... .......... 14% 58.5M 3s
 25650K .......... .......... .......... .......... .......... 14% 52.7M 3s
 25700K .......... .......... .......... .......... .......... 14% 70.5M 3s
 25750K .......... .......... .......... .......... .......... 14% 73.5M 3s
 25800K .......... .......... .......... .......... .......... 14% 76.4M 3s
 25850K .......... .......... .......... .......... .......... 14% 48.5M 3s
 25900K .......... .......... .......... .......... .......... 14% 50.2M 3s
 25950K .......... .......... .......... .......... .......... 14% 53.0M 3s
 26000K .......... .......... .......... .......... .......... 15% 59.5M 3s
 26050K .......... .......... .......... .......... .......... 15% 77.1M 3s
 26100K .......... .......... .......... .......... .......... 15% 77.3M 3s
 26150K .......... .......... .......... .......... .......... 15% 59.1M 3s
 26200K .......... .......... .......... .......... .......... 15% 52.2M 3s
 26250K .......... .......... .......... .......... .......... 15% 66.3M 3s
 26300K .......... .......... .......... .......... .......... 15% 45.3M 3s
 26350K .......... .......... .......... .......... .......... 15% 62.3M 3s
 26400K .......... .......... .......... .......... .......... 15% 65.4M 3s
 26450K .......... .......... .......... .......... .......... 15% 60.4M 3s
 26500K .......... .......... .......... .......... .......... 15% 82.3M 3s
 26550K .......... .......... .......... .......... .......... 15% 51.6M 3s
 26600K .......... .......... .......... .......... .......... 15% 61.8M 3s
 26650K .......... .......... .......... .......... .......... 15% 63.1M 3s
 26700K .......... .......... .......... .......... .......... 15% 61.0M 3s
 26750K .......... .......... .......... .......... .......... 15% 88.8M 3s
 26800K .......... .......... .......... .......... .......... 15% 68.7M 3s
 26850K .......... .......... .......... .......... .......... 15% 58.2M 3s
 26900K .......... .......... .......... .......... .......... 15% 65.9M 3s
 26950K .......... .......... .......... .......... .......... 15% 41.8M 3s
 27000K .......... .......... .......... .......... .......... 15% 46.0M 3s
 27050K .......... .......... .......... .......... .......... 15% 29.2M 3s
 27100K .......... .......... .......... .......... .......... 15% 16.7M 3s
 27150K .......... .......... .......... .......... .......... 15% 18.5M 3s
 27200K .......... .......... .......... .......... .......... 15% 27.8M 3s
 27250K .......... .......... .......... .......... .......... 15% 30.1M 3s
 27300K .......... .......... .......... .......... .......... 15% 41.5M 3s
 27350K .......... .......... .......... .......... .......... 15% 65.5M 3s
 27400K .......... .......... .......... .......... .......... 15% 57.7M 3s
 27450K .......... .......... .......... .......... .......... 15% 87.9M 3s
 27500K .......... .......... .......... .......... .......... 15% 89.7M 3s
 27550K .......... .......... .......... .......... .......... 15% 91.8M 3s
 27600K .......... .......... .......... .......... .......... 15% 80.9M 3s
 27650K .......... .......... .......... .......... .......... 15% 73.0M 3s
 27700K .......... .......... .......... .......... .......... 15% 95.9M 3s
 27750K .......... .......... .......... .......... .......... 16% 96.1M 3s
 27800K .......... .......... .......... .......... .......... 16% 91.9M 3s
 27850K .......... .......... .......... .......... .......... 16% 88.8M 3s
 27900K .......... .......... .......... .......... .......... 16% 81.5M 3s
 27950K .......... .......... .......... .......... .......... 16% 79.6M 3s
 28000K .......... .......... .......... .......... .......... 16%  120M 3s
 28050K .......... .......... .......... .......... .......... 16%  110M 3s
 28100K .......... .......... .......... .......... .......... 16%  111M 3s
 28150K .......... .......... .......... .......... .......... 16%  155M 3s
 28200K .......... .......... .......... .......... .......... 16%  102M 3s
 28250K .......... .......... .......... .......... .......... 16%  129M 3s
 28300K .......... .......... .......... .......... .......... 16%  103M 3s
 28350K .......... .......... .......... .......... .......... 16%  123M 3s
 28400K .......... .......... .......... .......... .......... 16% 78.3M 3s
 28450K .......... .......... .......... .......... .......... 16% 80.4M 3s
 28500K .......... .......... .......... .......... .......... 16% 89.8M 3s
 28550K .......... .......... .......... .......... .......... 16% 89.5M 3s
 28600K .......... .......... .......... .......... .......... 16% 82.6M 3s
 28650K .......... .......... .......... .......... .......... 16% 67.8M 3s
 28700K .......... .......... .......... .......... .......... 16% 90.4M 3s
 28750K .......... .......... .......... .......... .......... 16% 60.3M 3s
 28800K .......... .......... .......... .......... .......... 16% 70.9M 3s
 28850K .......... .......... .......... .......... .......... 16% 60.8M 3s
 28900K .......... .......... .......... .......... .......... 16% 71.4M 3s
 28950K .......... .......... .......... .......... .......... 16% 80.9M 3s
 29000K .......... .......... .......... .......... .......... 16% 96.2M 3s
 29050K .......... .......... .......... .......... .......... 16% 64.7M 3s
 29100K .......... .......... .......... .......... .......... 16% 54.4M 3s
 29150K .......... .......... .......... .......... .......... 16% 79.2M 3s
 29200K .......... .......... .......... .......... .......... 16% 78.4M 3s
 29250K .......... .......... .......... .......... .......... 16% 79.1M 3s
 29300K .......... .......... .......... .......... .......... 16% 62.7M 3s
 29350K .......... .......... .......... .......... .......... 16% 69.7M 3s
 29400K .......... .......... .......... .......... .......... 16% 71.5M 3s
 29450K .......... .......... .......... .......... .......... 17% 72.5M 3s
 29500K .......... .......... .......... .......... .......... 17% 62.7M 3s
 29550K .......... .......... .......... .......... .......... 17% 70.4M 3s
 29600K .......... .......... .......... .......... .......... 17% 79.4M 3s
 29650K .......... .......... .......... .......... .......... 17% 75.3M 3s
 29700K .......... .......... .......... .......... .......... 17% 82.5M 3s
 29750K .......... .......... .......... .......... .......... 17% 65.1M 3s
 29800K .......... .......... .......... .......... .......... 17% 73.1M 3s
 29850K .......... .......... .......... .......... .......... 17% 72.0M 3s
 29900K .......... .......... .......... .......... .......... 17% 68.8M 3s
 29950K .......... .......... .......... .......... .......... 17% 87.0M 3s
 30000K .......... .......... .......... .......... .......... 17% 69.2M 3s
 30050K .......... .......... .......... .......... .......... 17% 65.3M 3s
 30100K .......... .......... .......... .......... .......... 17% 79.7M 3s
 30150K .......... .......... .......... .......... .......... 17%  105M 3s
 30200K .......... .......... .......... .......... .......... 17% 60.0M 3s
 30250K .......... .......... .......... .......... .......... 17% 75.9M 3s
 30300K .......... .......... .......... .......... .......... 17% 85.3M 3s
 30350K .......... .......... .......... .......... .......... 17% 74.2M 3s
 30400K .......... .......... .......... .......... .......... 17% 77.1M 3s
 30450K .......... .......... .......... .......... .......... 17% 77.8M 3s
 30500K .......... .......... .......... .......... .......... 17% 75.5M 3s
 30550K .......... .......... .......... .......... .......... 17% 96.6M 3s
 30600K .......... .......... .......... .......... .......... 17% 60.5M 3s
 30650K .......... .......... .......... .......... .......... 17% 77.4M 3s
 30700K .......... .......... .......... .......... .......... 17% 70.3M 3s
 30750K .......... .......... .......... .......... .......... 17% 74.2M 3s
 30800K .......... .......... .......... .......... .......... 17% 83.6M 3s
 30850K .......... .......... .......... .......... .......... 17%  104M 3s
 30900K .......... .......... .......... .......... .......... 17% 71.7M 3s
 30950K .......... .......... .......... .......... .......... 17% 53.5M 3s
 31000K .......... .......... .......... .......... .......... 17% 70.5M 3s
 31050K .......... .......... .......... .......... .......... 17% 78.2M 3s
 31100K .......... .......... .......... .......... .......... 17% 93.9M 3s
 31150K .......... .......... .......... .......... .......... 17% 57.1M 3s
 31200K .......... .......... .......... .......... .......... 18% 85.7M 3s
 31250K .......... .......... .......... .......... .......... 18% 58.2M 3s
 31300K .......... .......... .......... .......... .......... 18% 69.8M 3s
 31350K .......... .......... .......... .......... .......... 18% 74.6M 3s
 31400K .......... .......... .......... .......... .......... 18% 72.2M 3s
 31450K .......... .......... .......... .......... .......... 18% 77.4M 3s
 31500K .......... .......... .......... .......... .......... 18% 76.6M 3s
 31550K .......... .......... .......... .......... .......... 18% 74.7M 3s
 31600K .......... .......... .......... .......... .......... 18% 65.0M 3s
 31650K .......... .......... .......... .......... .......... 18% 88.5M 3s
 31700K .......... .......... .......... .......... .......... 18% 67.3M 3s
 31750K .......... .......... .......... .......... .......... 18% 80.3M 3s
 31800K .......... .......... .......... .......... .......... 18% 82.4M 3s
 31850K .......... .......... .......... .......... .......... 18% 78.5M 3s
 31900K .......... .......... .......... .......... .......... 18% 77.3M 3s
 31950K .......... .......... .......... .......... .......... 18% 70.0M 3s
 32000K .......... .......... .......... .......... .......... 18% 73.7M 3s
 32050K .......... .......... .......... .......... .......... 18% 68.6M 3s
 32100K .......... .......... .......... .......... .......... 18% 78.9M 3s
 32150K .......... .......... .......... .......... .......... 18% 82.5M 3s
 32200K .......... .......... .......... .......... .......... 18% 55.3M 3s
 32250K .......... .......... .......... .......... .......... 18%  110M 3s
 32300K .......... .......... .......... .......... .......... 18% 95.3M 3s
 32350K .......... .......... .......... .......... .......... 18% 78.0M 3s
 32400K .......... .......... .......... .......... .......... 18% 50.0M 3s
 32450K .......... .......... .......... .......... .......... 18% 91.0M 3s
 32500K .......... .......... .......... .......... .......... 18% 74.5M 3s
 32550K .......... .......... .......... .......... .......... 18% 73.6M 3s
 32600K .......... .......... .......... .......... .......... 18% 33.3M 3s
 32650K .......... .......... .......... .......... .......... 18% 43.7M 3s
 32700K .......... .......... .......... .......... .......... 18% 29.3M 3s
 32750K .......... .......... .......... .......... .......... 18% 23.6M 3s
 32800K .......... .......... .......... .......... .......... 18% 36.7M 3s
 32850K .......... .......... .......... .......... .......... 18% 49.8M 3s
 32900K .......... .......... .......... .......... .......... 18% 86.0M 3s
 32950K .......... .......... .......... .......... .......... 19% 57.4M 3s
 33000K .......... .......... .......... .......... .......... 19% 63.0M 3s
 33050K .......... .......... .......... .......... .......... 19% 74.7M 3s
 33100K .......... .......... .......... .......... .......... 19% 51.2M 3s
 33150K .......... .......... .......... .......... .......... 19% 51.4M 3s
 33200K .......... .......... .......... .......... .......... 19% 62.7M 3s
 33250K .......... .......... .......... .......... .......... 19% 82.7M 3s
 33300K .......... .......... .......... .......... .......... 19% 82.4M 3s
 33350K .......... .......... .......... .......... .......... 19% 66.0M 3s
 33400K .......... .......... .......... .......... .......... 19% 86.2M 3s
 33450K .......... .......... .......... .......... .......... 19%  103M 3s
 33500K .......... .......... .......... .......... .......... 19% 64.1M 3s
 33550K .......... .......... .......... .......... .......... 19%  108M 3s
 33600K .......... .......... .......... .......... .......... 19% 97.4M 3s
 33650K .......... .......... .......... .......... .......... 19%  110M 3s
 33700K .......... .......... .......... .......... .......... 19%  112M 3s
 33750K .......... .......... .......... .......... .......... 19% 71.8M 3s
 33800K .......... .......... .......... .......... .......... 19%  100M 3s
 33850K .......... .......... .......... .......... .......... 19% 69.2M 3s
 33900K .......... .......... .......... .......... .......... 19% 73.4M 3s
 33950K .......... .......... .......... .......... .......... 19% 69.5M 3s
 34000K .......... .......... .......... .......... .......... 19% 76.6M 3s
 34050K .......... .......... .......... .......... .......... 19% 82.1M 3s
 34100K .......... .......... .......... .......... .......... 19% 61.5M 3s
 34150K .......... .......... .......... .......... .......... 19% 97.6M 3s
 34200K .......... .......... .......... .......... .......... 19% 56.1M 3s
 34250K .......... .......... .......... .......... .......... 19% 59.4M 3s
 34300K .......... .......... .......... .......... .......... 19% 74.7M 3s
 34350K .......... .......... .......... .......... .......... 19% 70.4M 3s
 34400K .......... .......... .......... .......... .......... 19% 58.3M 3s
 34450K .......... .......... .......... .......... .......... 19% 70.1M 3s
 34500K .......... .......... .......... .......... .......... 19% 60.9M 3s
 34550K .......... .......... .......... .......... .......... 19% 84.1M 3s
 34600K .......... .......... .......... .......... .......... 19% 50.1M 3s
 34650K .......... .......... .......... .......... .......... 20% 69.1M 3s
 34700K .......... .......... .......... .......... .......... 20% 68.5M 2s
 34750K .......... .......... .......... .......... .......... 20% 69.8M 2s
 34800K .......... .......... .......... .......... .......... 20% 74.3M 2s
 34850K .......... .......... .......... .......... .......... 20% 73.8M 2s
 34900K .......... .......... .......... .......... .......... 20% 62.5M 2s
 34950K .......... .......... .......... .......... .......... 20% 57.9M 2s
 35000K .......... .......... .......... .......... .......... 20% 88.1M 2s
 35050K .......... .......... .......... .......... .......... 20% 98.9M 2s
 35100K .......... .......... .......... .......... .......... 20% 64.0M 2s
 35150K .......... .......... .......... .......... .......... 20% 68.8M 2s
 35200K .......... .......... .......... .......... .......... 20% 66.7M 2s
 35250K .......... .......... .......... .......... .......... 20% 84.1M 2s
 35300K .......... .......... .......... .......... .......... 20% 68.0M 2s
 35350K .......... .......... .......... .......... .......... 20% 74.7M 2s
 35400K .......... .......... .......... .......... .......... 20% 69.6M 2s
 35450K .......... .......... .......... .......... .......... 20% 83.2M 2s
 35500K .......... .......... .......... .......... .......... 20% 69.8M 2s
 35550K .......... .......... .......... .......... .......... 20% 95.5M 2s
 35600K .......... .......... .......... .......... .......... 20% 79.9M 2s
 35650K .......... .......... .......... .......... .......... 20%  112M 2s
 35700K .......... .......... .......... .......... .......... 20% 59.0M 2s
 35750K .......... .......... .......... .......... .......... 20% 82.4M 2s
 35800K .......... .......... .......... .......... .......... 20% 88.2M 2s
 35850K .......... .......... .......... .......... .......... 20% 53.8M 2s
 35900K .......... .......... .......... .......... .......... 20% 86.9M 2s
 35950K .......... .......... .......... .......... .......... 20% 84.5M 2s
 36000K .......... .......... .......... .......... .......... 20% 76.9M 2s
 36050K .......... .......... .......... .......... .......... 20% 49.9M 2s
 36100K .......... .......... .......... .......... .......... 20% 86.8M 2s
 36150K .......... .......... .......... .......... .......... 20% 71.5M 2s
 36200K .......... .......... .......... .......... .......... 20% 52.8M 2s
 36250K .......... .......... .......... .......... .......... 20%  109M 2s
 36300K .......... .......... .......... .......... .......... 20% 65.9M 2s
 36350K .......... .......... .......... .......... .......... 20% 70.1M 2s
 36400K .......... .......... .......... .......... .......... 21% 52.8M 2s
 36450K .......... .......... .......... .......... .......... 21% 70.1M 2s
 36500K .......... .......... .......... .......... .......... 21% 60.8M 2s
 36550K .......... .......... .......... .......... .......... 21% 63.0M 2s
 36600K .......... .......... .......... .......... .......... 21% 67.6M 2s
 36650K .......... .......... .......... .......... .......... 21% 83.6M 2s
 36700K .......... .......... .......... .......... .......... 21% 73.7M 2s
 36750K .......... .......... .......... .......... .......... 21% 54.5M 2s
 36800K .......... .......... .......... .......... .......... 21% 85.2M 2s
 36850K .......... .......... .......... .......... .......... 21% 75.3M 2s
 36900K .......... .......... .......... .......... .......... 21% 61.0M 2s
 36950K .......... .......... .......... .......... .......... 21% 72.5M 2s
 37000K .......... .......... .......... .......... .......... 21% 78.0M 2s
 37050K .......... .......... .......... .......... .......... 21% 37.1M 2s
 37100K .......... .......... .......... .......... .......... 21% 84.3M 2s
 37150K .......... .......... .......... .......... .......... 21% 54.5M 2s
 37200K .......... .......... .......... .......... .......... 21% 86.4M 2s
 37250K .......... .......... .......... .......... .......... 21% 85.3M 2s
 37300K .......... .......... .......... .......... .......... 21% 56.8M 2s
 37350K .......... .......... .......... .......... .......... 21% 65.9M 2s
 37400K .......... .......... .......... .......... .......... 21% 70.0M 2s
 37450K .......... .......... .......... .......... .......... 21% 52.3M 2s
 37500K .......... .......... .......... .......... .......... 21% 49.2M 2s
 37550K .......... .......... .......... .......... .......... 21% 42.9M 2s
 37600K .......... .......... .......... .......... .......... 21% 63.5M 2s
 37650K .......... .......... .......... .......... .......... 21% 74.9M 2s
 37700K .......... .......... .......... .......... .......... 21% 98.0M 2s
 37750K .......... .......... .......... .......... .......... 21% 76.5M 2s
 37800K .......... .......... .......... .......... .......... 21% 55.0M 2s
 37850K .......... .......... .......... .......... .......... 21% 23.7M 2s
 37900K .......... .......... .......... .......... .......... 21% 33.1M 2s
 37950K .......... .......... .......... .......... .......... 21% 35.7M 2s
 38000K .......... .......... .......... .......... .......... 21% 38.4M 2s
 38050K .......... .......... .......... .......... .......... 21% 16.8M 2s
 38100K .......... .......... .......... .......... .......... 21% 72.3M 2s
 38150K .......... .......... .......... .......... .......... 22%  111M 2s
 38200K .......... .......... .......... .......... .......... 22% 75.6M 2s
 38250K .......... .......... .......... .......... .......... 22% 65.4M 2s
 38300K .......... .......... .......... .......... .......... 22%  101M 2s
 38350K .......... .......... .......... .......... .......... 22% 27.5M 2s
 38400K .......... .......... .......... .......... .......... 22% 68.6M 2s
 38450K .......... .......... .......... .......... .......... 22% 83.2M 2s
 38500K .......... .......... .......... .......... .......... 22% 75.6M 2s
 38550K .......... .......... .......... .......... .......... 22% 78.3M 2s
 38600K .......... .......... .......... .......... .......... 22% 70.7M 2s
 38650K .......... .......... .......... .......... .......... 22% 68.7M 2s
 38700K .......... .......... .......... .......... .......... 22% 78.6M 2s
 38750K .......... .......... .......... .......... .......... 22% 92.7M 2s
 38800K .......... .......... .......... .......... .......... 22% 71.1M 2s
 38850K .......... .......... .......... .......... .......... 22%  118M 2s
 38900K .......... .......... .......... .......... .......... 22% 96.7M 2s
 38950K .......... .......... .......... .......... .......... 22% 66.9M 2s
 39000K .......... .......... .......... .......... .......... 22% 76.8M 2s
 39050K .......... .......... .......... .......... .......... 22%  101M 2s
 39100K .......... .......... .......... .......... .......... 22% 77.5M 2s
 39150K .......... .......... .......... .......... .......... 22% 94.3M 2s
 39200K .......... .......... .......... .......... .......... 22% 89.0M 2s
 39250K .......... .......... .......... .......... .......... 22% 81.3M 2s
 39300K .......... .......... .......... .......... .......... 22% 78.8M 2s
 39350K .......... .......... .......... .......... .......... 22% 85.6M 2s
 39400K .......... .......... .......... .......... .......... 22% 91.5M 2s
 39450K .......... .......... .......... .......... .......... 22% 95.9M 2s
 39500K .......... .......... .......... .......... .......... 22% 71.4M 2s
 39550K .......... .......... .......... .......... .......... 22%  104M 2s
 39600K .......... .......... .......... .......... .......... 22% 91.2M 2s
 39650K .......... .......... .......... .......... .......... 22% 96.9M 2s
 39700K .......... .......... .......... .......... .......... 22% 67.8M 2s
 39750K .......... .......... .......... .......... .......... 22% 78.0M 2s
 39800K .......... .......... .......... .......... .......... 22% 79.2M 2s
 39850K .......... .......... .......... .......... .......... 23% 97.8M 2s
 39900K .......... .......... .......... .......... .......... 23% 80.5M 2s
 39950K .......... .......... .......... .......... .......... 23%  117M 2s
 40000K .......... .......... .......... .......... .......... 23% 84.2M 2s
 40050K .......... .......... .......... .......... .......... 23% 61.9M 2s
 40100K .......... .......... .......... .......... .......... 23% 74.0M 2s
 40150K .......... .......... .......... .......... .......... 23% 91.4M 2s
 40200K .......... .......... .......... .......... .......... 23%  104M 2s
 40250K .......... .......... .......... .......... .......... 23%  103M 2s
 40300K .......... .......... .......... .......... .......... 23% 93.7M 2s
 40350K .......... .......... .......... .......... .......... 23% 95.2M 2s
 40400K .......... .......... .......... .......... .......... 23% 71.1M 2s
 40450K .......... .......... .......... .......... .......... 23% 94.5M 2s
 40500K .......... .......... .......... .......... .......... 23% 96.3M 2s
 40550K .......... .......... .......... .......... .......... 23% 85.7M 2s
 40600K .......... .......... .......... .......... .......... 23% 75.7M 2s
 40650K .......... .......... .......... .......... .......... 23%  133M 2s
 40700K .......... .......... .......... .......... .......... 23% 86.6M 2s
 40750K .......... .......... .......... .......... .......... 23% 76.9M 2s
 40800K .......... .......... .......... .......... .......... 23% 72.8M 2s
 40850K .......... .......... .......... .......... .......... 23%  102M 2s
 40900K .......... .......... .......... .......... .......... 23% 81.6M 2s
 40950K .......... .......... .......... .......... .......... 23% 95.4M 2s
 41000K .......... .......... .......... .......... .......... 23% 90.4M 2s
 41050K .......... .......... .......... .......... .......... 23% 92.1M 2s
 41100K .......... .......... .......... .......... .......... 23% 89.8M 2s
 41150K .......... .......... .......... .......... .......... 23% 71.3M 2s
 41200K .......... .......... .......... .......... .......... 23% 80.3M 2s
 41250K .......... .......... .......... .......... .......... 23% 92.9M 2s
 41300K .......... .......... .......... .......... .......... 23% 78.0M 2s
 41350K .......... .......... .......... .......... .......... 23% 82.1M 2s
 41400K .......... .......... .......... .......... .......... 23% 87.6M 2s
 41450K .......... .......... .......... .......... .......... 23% 92.2M 2s
 41500K .......... .......... .......... .......... .......... 23% 69.0M 2s
 41550K .......... .......... .......... .......... .......... 23% 88.2M 2s
 41600K .......... .......... .......... .......... .......... 24% 85.0M 2s
 41650K .......... .......... .......... .......... .......... 24% 81.7M 2s
 41700K .......... .......... .......... .......... .......... 24%  112M 2s
 41750K .......... .......... .......... .......... .......... 24%  101M 2s
 41800K .......... .......... .......... .......... .......... 24% 73.8M 2s
 41850K .......... .......... .......... .......... .......... 24% 75.4M 2s
 41900K .......... .......... .......... .......... .......... 24% 99.6M 2s
 41950K .......... .......... .......... .......... .......... 24% 73.1M 2s
 42000K .......... .......... .......... .......... .......... 24%  123M 2s
 42050K .......... .......... .......... .......... .......... 24% 80.4M 2s
 42100K .......... .......... .......... .......... .......... 24% 90.0M 2s
 42150K .......... .......... .......... .......... .......... 24% 82.8M 2s
 42200K .......... .......... .......... .......... .......... 24% 67.4M 2s
 42250K .......... .......... .......... .......... .......... 24% 68.9M 2s
 42300K .......... .......... .......... .......... .......... 24% 97.3M 2s
 42350K .......... .......... .......... .......... .......... 24% 74.3M 2s
 42400K .......... .......... .......... .......... .......... 24% 80.8M 2s
 42450K .......... .......... .......... .......... .......... 24% 94.3M 2s
 42500K .......... .......... .......... .......... .......... 24% 75.9M 2s
 42550K .......... .......... .......... .......... .......... 24% 77.6M 2s
 42600K .......... .......... .......... .......... .......... 24% 70.8M 2s
 42650K .......... .......... .......... .......... .......... 24% 88.4M 2s
 42700K .......... .......... .......... .......... .......... 24% 82.0M 2s
 42750K .......... .......... .......... .......... .......... 24% 83.0M 2s
 42800K .......... .......... .......... .......... .......... 24% 72.8M 2s
 42850K .......... .......... .......... .......... .......... 24% 66.5M 2s
 42900K .......... .......... .......... .......... .......... 24% 99.6M 2s
 42950K .......... .......... .......... .......... .......... 24% 60.5M 2s
 43000K .......... .......... .......... .......... .......... 24% 87.8M 2s
 43050K .......... .......... .......... .......... .......... 24% 75.9M 2s
 43100K .......... .......... .......... .......... .......... 24% 33.0M 2s
 43150K .......... .......... .......... .......... .......... 24% 94.4M 2s
 43200K .......... .......... .......... .......... .......... 24% 56.8M 2s
 43250K .......... .......... .......... .......... .......... 24% 65.1M 2s
 43300K .......... .......... .......... .......... .......... 24% 76.6M 2s
 43350K .......... .......... .......... .......... .......... 25% 59.1M 2s
 43400K .......... .......... .......... .......... .......... 25% 64.3M 2s
 43450K .......... .......... .......... .......... .......... 25% 70.6M 2s
 43500K .......... .......... .......... .......... .......... 25% 60.3M 2s
 43550K .......... .......... .......... .......... .......... 25% 66.5M 2s
 43600K .......... .......... .......... .......... .......... 25% 56.2M 2s
 43650K .......... .......... .......... .......... .......... 25%  102M 2s
 43700K .......... .......... .......... .......... .......... 25% 65.3M 2s
 43750K .......... .......... .......... .......... .......... 25% 78.6M 2s
 43800K .......... .......... .......... .......... .......... 25% 45.8M 2s
 43850K .......... .......... .......... .......... .......... 25% 67.0M 2s
 43900K .......... .......... .......... .......... .......... 25% 83.4M 2s
 43950K .......... .......... .......... .......... .......... 25% 43.6M 2s
 44000K .......... .......... .......... .......... .......... 25% 82.7M 2s
 44050K .......... .......... .......... .......... .......... 25% 67.2M 2s
 44100K .......... .......... .......... .......... .......... 25% 93.8M 2s
 44150K .......... .......... .......... .......... .......... 25% 70.2M 2s
 44200K .......... .......... .......... .......... .......... 25% 50.4M 2s
 44250K .......... .......... .......... .......... .......... 25% 47.6M 2s
 44300K .......... .......... .......... .......... .......... 25% 38.8M 2s
 44350K .......... .......... .......... .......... .......... 25% 44.2M 2s
 44400K .......... .......... .......... .......... .......... 25% 31.2M 2s
 44450K .......... .......... .......... .......... .......... 25% 37.2M 2s
 44500K .......... .......... .......... .......... .......... 25% 47.2M 2s
 44550K .......... .......... .......... .......... .......... 25% 68.7M 2s
 44600K .......... .......... .......... .......... .......... 25% 79.1M 2s
 44650K .......... .......... .......... .......... .......... 25% 83.8M 2s
 44700K .......... .......... .......... .......... .......... 25% 75.8M 2s
 44750K .......... .......... .......... .......... .......... 25% 91.3M 2s
 44800K .......... .......... .......... .......... .......... 25% 69.0M 2s
 44850K .......... .......... .......... .......... .......... 25% 87.1M 2s
 44900K .......... .......... .......... .......... .......... 25% 92.0M 2s
 44950K .......... .......... .......... .......... .......... 25% 88.4M 2s
 45000K .......... .......... .......... .......... .......... 25% 80.3M 2s
 45050K .......... .......... .......... .......... .......... 26%  117M 2s
 45100K .......... .......... .......... .......... .......... 26% 90.2M 2s
 45150K .......... .......... .......... .......... .......... 26% 51.5M 2s
 45200K .......... .......... .......... .......... .......... 26%  104M 2s
 45250K .......... .......... .......... .......... .......... 26% 98.7M 2s
 45300K .......... .......... .......... .......... .......... 26%  101M 2s
 45350K .......... .......... .......... .......... .......... 26% 63.3M 2s
 45400K .......... .......... .......... .......... .......... 26% 71.7M 2s
 45450K .......... .......... .......... .......... .......... 26% 91.9M 2s
 45500K .......... .......... .......... .......... .......... 26% 79.3M 2s
 45550K .......... .......... .......... .......... .......... 26% 78.8M 2s
 45600K .......... .......... .......... .......... .......... 26% 84.1M 2s
 45650K .......... .......... .......... .......... .......... 26%  105M 2s
 45700K .......... .......... .......... .......... .......... 26% 73.2M 2s
 45750K .......... .......... .......... .......... .......... 26% 97.7M 2s
 45800K .......... .......... .......... .......... .......... 26% 77.7M 2s
 45850K .......... .......... .......... .......... .......... 26%  102M 2s
 45900K .......... .......... .......... .......... .......... 26% 67.1M 2s
 45950K .......... .......... .......... .......... .......... 26% 85.3M 2s
 46000K .......... .......... .......... .......... .......... 26% 67.7M 2s
 46050K .......... .......... .......... .......... .......... 26% 84.4M 2s
 46100K .......... .......... .......... .......... .......... 26% 75.8M 2s
 46150K .......... .......... .......... .......... .......... 26% 88.4M 2s
 46200K .......... .......... .......... .......... .......... 26% 75.1M 2s
 46250K .......... .......... .......... .......... .......... 26% 74.0M 2s
 46300K .......... .......... .......... .......... .......... 26% 84.4M 2s
 46350K .......... .......... .......... .......... .......... 26%  117M 2s
 46400K .......... .......... .......... .......... .......... 26% 85.2M 2s
 46450K .......... .......... .......... .......... .......... 26% 92.0M 2s
 46500K .......... .......... .......... .......... .......... 26% 88.7M 2s
 46550K .......... .......... .......... .......... .......... 26% 91.9M 2s
 46600K .......... .......... .......... .......... .......... 26% 64.6M 2s
 46650K .......... .......... .......... .......... .......... 26% 75.3M 2s
 46700K .......... .......... .......... .......... .......... 26%  101M 2s
 46750K .......... .......... .......... .......... .......... 26% 77.3M 2s
 46800K .......... .......... .......... .......... .......... 27% 84.4M 2s
 46850K .......... .......... .......... .......... .......... 27%  122M 2s
 46900K .......... .......... .......... .......... .......... 27% 73.7M 2s
 46950K .......... .......... .......... .......... .......... 27% 59.3M 2s
 47000K .......... .......... .......... .......... .......... 27% 85.5M 2s
 47050K .......... .......... .......... .......... .......... 27% 92.0M 2s
 47100K .......... .......... .......... .......... .......... 27% 80.7M 2s
 47150K .......... .......... .......... .......... .......... 27% 83.3M 2s
 47200K .......... .......... .......... .......... .......... 27%  135M 2s
 47250K .......... .......... .......... .......... .......... 27% 81.6M 2s
 47300K .......... .......... .......... .......... .......... 27% 88.4M 2s
 47350K .......... .......... .......... .......... .......... 27% 82.6M 2s
 47400K .......... .......... .......... .......... .......... 27% 75.5M 2s
 47450K .......... .......... .......... .......... .......... 27% 85.6M 2s
 47500K .......... .......... .......... .......... .......... 27% 74.7M 2s
 47550K .......... .......... .......... .......... .......... 27% 90.7M 2s
 47600K .......... .......... .......... .......... .......... 27% 89.4M 2s
 47650K .......... .......... .......... .......... .......... 27% 87.7M 2s
 47700K .......... .......... .......... .......... .......... 27% 37.6M 2s
 47750K .......... .......... .......... .......... .......... 27% 35.6M 2s
 47800K .......... .......... .......... .......... .......... 27% 46.8M 2s
 47850K .......... .......... .......... .......... .......... 27% 40.3M 2s
 47900K .......... .......... .......... .......... .......... 27% 55.7M 2s
 47950K .......... .......... .......... .......... .......... 27% 61.8M 2s
 48000K .......... .......... .......... .......... .......... 27% 72.5M 2s
 48050K .......... .......... .......... .......... .......... 27% 72.0M 2s
 48100K .......... .......... .......... .......... .......... 27% 83.3M 2s
 48150K .......... .......... .......... .......... .......... 27% 89.6M 2s
 48200K .......... .......... .......... .......... .......... 27% 78.1M 2s
 48250K .......... .......... .......... .......... .......... 27% 74.7M 2s
 48300K .......... .......... .......... .......... .......... 27% 90.0M 2s
 48350K .......... .......... .......... .......... .......... 27% 98.2M 2s
 48400K .......... .......... .......... .......... .......... 27% 86.5M 2s
 48450K .......... .......... .......... .......... .......... 27% 63.7M 2s
 48500K .......... .......... .......... .......... .......... 27%  116M 2s
 48550K .......... .......... .......... .......... .......... 28%  103M 2s
 48600K .......... .......... .......... .......... .......... 28%  113M 2s
 48650K .......... .......... .......... .......... .......... 28%  105M 2s
 48700K .......... .......... .......... .......... .......... 28%  107M 2s
 48750K .......... .......... .......... .......... .......... 28%  105M 2s
 48800K .......... .......... .......... .......... .......... 28% 88.0M 2s
 48850K .......... .......... .......... .......... .......... 28% 89.6M 2s
 48900K .......... .......... .......... .......... .......... 28%  120M 2s
 48950K .......... .......... .......... .......... .......... 28% 97.5M 2s
 49000K .......... .......... .......... .......... .......... 28% 90.7M 2s
 49050K .......... .......... .......... .......... .......... 28%  117M 2s
 49100K .......... .......... .......... .......... .......... 28%  112M 2s
 49150K .......... .......... .......... .......... .......... 28% 89.6M 2s
 49200K .......... .......... .......... .......... .......... 28%  111M 2s
 49250K .......... .......... .......... .......... .......... 28%  115M 2s
 49300K .......... .......... .......... .......... .......... 28%  110M 2s
 49350K .......... .......... .......... .......... .......... 28% 82.2M 2s
 49400K .......... .......... .......... .......... .......... 28%  139M 2s
 49450K .......... .......... .......... .......... .......... 28%  132M 2s
 49500K .......... .......... .......... .......... .......... 28% 82.7M 2s
 49550K .......... .......... .......... .......... .......... 28%  118M 2s
 49600K .......... .......... .......... .......... .......... 28%  111M 2s
 49650K .......... .......... .......... .......... .......... 28% 91.6M 2s
 49700K .......... .......... .......... .......... .......... 28%  107M 2s
 49750K .......... .......... .......... .......... .......... 28%  116M 2s
 49800K .......... .......... .......... .......... .......... 28%  108M 2s
 49850K .......... .......... .......... .......... .......... 28%  125M 2s
 49900K .......... .......... .......... .......... .......... 28% 79.6M 2s
 49950K .......... .......... .......... .......... .......... 28%  109M 2s
 50000K .......... .......... .......... .......... .......... 28% 97.4M 2s
 50050K .......... .......... .......... .......... .......... 28%  134M 2s
 50100K .......... .......... .......... .......... .......... 28% 86.9M 2s
 50150K .......... .......... .......... .......... .......... 28%  128M 2s
 50200K .......... .......... .......... .......... .......... 28% 90.4M 2s
 50250K .......... .......... .......... .......... .......... 28% 85.3M 2s
 50300K .......... .......... .......... .......... .......... 29% 96.5M 2s
 50350K .......... .......... .......... .......... .......... 29%  129M 2s
 50400K .......... .......... .......... .......... .......... 29%  153M 2s
 50450K .......... .......... .......... .......... .......... 29%  116M 2s
 50500K .......... .......... .......... .......... .......... 29% 87.7M 2s
 50550K .......... .......... .......... .......... .......... 29% 81.6M 2s
 50600K .......... .......... .......... .......... .......... 29% 87.0M 2s
 50650K .......... .......... .......... .......... .......... 29%  118M 2s
 50700K .......... .......... .......... .......... .......... 29%  104M 2s
 50750K .......... .......... .......... .......... .......... 29% 95.4M 2s
 50800K .......... .......... .......... .......... .......... 29%  104M 2s
 50850K .......... .......... .......... .......... .......... 29% 82.0M 2s
 50900K .......... .......... .......... .......... .......... 29% 78.3M 2s
 50950K .......... .......... .......... .......... .......... 29%  133M 2s
 51000K .......... .......... .......... .......... .......... 29% 85.3M 2s
 51050K .......... .......... .......... .......... .......... 29%  115M 2s
 51100K .......... .......... .......... .......... .......... 29%  115M 2s
 51150K .......... .......... .......... .......... .......... 29% 84.8M 2s
 51200K .......... .......... .......... .......... .......... 29% 87.7M 2s
 51250K .......... .......... .......... .......... .......... 29% 92.0M 2s
 51300K .......... .......... .......... .......... .......... 29%  112M 2s
 51350K .......... .......... .......... .......... .......... 29% 84.2M 2s
 51400K .......... .......... .......... .......... .......... 29%  116M 2s
 51450K .......... .......... .......... .......... .......... 29% 85.0M 2s
 51500K .......... .......... .......... .......... .......... 29%  117M 2s
 51550K .......... .......... .......... .......... .......... 29%  113M 2s
 51600K .......... .......... .......... .......... .......... 29%  110M 2s
 51650K .......... .......... .......... .......... .......... 29%  121M 2s
 51700K .......... .......... .......... .......... .......... 29% 76.6M 2s
 51750K .......... .......... .......... .......... .......... 29% 95.6M 2s
 51800K .......... .......... .......... .......... .......... 29% 67.1M 2s
 51850K .......... .......... .......... .......... .......... 29% 81.1M 2s
 51900K .......... .......... .......... .......... .......... 29%  104M 2s
 51950K .......... .......... .......... .......... .......... 29%  141M 2s
 52000K .......... .......... .......... .......... .......... 30%  101M 2s
 52050K .......... .......... .......... .......... .......... 30% 73.2M 2s
 52100K .......... .......... .......... .......... .......... 30%  127M 2s
 52150K .......... .......... .......... .......... .......... 30% 86.2M 2s
 52200K .......... .......... .......... .......... .......... 30%  113M 2s
 52250K .......... .......... .......... .......... .......... 30% 83.2M 2s
 52300K .......... .......... .......... .......... .......... 30%  109M 2s
 52350K .......... .......... .......... .......... .......... 30% 93.2M 2s
 52400K .......... .......... .......... .......... .......... 30% 94.1M 2s
 52450K .......... .......... .......... .......... .......... 30% 76.8M 2s
 52500K .......... .......... .......... .......... .......... 30%  130M 2s
 52550K .......... .......... .......... .......... .......... 30%  109M 2s
 52600K .......... .......... .......... .......... .......... 30% 88.9M 2s
 52650K .......... .......... .......... .......... .......... 30%  120M 2s
 52700K .......... .......... .......... .......... .......... 30%  115M 2s
 52750K .......... .......... .......... .......... .......... 30%  111M 2s
 52800K .......... .......... .......... .......... .......... 30% 79.4M 2s
 52850K .......... .......... .......... .......... .......... 30%  108M 2s
 52900K .......... .......... .......... .......... .......... 30%  133M 2s
 52950K .......... .......... .......... .......... .......... 30% 82.5M 2s
 53000K .......... .......... .......... .......... .......... 30%  130M 2s
 53050K .......... .......... .......... .......... .......... 30% 96.8M 2s
 53100K .......... .......... .......... .......... .......... 30%  112M 2s
 53150K .......... .......... .......... .......... .......... 30% 91.2M 2s
 53200K .......... .......... .......... .......... .......... 30% 97.6M 2s
 53250K .......... .......... .......... .......... .......... 30% 99.3M 2s
 53300K .......... .......... .......... .......... .......... 30% 89.4M 2s
 53350K .......... .......... .......... .......... .......... 30%  101M 2s
 53400K .......... .......... .......... .......... .......... 30% 96.7M 2s
 53450K .......... .......... .......... .......... .......... 30% 92.2M 2s
 53500K .......... .......... .......... .......... .......... 30% 76.4M 2s
 53550K .......... .......... .......... .......... .......... 30% 97.7M 2s
 53600K .......... .......... .......... .......... .......... 30% 87.4M 2s
 53650K .......... .......... .......... .......... .......... 30% 74.3M 2s
 53700K .......... .......... .......... .......... .......... 30% 97.9M 2s
 53750K .......... .......... .......... .......... .......... 31% 71.7M 2s
 53800K .......... .......... .......... .......... .......... 31%  143M 2s
 53850K .......... .......... .......... .......... .......... 31%  108M 2s
 53900K .......... .......... .......... .......... .......... 31% 86.9M 2s
 53950K .......... .......... .......... .......... .......... 31%  134M 2s
 54000K .......... .......... .......... .......... .......... 31%  131M 2s
 54050K .......... .......... .......... .......... .......... 31% 83.7M 2s
 54100K .......... .......... .......... .......... .......... 31%  126M 2s
 54150K .......... .......... .......... .......... .......... 31% 97.3M 2s
 54200K .......... .......... .......... .......... .......... 31%  104M 2s
 54250K .......... .......... .......... .......... .......... 31% 98.6M 2s
 54300K .......... .......... .......... .......... .......... 31%  114M 2s
 54350K .......... .......... .......... .......... .......... 31%  108M 2s
 54400K .......... .......... .......... .......... .......... 31% 90.8M 2s
 54450K .......... .......... .......... .......... .......... 31%  120M 2s
 54500K .......... .......... .......... .......... .......... 31% 74.0M 2s
 54550K .......... .......... .......... .......... .......... 31%  119M 2s
 54600K .......... .......... .......... .......... .......... 31% 77.2M 2s
 54650K .......... .......... .......... .......... .......... 31%  173M 2s
 54700K .......... .......... .......... .......... .......... 31%  128M 2s
 54750K .......... .......... .......... .......... .......... 31% 93.5M 2s
 54800K .......... .......... .......... .......... .......... 31% 78.9M 2s
 54850K .......... .......... .......... .......... .......... 31%  102M 2s
 54900K .......... .......... .......... .......... .......... 31%  106M 2s
 54950K .......... .......... .......... .......... .......... 31% 84.9M 2s
 55000K .......... .......... .......... .......... .......... 31% 79.8M 2s
 55050K .......... .......... .......... .......... .......... 31% 99.9M 2s
 55100K .......... .......... .......... .......... .......... 31% 79.8M 2s
 55150K .......... .......... .......... .......... .......... 31% 68.7M 2s
 55200K .......... .......... .......... .......... .......... 31% 62.6M 2s
 55250K .......... .......... .......... .......... .......... 31%  118M 2s
 55300K .......... .......... .......... .......... .......... 31% 83.4M 2s
 55350K .......... .......... .......... .......... .......... 31%  100M 2s
 55400K .......... .......... .......... .......... .......... 31% 91.9M 2s
 55450K .......... .......... .......... .......... .......... 31% 58.0M 2s
 55500K .......... .......... .......... .......... .......... 32%  150M 2s
 55550K .......... .......... .......... .......... .......... 32% 90.0M 2s
 55600K .......... .......... .......... .......... .......... 32%  115M 2s
 55650K .......... .......... .......... .......... .......... 32%  127M 2s
 55700K .......... .......... .......... .......... .......... 32% 90.3M 2s
 55750K .......... .......... .......... .......... .......... 32%  123M 2s
 55800K .......... .......... .......... .......... .......... 32%  101M 2s
 55850K .......... .......... .......... .......... .......... 32%  125M 2s
 55900K .......... .......... .......... .......... .......... 32%  124M 2s
 55950K .......... .......... .......... .......... .......... 32%  102M 2s
 56000K .......... .......... .......... .......... .......... 32%  120M 2s
 56050K .......... .......... .......... .......... .......... 32% 95.6M 2s
 56100K .......... .......... .......... .......... .......... 32%  101M 2s
 56150K .......... .......... .......... .......... .......... 32%  116M 2s
 56200K .......... .......... .......... .......... .......... 32%  104M 2s
 56250K .......... .......... .......... .......... .......... 32%  124M 2s
 56300K .......... .......... .......... .......... .......... 32%  116M 2s
 56350K .......... .......... .......... .......... .......... 32%  111M 2s
 56400K .......... .......... .......... .......... .......... 32%  103M 2s
 56450K .......... .......... .......... .......... .......... 32%  111M 2s
 56500K .......... .......... .......... .......... .......... 32%  113M 2s
 56550K .......... .......... .......... .......... .......... 32%  111M 2s
 56600K .......... .......... .......... .......... .......... 32% 99.6M 2s
 56650K .......... .......... .......... .......... .......... 32%  141M 2s
 56700K .......... .......... .......... .......... .......... 32%  125M 2s
 56750K .......... .......... .......... .......... .......... 32%  108M 2s
 56800K .......... .......... .......... .......... .......... 32% 88.7M 2s
 56850K .......... .......... .......... .......... .......... 32%  127M 2s
 56900K .......... .......... .......... .......... .......... 32%  113M 2s
 56950K .......... .......... .......... .......... .......... 32% 94.8M 2s
 57000K .......... .......... .......... .......... .......... 32%  150M 2s
 57050K .......... .......... .......... .......... .......... 32%  104M 2s
 57100K .......... .......... .......... .......... .......... 32%  124M 2s
 57150K .......... .......... .......... .......... .......... 32% 89.3M 2s
 57200K .......... .......... .......... .......... .......... 33%  127M 2s
 57250K .......... .......... .......... .......... .......... 33%  106M 2s
 57300K .......... .......... .......... .......... .......... 33%  112M 2s
 57350K .......... .......... .......... .......... .......... 33% 97.1M 2s
 57400K .......... .......... .......... .......... .......... 33%  112M 2s
 57450K .......... .......... .......... .......... .......... 33%  171M 2s
 57500K .......... .......... .......... .......... .......... 33% 93.5M 2s
 57550K .......... .......... .......... .......... .......... 33% 76.8M 2s
 57600K .......... .......... .......... .......... .......... 33%  114M 2s
 57650K .......... .......... .......... .......... .......... 33%  122M 2s
 57700K .......... .......... .......... .......... .......... 33%  103M 2s
 57750K .......... .......... .......... .......... .......... 33%  107M 2s
 57800K .......... .......... .......... .......... .......... 33%  103M 2s
 57850K .......... .......... .......... .......... .......... 33%  106M 2s
 57900K .......... .......... .......... .......... .......... 33% 96.7M 2s
 57950K .......... .......... .......... .......... .......... 33%  117M 2s
 58000K .......... .......... .......... .......... .......... 33%  131M 2s
 58050K .......... .......... .......... .......... .......... 33%  112M 2s
 58100K .......... .......... .......... .......... .......... 33% 95.6M 2s
 58150K .......... .......... .......... .......... .......... 33% 65.4M 2s
 58200K .......... .......... .......... .......... .......... 33%  124M 2s
 58250K .......... .......... .......... .......... .......... 33%  100M 2s
 58300K .......... .......... .......... .......... .......... 33% 94.9M 2s
 58350K .......... .......... .......... .......... .......... 33%  109M 2s
 58400K .......... .......... .......... .......... .......... 33%  152M 2s
 58450K .......... .......... .......... .......... .......... 33% 90.9M 2s
 58500K .......... .......... .......... .......... .......... 33% 90.3M 2s
 58550K .......... .......... .......... .......... .......... 33%  133M 2s
 58600K .......... .......... .......... .......... .......... 33%  116M 2s
 58650K .......... .......... .......... .......... .......... 33% 75.7M 2s
 58700K .......... .......... .......... .......... .......... 33%  120M 2s
 58750K .......... .......... .......... .......... .......... 33% 97.0M 2s
 58800K .......... .......... .......... .......... .......... 33%  110M 2s
 58850K .......... .......... .......... .......... .......... 33% 91.8M 2s
 58900K .......... .......... .......... .......... .......... 33%  115M 2s
 58950K .......... .......... .......... .......... .......... 34%  117M 2s
 59000K .......... .......... .......... .......... .......... 34% 61.4M 2s
 59050K .......... .......... .......... .......... .......... 34%  109M 2s
 59100K .......... .......... .......... .......... .......... 34%  114M 2s
 59150K .......... .......... .......... .......... .......... 34% 96.7M 2s
 59200K .......... .......... .......... .......... .......... 34%  123M 2s
 59250K .......... .......... .......... .......... .......... 34% 98.7M 2s
 59300K .......... .......... .......... .......... .......... 34%  104M 2s
 59350K .......... .......... .......... .......... .......... 34% 63.3M 2s
 59400K .......... .......... .......... .......... .......... 34%  120M 2s
 59450K .......... .......... .......... .......... .......... 34% 90.3M 2s
 59500K .......... .......... .......... .......... .......... 34% 84.7M 2s
 59550K .......... .......... .......... .......... .......... 34% 77.4M 2s
 59600K .......... .......... .......... .......... .......... 34% 98.6M 2s
 59650K .......... .......... .......... .......... .......... 34%  141M 2s
 59700K .......... .......... .......... .......... .......... 34% 73.7M 2s
 59750K .......... .......... .......... .......... .......... 34% 96.3M 2s
 59800K .......... .......... .......... .......... .......... 34%  114M 2s
 59850K .......... .......... .......... .......... .......... 34%  118M 2s
 59900K .......... .......... .......... .......... .......... 34%  122M 2s
 59950K .......... .......... .......... .......... .......... 34%  102M 2s
 60000K .......... .......... .......... .......... .......... 34%  136M 2s
 60050K .......... .......... .......... .......... .......... 34% 99.1M 2s
 60100K .......... .......... .......... .......... .......... 34% 39.8M 2s
 60150K .......... .......... .......... .......... .......... 34% 78.1M 2s
 60200K .......... .......... .......... .......... .......... 34% 66.3M 2s
 60250K .......... .......... .......... .......... .......... 34% 58.1M 2s
 60300K .......... .......... .......... .......... .......... 34%  100M 2s
 60350K .......... .......... .......... .......... .......... 34% 24.5M 2s
 60400K .......... .......... .......... .......... .......... 34% 71.8M 2s
 60450K .......... .......... .......... .......... .......... 34% 51.9M 2s
 60500K .......... .......... .......... .......... .......... 34% 66.9M 2s
 60550K .......... .......... .......... .......... .......... 34% 64.9M 2s
 60600K .......... .......... .......... .......... .......... 34% 86.7M 2s
 60650K .......... .......... .......... .......... .......... 34% 60.5M 2s
 60700K .......... .......... .......... .......... .......... 35% 74.3M 2s
 60750K .......... .......... .......... .......... .......... 35% 73.3M 2s
 60800K .......... .......... .......... .......... .......... 35% 64.6M 2s
 60850K .......... .......... .......... .......... .......... 35% 90.2M 2s
 60900K .......... .......... .......... .......... .......... 35% 79.2M 2s
 60950K .......... .......... .......... .......... .......... 35% 55.5M 2s
 61000K .......... .......... .......... .......... .......... 35% 64.1M 2s
 61050K .......... .......... .......... .......... .......... 35%  106M 2s
 61100K .......... .......... .......... .......... .......... 35% 63.6M 2s
 61150K .......... .......... .......... .......... .......... 35% 90.3M 2s
 61200K .......... .......... .......... .......... .......... 35% 55.0M 2s
 61250K .......... .......... .......... .......... .......... 35% 75.2M 2s
 61300K .......... .......... .......... .......... .......... 35% 78.2M 2s
 61350K .......... .......... .......... .......... .......... 35% 70.4M 2s
 61400K .......... .......... .......... .......... .......... 35% 73.7M 2s
 61450K .......... .......... .......... .......... .......... 35% 65.0M 2s
 61500K .......... .......... .......... .......... .......... 35% 83.2M 2s
 61550K .......... .......... .......... .......... .......... 35% 49.2M 2s
 61600K .......... .......... .......... .......... .......... 35% 75.5M 2s
 61650K .......... .......... .......... .......... .......... 35% 62.8M 2s
 61700K .......... .......... .......... .......... .......... 35%  100M 2s
 61750K .......... .......... .......... .......... .......... 35% 75.4M 2s
 61800K .......... .......... .......... .......... .......... 35% 74.6M 2s
 61850K .......... .......... .......... .......... .......... 35%  111M 2s
 61900K .......... .......... .......... .......... .......... 35% 17.3M 2s
 61950K .......... .......... .......... .......... .......... 35% 96.0M 2s
 62000K .......... .......... .......... .......... .......... 35%  131M 2s
 62050K .......... .......... .......... .......... .......... 35%  108M 2s
 62100K .......... .......... .......... .......... .......... 35% 90.5M 2s
 62150K .......... .......... .......... .......... .......... 35%  116M 2s
 62200K .......... .......... .......... .......... .......... 35%  118M 2s
 62250K .......... .......... .......... .......... .......... 35% 75.3M 2s
 62300K .......... .......... .......... .......... .......... 35% 99.1M 2s
 62350K .......... .......... .......... .......... .......... 35%  161M 2s
 62400K .......... .......... .......... .......... .......... 36%  108M 2s
 62450K .......... .......... .......... .......... .......... 36% 90.7M 2s
 62500K .......... .......... .......... .......... .......... 36%  105M 2s
 62550K .......... .......... .......... .......... .......... 36%  150M 2s
 62600K .......... .......... .......... .......... .......... 36%  113M 2s
 62650K .......... .......... .......... .......... .......... 36% 74.6M 2s
 62700K .......... .......... .......... .......... .......... 36%  144M 2s
 62750K .......... .......... .......... .......... .......... 36%  137M 2s
 62800K .......... .......... .......... .......... .......... 36% 75.0M 2s
 62850K .......... .......... .......... .......... .......... 36%  130M 2s
 62900K .......... .......... .......... .......... .......... 36%  105M 2s
 62950K .......... .......... .......... .......... .......... 36%  132M 2s
 63000K .......... .......... .......... .......... .......... 36%  104M 2s
 63050K .......... .......... .......... .......... .......... 36%  125M 2s
 63100K .......... .......... .......... .......... .......... 36% 80.0M 2s
 63150K .......... .......... .......... .......... .......... 36%  146M 2s
 63200K .......... .......... .......... .......... .......... 36%  105M 2s
 63250K .......... .......... .......... .......... .......... 36%  116M 2s
 63300K .......... .......... .......... .......... .......... 36%  142M 2s
 63350K .......... .......... .......... .......... .......... 36% 4.02M 2s
 63400K .......... .......... .......... .......... .......... 36% 99.5M 2s
 63450K .......... .......... .......... .......... .......... 36%  125M 2s
 63500K .......... .......... .......... .......... .......... 36% 94.0M 2s
 63550K .......... .......... .......... .......... .......... 36%  103M 2s
 63600K .......... .......... .......... .......... .......... 36% 74.6M 2s
 63650K .......... .......... .......... .......... .......... 36% 95.8M 2s
 63700K .......... .......... .......... .......... .......... 36%  130M 2s
 63750K .......... .......... .......... .......... .......... 36% 82.7M 2s
 63800K .......... .......... .......... .......... .......... 36% 99.5M 2s
 63850K .......... .......... .......... .......... .......... 36%  128M 2s
 63900K .......... .......... .......... .......... .......... 36% 65.7M 2s
 63950K .......... .......... .......... .......... .......... 36%  118M 2s
 64000K .......... .......... .......... .......... .......... 36%  123M 2s
 64050K .......... .......... .......... .......... .......... 36%  115M 2s
 64100K .......... .......... .......... .......... .......... 36% 93.0M 2s
 64150K .......... .......... .......... .......... .......... 37%  112M 2s
 64200K .......... .......... .......... .......... .......... 37%  116M 2s
 64250K .......... .......... .......... .......... .......... 37%  101M 2s
 64300K .......... .......... .......... .......... .......... 37% 95.4M 2s
 64350K .......... .......... .......... .......... .......... 37%  156M 2s
 64400K .......... .......... .......... .......... .......... 37%  104M 2s
 64450K .......... .......... .......... .......... .......... 37%  108M 2s
 64500K .......... .......... .......... .......... .......... 37% 8.30M 2s
 64550K .......... .......... .......... .......... .......... 37% 91.3M 2s
 64600K .......... .......... .......... .......... .......... 37%  124M 2s
 64650K .......... .......... .......... .......... .......... 37% 91.1M 2s
 64700K .......... .......... .......... .......... .......... 37%  179M 2s
 64750K .......... .......... .......... .......... .......... 37% 75.7M 2s
 64800K .......... .......... .......... .......... .......... 37%  119M 2s
 64850K .......... .......... .......... .......... .......... 37%  103M 2s
 64900K .......... .......... .......... .......... .......... 37%  104M 2s
 64950K .......... .......... .......... .......... .......... 37%  112M 2s
 65000K .......... .......... .......... .......... .......... 37%  159M 2s
 65050K .......... .......... .......... .......... .......... 37% 89.8M 2s
 65100K .......... .......... .......... .......... .......... 37%  101M 2s
 65150K .......... .......... .......... .......... .......... 37% 85.8M 2s
 65200K .......... .......... .......... .......... .......... 37% 88.5M 2s
 65250K .......... .......... .......... .......... .......... 37%  110M 2s
 65300K .......... .......... .......... .......... .......... 37% 90.4M 2s
 65350K .......... .......... .......... .......... .......... 37%  150M 2s
 65400K .......... .......... .......... .......... .......... 37% 94.0M 2s
 65450K .......... .......... .......... .......... .......... 37% 96.2M 2s
 65500K .......... .......... .......... .......... .......... 37%  121M 2s
 65550K .......... .......... .......... .......... .......... 37%  101M 2s
 65600K .......... .......... .......... .......... .......... 37%  118M 2s
 65650K .......... .......... .......... .......... .......... 37% 84.3M 2s
 65700K .......... .......... .......... .......... .......... 37%  123M 2s
 65750K .......... .......... .......... .......... .......... 37%  112M 2s
 65800K .......... .......... .......... .......... .......... 37% 77.6M 2s
 65850K .......... .......... .......... .......... .......... 37%  106M 2s
 65900K .......... .......... .......... .......... .......... 38% 73.6M 2s
 65950K .......... .......... .......... .......... .......... 38%  199M 2s
 66000K .......... .......... .......... .......... .......... 38%  117M 2s
 66050K .......... .......... .......... .......... .......... 38%  113M 2s
 66100K .......... .......... .......... .......... .......... 38%  106M 2s
 66150K .......... .......... .......... .......... .......... 38%  111M 2s
 66200K .......... .......... .......... .......... .......... 38% 92.4M 2s
 66250K .......... .......... .......... .......... .......... 38%  110M 2s
 66300K .......... .......... .......... .......... .......... 38% 91.2M 2s
 66350K .......... .......... .......... .......... .......... 38%  110M 2s
 66400K .......... .......... .......... .......... .......... 38%  107M 2s
 66450K .......... .......... .......... .......... .......... 38% 74.2M 2s
 66500K .......... .......... .......... .......... .......... 38%  173M 2s
 66550K .......... .......... .......... .......... .......... 38% 81.7M 2s
 66600K .......... .......... .......... .......... .......... 38%  114M 2s
 66650K .......... .......... .......... .......... .......... 38% 89.3M 2s
 66700K .......... .......... .......... .......... .......... 38%  106M 2s
 66750K .......... .......... .......... .......... .......... 38%  124M 2s
 66800K .......... .......... .......... .......... .......... 38% 83.6M 2s
 66850K .......... .......... .......... .......... .......... 38%  101M 2s
 66900K .......... .......... .......... .......... .......... 38%  105M 2s
 66950K .......... .......... .......... .......... .......... 38% 93.3M 2s
 67000K .......... .......... .......... .......... .......... 38%  119M 2s
 67050K .......... .......... .......... .......... .......... 38%  124M 2s
 67100K .......... .......... .......... .......... .......... 38%  120M 2s
 67150K .......... .......... .......... .......... .......... 38% 96.1M 2s
 67200K .......... .......... .......... .......... .......... 38%  127M 2s
 67250K .......... .......... .......... .......... .......... 38%  114M 2s
 67300K .......... .......... .......... .......... .......... 38% 97.6M 2s
 67350K .......... .......... .......... .......... .......... 38%  123M 2s
 67400K .......... .......... .......... .......... .......... 38% 90.0M 2s
 67450K .......... .......... .......... .......... .......... 38%  137M 2s
 67500K .......... .......... .......... .......... .......... 38% 90.9M 2s
 67550K .......... .......... .......... .......... .......... 38%  141M 2s
 67600K .......... .......... .......... .......... .......... 39% 92.0M 2s
 67650K .......... .......... .......... .......... .......... 39% 99.6M 2s
 67700K .......... .......... .......... .......... .......... 39%  134M 2s
 67750K .......... .......... .......... .......... .......... 39% 70.7M 2s
 67800K .......... .......... .......... .......... .......... 39%  133M 2s
 67850K .......... .......... .......... .......... .......... 39% 95.9M 2s
 67900K .......... .......... .......... .......... .......... 39%  108M 2s
 67950K .......... .......... .......... .......... .......... 39%  125M 2s
 68000K .......... .......... .......... .......... .......... 39%  102M 2s
 68050K .......... .......... .......... .......... .......... 39% 63.4M 2s
 68100K .......... .......... .......... .......... .......... 39%  163M 2s
 68150K .......... .......... .......... .......... .......... 39%  129M 2s
 68200K .......... .......... .......... .......... .......... 39% 77.4M 2s
 68250K .......... .......... .......... .......... .......... 39%  117M 2s
 68300K .......... .......... .......... .......... .......... 39% 68.9M 2s
 68350K .......... .......... .......... .......... .......... 39%  104M 2s
 68400K .......... .......... .......... .......... .......... 39%  106M 2s
 68450K .......... .......... .......... .......... .......... 39% 91.4M 2s
 68500K .......... .......... .......... .......... .......... 39%  108M 2s
 68550K .......... .......... .......... .......... .......... 39%  158M 2s
 68600K .......... .......... .......... .......... .......... 39%  106M 2s
 68650K .......... .......... .......... .......... .......... 39% 83.9M 2s
 68700K .......... .......... .......... .......... .......... 39% 89.7M 2s
 68750K .......... .......... .......... .......... .......... 39% 93.2M 2s
 68800K .......... .......... .......... .......... .......... 39%  121M 2s
 68850K .......... .......... .......... .......... .......... 39% 74.4M 2s
 68900K .......... .......... .......... .......... .......... 39%  128M 2s
 68950K .......... .......... .......... .......... .......... 39% 62.4M 2s
 69000K .......... .......... .......... .......... .......... 39% 74.6M 2s
 69050K .......... .......... .......... .......... .......... 39%  164M 2s
 69100K .......... .......... .......... .......... .......... 39% 68.6M 2s
 69150K .......... .......... .......... .......... .......... 39%  149M 2s
 69200K .......... .......... .......... .......... .......... 39% 85.5M 2s
 69250K .......... .......... .......... .......... .......... 39% 87.5M 2s
 69300K .......... .......... .......... .......... .......... 39%  123M 2s
 69350K .......... .......... .......... .......... .......... 40% 82.0M 2s
 69400K .......... .......... .......... .......... .......... 40% 78.3M 2s
 69450K .......... .......... .......... .......... .......... 40% 83.8M 2s
 69500K .......... .......... .......... .......... .......... 40% 87.4M 2s
 69550K .......... .......... .......... .......... .......... 40%  123M 2s
 69600K .......... .......... .......... .......... .......... 40%  138M 2s
 69650K .......... .......... .......... .......... .......... 40%  103M 2s
 69700K .......... .......... .......... .......... .......... 40%  101M 2s
 69750K .......... .......... .......... .......... .......... 40%  118M 2s
 69800K .......... .......... .......... .......... .......... 40% 48.9M 2s
 69850K .......... .......... .......... .......... .......... 40% 82.9M 2s
 69900K .......... .......... .......... .......... .......... 40%  122M 2s
 69950K .......... .......... .......... .......... .......... 40%  104M 2s
 70000K .......... .......... .......... .......... .......... 40%  165M 2s
 70050K .......... .......... .......... .......... .......... 40% 67.7M 2s
 70100K .......... .......... .......... .......... .......... 40%  106M 2s
 70150K .......... .......... .......... .......... .......... 40%  105M 2s
 70200K .......... .......... .......... .......... .......... 40%  105M 2s
 70250K .......... .......... .......... .......... .......... 40% 85.1M 2s
 70300K .......... .......... .......... .......... .......... 40%  104M 2s
 70350K .......... .......... .......... .......... .......... 40%  134M 2s
 70400K .......... .......... .......... .......... .......... 40% 90.6M 2s
 70450K .......... .......... .......... .......... .......... 40% 81.2M 2s
 70500K .......... .......... .......... .......... .......... 40%  108M 2s
 70550K .......... .......... .......... .......... .......... 40%  112M 2s
 70600K .......... .......... .......... .......... .......... 40% 98.3M 2s
 70650K .......... .......... .......... .......... .......... 40%  109M 2s
 70700K .......... .......... .......... .......... .......... 40%  119M 2s
 70750K .......... .......... .......... .......... .......... 40% 75.8M 2s
 70800K .......... .......... .......... .......... .......... 40%  108M 2s
 70850K .......... .......... .......... .......... .......... 40%  129M 2s
 70900K .......... .......... .......... .......... .......... 40%  127M 2s
 70950K .......... .......... .......... .......... .......... 40% 93.8M 2s
 71000K .......... .......... .......... .......... .......... 40% 90.4M 2s
 71050K .......... .......... .......... .......... .......... 40%  163M 2s
 71100K .......... .......... .......... .......... .......... 41% 85.7M 2s
 71150K .......... .......... .......... .......... .......... 41% 95.7M 2s
 71200K .......... .......... .......... .......... .......... 41% 77.7M 2s
 71250K .......... .......... .......... .......... .......... 41%  108M 2s
 71300K .......... .......... .......... .......... .......... 41% 72.5M 2s
 71350K .......... .......... .......... .......... .......... 41% 98.1M 2s
 71400K .......... .......... .......... .......... .......... 41%  104M 2s
 71450K .......... .......... .......... .......... .......... 41% 88.2M 2s
 71500K .......... .......... .......... .......... .......... 41%  149M 2s
 71550K .......... .......... .......... .......... .......... 41% 94.0M 2s
 71600K .......... .......... .......... .......... .......... 41%  123M 2s
 71650K .......... .......... .......... .......... .......... 41% 82.5M 2s
 71700K .......... .......... .......... .......... .......... 41%  110M 2s
 71750K .......... .......... .......... .......... .......... 41%  106M 2s
 71800K .......... .......... .......... .......... .......... 41% 82.2M 2s
 71850K .......... .......... .......... .......... .......... 41%  116M 2s
 71900K .......... .......... .......... .......... .......... 41% 98.6M 2s
 71950K .......... .......... .......... .......... .......... 41%  132M 2s
 72000K .......... .......... .......... .......... .......... 41%  113M 2s
 72050K .......... .......... .......... .......... .......... 41% 90.4M 2s
 72100K .......... .......... .......... .......... .......... 41% 92.1M 2s
 72150K .......... .......... .......... .......... .......... 41%  111M 2s
 72200K .......... .......... .......... .......... .......... 41%  103M 2s
 72250K .......... .......... .......... .......... .......... 41%  110M 2s
 72300K .......... .......... .......... .......... .......... 41%  105M 1s
 72350K .......... .......... .......... .......... .......... 41%  104M 1s
 72400K .......... .......... .......... .......... .......... 41% 83.5M 1s
 72450K .......... .......... .......... .......... .......... 41% 95.6M 1s
 72500K .......... .......... .......... .......... .......... 41%  108M 1s
 72550K .......... .......... .......... .......... .......... 41% 87.9M 1s
 72600K .......... .......... .......... .......... .......... 41% 92.0M 1s
 72650K .......... .......... .......... .......... .......... 41%  111M 1s
 72700K .......... .......... .......... .......... .......... 41%  114M 1s
 72750K .......... .......... .......... .......... .......... 41% 72.9M 1s
 72800K .......... .......... .......... .......... .......... 41%  143M 1s
 72850K .......... .......... .......... .......... .......... 42%  107M 1s
 72900K .......... .......... .......... .......... .......... 42% 92.7M 1s
 72950K .......... .......... .......... .......... .......... 42%  102M 1s
 73000K .......... .......... .......... .......... .......... 42%  101M 1s
 73050K .......... .......... .......... .......... .......... 42% 96.1M 1s
 73100K .......... .......... .......... .......... .......... 42%  116M 1s
 73150K .......... .......... .......... .......... .......... 42% 70.7M 1s
 73200K .......... .......... .......... .......... .......... 42%  111M 1s
 73250K .......... .......... .......... .......... .......... 42%  116M 1s
 73300K .......... .......... .......... .......... .......... 42%  136M 1s
 73350K .......... .......... .......... .......... .......... 42%  103M 1s
 73400K .......... .......... .......... .......... .......... 42%  113M 1s
 73450K .......... .......... .......... .......... .......... 42%  116M 1s
 73500K .......... .......... .......... .......... .......... 42% 94.9M 1s
 73550K .......... .......... .......... .......... .......... 42%  100M 1s
 73600K .......... .......... .......... .......... .......... 42%  130M 1s
 73650K .......... .......... .......... .......... .......... 42% 92.7M 1s
 73700K .......... .......... .......... .......... .......... 42%  128M 1s
 73750K .......... .......... .......... .......... .......... 42% 89.0M 1s
 73800K .......... .......... .......... .......... .......... 42%  103M 1s
 73850K .......... .......... .......... .......... .......... 42% 84.2M 1s
 73900K .......... .......... .......... .......... .......... 42%  109M 1s
 73950K .......... .......... .......... .......... .......... 42%  149M 1s
 74000K .......... .......... .......... .......... .......... 42%  107M 1s
 74050K .......... .......... .......... .......... .......... 42%  107M 1s
 74100K .......... .......... .......... .......... .......... 42% 69.4M 1s
 74150K .......... .......... .......... .......... .......... 42% 66.8M 1s
 74200K .......... .......... .......... .......... .......... 42% 82.0M 1s
 74250K .......... .......... .......... .......... .......... 42% 86.7M 1s
 74300K .......... .......... .......... .......... .......... 42%  129M 1s
 74350K .......... .......... .......... .......... .......... 42%  101M 1s
 74400K .......... .......... .......... .......... .......... 42%  114M 1s
 74450K .......... .......... .......... .......... .......... 42%  121M 1s
 74500K .......... .......... .......... .......... .......... 42% 75.9M 1s
 74550K .......... .......... .......... .......... .......... 43% 96.1M 1s
 74600K .......... .......... .......... .......... .......... 43%  104M 1s
 74650K .......... .......... .......... .......... .......... 43%  102M 1s
 74700K .......... .......... .......... .......... .......... 43% 76.1M 1s
 74750K .......... .......... .......... .......... .......... 43% 96.1M 1s
 74800K .......... .......... .......... .......... .......... 43%  108M 1s
 74850K .......... .......... .......... .......... .......... 43%  101M 1s
 74900K .......... .......... .......... .......... .......... 43% 86.1M 1s
 74950K .......... .......... .......... .......... .......... 43% 92.4M 1s
 75000K .......... .......... .......... .......... .......... 43%  148M 1s
 75050K .......... .......... .......... .......... .......... 43%  127M 1s
 75100K .......... .......... .......... .......... .......... 43%  138M 1s
 75150K .......... .......... .......... .......... .......... 43%  105M 1s
 75200K .......... .......... .......... .......... .......... 43%  124M 1s
 75250K .......... .......... .......... .......... .......... 43% 86.3M 1s
 75300K .......... .......... .......... .......... .......... 43% 91.7M 1s
 75350K .......... .......... .......... .......... .......... 43%  130M 1s
 75400K .......... .......... .......... .......... .......... 43% 78.2M 1s
 75450K .......... .......... .......... .......... .......... 43%  152M 1s
 75500K .......... .......... .......... .......... .......... 43%  101M 1s
 75550K .......... .......... .......... .......... .......... 43%  102M 1s
 75600K .......... .......... .......... .......... .......... 43%  120M 1s
 75650K .......... .......... .......... .......... .......... 43% 93.9M 1s
 75700K .......... .......... .......... .......... .......... 43% 88.1M 1s
 75750K .......... .......... .......... .......... .......... 43%  124M 1s
 75800K .......... .......... .......... .......... .......... 43% 90.9M 1s
 75850K .......... .......... .......... .......... .......... 43%  102M 1s
 75900K .......... .......... .......... .......... .......... 43%  153M 1s
 75950K .......... .......... .......... .......... .......... 43% 93.6M 1s
 76000K .......... .......... .......... .......... .......... 43%  115M 1s
 76050K .......... .......... .......... .......... .......... 43% 55.5M 1s
 76100K .......... .......... .......... .......... .......... 43%  134M 1s
 76150K .......... .......... .......... .......... .......... 43%  109M 1s
 76200K .......... .......... .......... .......... .......... 43% 78.0M 1s
 76250K .......... .......... .......... .......... .......... 43%  107M 1s
 76300K .......... .......... .......... .......... .......... 44%  123M 1s
 76350K .......... .......... .......... .......... .......... 44% 97.9M 1s
 76400K .......... .......... .......... .......... .......... 44%  122M 1s
 76450K .......... .......... .......... .......... .......... 44%  107M 1s
 76500K .......... .......... .......... .......... .......... 44% 92.4M 1s
 76550K .......... .......... .......... .......... .......... 44%  122M 1s
 76600K .......... .......... .......... .......... .......... 44% 77.3M 1s
 76650K .......... .......... .......... .......... .......... 44% 93.3M 1s
 76700K .......... .......... .......... .......... .......... 44%  123M 1s
 76750K .......... .......... .......... .......... .......... 44% 84.5M 1s
 76800K .......... .......... .......... .......... .......... 44% 98.6M 1s
 76850K .......... .......... .......... .......... .......... 44% 80.1M 1s
 76900K .......... .......... .......... .......... .......... 44% 95.6M 1s
 76950K .......... .......... .......... .......... .......... 44% 86.4M 1s
 77000K .......... .......... .......... .......... .......... 44% 70.0M 1s
 77050K .......... .......... .......... .......... .......... 44% 90.5M 1s
 77100K .......... .......... .......... .......... .......... 44% 93.1M 1s
 77150K .......... .......... .......... .......... .......... 44% 81.6M 1s
 77200K .......... .......... .......... .......... .......... 44%  107M 1s
 77250K .......... .......... .......... .......... .......... 44%  110M 1s
 77300K .......... .......... .......... .......... .......... 44% 98.8M 1s
 77350K .......... .......... .......... .......... .......... 44%  140M 1s
 77400K .......... .......... .......... .......... .......... 44%  107M 1s
 77450K .......... .......... .......... .......... .......... 44% 79.8M 1s
 77500K .......... .......... .......... .......... .......... 44% 78.1M 1s
 77550K .......... .......... .......... .......... .......... 44% 81.9M 1s
 77600K .......... .......... .......... .......... .......... 44%  169M 1s
 77650K .......... .......... .......... .......... .......... 44%  118M 1s
 77700K .......... .......... .......... .......... .......... 44% 90.4M 1s
 77750K .......... .......... .......... .......... .......... 44%  146M 1s
 77800K .......... .......... .......... .......... .......... 44%  123M 1s
 77850K .......... .......... .......... .......... .......... 44% 98.0M 1s
 77900K .......... .......... .......... .......... .......... 44% 78.1M 1s
 77950K .......... .......... .......... .......... .......... 44%  118M 1s
 78000K .......... .......... .......... .......... .......... 44%  107M 1s
 78050K .......... .......... .......... .......... .......... 45%  105M 1s
 78100K .......... .......... .......... .......... .......... 45% 95.8M 1s
 78150K .......... .......... .......... .......... .......... 45%  126M 1s
 78200K .......... .......... .......... .......... .......... 45%  105M 1s
 78250K .......... .......... .......... .......... .......... 45% 68.4M 1s
 78300K .......... .......... .......... .......... .......... 45%  145M 1s
 78350K .......... .......... .......... .......... .......... 45% 80.9M 1s
 78400K .......... .......... .......... .......... .......... 45%  123M 1s
 78450K .......... .......... .......... .......... .......... 45%  104M 1s
 78500K .......... .......... .......... .......... .......... 45%  113M 1s
 78550K .......... .......... .......... .......... .......... 45% 97.1M 1s
 78600K .......... .......... .......... .......... .......... 45% 91.5M 1s
 78650K .......... .......... .......... .......... .......... 45% 98.2M 1s
 78700K .......... .......... .......... .......... .......... 45%  118M 1s
 78750K .......... .......... .......... .......... .......... 45%  102M 1s
 78800K .......... .......... .......... .......... .......... 45%  128M 1s
 78850K .......... .......... .......... .......... .......... 45% 87.6M 1s
 78900K .......... .......... .......... .......... .......... 45%  128M 1s
 78950K .......... .......... .......... .......... .......... 45% 77.6M 1s
 79000K .......... .......... .......... .......... .......... 45% 94.3M 1s
 79050K .......... .......... .......... .......... .......... 45%  140M 1s
 79100K .......... .......... .......... .......... .......... 45%  128M 1s
 79150K .......... .......... .......... .......... .......... 45%  113M 1s
 79200K .......... .......... .......... .......... .......... 45% 76.3M 1s
 79250K .......... .......... .......... .......... .......... 45%  196M 1s
 79300K .......... .......... .......... .......... .......... 45% 65.7M 1s
 79350K .......... .......... .......... .......... .......... 45% 88.9M 1s
 79400K .......... .......... .......... .......... .......... 45%  107M 1s
 79450K .......... .......... .......... .......... .......... 45%  113M 1s
 79500K .......... .......... .......... .......... .......... 45%  146M 1s
 79550K .......... .......... .......... .......... .......... 45%  104M 1s
 79600K .......... .......... .......... .......... .......... 45% 99.3M 1s
 79650K .......... .......... .......... .......... .......... 45%  118M 1s
 79700K .......... .......... .......... .......... .......... 45% 91.9M 1s
 79750K .......... .......... .......... .......... .......... 46%  144M 1s
 79800K .......... .......... .......... .......... .......... 46%  109M 1s
 79850K .......... .......... .......... .......... .......... 46% 74.4M 1s
 79900K .......... .......... .......... .......... .......... 46%  116M 1s
 79950K .......... .......... .......... .......... .......... 46%  152M 1s
 80000K .......... .......... .......... .......... .......... 46%  117M 1s
 80050K .......... .......... .......... .......... .......... 46% 82.8M 1s
 80100K .......... .......... .......... .......... .......... 46%  138M 1s
 80150K .......... .......... .......... .......... .......... 46%  108M 1s
 80200K .......... .......... .......... .......... .......... 46%  127M 1s
 80250K .......... .......... .......... .......... .......... 46%  130M 1s
 80300K .......... .......... .......... .......... .......... 46% 97.2M 1s
 80350K .......... .......... .......... .......... .......... 46%  131M 1s
 80400K .......... .......... .......... .......... .......... 46% 83.6M 1s
 80450K .......... .......... .......... .......... .......... 46% 98.7M 1s
 80500K .......... .......... .......... .......... .......... 46%  104M 1s
 80550K .......... .......... .......... .......... .......... 46%  123M 1s
 80600K .......... .......... .......... .......... .......... 46% 91.7M 1s
 80650K .......... .......... .......... .......... .......... 46%  109M 1s
 80700K .......... .......... .......... .......... .......... 46%  124M 1s
 80750K .......... .......... .......... .......... .......... 46%  119M 1s
 80800K .......... .......... .......... .......... .......... 46% 96.0M 1s
 80850K .......... .......... .......... .......... .......... 46%  106M 1s
 80900K .......... .......... .......... .......... .......... 46%  132M 1s
 80950K .......... .......... .......... .......... .......... 46%  109M 1s
 81000K .......... .......... .......... .......... .......... 46%  115M 1s
 81050K .......... .......... .......... .......... .......... 46% 83.8M 1s
 81100K .......... .......... .......... .......... .......... 46%  135M 1s
 81150K .......... .......... .......... .......... .......... 46%  102M 1s
 81200K .......... .......... .......... .......... .......... 46% 82.0M 1s
 81250K .......... .......... .......... .......... .......... 46% 77.9M 1s
 81300K .......... .......... .......... .......... .......... 46%  156M 1s
 81350K .......... .......... .......... .......... .......... 46% 71.4M 1s
 81400K .......... .......... .......... .......... .......... 46%  107M 1s
 81450K .......... .......... .......... .......... .......... 46%  110M 1s
 81500K .......... .......... .......... .......... .......... 47% 81.4M 1s
 81550K .......... .......... .......... .......... .......... 47% 80.2M 1s
 81600K .......... .......... .......... .......... .......... 47%  118M 1s
 81650K .......... .......... .......... .......... .......... 47% 83.3M 1s
 81700K .......... .......... .......... .......... .......... 47%  113M 1s
 81750K .......... .......... .......... .......... .......... 47%  110M 1s
 81800K .......... .......... .......... .......... .......... 47% 78.2M 1s
 81850K .......... .......... .......... .......... .......... 47% 72.9M 1s
 81900K .......... .......... .......... .......... .......... 47% 99.7M 1s
 81950K .......... .......... .......... .......... .......... 47%  107M 1s
 82000K .......... .......... .......... .......... .......... 47% 98.2M 1s
 82050K .......... .......... .......... .......... .......... 47%  121M 1s
 82100K .......... .......... .......... .......... .......... 47% 90.5M 1s
 82150K .......... .......... .......... .......... .......... 47%  114M 1s
 82200K .......... .......... .......... .......... .......... 47%  118M 1s
 82250K .......... .......... .......... .......... .......... 47% 87.9M 1s
 82300K .......... .......... .......... .......... .......... 47% 98.7M 1s
 82350K .......... .......... .......... .......... .......... 47%  147M 1s
 82400K .......... .......... .......... .......... .......... 47% 82.3M 1s
 82450K .......... .......... .......... .......... .......... 47%  220M 1s
 82500K .......... .......... .......... .......... .......... 47%  120M 1s
 82550K .......... .......... .......... .......... .......... 47%  107M 1s
 82600K .......... .......... .......... .......... .......... 47% 88.6M 1s
 82650K .......... .......... .......... .......... .......... 47%  113M 1s
 82700K .......... .......... .......... .......... .......... 47%  129M 1s
 82750K .......... .......... .......... .......... .......... 47% 95.2M 1s
 82800K .......... .......... .......... .......... .......... 47%  110M 1s
 82850K .......... .......... .......... .......... .......... 47%  160M 1s
 82900K .......... .......... .......... .......... .......... 47%  107M 1s
 82950K .......... .......... .......... .......... .......... 47% 75.7M 1s
 83000K .......... .......... .......... .......... .......... 47%  166M 1s
 83050K .......... .......... .......... .......... .......... 47% 80.9M 1s
 83100K .......... .......... .......... .......... .......... 47%  121M 1s
 83150K .......... .......... .......... .......... .......... 47%  102M 1s
 83200K .......... .......... .......... .......... .......... 47% 82.1M 1s
 83250K .......... .......... .......... .......... .......... 48%  123M 1s
 83300K .......... .......... .......... .......... .......... 48%  117M 1s
 83350K .......... .......... .......... .......... .......... 48% 84.5M 1s
 83400K .......... .......... .......... .......... .......... 48%  118M 1s
 83450K .......... .......... .......... .......... .......... 48%  114M 1s
 83500K .......... .......... .......... .......... .......... 48%  104M 1s
 83550K .......... .......... .......... .......... .......... 48% 90.0M 1s
 83600K .......... .......... .......... .......... .......... 48%  129M 1s
 83650K .......... .......... .......... .......... .......... 48%  127M 1s
 83700K .......... .......... .......... .......... .......... 48% 76.6M 1s
 83750K .......... .......... .......... .......... .......... 48%  128M 1s
 83800K .......... .......... .......... .......... .......... 48% 95.9M 1s
 83850K .......... .......... .......... .......... .......... 48% 96.2M 1s
 83900K .......... .......... .......... .......... .......... 48%  105M 1s
 83950K .......... .......... .......... .......... .......... 48%  115M 1s
 84000K .......... .......... .......... .......... .......... 48%  105M 1s
 84050K .......... .......... .......... .......... .......... 48% 80.0M 1s
 84100K .......... .......... .......... .......... .......... 48%  116M 1s
 84150K .......... .......... .......... .......... .......... 48%  104M 1s
 84200K .......... .......... .......... .......... .......... 48%  133M 1s
 84250K .......... .......... .......... .......... .......... 48% 74.5M 1s
 84300K .......... .......... .......... .......... .......... 48%  235M 1s
 84350K .......... .......... .......... .......... .......... 48%  101M 1s
 84400K .......... .......... .......... .......... .......... 48% 75.0M 1s
 84450K .......... .......... .......... .......... .......... 48%  121M 1s
 84500K .......... .......... .......... .......... .......... 48%  128M 1s
 84550K .......... .......... .......... .......... .......... 48%  128M 1s
 84600K .......... .......... .......... .......... .......... 48% 69.6M 1s
 84650K .......... .......... .......... .......... .......... 48%  160M 1s
 84700K .......... .......... .......... .......... .......... 48%  106M 1s
 84750K .......... .......... .......... .......... .......... 48% 86.9M 1s
 84800K .......... .......... .......... .......... .......... 48%  113M 1s
 84850K .......... .......... .......... .......... .......... 48%  108M 1s
 84900K .......... .......... .......... .......... .......... 48%  104M 1s
 84950K .......... .......... .......... .......... .......... 49%  102M 1s
 85000K .......... .......... .......... .......... .......... 49% 83.8M 1s
 85050K .......... .......... .......... .......... .......... 49%  104M 1s
 85100K .......... .......... .......... .......... .......... 49%  130M 1s
 85150K .......... .......... .......... .......... .......... 49% 90.2M 1s
 85200K .......... .......... .......... .......... .......... 49% 81.1M 1s
 85250K .......... .......... .......... .......... .......... 49%  115M 1s
 85300K .......... .......... .......... .......... .......... 49% 62.0M 1s
 85350K .......... .......... .......... .......... .......... 49%  172M 1s
 85400K .......... .......... .......... .......... .......... 49%  120M 1s
 85450K .......... .......... .......... .......... .......... 49%  108M 1s
 85500K .......... .......... .......... .......... .......... 49% 76.4M 1s
 85550K .......... .......... .......... .......... .......... 49%  112M 1s
 85600K .......... .......... .......... .......... .......... 49% 76.7M 1s
 85650K .......... .......... .......... .......... .......... 49%  133M 1s
 85700K .......... .......... .......... .......... .......... 49% 94.9M 1s
 85750K .......... .......... .......... .......... .......... 49% 76.9M 1s
 85800K .......... .......... .......... .......... .......... 49%  128M 1s
 85850K .......... .......... .......... .......... .......... 49% 77.2M 1s
 85900K .......... .......... .......... .......... .......... 49% 88.2M 1s
 85950K .......... .......... .......... .......... .......... 49%  110M 1s
 86000K .......... .......... .......... .......... .......... 49% 83.9M 1s
 86050K .......... .......... .......... .......... .......... 49%  109M 1s
 86100K .......... .......... .......... .......... .......... 49% 89.7M 1s
 86150K .......... .......... .......... .......... .......... 49%  128M 1s
 86200K .......... .......... .......... .......... .......... 49%  165M 1s
 86250K .......... .......... .......... .......... .......... 49% 77.8M 1s
 86300K .......... .......... .......... .......... .......... 49%  113M 1s
 86350K .......... .......... .......... .......... .......... 49% 90.0M 1s
 86400K .......... .......... .......... .......... .......... 49%  111M 1s
 86450K .......... .......... .......... .......... .......... 49% 90.1M 1s
 86500K .......... .......... .......... .......... .......... 49%  139M 1s
 86550K .......... .......... .......... .......... .......... 49%  119M 1s
 86600K .......... .......... .......... .......... .......... 49% 81.7M 1s
 86650K .......... .......... .......... .......... .......... 49%  124M 1s
 86700K .......... .......... .......... .......... .......... 50%  106M 1s
 86750K .......... .......... .......... .......... .......... 50%  115M 1s
 86800K .......... .......... .......... .......... .......... 50%  136M 1s
 86850K .......... .......... .......... .......... .......... 50%  108M 1s
 86900K .......... .......... .......... .......... .......... 50% 95.7M 1s
 86950K .......... .......... .......... .......... .......... 50% 68.8M 1s
 87000K .......... .......... .......... .......... .......... 50% 77.7M 1s
 87050K .......... .......... .......... .......... .......... 50%  121M 1s
 87100K .......... .......... .......... .......... .......... 50% 96.8M 1s
 87150K .......... .......... .......... .......... .......... 50%  118M 1s
 87200K .......... .......... .......... .......... .......... 50% 85.6M 1s
 87250K .......... .......... .......... .......... .......... 50% 88.8M 1s
 87300K .......... .......... .......... .......... .......... 50%  122M 1s
 87350K .......... .......... .......... .......... .......... 50% 72.2M 1s
 87400K .......... .......... .......... .......... .......... 50%  120M 1s
 87450K .......... .......... .......... .......... .......... 50% 98.6M 1s
 87500K .......... .......... .......... .......... .......... 50% 86.7M 1s
 87550K .......... .......... .......... .......... .......... 50%  137M 1s
 87600K .......... .......... .......... .......... .......... 50% 92.5M 1s
 87650K .......... .......... .......... .......... .......... 50%  128M 1s
 87700K .......... .......... .......... .......... .......... 50% 83.2M 1s
 87750K .......... .......... .......... .......... .......... 50% 91.0M 1s
 87800K .......... .......... .......... .......... .......... 50%  125M 1s
 87850K .......... .......... .......... .......... .......... 50% 79.3M 1s
 87900K .......... .......... .......... .......... .......... 50%  106M 1s
 87950K .......... .......... .......... .......... .......... 50%  126M 1s
 88000K .......... .......... .......... .......... .......... 50% 91.1M 1s
 88050K .......... .......... .......... .......... .......... 50% 79.8M 1s
 88100K .......... .......... .......... .......... .......... 50%  117M 1s
 88150K .......... .......... .......... .......... .......... 50% 93.0M 1s
 88200K .......... .......... .......... .......... .......... 50%  120M 1s
 88250K .......... .......... .......... .......... .......... 50% 86.2M 1s
 88300K .......... .......... .......... .......... .......... 50% 94.2M 1s
 88350K .......... .......... .......... .......... .......... 50%  121M 1s
 88400K .......... .......... .......... .......... .......... 50%  120M 1s
 88450K .......... .......... .......... .......... .......... 51% 78.6M 1s
 88500K .......... .......... .......... .......... .......... 51%  110M 1s
 88550K .......... .......... .......... .......... .......... 51%  106M 1s
 88600K .......... .......... .......... .......... .......... 51%  100M 1s
 88650K .......... .......... .......... .......... .......... 51%  106M 1s
 88700K .......... .......... .......... .......... .......... 51%  102M 1s
 88750K .......... .......... .......... .......... .......... 51%  166M 1s
 88800K .......... .......... .......... .......... .......... 51% 80.1M 1s
 88850K .......... .......... .......... .......... .......... 51%  123M 1s
 88900K .......... .......... .......... .......... .......... 51%  142M 1s
 88950K .......... .......... .......... .......... .......... 51%  158M 1s
 89000K .......... .......... .......... .......... .......... 51% 97.8M 1s
 89050K .......... .......... .......... .......... .......... 51%  127M 1s
 89100K .......... .......... .......... .......... .......... 51%  110M 1s
 89150K .......... .......... .......... .......... .......... 51% 73.5M 1s
 89200K .......... .......... .......... .......... .......... 51% 91.2M 1s
 89250K .......... .......... .......... .......... .......... 51% 90.0M 1s
 89300K .......... .......... .......... .......... .......... 51% 93.6M 1s
 89350K .......... .......... .......... .......... .......... 51%  186M 1s
 89400K .......... .......... .......... .......... .......... 51%  115M 1s
 89450K .......... .......... .......... .......... .......... 51%  105M 1s
 89500K .......... .......... .......... .......... .......... 51%  131M 1s
 89550K .......... .......... .......... .......... .......... 51%  103M 1s
 89600K .......... .......... .......... .......... .......... 51%  125M 1s
 89650K .......... .......... .......... .......... .......... 51%  129M 1s
 89700K .......... .......... .......... .......... .......... 51% 97.4M 1s
 89750K .......... .......... .......... .......... .......... 51%  105M 1s
 89800K .......... .......... .......... .......... .......... 51%  111M 1s
 89850K .......... .......... .......... .......... .......... 51%  123M 1s
 89900K .......... .......... .......... .......... .......... 51% 76.5M 1s
 89950K .......... .......... .......... .......... .......... 51%  116M 1s
 90000K .......... .......... .......... .......... .......... 51%  156M 1s
 90050K .......... .......... .......... .......... .......... 51% 95.7M 1s
 90100K .......... .......... .......... .......... .......... 51% 95.3M 1s
 90150K .......... .......... .......... .......... .......... 52%  101M 1s
 90200K .......... .......... .......... .......... .......... 52%  112M 1s
 90250K .......... .......... .......... .......... .......... 52%  105M 1s
 90300K .......... .......... .......... .......... .......... 52%  125M 1s
 90350K .......... .......... .......... .......... .......... 52%  105M 1s
 90400K .......... .......... .......... .......... .......... 52%  129M 1s
 90450K .......... .......... .......... .......... .......... 52%  103M 1s
 90500K .......... .......... .......... .......... .......... 52%  111M 1s
 90550K .......... .......... .......... .......... .......... 52%  119M 1s
 90600K .......... .......... .......... .......... .......... 52% 93.0M 1s
 90650K .......... .......... .......... .......... .......... 52%  106M 1s
 90700K .......... .......... .......... .......... .......... 52% 82.2M 1s
 90750K .......... .......... .......... .......... .......... 52% 77.6M 1s
 90800K .......... .......... .......... .......... .......... 52%  114M 1s
 90850K .......... .......... .......... .......... .......... 52% 76.5M 1s
 90900K .......... .......... .......... .......... .......... 52%  186M 1s
 90950K .......... .......... .......... .......... .......... 52% 83.9M 1s
 91000K .......... .......... .......... .......... .......... 52%  117M 1s
 91050K .......... .......... .......... .......... .......... 52% 93.1M 1s
 91100K .......... .......... .......... .......... .......... 52%  125M 1s
 91150K .......... .......... .......... .......... .......... 52% 82.2M 1s
 91200K .......... .......... .......... .......... .......... 52% 94.5M 1s
 91250K .......... .......... .......... .......... .......... 52%  146M 1s
 91300K .......... .......... .......... .......... .......... 52% 94.0M 1s
 91350K .......... .......... .......... .......... .......... 52%  107M 1s
 91400K .......... .......... .......... .......... .......... 52%  119M 1s
 91450K .......... .......... .......... .......... .......... 52%  142M 1s
 91500K .......... .......... .......... .......... .......... 52%  121M 1s
 91550K .......... .......... .......... .......... .......... 52%  106M 1s
 91600K .......... .......... .......... .......... .......... 52% 79.2M 1s
 91650K .......... .......... .......... .......... .......... 52%  171M 1s
 91700K .......... .......... .......... .......... .......... 52% 92.7M 1s
 91750K .......... .......... .......... .......... .......... 52%  123M 1s
 91800K .......... .......... .......... .......... .......... 52% 97.7M 1s
 91850K .......... .......... .......... .......... .......... 52%  102M 1s
 91900K .......... .......... .......... .......... .......... 53% 99.2M 1s
 91950K .......... .......... .......... .......... .......... 53% 96.0M 1s
 92000K .......... .......... .......... .......... .......... 53% 80.2M 1s
 92050K .......... .......... .......... .......... .......... 53%  139M 1s
 92100K .......... .......... .......... .......... .......... 53% 94.6M 1s
 92150K .......... .......... .......... .......... .......... 53%  121M 1s
 92200K .......... .......... .......... .......... .......... 53%  118M 1s
 92250K .......... .......... .......... .......... .......... 53% 99.5M 1s
 92300K .......... .......... .......... .......... .......... 53%  107M 1s
 92350K .......... .......... .......... .......... .......... 53%  101M 1s
 92400K .......... .......... .......... .......... .......... 53%  136M 1s
 92450K .......... .......... .......... .......... .......... 53% 75.4M 1s
 92500K .......... .......... .......... .......... .......... 53%  152M 1s
 92550K .......... .......... .......... .......... .......... 53%  120M 1s
 92600K .......... .......... .......... .......... .......... 53%  113M 1s
 92650K .......... .......... .......... .......... .......... 53%  107M 1s
 92700K .......... .......... .......... .......... .......... 53%  109M 1s
 92750K .......... .......... .......... .......... .......... 53% 96.0M 1s
 92800K .......... .......... .......... .......... .......... 53% 51.1M 1s
 92850K .......... .......... .......... .......... .......... 53%  118M 1s
 92900K .......... .......... .......... .......... .......... 53% 83.6M 1s
 92950K .......... .......... .......... .......... .......... 53%  110M 1s
 93000K .......... .......... .......... .......... .......... 53% 95.5M 1s
 93050K .......... .......... .......... .......... .......... 53%  105M 1s
 93100K .......... .......... .......... .......... .......... 53%  129M 1s
 93150K .......... .......... .......... .......... .......... 53%  122M 1s
 93200K .......... .......... .......... .......... .......... 53% 99.3M 1s
 93250K .......... .......... .......... .......... .......... 53%  111M 1s
 93300K .......... .......... .......... .......... .......... 53%  128M 1s
 93350K .......... .......... .......... .......... .......... 53%  123M 1s
 93400K .......... .......... .......... .......... .......... 53% 93.1M 1s
 93450K .......... .......... .......... .......... .......... 53%  119M 1s
 93500K .......... .......... .......... .......... .......... 53%  110M 1s
 93550K .......... .......... .......... .......... .......... 53% 88.7M 1s
 93600K .......... .......... .......... .......... .......... 53% 94.6M 1s
 93650K .......... .......... .......... .......... .......... 54%  112M 1s
 93700K .......... .......... .......... .......... .......... 54%  106M 1s
 93750K .......... .......... .......... .......... .......... 54% 94.7M 1s
 93800K .......... .......... .......... .......... .......... 54%  134M 1s
 93850K .......... .......... .......... .......... .......... 54%  118M 1s
 93900K .......... .......... .......... .......... .......... 54% 94.2M 1s
 93950K .......... .......... .......... .......... .......... 54%  111M 1s
 94000K .......... .......... .......... .......... .......... 54%  125M 1s
 94050K .......... .......... .......... .......... .......... 54%  132M 1s
 94100K .......... .......... .......... .......... .......... 54% 92.7M 1s
 94150K .......... .......... .......... .......... .......... 54%  150M 1s
 94200K .......... .......... .......... .......... .......... 54%  112M 1s
 94250K .......... .......... .......... .......... .......... 54% 85.1M 1s
 94300K .......... .......... .......... .......... .......... 54%  107M 1s
 94350K .......... .......... .......... .......... .......... 54%  118M 1s
 94400K .......... .......... .......... .......... .......... 54%  124M 1s
 94450K .......... .......... .......... .......... .......... 54%  109M 1s
 94500K .......... .......... .......... .......... .......... 54%  131M 1s
 94550K .......... .......... .......... .......... .......... 54% 85.2M 1s
 94600K .......... .......... .......... .......... .......... 54% 76.8M 1s
 94650K .......... .......... .......... .......... .......... 54%  104M 1s
 94700K .......... .......... .......... .......... .......... 54%  112M 1s
 94750K .......... .......... .......... .......... .......... 54%  132M 1s
 94800K .......... .......... .......... .......... .......... 54% 44.0M 1s
 94850K .......... .......... .......... .......... .......... 54%  109M 1s
 94900K .......... .......... .......... .......... .......... 54%  158M 1s
 94950K .......... .......... .......... .......... .......... 54% 89.9M 1s
 95000K .......... .......... .......... .......... .......... 54% 69.6M 1s
 95050K .......... .......... .......... .......... .......... 54% 92.4M 1s
 95100K .......... .......... .......... .......... .......... 54%  153M 1s
 95150K .......... .......... .......... .......... .......... 54%  113M 1s
 95200K .......... .......... .......... .......... .......... 54%  112M 1s
 95250K .......... .......... .......... .......... .......... 54% 99.2M 1s
 95300K .......... .......... .......... .......... .......... 54%  146M 1s
 95350K .......... .......... .......... .......... .......... 54%  105M 1s
 95400K .......... .......... .......... .......... .......... 55%  123M 1s
 95450K .......... .......... .......... .......... .......... 55% 97.4M 1s
 95500K .......... .......... .......... .......... .......... 55%  112M 1s
 95550K .......... .......... .......... .......... .......... 55% 96.5M 1s
 95600K .......... .......... .......... .......... .......... 55%  127M 1s
 95650K .......... .......... .......... .......... .......... 55%  113M 1s
 95700K .......... .......... .......... .......... .......... 55% 95.5M 1s
 95750K .......... .......... .......... .......... .......... 55%  137M 1s
 95800K .......... .......... .......... .......... .......... 55%  125M 1s
 95850K .......... .......... .......... .......... .......... 55% 94.7M 1s
 95900K .......... .......... .......... .......... .......... 55%  117M 1s
 95950K .......... .......... .......... .......... .......... 55%  101M 1s
 96000K .......... .......... .......... .......... .......... 55%  114M 1s
 96050K .......... .......... .......... .......... .......... 55% 96.4M 1s
 96100K .......... .......... .......... .......... .......... 55%  106M 1s
 96150K .......... .......... .......... .......... .......... 55%  118M 1s
 96200K .......... .......... .......... .......... .......... 55% 96.0M 1s
 96250K .......... .......... .......... .......... .......... 55%  126M 1s
 96300K .......... .......... .......... .......... .......... 55%  107M 1s
 96350K .......... .......... .......... .......... .......... 55%  159M 1s
 96400K .......... .......... .......... .......... .......... 55% 93.5M 1s
 96450K .......... .......... .......... .......... .......... 55% 89.0M 1s
 96500K .......... .......... .......... .......... .......... 55%  107M 1s
 96550K .......... .......... .......... .......... .......... 55%  108M 1s
 96600K .......... .......... .......... .......... .......... 55% 89.2M 1s
 96650K .......... .......... .......... .......... .......... 55%  138M 1s
 96700K .......... .......... .......... .......... .......... 55% 56.8M 1s
 96750K .......... .......... .......... .......... .......... 55%  121M 1s
 96800K .......... .......... .......... .......... .......... 55% 80.4M 1s
 96850K .......... .......... .......... .......... .......... 55% 93.8M 1s
 96900K .......... .......... .......... .......... .......... 55%  135M 1s
 96950K .......... .......... .......... .......... .......... 55%  108M 1s
 97000K .......... .......... .......... .......... .......... 55% 96.9M 1s
 97050K .......... .......... .......... .......... .......... 55%  160M 1s
 97100K .......... .......... .......... .......... .......... 56%  107M 1s
 97150K .......... .......... .......... .......... .......... 56% 78.3M 1s
 97200K .......... .......... .......... .......... .......... 56%  145M 1s
 97250K .......... .......... .......... .......... .......... 56%  127M 1s
 97300K .......... .......... .......... .......... .......... 56%  100M 1s
 97350K .......... .......... .......... .......... .......... 56%  129M 1s
 97400K .......... .......... .......... .......... .......... 56%  120M 1s
 97450K .......... .......... .......... .......... .......... 56%  105M 1s
 97500K .......... .......... .......... .......... .......... 56%  122M 1s
 97550K .......... .......... .......... .......... .......... 56% 76.2M 1s
 97600K .......... .......... .......... .......... .......... 56%  141M 1s
 97650K .......... .......... .......... .......... .......... 56%  141M 1s
 97700K .......... .......... .......... .......... .......... 56%  113M 1s
 97750K .......... .......... .......... .......... .......... 56% 95.2M 1s
 97800K .......... .......... .......... .......... .......... 56%  118M 1s
 97850K .......... .......... .......... .......... .......... 56% 78.3M 1s
 97900K .......... .......... .......... .......... .......... 56% 93.5M 1s
 97950K .......... .......... .......... .......... .......... 56%  126M 1s
 98000K .......... .......... .......... .......... .......... 56% 92.8M 1s
 98050K .......... .......... .......... .......... .......... 56%  127M 1s
 98100K .......... .......... .......... .......... .......... 56%  102M 1s
 98150K .......... .......... .......... .......... .......... 56%  103M 1s
 98200K .......... .......... .......... .......... .......... 56%  109M 1s
 98250K .......... .......... .......... .......... .......... 56% 75.1M 1s
 98300K .......... .......... .......... .......... .......... 56%  126M 1s
 98350K .......... .......... .......... .......... .......... 56%  145M 1s
 98400K .......... .......... .......... .......... .......... 56% 72.1M 1s
 98450K .......... .......... .......... .......... .......... 56% 87.2M 1s
 98500K .......... .......... .......... .......... .......... 56%  129M 1s
 98550K .......... .......... .......... .......... .......... 56% 80.0M 1s
 98600K .......... .......... .......... .......... .......... 56%  121M 1s
 98650K .......... .......... .......... .......... .......... 56%  103M 1s
 98700K .......... .......... .......... .......... .......... 56%  101M 1s
 98750K .......... .......... .......... .......... .......... 56%  109M 1s
 98800K .......... .......... .......... .......... .......... 56%  110M 1s
 98850K .......... .......... .......... .......... .......... 57%  120M 1s
 98900K .......... .......... .......... .......... .......... 57% 98.1M 1s
 98950K .......... .......... .......... .......... .......... 57%  112M 1s
 99000K .......... .......... .......... .......... .......... 57% 89.3M 1s
 99050K .......... .......... .......... .......... .......... 57%  101M 1s
 99100K .......... .......... .......... .......... .......... 57%  118M 1s
 99150K .......... .......... .......... .......... .......... 57%  121M 1s
 99200K .......... .......... .......... .......... .......... 57% 83.4M 1s
 99250K .......... .......... .......... .......... .......... 57% 97.2M 1s
 99300K .......... .......... .......... .......... .......... 57%  175M 1s
 99350K .......... .......... .......... .......... .......... 57%  101M 1s
 99400K .......... .......... .......... .......... .......... 57%  109M 1s
 99450K .......... .......... .......... .......... .......... 57% 72.4M 1s
 99500K .......... .......... .......... .......... .......... 57%  169M 1s
 99550K .......... .......... .......... .......... .......... 57%  102M 1s
 99600K .......... .......... .......... .......... .......... 57%  126M 1s
 99650K .......... .......... .......... .......... .......... 57%  119M 1s
 99700K .......... .......... .......... .......... .......... 57% 67.6M 1s
 99750K .......... .......... .......... .......... .......... 57%  107M 1s
 99800K .......... .......... .......... .......... .......... 57% 81.4M 1s
 99850K .......... .......... .......... .......... .......... 57%  120M 1s
 99900K .......... .......... .......... .......... .......... 57% 94.8M 1s
 99950K .......... .......... .......... .......... .......... 57%  113M 1s
100000K .......... .......... .......... .......... .......... 57% 94.0M 1s
100050K .......... .......... .......... .......... .......... 57%  127M 1s
100100K .......... .......... .......... .......... .......... 57% 84.8M 1s
100150K .......... .......... .......... .......... .......... 57%  102M 1s
100200K .......... .......... .......... .......... .......... 57%  113M 1s
100250K .......... .......... .......... .......... .......... 57%  107M 1s
100300K .......... .......... .......... .......... .......... 57% 94.8M 1s
100350K .......... .......... .......... .......... .......... 57%  123M 1s
100400K .......... .......... .......... .......... .......... 57%  125M 1s
100450K .......... .......... .......... .......... .......... 57% 73.5M 1s
100500K .......... .......... .......... .......... .......... 57% 97.2M 1s
100550K .......... .......... .......... .......... .......... 57% 85.9M 1s
100600K .......... .......... .......... .......... .......... 58%  106M 1s
100650K .......... .......... .......... .......... .......... 58%  151M 1s
100700K .......... .......... .......... .......... .......... 58%  108M 1s
100750K .......... .......... .......... .......... .......... 58%  109M 1s
100800K .......... .......... .......... .......... .......... 58% 82.7M 1s
100850K .......... .......... .......... .......... .......... 58%  120M 1s
100900K .......... .......... .......... .......... .......... 58% 92.4M 1s
100950K .......... .......... .......... .......... .......... 58%  149M 1s
101000K .......... .......... .......... .......... .......... 58% 93.3M 1s
101050K .......... .......... .......... .......... .......... 58%  120M 1s
101100K .......... .......... .......... .......... .......... 58%  112M 1s
101150K .......... .......... .......... .......... .......... 58%  108M 1s
101200K .......... .......... .......... .......... .......... 58%  106M 1s
101250K .......... .......... .......... .......... .......... 58% 75.2M 1s
101300K .......... .......... .......... .......... .......... 58%  125M 1s
101350K .......... .......... .......... .......... .......... 58%  111M 1s
101400K .......... .......... .......... .......... .......... 58%  105M 1s
101450K .......... .......... .......... .......... .......... 58%  100M 1s
101500K .......... .......... .......... .......... .......... 58% 99.1M 1s
101550K .......... .......... .......... .......... .......... 58%  107M 1s
101600K .......... .......... .......... .......... .......... 58%  116M 1s
101650K .......... .......... .......... .......... .......... 58%  116M 1s
101700K .......... .......... .......... .......... .......... 58%  104M 1s
101750K .......... .......... .......... .......... .......... 58%  147M 1s
101800K .......... .......... .......... .......... .......... 58%  117M 1s
101850K .......... .......... .......... .......... .......... 58%  134M 1s
101900K .......... .......... .......... .......... .......... 58% 80.6M 1s
101950K .......... .......... .......... .......... .......... 58% 99.5M 1s
102000K .......... .......... .......... .......... .......... 58%  141M 1s
102050K .......... .......... .......... .......... .......... 58%  110M 1s
102100K .......... .......... .......... .......... .......... 58% 82.7M 1s
102150K .......... .......... .......... .......... .......... 58%  126M 1s
102200K .......... .......... .......... .......... .......... 58% 86.7M 1s
102250K .......... .......... .......... .......... .......... 58%  116M 1s
102300K .......... .......... .......... .......... .......... 59%  105M 1s
102350K .......... .......... .......... .......... .......... 59%  130M 1s
102400K .......... .......... .......... .......... .......... 59%  126M 1s
102450K .......... .......... .......... .......... .......... 59%  112M 1s
102500K .......... .......... .......... .......... .......... 59% 89.0M 1s
102550K .......... .......... .......... .......... .......... 59% 90.1M 1s
102600K .......... .......... .......... .......... .......... 59%  117M 1s
102650K .......... .......... .......... .......... .......... 59% 84.0M 1s
102700K .......... .......... .......... .......... .......... 59%  142M 1s
102750K .......... .......... .......... .......... .......... 59%  106M 1s
102800K .......... .......... .......... .......... .......... 59%  101M 1s
102850K .......... .......... .......... .......... .......... 59% 74.2M 1s
102900K .......... .......... .......... .......... .......... 59%  102M 1s
102950K .......... .......... .......... .......... .......... 59%  131M 1s
103000K .......... .......... .......... .......... .......... 59% 99.9M 1s
103050K .......... .......... .......... .......... .......... 59% 99.1M 1s
103100K .......... .......... .......... .......... .......... 59%  105M 1s
103150K .......... .......... .......... .......... .......... 59% 99.0M 1s
103200K .......... .......... .......... .......... .......... 59%  113M 1s
103250K .......... .......... .......... .......... .......... 59%  116M 1s
103300K .......... .......... .......... .......... .......... 59%  139M 1s
103350K .......... .......... .......... .......... .......... 59% 81.7M 1s
103400K .......... .......... .......... .......... .......... 59%  124M 1s
103450K .......... .......... .......... .......... .......... 59%  114M 1s
103500K .......... .......... .......... .......... .......... 59%  103M 1s
103550K .......... .......... .......... .......... .......... 59% 93.5M 1s
103600K .......... .......... .......... .......... .......... 59%  135M 1s
103650K .......... .......... .......... .......... .......... 59%  129M 1s
103700K .......... .......... .......... .......... .......... 59%  110M 1s
103750K .......... .......... .......... .......... .......... 59% 85.7M 1s
103800K .......... .......... .......... .......... .......... 59% 95.6M 1s
103850K .......... .......... .......... .......... .......... 59%  129M 1s
103900K .......... .......... .......... .......... .......... 59% 90.2M 1s
103950K .......... .......... .......... .......... .......... 59%  121M 1s
104000K .......... .......... .......... .......... .......... 59%  112M 1s
104050K .......... .......... .......... .......... .......... 60% 94.0M 1s
104100K .......... .......... .......... .......... .......... 60%  104M 1s
104150K .......... .......... .......... .......... .......... 60%  102M 1s
104200K .......... .......... .......... .......... .......... 60% 94.7M 1s
104250K .......... .......... .......... .......... .......... 60%  135M 1s
104300K .......... .......... .......... .......... .......... 60% 93.9M 1s
104350K .......... .......... .......... .......... .......... 60%  118M 1s
104400K .......... .......... .......... .......... .......... 60% 82.2M 1s
104450K .......... .......... .......... .......... .......... 60% 85.4M 1s
104500K .......... .......... .......... .......... .......... 60%  147M 1s
104550K .......... .......... .......... .......... .......... 60% 98.3M 1s
104600K .......... .......... .......... .......... .......... 60%  129M 1s
104650K .......... .......... .......... .......... .......... 60%  108M 1s
104700K .......... .......... .......... .......... .......... 60% 86.8M 1s
104750K .......... .......... .......... .......... .......... 60%  145M 1s
104800K .......... .......... .......... .......... .......... 60% 82.1M 1s
104850K .......... .......... .......... .......... .......... 60%  100M 1s
104900K .......... .......... .......... .......... .......... 60%  115M 1s
104950K .......... .......... .......... .......... .......... 60%  152M 1s
105000K .......... .......... .......... .......... .......... 60%  108M 1s
105050K .......... .......... .......... .......... .......... 60%  105M 1s
105100K .......... .......... .......... .......... .......... 60%  116M 1s
105150K .......... .......... .......... .......... .......... 60%  105M 1s
105200K .......... .......... .......... .......... .......... 60%  119M 1s
105250K .......... .......... .......... .......... .......... 60%  135M 1s
105300K .......... .......... .......... .......... .......... 60% 94.5M 1s
105350K .......... .......... .......... .......... .......... 60%  117M 1s
105400K .......... .......... .......... .......... .......... 60%  103M 1s
105450K .......... .......... .......... .......... .......... 60%  122M 1s
105500K .......... .......... .......... .......... .......... 60%  122M 1s
105550K .......... .......... .......... .......... .......... 60% 82.5M 1s
105600K .......... .......... .......... .......... .......... 60%  112M 1s
105650K .......... .......... .......... .......... .......... 60%  121M 1s
105700K .......... .......... .......... .......... .......... 60% 86.7M 1s
105750K .......... .......... .......... .......... .......... 60%  132M 1s
105800K .......... .......... .......... .......... .......... 61%  100M 1s
105850K .......... .......... .......... .......... .......... 61%  105M 1s
105900K .......... .......... .......... .......... .......... 61% 76.8M 1s
105950K .......... .......... .......... .......... .......... 61%  156M 1s
106000K .......... .......... .......... .......... .......... 61%  119M 1s
106050K .......... .......... .......... .......... .......... 61%  119M 1s
106100K .......... .......... .......... .......... .......... 61% 87.6M 1s
106150K .......... .......... .......... .......... .......... 61% 97.1M 1s
106200K .......... .......... .......... .......... .......... 61%  161M 1s
106250K .......... .......... .......... .......... .......... 61%  103M 1s
106300K .......... .......... .......... .......... .......... 61% 76.0M 1s
106350K .......... .......... .......... .......... .......... 61%  152M 1s
106400K .......... .......... .......... .......... .......... 61%  108M 1s
106450K .......... .......... .......... .......... .......... 61%  107M 1s
106500K .......... .......... .......... .......... .......... 61%  141M 1s
106550K .......... .......... .......... .......... .......... 61% 90.3M 1s
106600K .......... .......... .......... .......... .......... 61% 92.0M 1s
106650K .......... .......... .......... .......... .......... 61%  109M 1s
106700K .......... .......... .......... .......... .......... 61% 85.3M 1s
106750K .......... .......... .......... .......... .......... 61% 86.2M 1s
106800K .......... .......... .......... .......... .......... 61%  119M 1s
106850K .......... .......... .......... .......... .......... 61% 76.4M 1s
106900K .......... .......... .......... .......... .......... 61%  121M 1s
106950K .......... .......... .......... .......... .......... 61% 96.9M 1s
107000K .......... .......... .......... .......... .......... 61% 78.0M 1s
107050K .......... .......... .......... .......... .......... 61% 82.5M 1s
107100K .......... .......... .......... .......... .......... 61% 88.0M 1s
107150K .......... .......... .......... .......... .......... 61% 95.6M 1s
107200K .......... .......... .......... .......... .......... 61% 92.5M 1s
107250K .......... .......... .......... .......... .......... 61%  129M 1s
107300K .......... .......... .......... .......... .......... 61%  135M 1s
107350K .......... .......... .......... .......... .......... 61% 79.7M 1s
107400K .......... .......... .......... .......... .......... 61%  109M 1s
107450K .......... .......... .......... .......... .......... 61%  120M 1s
107500K .......... .......... .......... .......... .......... 62%  104M 1s
107550K .......... .......... .......... .......... .......... 62%  120M 1s
107600K .......... .......... .......... .......... .......... 62% 67.7M 1s
107650K .......... .......... .......... .......... .......... 62% 82.2M 1s
107700K .......... .......... .......... .......... .......... 62% 43.1M 1s
107750K .......... .......... .......... .......... .......... 62% 58.0M 1s
107800K .......... .......... .......... .......... .......... 62% 43.9M 1s
107850K .......... .......... .......... .......... .......... 62% 51.1M 1s
107900K .......... .......... .......... .......... .......... 62% 57.9M 1s
107950K .......... .......... .......... .......... .......... 62% 59.9M 1s
108000K .......... .......... .......... .......... .......... 62% 95.5M 1s
108050K .......... .......... .......... .......... .......... 62%  109M 1s
108100K .......... .......... .......... .......... .......... 62% 93.7M 1s
108150K .......... .......... .......... .......... .......... 62% 88.9M 1s
108200K .......... .......... .......... .......... .......... 62% 88.7M 1s
108250K .......... .......... .......... .......... .......... 62%  139M 1s
108300K .......... .......... .......... .......... .......... 62%  128M 1s
108350K .......... .......... .......... .......... .......... 62% 93.5M 1s
108400K .......... .......... .......... .......... .......... 62%  144M 1s
108450K .......... .......... .......... .......... .......... 62% 49.9M 1s
108500K .......... .......... .......... .......... .......... 62%  118M 1s
108550K .......... .......... .......... .......... .......... 62%  114M 1s
108600K .......... .......... .......... .......... .......... 62% 93.3M 1s
108650K .......... .......... .......... .......... .......... 62%  121M 1s
108700K .......... .......... .......... .......... .......... 62%  112M 1s
108750K .......... .......... .......... .......... .......... 62%  101M 1s
108800K .......... .......... .......... .......... .......... 62%  169M 1s
108850K .......... .......... .......... .......... .......... 62% 90.0M 1s
108900K .......... .......... .......... .......... .......... 62% 98.9M 1s
108950K .......... .......... .......... .......... .......... 62%  139M 1s
109000K .......... .......... .......... .......... .......... 62%  131M 1s
109050K .......... .......... .......... .......... .......... 62%  104M 1s
109100K .......... .......... .......... .......... .......... 62%  125M 1s
109150K .......... .......... .......... .......... .......... 62%  127M 1s
109200K .......... .......... .......... .......... .......... 62% 86.6M 1s
109250K .......... .......... .......... .......... .......... 63%  114M 1s
109300K .......... .......... .......... .......... .......... 63%  121M 1s
109350K .......... .......... .......... .......... .......... 63%  126M 1s
109400K .......... .......... .......... .......... .......... 63% 91.1M 1s
109450K .......... .......... .......... .......... .......... 63%  124M 1s
109500K .......... .......... .......... .......... .......... 63%  107M 1s
109550K .......... .......... .......... .......... .......... 63%  108M 1s
109600K .......... .......... .......... .......... .......... 63%  104M 1s
109650K .......... .......... .......... .......... .......... 63%  123M 1s
109700K .......... .......... .......... .......... .......... 63%  138M 1s
109750K .......... .......... .......... .......... .......... 63% 88.2M 1s
109800K .......... .......... .......... .......... .......... 63%  138M 1s
109850K .......... .......... .......... .......... .......... 63%  105M 1s
109900K .......... .......... .......... .......... .......... 63% 76.2M 1s
109950K .......... .......... .......... .......... .......... 63%  117M 1s
110000K .......... .......... .......... .......... .......... 63%  121M 1s
110050K .......... .......... .......... .......... .......... 63%  155M 1s
110100K .......... .......... .......... .......... .......... 63% 94.0M 1s
110150K .......... .......... .......... .......... .......... 63%  109M 1s
110200K .......... .......... .......... .......... .......... 63%  148M 1s
110250K .......... .......... .......... .......... .......... 63%  106M 1s
110300K .......... .......... .......... .......... .......... 63% 84.5M 1s
110350K .......... .......... .......... .......... .......... 63%  110M 1s
110400K .......... .......... .......... .......... .......... 63%  134M 1s
110450K .......... .......... .......... .......... .......... 63% 77.3M 1s
110500K .......... .......... .......... .......... .......... 63%  203M 1s
110550K .......... .......... .......... .......... .......... 63%  102M 1s
110600K .......... .......... .......... .......... .......... 63% 90.7M 1s
110650K .......... .......... .......... .......... .......... 63%  117M 1s
110700K .......... .......... .......... .......... .......... 63%  105M 1s
110750K .......... .......... .......... .......... .......... 63%  126M 1s
110800K .......... .......... .......... .......... .......... 63%  107M 1s
110850K .......... .......... .......... .......... .......... 63%  120M 1s
110900K .......... .......... .......... .......... .......... 63% 97.0M 1s
110950K .......... .......... .......... .......... .......... 63%  118M 1s
111000K .......... .......... .......... .......... .......... 64% 87.6M 1s
111050K .......... .......... .......... .......... .......... 64%  160M 1s
111100K .......... .......... .......... .......... .......... 64% 89.3M 1s
111150K .......... .......... .......... .......... .......... 64%  113M 1s
111200K .......... .......... .......... .......... .......... 64%  111M 1s
111250K .......... .......... .......... .......... .......... 64%  117M 1s
111300K .......... .......... .......... .......... .......... 64% 99.8M 1s
111350K .......... .......... .......... .......... .......... 64%  131M 1s
111400K .......... .......... .......... .......... .......... 64% 89.5M 1s
111450K .......... .......... .......... .......... .......... 64% 91.5M 1s
111500K .......... .......... .......... .......... .......... 64%  129M 1s
111550K .......... .......... .......... .......... .......... 64% 97.0M 1s
111600K .......... .......... .......... .......... .......... 64% 82.5M 1s
111650K .......... .......... .......... .......... .......... 64% 93.6M 1s
111700K .......... .......... .......... .......... .......... 64%  135M 1s
111750K .......... .......... .......... .......... .......... 64% 92.4M 1s
111800K .......... .......... .......... .......... .......... 64%  119M 1s
111850K .......... .......... .......... .......... .......... 64%  104M 1s
111900K .......... .......... .......... .......... .......... 64% 76.2M 1s
111950K .......... .......... .......... .......... .......... 64% 52.9M 1s
112000K .......... .......... .......... .......... .......... 64%  145M 1s
112050K .......... .......... .......... .......... .......... 64%  112M 1s
112100K .......... .......... .......... .......... .......... 64% 86.9M 1s
112150K .......... .......... .......... .......... .......... 64% 58.0M 1s
112200K .......... .......... .......... .......... .......... 64%  129M 1s
112250K .......... .......... .......... .......... .......... 64%  109M 1s
112300K .......... .......... .......... .......... .......... 64%  113M 1s
112350K .......... .......... .......... .......... .......... 64% 99.0M 1s
112400K .......... .......... .......... .......... .......... 64%  104M 1s
112450K .......... .......... .......... .......... .......... 64% 69.7M 1s
112500K .......... .......... .......... .......... .......... 64%  122M 1s
112550K .......... .......... .......... .......... .......... 64%  125M 1s
112600K .......... .......... .......... .......... .......... 64%  117M 1s
112650K .......... .......... .......... .......... .......... 64% 91.0M 1s
112700K .......... .......... .......... .......... .......... 65% 78.3M 1s
112750K .......... .......... .......... .......... .......... 65%  129M 1s
112800K .......... .......... .......... .......... .......... 65%  111M 1s
112850K .......... .......... .......... .......... .......... 65% 82.2M 1s
112900K .......... .......... .......... .......... .......... 65% 83.0M 1s
112950K .......... .......... .......... .......... .......... 65%  110M 1s
113000K .......... .......... .......... .......... .......... 65% 81.8M 1s
113050K .......... .......... .......... .......... .......... 65%  134M 1s
113100K .......... .......... .......... .......... .......... 65%  114M 1s
113150K .......... .......... .......... .......... .......... 65%  105M 1s
113200K .......... .......... .......... .......... .......... 65% 65.5M 1s
113250K .......... .......... .......... .......... .......... 65%  108M 1s
113300K .......... .......... .......... .......... .......... 65%  115M 1s
113350K .......... .......... .......... .......... .......... 65%  122M 1s
113400K .......... .......... .......... .......... .......... 65% 87.6M 1s
113450K .......... .......... .......... .......... .......... 65%  182M 1s
113500K .......... .......... .......... .......... .......... 65%  100M 1s
113550K .......... .......... .......... .......... .......... 65% 79.9M 1s
113600K .......... .......... .......... .......... .......... 65%  125M 1s
113650K .......... .......... .......... .......... .......... 65% 82.5M 1s
113700K .......... .......... .......... .......... .......... 65%  120M 1s
113750K .......... .......... .......... .......... .......... 65%  105M 1s
113800K .......... .......... .......... .......... .......... 65%  106M 1s
113850K .......... .......... .......... .......... .......... 65%  137M 1s
113900K .......... .......... .......... .......... .......... 65%  110M 1s
113950K .......... .......... .......... .......... .......... 65% 71.7M 1s
114000K .......... .......... .......... .......... .......... 65%  164M 1s
114050K .......... .......... .......... .......... .......... 65%  121M 1s
114100K .......... .......... .......... .......... .......... 65% 88.9M 1s
114150K .......... .......... .......... .......... .......... 65%  166M 1s
114200K .......... .......... .......... .......... .......... 65% 78.7M 1s
114250K .......... .......... .......... .......... .......... 65%  158M 1s
114300K .......... .......... .......... .......... .......... 65%  102M 1s
114350K .......... .......... .......... .......... .......... 65%  107M 1s
114400K .......... .......... .......... .......... .......... 65% 95.9M 1s
114450K .......... .......... .......... .......... .......... 66%  113M 1s
114500K .......... .......... .......... .......... .......... 66% 88.6M 1s
114550K .......... .......... .......... .......... .......... 66%  168M 1s
114600K .......... .......... .......... .......... .......... 66%  111M 1s
114650K .......... .......... .......... .......... .......... 66% 93.7M 1s
114700K .......... .......... .......... .......... .......... 66% 99.8M 1s
114750K .......... .......... .......... .......... .......... 66%  113M 1s
114800K .......... .......... .......... .......... .......... 66%  105M 1s
114850K .......... .......... .......... .......... .......... 66%  108M 1s
114900K .......... .......... .......... .......... .......... 66%  165M 1s
114950K .......... .......... .......... .......... .......... 66%  108M 1s
115000K .......... .......... .......... .......... .......... 66%  102M 1s
115050K .......... .......... .......... .......... .......... 66% 97.5M 1s
115100K .......... .......... .......... .......... .......... 66%  119M 1s
115150K .......... .......... .......... .......... .......... 66%  109M 1s
115200K .......... .......... .......... .......... .......... 66%  119M 1s
115250K .......... .......... .......... .......... .......... 66%  127M 1s
115300K .......... .......... .......... .......... .......... 66% 86.2M 1s
115350K .......... .......... .......... .......... .......... 66% 93.4M 1s
115400K .......... .......... .......... .......... .......... 66% 90.8M 1s
115450K .......... .......... .......... .......... .......... 66%  112M 1s
115500K .......... .......... .......... .......... .......... 66%  121M 1s
115550K .......... .......... .......... .......... .......... 66%  121M 1s
115600K .......... .......... .......... .......... .......... 66%  102M 1s
115650K .......... .......... .......... .......... .......... 66% 54.1M 1s
115700K .......... .......... .......... .......... .......... 66%  243M 1s
115750K .......... .......... .......... .......... .......... 66% 83.4M 1s
115800K .......... .......... .......... .......... .......... 66%  121M 1s
115850K .......... .......... .......... .......... .......... 66%  109M 1s
115900K .......... .......... .......... .......... .......... 66% 88.2M 1s
115950K .......... .......... .......... .......... .......... 66% 82.4M 1s
116000K .......... .......... .......... .......... .......... 66%  102M 1s
116050K .......... .......... .......... .......... .......... 66%  123M 1s
116100K .......... .......... .......... .......... .......... 66% 60.2M 1s
116150K .......... .......... .......... .......... .......... 66%  125M 1s
116200K .......... .......... .......... .......... .......... 67% 97.2M 1s
116250K .......... .......... .......... .......... .......... 67%  103M 1s
116300K .......... .......... .......... .......... .......... 67%  132M 1s
116350K .......... .......... .......... .......... .......... 67%  112M 1s
116400K .......... .......... .......... .......... .......... 67%  140M 1s
116450K .......... .......... .......... .......... .......... 67% 93.0M 1s
116500K .......... .......... .......... .......... .......... 67% 96.1M 1s
116550K .......... .......... .......... .......... .......... 67%  135M 1s
116600K .......... .......... .......... .......... .......... 67% 78.5M 1s
116650K .......... .......... .......... .......... .......... 67%  113M 1s
116700K .......... .......... .......... .......... .......... 67% 89.3M 1s
116750K .......... .......... .......... .......... .......... 67%  111M 1s
116800K .......... .......... .......... .......... .......... 67%  109M 1s
116850K .......... .......... .......... .......... .......... 67%  116M 1s
116900K .......... .......... .......... .......... .......... 67%  109M 1s
116950K .......... .......... .......... .......... .......... 67%  104M 1s
117000K .......... .......... .......... .......... .......... 67%  115M 1s
117050K .......... .......... .......... .......... .......... 67%  101M 1s
117100K .......... .......... .......... .......... .......... 67% 92.0M 1s
117150K .......... .......... .......... .......... .......... 67% 99.2M 1s
117200K .......... .......... .......... .......... .......... 67% 99.8M 1s
117250K .......... .......... .......... .......... .......... 67%  134M 1s
117300K .......... .......... .......... .......... .......... 67%  130M 1s
117350K .......... .......... .......... .......... .......... 67%  112M 1s
117400K .......... .......... .......... .......... .......... 67% 91.7M 1s
117450K .......... .......... .......... .......... .......... 67% 97.7M 1s
117500K .......... .......... .......... .......... .......... 67%  242M 1s
117550K .......... .......... .......... .......... .......... 67% 79.5M 1s
117600K .......... .......... .......... .......... .......... 67%  103M 1s
117650K .......... .......... .......... .......... .......... 67%  134M 1s
117700K .......... .......... .......... .......... .......... 67%  113M 1s
117750K .......... .......... .......... .......... .......... 67%  110M 1s
117800K .......... .......... .......... .......... .......... 67%  130M 1s
117850K .......... .......... .......... .......... .......... 67%  110M 1s
117900K .......... .......... .......... .......... .......... 68%  101M 1s
117950K .......... .......... .......... .......... .......... 68% 90.7M 1s
118000K .......... .......... .......... .......... .......... 68%  119M 1s
118050K .......... .......... .......... .......... .......... 68%  123M 1s
118100K .......... .......... .......... .......... .......... 68%  117M 1s
118150K .......... .......... .......... .......... .......... 68%  104M 1s
118200K .......... .......... .......... .......... .......... 68%  103M 1s
118250K .......... .......... .......... .......... .......... 68% 83.1M 1s
118300K .......... .......... .......... .......... .......... 68% 89.6M 1s
118350K .......... .......... .......... .......... .......... 68% 88.3M 1s
118400K .......... .......... .......... .......... .......... 68%  146M 1s
118450K .......... .......... .......... .......... .......... 68%  141M 1s
118500K .......... .......... .......... .......... .......... 68% 94.9M 1s
118550K .......... .......... .......... .......... .......... 68%  115M 1s
118600K .......... .......... .......... .......... .......... 68% 85.5M 1s
118650K .......... .......... .......... .......... .......... 68% 88.5M 1s
118700K .......... .......... .......... .......... .......... 68%  147M 1s
118750K .......... .......... .......... .......... .......... 68% 94.8M 1s
118800K .......... .......... .......... .......... .......... 68% 98.8M 1s
118850K .......... .......... .......... .......... .......... 68%  108M 1s
118900K .......... .......... .......... .......... .......... 68%  110M 1s
118950K .......... .......... .......... .......... .......... 68%  118M 1s
119000K .......... .......... .......... .......... .......... 68%  106M 1s
119050K .......... .......... .......... .......... .......... 68% 78.0M 1s
119100K .......... .......... .......... .......... .......... 68%  107M 1s
119150K .......... .......... .......... .......... .......... 68%  135M 1s
119200K .......... .......... .......... .......... .......... 68%  124M 1s
119250K .......... .......... .......... .......... .......... 68%  117M 1s
119300K .......... .......... .......... .......... .......... 68% 97.2M 1s
119350K .......... .......... .......... .......... .......... 68% 96.9M 1s
119400K .......... .......... .......... .......... .......... 68%  109M 1s
119450K .......... .......... .......... .......... .......... 68% 80.7M 1s
119500K .......... .......... .......... .......... .......... 68%  156M 1s
119550K .......... .......... .......... .......... .......... 68%  114M 1s
119600K .......... .......... .......... .......... .......... 68%  103M 1s
119650K .......... .......... .......... .......... .......... 69%  135M 1s
119700K .......... .......... .......... .......... .......... 69%  127M 1s
119750K .......... .......... .......... .......... .......... 69% 89.2M 1s
119800K .......... .......... .......... .......... .......... 69% 86.7M 1s
119850K .......... .......... .......... .......... .......... 69% 94.0M 1s
119900K .......... .......... .......... .......... .......... 69%  102M 1s
119950K .......... .......... .......... .......... .......... 69%  142M 1s
120000K .......... .......... .......... .......... .......... 69%  140M 1s
120050K .......... .......... .......... .......... .......... 69%  129M 1s
120100K .......... .......... .......... .......... .......... 69% 84.2M 1s
120150K .......... .......... .......... .......... .......... 69% 71.0M 1s
120200K .......... .......... .......... .......... .......... 69%  241M 1s
120250K .......... .......... .......... .......... .......... 69%  212M 1s
120300K .......... .......... .......... .......... .......... 69%  190M 1s
120350K .......... .......... .......... .......... .......... 69%  198M 1s
120400K .......... .......... .......... .......... .......... 69%  277M 1s
120450K .......... .......... .......... .......... .......... 69%  249M 1s
120500K .......... .......... .......... .......... .......... 69%  204M 1s
120550K .......... .......... .......... .......... .......... 69% 61.6M 1s
120600K .......... .......... .......... .......... .......... 69%  138M 1s
120650K .......... .......... .......... .......... .......... 69%  239M 1s
120700K .......... .......... .......... .......... .......... 69% 77.9M 1s
120750K .......... .......... .......... .......... .......... 69%  148M 1s
120800K .......... .......... .......... .......... .......... 69% 87.3M 1s
120850K .......... .......... .......... .......... .......... 69% 86.0M 1s
120900K .......... .......... .......... .......... .......... 69%  254M 1s
120950K .......... .......... .......... .......... .......... 69% 80.4M 1s
121000K .......... .......... .......... .......... .......... 69%  140M 1s
121050K .......... .......... .......... .......... .......... 69% 63.0M 1s
121100K .......... .......... .......... .......... .......... 69%  337M 1s
121150K .......... .......... .......... .......... .......... 69%  183M 1s
121200K .......... .......... .......... .......... .......... 69% 68.0M 1s
121250K .......... .......... .......... .......... .......... 69%  142M 1s
121300K .......... .......... .......... .......... .......... 69% 70.5M 1s
121350K .......... .......... .......... .......... .......... 69%  308M 1s
121400K .......... .......... .......... .......... .......... 70%  169M 1s
121450K .......... .......... .......... .......... .......... 70% 75.4M 1s
121500K .......... .......... .......... .......... .......... 70%  101M 1s
121550K .......... .......... .......... .......... .......... 70%  309M 1s
121600K .......... .......... .......... .......... .......... 70% 81.2M 1s
121650K .......... .......... .......... .......... .......... 70%  127M 1s
121700K .......... .......... .......... .......... .......... 70%  123M 1s
121750K .......... .......... .......... .......... .......... 70%  142M 1s
121800K .......... .......... .......... .......... .......... 70%  250M 1s
121850K .......... .......... .......... .......... .......... 70% 50.9M 1s
121900K .......... .......... .......... .......... .......... 70%  252M 1s
121950K .......... .......... .......... .......... .......... 70%  212M 1s
122000K .......... .......... .......... .......... .......... 70%  228M 1s
122050K .......... .......... .......... .......... .......... 70%  250M 1s
122100K .......... .......... .......... .......... .......... 70%  223M 1s
122150K .......... .......... .......... .......... .......... 70%  185M 1s
122200K .......... .......... .......... .......... .......... 70%  210M 1s
122250K .......... .......... .......... .......... .......... 70%  308M 1s
122300K .......... .......... .......... .......... .......... 70% 66.6M 1s
122350K .......... .......... .......... .......... .......... 70%  119M 1s
122400K .......... .......... .......... .......... .......... 70% 87.3M 1s
122450K .......... .......... .......... .......... .......... 70%  120M 1s
122500K .......... .......... .......... .......... .......... 70%  278M 1s
122550K .......... .......... .......... .......... .......... 70% 87.0M 1s
122600K .......... .......... .......... .......... .......... 70% 94.7M 1s
122650K .......... .......... .......... .......... .......... 70% 77.7M 1s
122700K .......... .......... .......... .......... .......... 70%  133M 1s
122750K .......... .......... .......... .......... .......... 70%  268M 1s
122800K .......... .......... .......... .......... .......... 70% 46.8M 1s
122850K .......... .......... .......... .......... .......... 70%  262M 1s
122900K .......... .......... .......... .......... .......... 70%  105M 1s
122950K .......... .......... .......... .......... .......... 70%  309M 1s
123000K .......... .......... .......... .......... .......... 70%  138M 1s
123050K .......... .......... .......... .......... .......... 70% 72.1M 1s
123100K .......... .......... .......... .......... .......... 70%  211M 1s
123150K .......... .......... .......... .......... .......... 71% 72.6M 1s
123200K .......... .......... .......... .......... .......... 71%  295M 1s
123250K .......... .......... .......... .......... .......... 71% 93.8M 1s
123300K .......... .......... .......... .......... .......... 71% 81.1M 1s
123350K .......... .......... .......... .......... .......... 71%  111M 1s
123400K .......... .......... .......... .......... .......... 71%  155M 1s
123450K .......... .......... .......... .......... .......... 71%  298M 1s
123500K .......... .......... .......... .......... .......... 71%  169M 1s
123550K .......... .......... .......... .......... .......... 71% 51.0M 1s
123600K .......... .......... .......... .......... .......... 71%  297M 1s
123650K .......... .......... .......... .......... .......... 71%  240M 1s
123700K .......... .......... .......... .......... .......... 71%  248M 1s
123750K .......... .......... .......... .......... .......... 71%  262M 1s
123800K .......... .......... .......... .......... .......... 71%  246M 1s
123850K .......... .......... .......... .......... .......... 71%  237M 1s
123900K .......... .......... .......... .......... .......... 71%  263M 1s
123950K .......... .......... .......... .......... .......... 71%  180M 1s
124000K .......... .......... .......... .......... .......... 71%  267M 1s
124050K .......... .......... .......... .......... .......... 71% 67.0M 1s
124100K .......... .......... .......... .......... .......... 71%  297M 1s
124150K .......... .......... .......... .......... .......... 71% 99.0M 1s
124200K .......... .......... .......... .......... .......... 71%  105M 1s
124250K .......... .......... .......... .......... .......... 71% 50.7M 1s
124300K .......... .......... .......... .......... .......... 71%  193M 1s
124350K .......... .......... .......... .......... .......... 71%  320M 1s
124400K .......... .......... .......... .......... .......... 71%  115M 1s
124450K .......... .......... .......... .......... .......... 71% 54.9M 1s
124500K .......... .......... .......... .......... .......... 71%  242M 1s
124550K .......... .......... .......... .......... .......... 71% 50.6M 1s
124600K .......... .......... .......... .......... .......... 71%  234M 1s
124650K .......... .......... .......... .......... .......... 71%  315M 1s
124700K .......... .......... .......... .......... .......... 71% 58.1M 1s
124750K .......... .......... .......... .......... .......... 71%  310M 1s
124800K .......... .......... .......... .......... .......... 71%  321M 1s
124850K .......... .......... .......... .......... .......... 72% 78.2M 1s
124900K .......... .......... .......... .......... .......... 72%  309M 1s
124950K .......... .......... .......... .......... .......... 72% 94.3M 1s
125000K .......... .......... .......... .......... .......... 72%  149M 1s
125050K .......... .......... .......... .......... .......... 72% 61.6M 1s
125100K .......... .......... .......... .......... .......... 72%  229M 1s
125150K .......... .......... .......... .......... .......... 72%  104M 1s
125200K .......... .......... .......... .......... .......... 72%  185M 1s
125250K .......... .......... .......... .......... .......... 72%  144M 1s
125300K .......... .......... .......... .......... .......... 72% 62.0M 1s
125350K .......... .......... .......... .......... .......... 72%  183M 1s
125400K .......... .......... .......... .......... .......... 72%  190M 1s
125450K .......... .......... .......... .......... .......... 72%  260M 1s
125500K .......... .......... .......... .......... .......... 72%  194M 1s
125550K .......... .......... .......... .......... .......... 72%  233M 1s
125600K .......... .......... .......... .......... .......... 72%  180M 1s
125650K .......... .......... .......... .......... .......... 72%  209M 1s
125700K .......... .......... .......... .......... .......... 72%  216M 1s
125750K .......... .......... .......... .......... .......... 72%  243M 1s
125800K .......... .......... .......... .......... .......... 72%  298M 1s
125850K .......... .......... .......... .......... .......... 72% 81.0M 1s
125900K .......... .......... .......... .......... .......... 72%  135M 1s
125950K .......... .......... .......... .......... .......... 72% 87.5M 1s
126000K .......... .......... .......... .......... .......... 72% 54.4M 1s
126050K .......... .......... .......... .......... .......... 72%  179M 1s
126100K .......... .......... .......... .......... .......... 72%  291M 1s
126150K .......... .......... .......... .......... .......... 72%  127M 1s
126200K .......... .......... .......... .......... .......... 72% 52.0M 1s
126250K .......... .......... .......... .......... .......... 72%  225M 1s
126300K .......... .......... .......... .......... .......... 72%  249M 1s
126350K .......... .......... .......... .......... .......... 72% 47.3M 1s
126400K .......... .......... .......... .......... .......... 72%  294M 1s
126450K .......... .......... .......... .......... .......... 72% 68.4M 1s
126500K .......... .......... .......... .......... .......... 72%  217M 1s
126550K .......... .......... .......... .......... .......... 72%  296M 1s
126600K .......... .......... .......... .......... .......... 73% 87.4M 1s
126650K .......... .......... .......... .......... .......... 73%  256M 1s
126700K .......... .......... .......... .......... .......... 73%  108M 1s
126750K .......... .......... .......... .......... .......... 73%  232M 1s
126800K .......... .......... .......... .......... .......... 73%  168M 1s
126850K .......... .......... .......... .......... .......... 73% 44.7M 1s
126900K .......... .......... .......... .......... .......... 73%  218M 1s
126950K .......... .......... .......... .......... .......... 73%  235M 1s
127000K .......... .......... .......... .......... .......... 73%  243M 1s
127050K .......... .......... .......... .......... .......... 73%  282M 1s
127100K .......... .......... .......... .......... .......... 73%  180M 1s
127150K .......... .......... .......... .......... .......... 73% 68.2M 1s
127200K .......... .......... .......... .......... .......... 73%  285M 1s
127250K .......... .......... .......... .......... .......... 73%  210M 1s
127300K .......... .......... .......... .......... .......... 73%  322M 1s
127350K .......... .......... .......... .......... .......... 73%  255M 1s
127400K .......... .......... .......... .......... .......... 73%  206M 1s
127450K .......... .......... .......... .......... .......... 73%  358M 1s
127500K .......... .......... .......... .......... .......... 73% 63.4M 1s
127550K .......... .......... .......... .......... .......... 73%  352M 1s
127600K .......... .......... .......... .......... .......... 73%  326M 1s
127650K .......... .......... .......... .......... .......... 73%  365M 1s
127700K .......... .......... .......... .......... .......... 73%  115M 1s
127750K .......... .......... .......... .......... .......... 73% 69.1M 1s
127800K .......... .......... .......... .......... .......... 73%  109M 1s
127850K .......... .......... .......... .......... .......... 73% 63.0M 1s
127900K .......... .......... .......... .......... .......... 73%  255M 1s
127950K .......... .......... .......... .......... .......... 73%  315M 1s
128000K .......... .......... .......... .......... .......... 73%  105M 1s
128050K .......... .......... .......... .......... .......... 73% 63.6M 1s
128100K .......... .......... .......... .......... .......... 73%  234M 1s
128150K .......... .......... .......... .......... .......... 73%  280M 1s
128200K .......... .......... .......... .......... .......... 73% 40.7M 1s
128250K .......... .......... .......... .......... .......... 73%  354M 1s
128300K .......... .......... .......... .......... .......... 73% 78.0M 1s
128350K .......... .......... .......... .......... .......... 74%  206M 1s
128400K .......... .......... .......... .......... .......... 74%  328M 1s
128450K .......... .......... .......... .......... .......... 74% 84.2M 1s
128500K .......... .......... .......... .......... .......... 74%  209M 1s
128550K .......... .......... .......... .......... .......... 74% 74.5M 1s
128600K .......... .......... .......... .......... .......... 74%  289M 1s
128650K .......... .......... .......... .......... .......... 74%  203M 1s
128700K .......... .......... .......... .......... .......... 74% 44.5M 1s
128750K .......... .......... .......... .......... .......... 74%  214M 1s
128800K .......... .......... .......... .......... .......... 74%  219M 1s
128850K .......... .......... .......... .......... .......... 74%  255M 1s
128900K .......... .......... .......... .......... .......... 74%  334M 1s
128950K .......... .......... .......... .......... .......... 74%  205M 1s
129000K .......... .......... .......... .......... .......... 74% 70.5M 1s
129050K .......... .......... .......... .......... .......... 74%  201M 1s
129100K .......... .......... .......... .......... .......... 74%  322M 1s
129150K .......... .......... .......... .......... .......... 74%  236M 1s
129200K .......... .......... .......... .......... .......... 74%  188M 1s
129250K .......... .......... .......... .......... .......... 74% 83.4M 1s
129300K .......... .......... .......... .......... .......... 74%  156M 1s
129350K .......... .......... .......... .......... .......... 74%  164M 1s
129400K .......... .......... .......... .......... .......... 74%  286M 1s
129450K .......... .......... .......... .......... .......... 74%  321M 1s
129500K .......... .......... .......... .......... .......... 74%  125M 1s
129550K .......... .......... .......... .......... .......... 74%  257M 1s
129600K .......... .......... .......... .......... .......... 74%  164M 1s
129650K .......... .......... .......... .......... .......... 74% 61.9M 1s
129700K .......... .......... .......... .......... .......... 74% 79.8M 1s
129750K .......... .......... .......... .......... .......... 74%  257M 1s
129800K .......... .......... .......... .......... .......... 74%  301M 1s
129850K .......... .......... .......... .......... .......... 74%  106M 1s
129900K .......... .......... .......... .......... .......... 74% 60.7M 1s
129950K .......... .......... .......... .......... .......... 74%  224M 1s
130000K .......... .......... .......... .......... .......... 74% 38.2M 1s
130050K .......... .......... .......... .......... .......... 75%  310M 1s
130100K .......... .......... .......... .......... .......... 75%  364M 1s
130150K .......... .......... .......... .......... .......... 75% 90.3M 1s
130200K .......... .......... .......... .......... .......... 75%  233M 1s
130250K .......... .......... .......... .......... .......... 75%  360M 1s
130300K .......... .......... .......... .......... .......... 75% 54.2M 1s
130350K .......... .......... .......... .......... .......... 75%  355M 1s
130400K .......... .......... .......... .......... .......... 75% 86.8M 1s
130450K .......... .......... .......... .......... .......... 75%  304M 1s
130500K .......... .......... .......... .......... .......... 75%  278M 1s
130550K .......... .......... .......... .......... .......... 75% 45.4M 1s
130600K .......... .......... .......... .......... .......... 75%  214M 1s
130650K .......... .......... .......... .......... .......... 75%  273M 1s
130700K .......... .......... .......... .......... .......... 75%  296M 1s
130750K .......... .......... .......... .......... .......... 75%  370M 1s
130800K .......... .......... .......... .......... .......... 75%  131M 1s
130850K .......... .......... .......... .......... .......... 75% 77.4M 1s
130900K .......... .......... .......... .......... .......... 75%  243M 1s
130950K .......... .......... .......... .......... .......... 75%  288M 1s
131000K .......... .......... .......... .......... .......... 75%  257M 1s
131050K .......... .......... .......... .......... .......... 75%  311M 1s
131100K .......... .......... .......... .......... .......... 75%  238M 1s
131150K .......... .......... .......... .......... .......... 75%  114M 1s
131200K .......... .......... .......... .......... .......... 75%  256M 1s
131250K .......... .......... .......... .......... .......... 75% 77.3M 1s
131300K .......... .......... .......... .......... .......... 75%  307M 1s
131350K .......... .......... .......... .......... .......... 75%  324M 1s
131400K .......... .......... .......... .......... .......... 75%  176M 1s
131450K .......... .......... .......... .......... .......... 75%  242M 1s
131500K .......... .......... .......... .......... .......... 75%  145M 1s
131550K .......... .......... .......... .......... .......... 75% 62.5M 1s
131600K .......... .......... .......... .......... .......... 75% 96.6M 1s
131650K .......... .......... .......... .......... .......... 75%  192M 1s
131700K .......... .......... .......... .......... .......... 75%  299M 1s
131750K .......... .......... .......... .......... .......... 75%  120M 1s
131800K .......... .......... .......... .......... .......... 76% 47.9M 1s
131850K .......... .......... .......... .......... .......... 76%  313M 1s
131900K .......... .......... .......... .......... .......... 76%  316M 1s
131950K .......... .......... .......... .......... .......... 76% 45.0M 1s
132000K .......... .......... .......... .......... .......... 76%  207M 1s
132050K .......... .......... .......... .......... .......... 76% 98.1M 1s
132100K .......... .......... .......... .......... .......... 76%  235M 1s
132150K .......... .......... .......... .......... .......... 76%  202M 1s
132200K .......... .......... .......... .......... .......... 76% 65.5M 1s
132250K .......... .......... .......... .......... .......... 76% 75.0M 1s
132300K .......... .......... .......... .......... .......... 76%  229M 1s
132350K .......... .......... .......... .......... .......... 76%  196M 1s
132400K .......... .......... .......... .......... .......... 76%  317M 1s
132450K .......... .......... .......... .......... .......... 76% 42.7M 1s
132500K .......... .......... .......... .......... .......... 76%  150M 1s
132550K .......... .......... .......... .......... .......... 76%  305M 0s
132600K .......... .......... .......... .......... .......... 76%  312M 0s
132650K .......... .......... .......... .......... .......... 76%  205M 0s
132700K .......... .......... .......... .......... .......... 76%  259M 0s
132750K .......... .......... .......... .......... .......... 76% 87.8M 0s
132800K .......... .......... .......... .......... .......... 76%  148M 0s
132850K .......... .......... .......... .......... .......... 76%  240M 0s
132900K .......... .......... .......... .......... .......... 76%  242M 0s
132950K .......... .......... .......... .......... .......... 76%  253M 0s
133000K .......... .......... .......... .......... .......... 76%  298M 0s
133050K .......... .......... .......... .......... .......... 76%  201M 0s
133100K .......... .......... .......... .......... .......... 76%  309M 0s
133150K .......... .......... .......... .......... .......... 76% 89.0M 0s
133200K .......... .......... .......... .......... .......... 76%  298M 0s
133250K .......... .......... .......... .......... .......... 76%  113M 0s
133300K .......... .......... .......... .......... .......... 76%  256M 0s
133350K .......... .......... .......... .......... .......... 76%  185M 0s
133400K .......... .......... .......... .......... .......... 76%  228M 0s
133450K .......... .......... .......... .......... .......... 76% 63.4M 0s
133500K .......... .......... .......... .......... .......... 76%  268M 0s
133550K .......... .......... .......... .......... .......... 77%  108M 0s
133600K .......... .......... .......... .......... .......... 77%  238M 0s
133650K .......... .......... .......... .......... .......... 77%  189M 0s
133700K .......... .......... .......... .......... .......... 77% 50.2M 0s
133750K .......... .......... .......... .......... .......... 77%  199M 0s
133800K .......... .......... .......... .......... .......... 77%  260M 0s
133850K .......... .......... .......... .......... .......... 77% 42.2M 0s
133900K .......... .......... .......... .......... .......... 77%  173M 0s
133950K .......... .......... .......... .......... .......... 77%  110M 0s
134000K .......... .......... .......... .......... .......... 77%  216M 0s
134050K .......... .......... .......... .......... .......... 77%  258M 0s
134100K .......... .......... .......... .......... .......... 77% 71.8M 0s
134150K .......... .......... .......... .......... .......... 77% 67.9M 0s
134200K .......... .......... .......... .......... .......... 77%  247M 0s
134250K .......... .......... .......... .......... .......... 77%  260M 0s
134300K .......... .......... .......... .......... .......... 77%  331M 0s
134350K .......... .......... .......... .......... .......... 77% 44.9M 0s
134400K .......... .......... .......... .......... .......... 77%  215M 0s
134450K .......... .......... .......... .......... .......... 77%  233M 0s
134500K .......... .......... .......... .......... .......... 77%  307M 0s
134550K .......... .......... .......... .......... .......... 77%  381M 0s
134600K .......... .......... .......... .......... .......... 77%  147M 0s
134650K .......... .......... .......... .......... .......... 77% 63.8M 0s
134700K .......... .......... .......... .......... .......... 77%  256M 0s
134750K .......... .......... .......... .......... .......... 77%  267M 0s
134800K .......... .......... .......... .......... .......... 77%  379M 0s
134850K .......... .......... .......... .......... .......... 77%  388M 0s
134900K .......... .......... .......... .......... .......... 77%  252M 0s
134950K .......... .......... .......... .......... .......... 77%  331M 0s
135000K .......... .......... .......... .......... .......... 77%  152M 0s
135050K .......... .......... .......... .......... .......... 77% 76.4M 0s
135100K .......... .......... .......... .......... .......... 77%  261M 0s
135150K .......... .......... .......... .......... .......... 77%  385M 0s
135200K .......... .......... .......... .......... .......... 77%  384M 0s
135250K .......... .......... .......... .......... .......... 78% 88.5M 0s
135300K .......... .......... .......... .......... .......... 78%  333M 0s
135350K .......... .......... .......... .......... .......... 78%  218M 0s
135400K .......... .......... .......... .......... .......... 78% 66.9M 0s
135450K .......... .......... .......... .......... .......... 78%  316M 0s
135500K .......... .......... .......... .......... .......... 78%  102M 0s
135550K .......... .......... .......... .......... .......... 78%  343M 0s
135600K .......... .......... .......... .......... .......... 78%  165M 0s
135650K .......... .......... .......... .......... .......... 78%  303M 0s
135700K .......... .......... .......... .......... .......... 78% 42.3M 0s
135750K .......... .......... .......... .......... .......... 78%  315M 0s
135800K .......... .......... .......... .......... .......... 78% 44.5M 0s
135850K .......... .......... .......... .......... .......... 78%  150M 0s
135900K .......... .......... .......... .......... .......... 78%  347M 0s
135950K .......... .......... .......... .......... .......... 78%  120M 0s
136000K .......... .......... .......... .......... .......... 78% 88.9M 0s
136050K .......... .......... .......... .......... .......... 78%  149M 0s
136100K .......... .......... .......... .......... .......... 78%  324M 0s
136150K .......... .......... .......... .......... .......... 78% 70.7M 0s
136200K .......... .......... .......... .......... .......... 78%  130M 0s
136250K .......... .......... .......... .......... .......... 78%  260M 0s
136300K .......... .......... .......... .......... .......... 78% 49.0M 0s
136350K .......... .......... .......... .......... .......... 78%  232M 0s
136400K .......... .......... .......... .......... .......... 78%  211M 0s
136450K .......... .......... .......... .......... .......... 78%  264M 0s
136500K .......... .......... .......... .......... .......... 78%  273M 0s
136550K .......... .......... .......... .......... .......... 78%  198M 0s
136600K .......... .......... .......... .......... .......... 78%  316M 0s
136650K .......... .......... .......... .......... .......... 78% 66.2M 0s
136700K .......... .......... .......... .......... .......... 78%  195M 0s
136750K .......... .......... .......... .......... .......... 78%  275M 0s
136800K .......... .......... .......... .......... .......... 78%  335M 0s
136850K .......... .......... .......... .......... .......... 78%  330M 0s
136900K .......... .......... .......... .......... .......... 78%  136M 0s
136950K .......... .......... .......... .......... .......... 78% 59.8M 0s
137000K .......... .......... .......... .......... .......... 79%  230M 0s
137050K .......... .......... .......... .......... .......... 79%  286M 0s
137100K .......... .......... .......... .......... .......... 79%  338M 0s
137150K .......... .......... .......... .......... .......... 79%  323M 0s
137200K .......... .......... .......... .......... .......... 79%  161M 0s
137250K .......... .......... .......... .......... .......... 79%  233M 0s
137300K .......... .......... .......... .......... .......... 79%  271M 0s
137350K .......... .......... .......... .......... .......... 79%  340M 0s
137400K .......... .......... .......... .......... .......... 79% 65.1M 0s
137450K .......... .......... .......... .......... .......... 79%  235M 0s
137500K .......... .......... .......... .......... .......... 79%  222M 0s
137550K .......... .......... .......... .......... .......... 79%  313M 0s
137600K .......... .......... .......... .......... .......... 79%  123M 0s
137650K .......... .......... .......... .......... .......... 79% 41.8M 0s
137700K .......... .......... .......... .......... .......... 79%  323M 0s
137750K .......... .......... .......... .......... .......... 79%  343M 0s
137800K .......... .......... .......... .......... .......... 79% 48.7M 0s
137850K .......... .......... .......... .......... .......... 79%  144M 0s
137900K .......... .......... .......... .......... .......... 79%  139M 0s
137950K .......... .......... .......... .......... .......... 79% 83.4M 0s
138000K .......... .......... .......... .......... .......... 79%  301M 0s
138050K .......... .......... .......... .......... .......... 79%  145M 0s
138100K .......... .......... .......... .......... .......... 79% 71.2M 0s
138150K .......... .......... .......... .......... .......... 79%  198M 0s
138200K .......... .......... .......... .......... .......... 79%  233M 0s
138250K .......... .......... .......... .......... .......... 79%  285M 0s
138300K .......... .......... .......... .......... .......... 79% 44.2M 0s
138350K .......... .......... .......... .......... .......... 79%  149M 0s
138400K .......... .......... .......... .......... .......... 79%  307M 0s
138450K .......... .......... .......... .......... .......... 79%  346M 0s
138500K .......... .......... .......... .......... .......... 79%  307M 0s
138550K .......... .......... .......... .......... .......... 79%  204M 0s
138600K .......... .......... .......... .......... .......... 79% 70.1M 0s
138650K .......... .......... .......... .......... .......... 79%  217M 0s
138700K .......... .......... .......... .......... .......... 79%  175M 0s
138750K .......... .......... .......... .......... .......... 80%  278M 0s
138800K .......... .......... .......... .......... .......... 80%  320M 0s
138850K .......... .......... .......... .......... .......... 80%  319M 0s
138900K .......... .......... .......... .......... .......... 80%  205M 0s
138950K .......... .......... .......... .......... .......... 80%  337M 0s
139000K .......... .......... .......... .......... .......... 80% 76.6M 0s
139050K .......... .......... .......... .......... .......... 80%  155M 0s
139100K .......... .......... .......... .......... .......... 80%  316M 0s
139150K .......... .......... .......... .......... .......... 80%  318M 0s
139200K .......... .......... .......... .......... .......... 80%  329M 0s
139250K .......... .......... .......... .......... .......... 80%  159M 0s
139300K .......... .......... .......... .......... .......... 80%  244M 0s
139350K .......... .......... .......... .......... .......... 80%  313M 0s
139400K .......... .......... .......... .......... .......... 80%  318M 0s
139450K .......... .......... .......... .......... .......... 80% 50.7M 0s
139500K .......... .......... .......... .......... .......... 80%  154M 0s
139550K .......... .......... .......... .......... .......... 80%  313M 0s
139600K .......... .......... .......... .......... .......... 80%  194M 0s
139650K .......... .......... .......... .......... .......... 80%  317M 0s
139700K .......... .......... .......... .......... .......... 80% 41.5M 0s
139750K .......... .......... .......... .......... .......... 80%  316M 0s
139800K .......... .......... .......... .......... .......... 80% 52.2M 0s
139850K .......... .......... .......... .......... .......... 80% 80.5M 0s
139900K .......... .......... .......... .......... .......... 80%  222M 0s
139950K .......... .......... .......... .......... .......... 80%  321M 0s
140000K .......... .......... .......... .......... .......... 80% 81.2M 0s
140050K .......... .......... .......... .......... .......... 80%  241M 0s
140100K .......... .......... .......... .......... .......... 80%  275M 0s
140150K .......... .......... .......... .......... .......... 80% 83.8M 0s
140200K .......... .......... .......... .......... .......... 80%  242M 0s
140250K .......... .......... .......... .......... .......... 80%  244M 0s
140300K .......... .......... .......... .......... .......... 80% 40.7M 0s
140350K .......... .......... .......... .......... .......... 80%  281M 0s
140400K .......... .......... .......... .......... .......... 80%  123M 0s
140450K .......... .......... .......... .......... .......... 81%  359M 0s
140500K .......... .......... .......... .......... .......... 81%  283M 0s
140550K .......... .......... .......... .......... .......... 81%  258M 0s
140600K .......... .......... .......... .......... .......... 81%  362M 0s
140650K .......... .......... .......... .......... .......... 81% 76.7M 0s
140700K .......... .......... .......... .......... .......... 81%  203M 0s
140750K .......... .......... .......... .......... .......... 81%  331M 0s
140800K .......... .......... .......... .......... .......... 81%  229M 0s
140850K .......... .......... .......... .......... .......... 81%  361M 0s
140900K .......... .......... .......... .......... .......... 81%  309M 0s
140950K .......... .......... .......... .......... .......... 81%  126M 0s
141000K .......... .......... .......... .......... .......... 81%  104M 0s
141050K .......... .......... .......... .......... .......... 81%  220M 0s
141100K .......... .......... .......... .......... .......... 81%  225M 0s
141150K .......... .......... .......... .......... .......... 81%  326M 0s
141200K .......... .......... .......... .......... .......... 81%  367M 0s
141250K .......... .......... .......... .......... .......... 81%  103M 0s
141300K .......... .......... .......... .......... .......... 81%  308M 0s
141350K .......... .......... .......... .......... .......... 81%  245M 0s
141400K .......... .......... .......... .......... .......... 81%  358M 0s
141450K .......... .......... .......... .......... .......... 81% 60.8M 0s
141500K .......... .......... .......... .......... .......... 81%  322M 0s
141550K .......... .......... .......... .......... .......... 81%  126M 0s
141600K .......... .......... .......... .......... .......... 81%  309M 0s
141650K .......... .......... .......... .......... .......... 81%  232M 0s
141700K .......... .......... .......... .......... .......... 81% 31.1M 0s
141750K .......... .......... .......... .......... .......... 81%  283M 0s
141800K .......... .......... .......... .......... .......... 81%  289M 0s
141850K .......... .......... .......... .......... .......... 81% 84.1M 0s
141900K .......... .......... .......... .......... .......... 81% 78.4M 0s
141950K .......... .......... .......... .......... .......... 81%  240M 0s
142000K .......... .......... .......... .......... .......... 81%  293M 0s
142050K .......... .......... .......... .......... .......... 81% 86.1M 0s
142100K .......... .......... .......... .......... .......... 81%  200M 0s
142150K .......... .......... .......... .......... .......... 81% 63.5M 0s
142200K .......... .......... .......... .......... .......... 82%  220M 0s
142250K .......... .......... .......... .......... .......... 82%  245M 0s
142300K .......... .......... .......... .......... .......... 82%  327M 0s
142350K .......... .......... .......... .......... .......... 82% 42.4M 0s
142400K .......... .......... .......... .......... .......... 82%  182M 0s
142450K .......... .......... .......... .......... .......... 82%  213M 0s
142500K .......... .......... .......... .......... .......... 82%  334M 0s
142550K .......... .......... .......... .......... .......... 82%  291M 0s
142600K .......... .......... .......... .......... .......... 82%  239M 0s
142650K .......... .......... .......... .......... .......... 82%  105M 0s
142700K .......... .......... .......... .......... .......... 82%  170M 0s
142750K .......... .......... .......... .......... .......... 82%  296M 0s
142800K .......... .......... .......... .......... .......... 82%  187M 0s
142850K .......... .......... .......... .......... .......... 82%  265M 0s
142900K .......... .......... .......... .......... .......... 82%  300M 0s
142950K .......... .......... .......... .......... .......... 82%  329M 0s
143000K .......... .......... .......... .......... .......... 82%  340M 0s
143050K .......... .......... .......... .......... .......... 82% 49.7M 0s
143100K .......... .......... .......... .......... .......... 82%  210M 0s
143150K .......... .......... .......... .......... .......... 82%  265M 0s
143200K .......... .......... .......... .......... .......... 82%  332M 0s
143250K .......... .......... .......... .......... .......... 82%  273M 0s
143300K .......... .......... .......... .......... .......... 82%  225M 0s
143350K .......... .......... .......... .......... .......... 82%  265M 0s
143400K .......... .......... .......... .......... .......... 82%  300M 0s
143450K .......... .......... .......... .......... .......... 82%  289M 0s
143500K .......... .......... .......... .......... .......... 82%  343M 0s
143550K .......... .......... .......... .......... .......... 82%  103M 0s
143600K .......... .......... .......... .......... .......... 82%  109M 0s
143650K .......... .......... .......... .......... .......... 82%  304M 0s
143700K .......... .......... .......... .......... .......... 82%  393M 0s
143750K .......... .......... .......... .......... .......... 82%  160M 0s
143800K .......... .......... .......... .......... .......... 82% 30.5M 0s
143850K .......... .......... .......... .......... .......... 82%  349M 0s
143900K .......... .......... .......... .......... .......... 82%  387M 0s
143950K .......... .......... .......... .......... .......... 83% 85.1M 0s
144000K .......... .......... .......... .......... .......... 83% 69.0M 0s
144050K .......... .......... .......... .......... .......... 83% 77.9M 0s
144100K .......... .......... .......... .......... .......... 83%  281M 0s
144150K .......... .......... .......... .......... .......... 83%  265M 0s
144200K .......... .......... .......... .......... .......... 83%  389M 0s
144250K .......... .......... .......... .......... .......... 83% 45.2M 0s
144300K .......... .......... .......... .......... .......... 83%  346M 0s
144350K .......... .......... .......... .......... .......... 83%  351M 0s
144400K .......... .......... .......... .......... .......... 83%  399M 0s
144450K .......... .......... .......... .......... .......... 83% 51.8M 0s
144500K .......... .......... .......... .......... .......... 83%  143M 0s
144550K .......... .......... .......... .......... .......... 83%  198M 0s
144600K .......... .......... .......... .......... .......... 83%  394M 0s
144650K .......... .......... .......... .......... .......... 83%  395M 0s
144700K .......... .......... .......... .......... .......... 83%  241M 0s
144750K .......... .......... .......... .......... .......... 83% 80.7M 0s
144800K .......... .......... .......... .......... .......... 83%  307M 0s
144850K .......... .......... .......... .......... .......... 83%  267M 0s
144900K .......... .......... .......... .......... .......... 83%  225M 0s
144950K .......... .......... .......... .......... .......... 83%  278M 0s
145000K .......... .......... .......... .......... .......... 83%  337M 0s
145050K .......... .......... .......... .......... .......... 83%  385M 0s
145100K .......... .......... .......... .......... .......... 83%  363M 0s
145150K .......... .......... .......... .......... .......... 83% 65.7M 0s
145200K .......... .......... .......... .......... .......... 83%  128M 0s
145250K .......... .......... .......... .......... .......... 83%  186M 0s
145300K .......... .......... .......... .......... .......... 83%  289M 0s
145350K .......... .......... .......... .......... .......... 83%  334M 0s
145400K .......... .......... .......... .......... .......... 83%  215M 0s
145450K .......... .......... .......... .......... .......... 83%  254M 0s
145500K .......... .......... .......... .......... .......... 83%  220M 0s
145550K .......... .......... .......... .......... .......... 83%  312M 0s
145600K .......... .......... .......... .......... .......... 83%  279M 0s
145650K .......... .......... .......... .......... .......... 83% 94.7M 0s
145700K .......... .......... .......... .......... .......... 84% 80.0M 0s
145750K .......... .......... .......... .......... .......... 84%  174M 0s
145800K .......... .......... .......... .......... .......... 84%  285M 0s
145850K .......... .......... .......... .......... .......... 84% 89.9M 0s
145900K .......... .......... .......... .......... .......... 84% 48.7M 0s
145950K .......... .......... .......... .......... .......... 84%  211M 0s
146000K .......... .......... .......... .......... .......... 84%  276M 0s
146050K .......... .......... .......... .......... .......... 84%  127M 0s
146100K .......... .......... .......... .......... .......... 84% 73.9M 0s
146150K .......... .......... .......... .......... .......... 84% 77.9M 0s
146200K .......... .......... .......... .......... .......... 84%  216M 0s
146250K .......... .......... .......... .......... .......... 84%  277M 0s
146300K .......... .......... .......... .......... .......... 84%  261M 0s
146350K .......... .......... .......... .......... .......... 84% 38.5M 0s
146400K .......... .......... .......... .......... .......... 84%  318M 0s
146450K .......... .......... .......... .......... .......... 84%  333M 0s
146500K .......... .......... .......... .......... .......... 84%  326M 0s
146550K .......... .......... .......... .......... .......... 84% 72.4M 0s
146600K .......... .......... .......... .......... .......... 84%  144M 0s
146650K .......... .......... .......... .......... .......... 84%  219M 0s
146700K .......... .......... .......... .......... .......... 84%  290M 0s
146750K .......... .......... .......... .......... .......... 84%  326M 0s
146800K .......... .......... .......... .......... .......... 84%  279M 0s
146850K .......... .......... .......... .......... .......... 84% 86.7M 0s
146900K .......... .......... .......... .......... .......... 84%  193M 0s
146950K .......... .......... .......... .......... .......... 84%  271M 0s
147000K .......... .......... .......... .......... .......... 84%  188M 0s
147050K .......... .......... .......... .......... .......... 84%  271M 0s
147100K .......... .......... .......... .......... .......... 84%  315M 0s
147150K .......... .......... .......... .......... .......... 84%  360M 0s
147200K .......... .......... .......... .......... .......... 84%  364M 0s
147250K .......... .......... .......... .......... .......... 84%  323M 0s
147300K .......... .......... .......... .......... .......... 84% 72.2M 0s
147350K .......... .......... .......... .......... .......... 84%  202M 0s
147400K .......... .......... .......... .......... .......... 85%  233M 0s
147450K .......... .......... .......... .......... .......... 85%  301M 0s
147500K .......... .......... .......... .......... .......... 85%  358M 0s
147550K .......... .......... .......... .......... .......... 85%  313M 0s
147600K .......... .......... .......... .......... .......... 85%  270M 0s
147650K .......... .......... .......... .......... .......... 85%  222M 0s
147700K .......... .......... .......... .......... .......... 85%  361M 0s
147750K .......... .......... .......... .......... .......... 85%  163M 0s
147800K .......... .......... .......... .......... .......... 85%  112M 0s
147850K .......... .......... .......... .......... .......... 85% 31.2M 0s
147900K .......... .......... .......... .......... .......... 85%  246M 0s
147950K .......... .......... .......... .......... .......... 85%  344M 0s
148000K .......... .......... .......... .......... .......... 85% 55.2M 0s
148050K .......... .......... .......... .......... .......... 85%  254M 0s
148100K .......... .......... .......... .......... .......... 85%  346M 0s
148150K .......... .......... .......... .......... .......... 85%  319M 0s
148200K .......... .......... .......... .......... .......... 85%  161M 0s
148250K .......... .......... .......... .......... .......... 85% 93.3M 0s
148300K .......... .......... .......... .......... .......... 85% 55.8M 0s
148350K .......... .......... .......... .......... .......... 85%  280M 0s
148400K .......... .......... .......... .......... .......... 85%  359M 0s
148450K .......... .......... .......... .......... .......... 85% 42.8M 0s
148500K .......... .......... .......... .......... .......... 85%  102M 0s
148550K .......... .......... .......... .......... .......... 85%  186M 0s
148600K .......... .......... .......... .......... .......... 85%  331M 0s
148650K .......... .......... .......... .......... .......... 85%  343M 0s
148700K .......... .......... .......... .......... .......... 85% 88.9M 0s
148750K .......... .......... .......... .......... .......... 85%  177M 0s
148800K .......... .......... .......... .......... .......... 85%  298M 0s
148850K .......... .......... .......... .......... .......... 85%  324M 0s
148900K .......... .......... .......... .......... .......... 85%  290M 0s
148950K .......... .......... .......... .......... .......... 85% 90.9M 0s
149000K .......... .......... .......... .......... .......... 85%  213M 0s
149050K .......... .......... .......... .......... .......... 85%  200M 0s
149100K .......... .......... .......... .......... .......... 85%  287M 0s
149150K .......... .......... .......... .......... .......... 86%  239M 0s
149200K .......... .......... .......... .......... .......... 86%  195M 0s
149250K .......... .......... .......... .......... .......... 86%  239M 0s
149300K .......... .......... .......... .......... .......... 86%  321M 0s
149350K .......... .......... .......... .......... .......... 86%  322M 0s
149400K .......... .......... .......... .......... .......... 86%  345M 0s
149450K .......... .......... .......... .......... .......... 86%  108M 0s
149500K .......... .......... .......... .......... .......... 86%  195M 0s
149550K .......... .......... .......... .......... .......... 86%  206M 0s
149600K .......... .......... .......... .......... .......... 86%  306M 0s
149650K .......... .......... .......... .......... .......... 86%  262M 0s
149700K .......... .......... .......... .......... .......... 86%  261M 0s
149750K .......... .......... .......... .......... .......... 86%  268M 0s
149800K .......... .......... .......... .......... .......... 86%  243M 0s
149850K .......... .......... .......... .......... .......... 86%  330M 0s
149900K .......... .......... .......... .......... .......... 86%  243M 0s
149950K .......... .......... .......... .......... .......... 86%  341M 0s
150000K .......... .......... .......... .......... .......... 86%  136M 0s
150050K .......... .......... .......... .......... .......... 86% 31.3M 0s
150100K .......... .......... .......... .......... .......... 86%  355M 0s
150150K .......... .......... .......... .......... .......... 86% 49.8M 0s
150200K .......... .......... .......... .......... .......... 86%  123M 0s
150250K .......... .......... .......... .......... .......... 86%  368M 0s
150300K .......... .......... .......... .......... .......... 86%  330M 0s
150350K .......... .......... .......... .......... .......... 86%  247M 0s
150400K .......... .......... .......... .......... .......... 86%  114M 0s
150450K .......... .......... .......... .......... .......... 86%  334M 0s
150500K .......... .......... .......... .......... .......... 86% 56.3M 0s
150550K .......... .......... .......... .......... .......... 86%  327M 0s
150600K .......... .......... .......... .......... .......... 86% 45.3M 0s
150650K .......... .......... .......... .......... .......... 86%  300M 0s
150700K .......... .......... .......... .......... .......... 86% 78.8M 0s
150750K .......... .......... .......... .......... .......... 86%  365M 0s
150800K .......... .......... .......... .......... .......... 86% 78.4M 0s
150850K .......... .......... .......... .......... .......... 86%  261M 0s
150900K .......... .......... .......... .......... .......... 87%  228M 0s
150950K .......... .......... .......... .......... .......... 87%  239M 0s
151000K .......... .......... .......... .......... .......... 87%  380M 0s
151050K .......... .......... .......... .......... .......... 87%  370M 0s
151100K .......... .......... .......... .......... .......... 87% 97.8M 0s
151150K .......... .......... .......... .......... .......... 87%  287M 0s
151200K .......... .......... .......... .......... .......... 87%  243M 0s
151250K .......... .......... .......... .......... .......... 87%  232M 0s
151300K .......... .......... .......... .......... .......... 87%  288M 0s
151350K .......... .......... .......... .......... .......... 87%  297M 0s
151400K .......... .......... .......... .......... .......... 87%  197M 0s
151450K .......... .......... .......... .......... .......... 87%  224M 0s
151500K .......... .......... .......... .......... .......... 87%  374M 0s
151550K .......... .......... .......... .......... .......... 87%  371M 0s
151600K .......... .......... .......... .......... .......... 87%  364M 0s
151650K .......... .......... .......... .......... .......... 87%  338M 0s
151700K .......... .......... .......... .......... .......... 87% 96.1M 0s
151750K .......... .......... .......... .......... .......... 87%  210M 0s
151800K .......... .......... .......... .......... .......... 87%  160M 0s
151850K .......... .......... .......... .......... .......... 87%  258M 0s
151900K .......... .......... .......... .......... .......... 87%  352M 0s
151950K .......... .......... .......... .......... .......... 87%  357M 0s
152000K .......... .......... .......... .......... .......... 87%  311M 0s
152050K .......... .......... .......... .......... .......... 87%  173M 0s
152100K .......... .......... .......... .......... .......... 87%  306M 0s
152150K .......... .......... .......... .......... .......... 87%  103M 0s
152200K .......... .......... .......... .......... .......... 87%  307M 0s
152250K .......... .......... .......... .......... .......... 87% 35.6M 0s
152300K .......... .......... .......... .......... .......... 87% 48.8M 0s
152350K .......... .......... .......... .......... .......... 87%  250M 0s
152400K .......... .......... .......... .......... .......... 87%  130M 0s
152450K .......... .......... .......... .......... .......... 87%  205M 0s
152500K .......... .......... .......... .......... .......... 87%  317M 0s
152550K .......... .......... .......... .......... .......... 87%  240M 0s
152600K .......... .......... .......... .......... .......... 88%  340M 0s
152650K .......... .......... .......... .......... .......... 88%  162M 0s
152700K .......... .......... .......... .......... .......... 88% 63.3M 0s
152750K .......... .......... .......... .......... .......... 88%  231M 0s
152800K .......... .......... .......... .......... .......... 88%  346M 0s
152850K .......... .......... .......... .......... .......... 88% 43.3M 0s
152900K .......... .......... .......... .......... .......... 88% 68.4M 0s
152950K .......... .......... .......... .......... .......... 88%  338M 0s
153000K .......... .......... .......... .......... .......... 88%  109M 0s
153050K .......... .......... .......... .......... .......... 88%  275M 0s
153100K .......... .......... .......... .......... .......... 88%  239M 0s
153150K .......... .......... .......... .......... .......... 88%  224M 0s
153200K .......... .......... .......... .......... .......... 88%  231M 0s
153250K .......... .......... .......... .......... .......... 88%  304M 0s
153300K .......... .......... .......... .......... .......... 88%  342M 0s
153350K .......... .......... .......... .......... .......... 88% 97.6M 0s
153400K .......... .......... .......... .......... .......... 88%  201M 0s
153450K .......... .......... .......... .......... .......... 88%  271M 0s
153500K .......... .......... .......... .......... .......... 88%  220M 0s
153550K .......... .......... .......... .......... .......... 88%  297M 0s
153600K .......... .......... .......... .......... .......... 88%  163M 0s
153650K .......... .......... .......... .......... .......... 88%  258M 0s
153700K .......... .......... .......... .......... .......... 88%  338M 0s
153750K .......... .......... .......... .......... .......... 88%  346M 0s
153800K .......... .......... .......... .......... .......... 88%  303M 0s
153850K .......... .......... .......... .......... .......... 88%  329M 0s
153900K .......... .......... .......... .......... .......... 88%  140M 0s
153950K .......... .......... .......... .......... .......... 88%  219M 0s
154000K .......... .......... .......... .......... .......... 88%  202M 0s
154050K .......... .......... .......... .......... .......... 88%  337M 0s
154100K .......... .......... .......... .......... .......... 88%  176M 0s
154150K .......... .......... .......... .......... .......... 88%  309M 0s
154200K .......... .......... .......... .......... .......... 88%  302M 0s
154250K .......... .......... .......... .......... .......... 88%  357M 0s
154300K .......... .......... .......... .......... .......... 88% 82.9M 0s
154350K .......... .......... .......... .......... .......... 89%  183M 0s
154400K .......... .......... .......... .......... .......... 89%  333M 0s
154450K .......... .......... .......... .......... .......... 89%  343M 0s
154500K .......... .......... .......... .......... .......... 89% 38.4M 0s
154550K .......... .......... .......... .......... .......... 89% 46.4M 0s
154600K .......... .......... .......... .......... .......... 89%  133M 0s
154650K .......... .......... .......... .......... .......... 89%  216M 0s
154700K .......... .......... .......... .......... .......... 89%  338M 0s
154750K .......... .......... .......... .......... .......... 89%  235M 0s
154800K .......... .......... .......... .......... .......... 89%  331M 0s
154850K .......... .......... .......... .......... .......... 89%  206M 0s
154900K .......... .......... .......... .......... .......... 89%  299M 0s
154950K .......... .......... .......... .......... .......... 89% 47.6M 0s
155000K .......... .......... .......... .......... .......... 89%  334M 0s
155050K .......... .......... .......... .......... .......... 89% 50.5M 0s
155100K .......... .......... .......... .......... .......... 89% 70.0M 0s
155150K .......... .......... .......... .......... .......... 89%  329M 0s
155200K .......... .......... .......... .......... .......... 89%  363M 0s
155250K .......... .......... .......... .......... .......... 89%  136M 0s
155300K .......... .......... .......... .......... .......... 89%  170M 0s
155350K .......... .......... .......... .......... .......... 89%  304M 0s
155400K .......... .......... .......... .......... .......... 89%  225M 0s
155450K .......... .......... .......... .......... .......... 89%  225M 0s
155500K .......... .......... .......... .......... .......... 89%  372M 0s
155550K .......... .......... .......... .......... .......... 89% 81.3M 0s
155600K .......... .......... .......... .......... .......... 89%  259M 0s
155650K .......... .......... .......... .......... .......... 89%  255M 0s
155700K .......... .......... .......... .......... .......... 89%  268M 0s
155750K .......... .......... .......... .......... .......... 89%  222M 0s
155800K .......... .......... .......... .......... .......... 89%  260M 0s
155850K .......... .......... .......... .......... .......... 89%  244M 0s
155900K .......... .......... .......... .......... .......... 89%  229M 0s
155950K .......... .......... .......... .......... .......... 89%  369M 0s
156000K .......... .......... .......... .......... .......... 89%  329M 0s
156050K .......... .......... .......... .......... .......... 89%  367M 0s
156100K .......... .......... .......... .......... .......... 90%  378M 0s
156150K .......... .......... .......... .......... .......... 90%  364M 0s
156200K .......... .......... .......... .......... .......... 90%  165M 0s
156250K .......... .......... .......... .......... .......... 90%  142M 0s
156300K .......... .......... .......... .......... .......... 90%  283M 0s
156350K .......... .......... .......... .......... .......... 90%  209M 0s
156400K .......... .......... .......... .......... .......... 90%  239M 0s
156450K .......... .......... .......... .......... .......... 90%  371M 0s
156500K .......... .......... .......... .......... .......... 90%  372M 0s
156550K .......... .......... .......... .......... .......... 90% 77.0M 0s
156600K .......... .......... .......... .......... .......... 90%  318M 0s
156650K .......... .......... .......... .......... .......... 90%  232M 0s
156700K .......... .......... .......... .......... .......... 90%  370M 0s
156750K .......... .......... .......... .......... .......... 90% 48.3M 0s
156800K .......... .......... .......... .......... .......... 90% 43.6M 0s
156850K .......... .......... .......... .......... .......... 90%  161M 0s
156900K .......... .......... .......... .......... .......... 90%  139M 0s
156950K .......... .......... .......... .......... .......... 90%  203M 0s
157000K .......... .......... .......... .......... .......... 90%  315M 0s
157050K .......... .......... .......... .......... .......... 90%  275M 0s
157100K .......... .......... .......... .......... .......... 90%  313M 0s
157150K .......... .......... .......... .......... .......... 90%  182M 0s
157200K .......... .......... .......... .......... .......... 90% 50.5M 0s
157250K .......... .......... .......... .......... .......... 90%  254M 0s
157300K .......... .......... .......... .......... .......... 90%  302M 0s
157350K .......... .......... .......... .......... .......... 90% 41.5M 0s
157400K .......... .......... .......... .......... .......... 90% 60.7M 0s
157450K .......... .......... .......... .......... .......... 90%  242M 0s
157500K .......... .......... .......... .......... .......... 90%  249M 0s
157550K .......... .......... .......... .......... .......... 90%  248M 0s
157600K .......... .......... .......... .......... .......... 90%  222M 0s
157650K .......... .......... .......... .......... .......... 90%  204M 0s
157700K .......... .......... .......... .......... .......... 90%  347M 0s
157750K .......... .......... .......... .......... .......... 90%  345M 0s
157800K .......... .......... .......... .......... .......... 91%  356M 0s
157850K .......... .......... .......... .......... .......... 91% 57.6M 0s
157900K .......... .......... .......... .......... .......... 91%  180M 0s
157950K .......... .......... .......... .......... .......... 91%  288M 0s
158000K .......... .......... .......... .......... .......... 91%  189M 0s
158050K .......... .......... .......... .......... .......... 91%  198M 0s
158100K .......... .......... .......... .......... .......... 91%  341M 0s
158150K .......... .......... .......... .......... .......... 91%  339M 0s
158200K .......... .......... .......... .......... .......... 91%  313M 0s
158250K .......... .......... .......... .......... .......... 91%  348M 0s
158300K .......... .......... .......... .......... .......... 91%  350M 0s
158350K .......... .......... .......... .......... .......... 91%  281M 0s
158400K .......... .......... .......... .......... .......... 91%  262M 0s
158450K .......... .......... .......... .......... .......... 91%  347M 0s
158500K .......... .......... .......... .......... .......... 91%  244M 0s
158550K .......... .......... .......... .......... .......... 91%  190M 0s
158600K .......... .......... .......... .......... .......... 91%  203M 0s
158650K .......... .......... .......... .......... .......... 91%  307M 0s
158700K .......... .......... .......... .......... .......... 91%  345M 0s
158750K .......... .......... .......... .......... .......... 91%  300M 0s
158800K .......... .......... .......... .......... .......... 91%  354M 0s
158850K .......... .......... .......... .......... .......... 91%  139M 0s
158900K .......... .......... .......... .......... .......... 91%  180M 0s
158950K .......... .......... .......... .......... .......... 91%  342M 0s
159000K .......... .......... .......... .......... .......... 91%  425M 0s
159050K .......... .......... .......... .......... .......... 91% 33.0M 0s
159100K .......... .......... .......... .......... .......... 91% 64.6M 0s
159150K .......... .......... .......... .......... .......... 91%  122M 0s
159200K .......... .......... .......... .......... .......... 91%  239M 0s
159250K .......... .......... .......... .......... .......... 91%  263M 0s
159300K .......... .......... .......... .......... .......... 91%  241M 0s
159350K .......... .......... .......... .......... .......... 91%  439M 0s
159400K .......... .......... .......... .......... .......... 91%  413M 0s
159450K .......... .......... .......... .......... .......... 91%  173M 0s
159500K .......... .......... .......... .......... .......... 91% 51.0M 0s
159550K .......... .......... .......... .......... .......... 92%  312M 0s
159600K .......... .......... .......... .......... .......... 92%  390M 0s
159650K .......... .......... .......... .......... .......... 92% 39.2M 0s
159700K .......... .......... .......... .......... .......... 92% 86.6M 0s
159750K .......... .......... .......... .......... .......... 92%  121M 0s
159800K .......... .......... .......... .......... .......... 92%  238M 0s
159850K .......... .......... .......... .......... .......... 92%  306M 0s
159900K .......... .......... .......... .......... .......... 92%  320M 0s
159950K .......... .......... .......... .......... .......... 92%  418M 0s
160000K .......... .......... .......... .......... .......... 92%  251M 0s
160050K .......... .......... .......... .......... .......... 92%  420M 0s
160100K .......... .......... .......... .......... .......... 92%  432M 0s
160150K .......... .......... .......... .......... .......... 92% 66.9M 0s
160200K .......... .......... .......... .......... .......... 92%  155M 0s
160250K .......... .......... .......... .......... .......... 92%  288M 0s
160300K .......... .......... .......... .......... .......... 92%  385M 0s
160350K .......... .......... .......... .......... .......... 92%  243M 0s
160400K .......... .......... .......... .......... .......... 92%  291M 0s
160450K .......... .......... .......... .......... .......... 92%  285M 0s
160500K .......... .......... .......... .......... .......... 92%  176M 0s
160550K .......... .......... .......... .......... .......... 92%  280M 0s
160600K .......... .......... .......... .......... .......... 92%  340M 0s
160650K .......... .......... .......... .......... .......... 92%  338M 0s
160700K .......... .......... .......... .......... .......... 92%  341M 0s
160750K .......... .......... .......... .......... .......... 92%  301M 0s
160800K .......... .......... .......... .......... .......... 92%  223M 0s
160850K .......... .......... .......... .......... .......... 92%  305M 0s
160900K .......... .......... .......... .......... .......... 92%  208M 0s
160950K .......... .......... .......... .......... .......... 92%  233M 0s
161000K .......... .......... .......... .......... .......... 92%  303M 0s
161050K .......... .......... .......... .......... .......... 92%  209M 0s
161100K .......... .......... .......... .......... .......... 92%  302M 0s
161150K .......... .......... .......... .......... .......... 92%  332M 0s
161200K .......... .......... .......... .......... .......... 92% 71.0M 0s
161250K .......... .......... .......... .......... .......... 92%  287M 0s
161300K .......... .......... .......... .......... .......... 93%  287M 0s
161350K .......... .......... .......... .......... .......... 93%  337M 0s
161400K .......... .......... .......... .......... .......... 93% 38.0M 0s
161450K .......... .......... .......... .......... .......... 93% 64.9M 0s
161500K .......... .......... .......... .......... .......... 93%  297M 0s
161550K .......... .......... .......... .......... .......... 93%  114M 0s
161600K .......... .......... .......... .......... .......... 93%  182M 0s
161650K .......... .......... .......... .......... .......... 93%  211M 0s
161700K .......... .......... .......... .......... .......... 93%  342M 0s
161750K .......... .......... .......... .......... .......... 93%  344M 0s
161800K .......... .......... .......... .......... .......... 93%  185M 0s
161850K .......... .......... .......... .......... .......... 93% 53.8M 0s
161900K .......... .......... .......... .......... .......... 93%  258M 0s
161950K .......... .......... .......... .......... .......... 93%  334M 0s
162000K .......... .......... .......... .......... .......... 93% 41.4M 0s
162050K .......... .......... .......... .......... .......... 93% 68.7M 0s
162100K .......... .......... .......... .......... .......... 93%  113M 0s
162150K .......... .......... .......... .......... .......... 93%  230M 0s
162200K .......... .......... .......... .......... .......... 93%  315M 0s
162250K .......... .......... .......... .......... .......... 93%  200M 0s
162300K .......... .......... .......... .......... .......... 93%  313M 0s
162350K .......... .......... .......... .......... .......... 93%  335M 0s
162400K .......... .......... .......... .......... .......... 93%  303M 0s
162450K .......... .......... .......... .......... .......... 93%  344M 0s
162500K .......... .......... .......... .......... .......... 93%  132M 0s
162550K .......... .......... .......... .......... .......... 93%  142M 0s
162600K .......... .......... .......... .......... .......... 93%  236M 0s
162650K .......... .......... .......... .......... .......... 93%  218M 0s
162700K .......... .......... .......... .......... .......... 93%  251M 0s
162750K .......... .......... .......... .......... .......... 93%  292M 0s
162800K .......... .......... .......... .......... .......... 93%  194M 0s
162850K .......... .......... .......... .......... .......... 93%  341M 0s
162900K .......... .......... .......... .......... .......... 93%  349M 0s
162950K .......... .......... .......... .......... .......... 93%  306M 0s
163000K .......... .......... .......... .......... .......... 94%  346M 0s
163050K .......... .......... .......... .......... .......... 94%  351M 0s
163100K .......... .......... .......... .......... .......... 94%  302M 0s
163150K .......... .......... .......... .......... .......... 94%  264M 0s
163200K .......... .......... .......... .......... .......... 94%  314M 0s
163250K .......... .......... .......... .......... .......... 94%  169M 0s
163300K .......... .......... .......... .......... .......... 94%  303M 0s
163350K .......... .......... .......... .......... .......... 94%  334M 0s
163400K .......... .......... .......... .......... .......... 94%  356M 0s
163450K .......... .......... .......... .......... .......... 94% 52.7M 0s
163500K .......... .......... .......... .......... .......... 94%  248M 0s
163550K .......... .......... .......... .......... .......... 94%  211M 0s
163600K .......... .......... .......... .......... .......... 94%  311M 0s
163650K .......... .......... .......... .......... .......... 94%  315M 0s
163700K .......... .......... .......... .......... .......... 94%  341M 0s
163750K .......... .......... .......... .......... .......... 94% 45.9M 0s
163800K .......... .......... .......... .......... .......... 94%  319M 0s
163850K .......... .......... .......... .......... .......... 94% 80.1M 0s
163900K .......... .......... .......... .......... .......... 94% 95.8M 0s
163950K .......... .......... .......... .......... .......... 94%  153M 0s
164000K .......... .......... .......... .......... .......... 94%  216M 0s
164050K .......... .......... .......... .......... .......... 94%  253M 0s
164100K .......... .......... .......... .......... .......... 94%  281M 0s
164150K .......... .......... .......... .......... .......... 94%  273M 0s
164200K .......... .......... .......... .......... .......... 94% 68.4M 0s
164250K .......... .......... .......... .......... .......... 94%  303M 0s
164300K .......... .......... .......... .......... .......... 94%  389M 0s
164350K .......... .......... .......... .......... .......... 94% 27.8M 0s
164400K .......... .......... .......... .......... .......... 94%  343M 0s
164450K .......... .......... .......... .......... .......... 94% 66.7M 0s
164500K .......... .......... .......... .......... .......... 94%  337M 0s
164550K .......... .......... .......... .......... .......... 94%  329M 0s
164600K .......... .......... .......... .......... .......... 94%  328M 0s
164650K .......... .......... .......... .......... .......... 94%  429M 0s
164700K .......... .......... .......... .......... .......... 94%  417M 0s
164750K .......... .......... .......... .......... .......... 95% 98.0M 0s
164800K .......... .......... .......... .......... .......... 95%  274M 0s
164850K .......... .......... .......... .......... .......... 95%  425M 0s
164900K .......... .......... .......... .......... .......... 95%  244M 0s
164950K .......... .......... .......... .......... .......... 95%  314M 0s
165000K .......... .......... .......... .......... .......... 95%  142M 0s
165050K .......... .......... .......... .......... .......... 95%  218M 0s
165100K .......... .......... .......... .......... .......... 95%  282M 0s
165150K .......... .......... .......... .......... .......... 95%  304M 0s
165200K .......... .......... .......... .......... .......... 95%  373M 0s
165250K .......... .......... .......... .......... .......... 95%  265M 0s
165300K .......... .......... .......... .......... .......... 95%  361M 0s
165350K .......... .......... .......... .......... .......... 95%  424M 0s
165400K .......... .......... .......... .......... .......... 95%  424M 0s
165450K .......... .......... .......... .......... .......... 95%  412M 0s
165500K .......... .......... .......... .......... .......... 95%  395M 0s
165550K .......... .......... .......... .......... .......... 95%  425M 0s
165600K .......... .......... .......... .......... .......... 95%  130M 0s
165650K .......... .......... .......... .......... .......... 95%  156M 0s
165700K .......... .......... .......... .......... .......... 95%  293M 0s
165750K .......... .......... .......... .......... .......... 95%  422M 0s
165800K .......... .......... .......... .......... .......... 95%  435M 0s
165850K .......... .......... .......... .......... .......... 95% 46.0M 0s
165900K .......... .......... .......... .......... .......... 95%  298M 0s
165950K .......... .......... .......... .......... .......... 95%  314M 0s
166000K .......... .......... .......... .......... .......... 95%  253M 0s
166050K .......... .......... .......... .......... .......... 95%  425M 0s
166100K .......... .......... .......... .......... .......... 95%  438M 0s
166150K .......... .......... .......... .......... .......... 95% 88.9M 0s
166200K .......... .......... .......... .......... .......... 95% 88.1M 0s
166250K .......... .......... .......... .......... .......... 95% 79.9M 0s
166300K .......... .......... .......... .......... .......... 95%  120M 0s
166350K .......... .......... .......... .......... .......... 95%  125M 0s
166400K .......... .......... .......... .......... .......... 95%  211M 0s
166450K .......... .......... .......... .......... .......... 95%  246M 0s
166500K .......... .......... .......... .......... .......... 96%  378M 0s
166550K .......... .......... .......... .......... .......... 96%  246M 0s
166600K .......... .......... .......... .......... .......... 96% 68.8M 0s
166650K .......... .......... .......... .......... .......... 96%  292M 0s
166700K .......... .......... .......... .......... .......... 96%  268M 0s
166750K .......... .......... .......... .......... .......... 96% 27.7M 0s
166800K .......... .......... .......... .......... .......... 96%  348M 0s
166850K .......... .......... .......... .......... .......... 96%  123M 0s
166900K .......... .......... .......... .......... .......... 96%  290M 0s
166950K .......... .......... .......... .......... .......... 96%  116M 0s
167000K .......... .......... .......... .......... .......... 96%  267M 0s
167050K .......... .......... .......... .......... .......... 96%  329M 0s
167100K .......... .......... .......... .......... .......... 96%  312M 0s
167150K .......... .......... .......... .......... .......... 96%  377M 0s
167200K .......... .......... .......... .......... .......... 96% 91.6M 0s
167250K .......... .......... .......... .......... .......... 96%  330M 0s
167300K .......... .......... .......... .......... .......... 96%  233M 0s
167350K .......... .......... .......... .......... .......... 96%  370M 0s
167400K .......... .......... .......... .......... .......... 96%  372M 0s
167450K .......... .......... .......... .......... .......... 96%  190M 0s
167500K .......... .......... .......... .......... .......... 96%  176M 0s
167550K .......... .......... .......... .......... .......... 96%  370M 0s
167600K .......... .......... .......... .......... .......... 96%  290M 0s
167650K .......... .......... .......... .......... .......... 96%  300M 0s
167700K .......... .......... .......... .......... .......... 96%  361M 0s
167750K .......... .......... .......... .......... .......... 96%  371M 0s
167800K .......... .......... .......... .......... .......... 96%  379M 0s
167850K .......... .......... .......... .......... .......... 96%  320M 0s
167900K .......... .......... .......... .......... .......... 96% 74.2M 0s
167950K .......... .......... .......... .......... .......... 96%  266M 0s
168000K .......... .......... .......... .......... .......... 96%  245M 0s
168050K .......... .......... .......... .......... .......... 96%  274M 0s
168100K .......... .......... .......... .......... .......... 96%  355M 0s
168150K .......... .......... .......... .......... .......... 96%  348M 0s
168200K .......... .......... .......... .......... .......... 96%  287M 0s
168250K .......... .......... .......... .......... .......... 97% 89.6M 0s
168300K .......... .......... .......... .......... .......... 97%  246M 0s
168350K .......... .......... .......... .......... .......... 97%  269M 0s
168400K .......... .......... .......... .......... .......... 97%  250M 0s
168450K .......... .......... .......... .......... .......... 97%  342M 0s
168500K .......... .......... .......... .......... .......... 97%  346M 0s
168550K .......... .......... .......... .......... .......... 97% 80.4M 0s
168600K .......... .......... .......... .......... .......... 97%  349M 0s
168650K .......... .......... .......... .......... .......... 97% 52.6M 0s
168700K .......... .......... .......... .......... .......... 97%  115M 0s
168750K .......... .......... .......... .......... .......... 97%  237M 0s
168800K .......... .......... .......... .......... .......... 97%  353M 0s
168850K .......... .......... .......... .......... .......... 97%  209M 0s
168900K .......... .......... .......... .......... .......... 97%  163M 0s
168950K .......... .......... .......... .......... .......... 97%  243M 0s
169000K .......... .......... .......... .......... .......... 97%  269M 0s
169050K .......... .......... .......... .......... .......... 97%  354M 0s
169100K .......... .......... .......... .......... .......... 97% 65.6M 0s
169150K .......... .......... .......... .......... .......... 97% 28.9M 0s
169200K .......... .......... .......... .......... .......... 97% 75.9M 0s
169250K .......... .......... .......... .......... .......... 97%  222M 0s
169300K .......... .......... .......... .......... .......... 97%  295M 0s
169350K .......... .......... .......... .......... .......... 97%  188M 0s
169400K .......... .......... .......... .......... .......... 97%  233M 0s
169450K .......... .......... .......... .......... .......... 97%  265M 0s
169500K .......... .......... .......... .......... .......... 97%  306M 0s
169550K .......... .......... .......... .......... .......... 97%  368M 0s
169600K .......... .......... .......... .......... .......... 97%  344M 0s
169650K .......... .......... .......... .......... .......... 97%  131M 0s
169700K .......... .......... .......... .......... .......... 97%  243M 0s
169750K .......... .......... .......... .......... .......... 97%  244M 0s
169800K .......... .......... .......... .......... .......... 97%  306M 0s
169850K .......... .......... .......... .......... .......... 97%  304M 0s
169900K .......... .......... .......... .......... .......... 97%  159M 0s
169950K .......... .......... .......... .......... .......... 98%  219M 0s
170000K .......... .......... .......... .......... .......... 98%  241M 0s
170050K .......... .......... .......... .......... .......... 98%  287M 0s
170100K .......... .......... .......... .......... .......... 98%  342M 0s
170150K .......... .......... .......... .......... .......... 98%  341M 0s
170200K .......... .......... .......... .......... .......... 98%  294M 0s
170250K .......... .......... .......... .......... .......... 98%  338M 0s
170300K .......... .......... .......... .......... .......... 98%  327M 0s
170350K .......... .......... .......... .......... .......... 98%  202M 0s
170400K .......... .......... .......... .......... .......... 98%  149M 0s
170450K .......... .......... .......... .......... .......... 98%  214M 0s
170500K .......... .......... .......... .......... .......... 98%  303M 0s
170550K .......... .......... .......... .......... .......... 98%  337M 0s
170600K .......... .......... .......... .......... .......... 98%  315M 0s
170650K .......... .......... .......... .......... .......... 98%  348M 0s
170700K .......... .......... .......... .......... .......... 98%  337M 0s
170750K .......... .......... .......... .......... .......... 98% 64.8M 0s
170800K .......... .......... .......... .......... .......... 98%  208M 0s
170850K .......... .......... .......... .......... .......... 98%  328M 0s
170900K .......... .......... .......... .......... .......... 98%  344M 0s
170950K .......... .......... .......... .......... .......... 98%  298M 0s
171000K .......... .......... .......... .......... .......... 98% 95.0M 0s
171050K .......... .......... .......... .......... .......... 98%  335M 0s
171100K .......... .......... .......... .......... .......... 98% 49.9M 0s
171150K .......... .......... .......... .......... .......... 98%  298M 0s
171200K .......... .......... .......... .......... .......... 98% 58.9M 0s
171250K .......... .......... .......... .......... .......... 98%  292M 0s
171300K .......... .......... .......... .......... .......... 98%  204M 0s
171350K .......... .......... .......... .......... .......... 98%  245M 0s
171400K .......... .......... .......... .......... .......... 98%  341M 0s
171450K .......... .......... .......... .......... .......... 98%  338M 0s
171500K .......... .......... .......... .......... .......... 98%  282M 0s
171550K .......... .......... .......... .......... .......... 98%  164M 0s
171600K .......... .......... .......... .......... .......... 98%  300M 0s
171650K .......... .......... .......... .......... .......... 98% 22.2M 0s
171700K .......... .......... .......... .......... .......... 99% 66.9M 0s
171750K .......... .......... .......... .......... .......... 99%  193M 0s
171800K .......... .......... .......... .......... .......... 99%  394M 0s
171850K .......... .......... .......... .......... .......... 99%  347M 0s
171900K .......... .......... .......... .......... .......... 99%  410M 0s
171950K .......... .......... .......... .......... .......... 99%  405M 0s
172000K .......... .......... .......... .......... .......... 99%  397M 0s
172050K .......... .......... .......... .......... .......... 99%  214M 0s
172100K .......... .......... .......... .......... .......... 99%  399M 0s
172150K .......... .......... .......... .......... .......... 99%  288M 0s
172200K .......... .......... .......... .......... .......... 99%  295M 0s
172250K .......... .......... .......... .......... .......... 99%  298M 0s
172300K .......... .......... .......... .......... .......... 99%  406M 0s
172350K .......... .......... .......... .......... .......... 99%  278M 0s
172400K .......... .......... .......... .......... .......... 99%  291M 0s
172450K .......... .......... .......... .......... .......... 99%  231M 0s
172500K .......... .......... .......... .......... .......... 99%  236M 0s
172550K .......... .......... .......... .......... .......... 99%  358M 0s
172600K .......... .......... .......... .......... .......... 99%  349M 0s
172650K .......... .......... .......... .......... .......... 99%  395M 0s
172700K .......... .......... .......... .......... .......... 99%  408M 0s
172750K .......... .......... .......... .......... .......... 99%  335M 0s
172800K .......... .......... .......... .......... .......... 99%  408M 0s
172850K .......... .......... .......... .......... .......... 99%  110M 0s
172900K .......... .......... .......... .......... .......... 99%  135M 0s
172950K .......... .......... .......... .......... .......... 99%  250M 0s
173000K .......... .......... .......... .......... .......... 99%  306M 0s
173050K .......... .......... .......... .......... .......... 99%  403M 0s
173100K .......... .......... .......... .......... .......... 99%  407M 0s
173150K .......... .......... .......... .......... .......... 99%  345M 0s
173200K .......... .......... .......... .......... .......... 99% 70.9M 0s
173250K .......... .......... .......... .......... .......... 99%  261M 0s
173300K .......... .......... .......... .......... .......... 99%  342M 0s
173350K .......... .......... .......... .......... .......... 99%  204M 0s
173400K .......... .......... .......... .......... .......... 99%  404M 0s
173450K .....                                                 100%  207M=1.9s

2023-11-28 08:14:48 (90.4 MB/s) - ‘imagenet-o.tar’ saved [177617920/177617920]

/home/users/akane/miniconda3/envs/nattome/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 7
Added key: store_based_barrier_key:1 to store for rank: 5
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 6
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.

Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
WARNING: Unsupported operator aten::add encountered 25 time(s)
WARNING: Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)
WARNING: Unsupported operator aten::gelu encountered 12 time(s)
WARNING: The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
blocks.0.attn.attn_drop, blocks.1.attn.attn_drop, blocks.10.attn.attn_drop, blocks.11.attn.attn_drop, blocks.2.attn.attn_drop, blocks.3.attn.attn_drop, blocks.4.attn.attn_drop, blocks.5.attn.attn_drop, blocks.6.attn.attn_drop, blocks.7.attn.attn_drop, blocks.8.attn.attn_drop, blocks.9.attn.attn_drop
Model vit_small_patch16_224_augreg_in21k created.
22.051M Params and 4.251GFLOPs
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.5, 0.5, 0.5)
	std: (0.5, 0.5, 0.5)
	crop_pct: 0.875
	crop_mode: center
Using native Torch AMP. Training in mixed precision.
Using native Torch DistributedDataParallel.
Scheduled epochs: 310
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /workspace/akane/NAT/classification/wandb/run-20231202_173654-20231202-173652-vit_small_patch16_224_augreg_in21k-224
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vit_small_patch16_224_augreg_in21k
wandb: ⭐️ View project at https://wandb.ai/compyle/wintome-dinat-s-IN1k
wandb: 🚀 View run at https://wandb.ai/compyle/wintome-dinat-s-IN1k/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224
Train: 0 [   0/1251 (  0%)]  Loss: 6.974 (6.97)  Time: 2.218s,  461.61/s  (2.218s,  461.61/s)  LR: 1.040e-06  Data: 1.576 (1.576)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 0 [  50/1251 (  4%)]  Loss: 6.947 (6.96)  Time: 0.248s, 4130.65/s  (0.289s, 3540.67/s)  LR: 3.036e-06  Data: 0.036 (0.061)
Train: 0 [ 100/1251 (  8%)]  Loss: 6.944 (6.95)  Time: 0.256s, 3999.60/s  (0.275s, 3729.98/s)  LR: 5.033e-06  Data: 0.040 (0.047)
Train: 0 [ 150/1251 ( 12%)]  Loss: 6.932 (6.95)  Time: 0.255s, 4008.26/s  (0.269s, 3806.24/s)  LR: 7.029e-06  Data: 0.035 (0.041)
Train: 0 [ 200/1251 ( 16%)]  Loss: 6.926 (6.94)  Time: 0.255s, 4015.23/s  (0.266s, 3843.71/s)  LR: 9.026e-06  Data: 0.021 (0.038)
Train: 0 [ 250/1251 ( 20%)]  Loss: 6.913 (6.94)  Time: 0.272s, 3765.01/s  (0.266s, 3846.04/s)  LR: 1.102e-05  Data: 0.034 (0.035)
Train: 0 [ 300/1251 ( 24%)]  Loss: 6.886 (6.93)  Time: 0.265s, 3858.90/s  (0.266s, 3854.82/s)  LR: 1.302e-05  Data: 0.027 (0.034)
Train: 0 [ 350/1251 ( 28%)]  Loss: 6.877 (6.92)  Time: 0.265s, 3869.01/s  (0.266s, 3853.92/s)  LR: 1.501e-05  Data: 0.024 (0.032)
Train: 0 [ 400/1251 ( 32%)]  Loss: 6.880 (6.92)  Time: 0.270s, 3798.43/s  (0.267s, 3838.29/s)  LR: 1.701e-05  Data: 0.021 (0.031)
Train: 0 [ 450/1251 ( 36%)]  Loss: 6.869 (6.91)  Time: 0.279s, 3672.72/s  (0.268s, 3820.61/s)  LR: 1.901e-05  Data: 0.021 (0.030)
Train: 0 [ 500/1251 ( 40%)]  Loss: 6.871 (6.91)  Time: 0.278s, 3677.80/s  (0.270s, 3799.55/s)  LR: 2.100e-05  Data: 0.023 (0.029)
Train: 0 [ 550/1251 ( 44%)]  Loss: 6.844 (6.91)  Time: 0.284s, 3604.12/s  (0.271s, 3776.00/s)  LR: 2.300e-05  Data: 0.022 (0.028)
Train: 0 [ 600/1251 ( 48%)]  Loss: 6.855 (6.90)  Time: 0.288s, 3559.77/s  (0.273s, 3751.76/s)  LR: 2.500e-05  Data: 0.020 (0.027)
Train: 0 [ 650/1251 ( 52%)]  Loss: 6.837 (6.90)  Time: 0.294s, 3478.36/s  (0.275s, 3729.25/s)  LR: 2.699e-05  Data: 0.022 (0.027)
Train: 0 [ 700/1251 ( 56%)]  Loss: 6.834 (6.89)  Time: 0.302s, 3395.97/s  (0.276s, 3706.94/s)  LR: 2.899e-05  Data: 0.017 (0.026)
Train: 0 [ 750/1251 ( 60%)]  Loss: 6.798 (6.89)  Time: 0.299s, 3428.97/s  (0.278s, 3684.80/s)  LR: 3.099e-05  Data: 0.017 (0.026)
Train: 0 [ 800/1251 ( 64%)]  Loss: 6.827 (6.88)  Time: 0.308s, 3323.36/s  (0.280s, 3663.23/s)  LR: 3.298e-05  Data: 0.016 (0.025)
Train: 0 [ 850/1251 ( 68%)]  Loss: 6.804 (6.88)  Time: 0.301s, 3404.42/s  (0.281s, 3643.86/s)  LR: 3.498e-05  Data: 0.019 (0.025)
Train: 0 [ 900/1251 ( 72%)]  Loss: 6.753 (6.87)  Time: 0.309s, 3316.49/s  (0.282s, 3626.00/s)  LR: 3.698e-05  Data: 0.018 (0.025)
Train: 0 [ 950/1251 ( 76%)]  Loss: 6.762 (6.87)  Time: 0.318s, 3222.50/s  (0.284s, 3609.78/s)  LR: 3.897e-05  Data: 0.019 (0.024)
Train: 0 [1000/1251 ( 80%)]  Loss: 6.766 (6.86)  Time: 0.306s, 3342.11/s  (0.285s, 3592.98/s)  LR: 4.097e-05  Data: 0.021 (0.024)
Train: 0 [1050/1251 ( 84%)]  Loss: 6.704 (6.85)  Time: 0.317s, 3229.94/s  (0.286s, 3578.91/s)  LR: 4.296e-05  Data: 0.022 (0.024)
Train: 0 [1100/1251 ( 88%)]  Loss: 6.716 (6.85)  Time: 0.325s, 3147.27/s  (0.287s, 3563.97/s)  LR: 4.496e-05  Data: 0.021 (0.023)
Train: 0 [1150/1251 ( 92%)]  Loss: 6.703 (6.84)  Time: 0.316s, 3244.33/s  (0.289s, 3549.24/s)  LR: 4.696e-05  Data: 0.016 (0.023)
Train: 0 [1200/1251 ( 96%)]  Loss: 6.728 (6.84)  Time: 0.312s, 3280.45/s  (0.290s, 3535.67/s)  LR: 4.895e-05  Data: 0.016 (0.023)
Train: 0 [1250/1251 (100%)]  Loss: 6.676 (6.83)  Time: 0.293s, 3491.36/s  (0.290s, 3525.04/s)  LR: 5.095e-05  Data: 0.000 (0.023)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.178 (3.178)  Loss:  6.1094 (6.1094)  Acc@1:  1.0742 ( 1.0742)  Acc@5: 12.6953 (12.6953)
Test: [  48/48]  Time: 0.348 (0.266)  Loss:  5.7383 (6.3630)  Acc@1: 14.5047 ( 2.0920)  Acc@5: 24.7642 ( 7.0880)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-0.pth.tar', 2.091999998168945)

Train: 1 [   0/1251 (  0%)]  Loss: 6.674 (6.67)  Time: 2.364s,  433.11/s  (2.364s,  433.11/s)  LR: 5.099e-05  Data: 1.998 (1.998)
Train: 1 [  50/1251 (  4%)]  Loss: 6.679 (6.68)  Time: 0.312s, 3282.92/s  (0.339s, 3018.65/s)  LR: 5.299e-05  Data: 0.016 (0.060)
Train: 1 [ 100/1251 (  8%)]  Loss: 6.687 (6.68)  Time: 0.308s, 3325.09/s  (0.321s, 3186.81/s)  LR: 5.498e-05  Data: 0.014 (0.038)
Train: 1 [ 150/1251 ( 12%)]  Loss: 6.727 (6.69)  Time: 0.316s, 3235.68/s  (0.317s, 3232.00/s)  LR: 5.698e-05  Data: 0.016 (0.032)
Train: 1 [ 200/1251 ( 16%)]  Loss: 6.640 (6.68)  Time: 0.309s, 3311.76/s  (0.315s, 3250.50/s)  LR: 5.898e-05  Data: 0.017 (0.028)
Train: 1 [ 250/1251 ( 20%)]  Loss: 6.665 (6.68)  Time: 0.308s, 3323.96/s  (0.314s, 3262.71/s)  LR: 6.097e-05  Data: 0.017 (0.026)
Train: 1 [ 300/1251 ( 24%)]  Loss: 6.639 (6.67)  Time: 0.315s, 3252.80/s  (0.313s, 3269.10/s)  LR: 6.297e-05  Data: 0.019 (0.024)
Train: 1 [ 350/1251 ( 28%)]  Loss: 6.572 (6.66)  Time: 0.308s, 3321.23/s  (0.313s, 3269.88/s)  LR: 6.496e-05  Data: 0.017 (0.023)
Train: 1 [ 400/1251 ( 32%)]  Loss: 6.612 (6.66)  Time: 0.314s, 3263.89/s  (0.313s, 3270.66/s)  LR: 6.696e-05  Data: 0.021 (0.023)
Train: 1 [ 450/1251 ( 36%)]  Loss: 6.516 (6.64)  Time: 0.317s, 3229.56/s  (0.313s, 3269.85/s)  LR: 6.896e-05  Data: 0.014 (0.022)
Train: 1 [ 500/1251 ( 40%)]  Loss: 6.596 (6.64)  Time: 0.315s, 3245.96/s  (0.313s, 3268.60/s)  LR: 7.095e-05  Data: 0.014 (0.022)
Train: 1 [ 550/1251 ( 44%)]  Loss: 6.509 (6.63)  Time: 0.315s, 3252.01/s  (0.314s, 3265.65/s)  LR: 7.295e-05  Data: 0.017 (0.022)
Train: 1 [ 600/1251 ( 48%)]  Loss: 6.661 (6.63)  Time: 0.319s, 3213.80/s  (0.314s, 3262.40/s)  LR: 7.495e-05  Data: 0.021 (0.021)
Train: 1 [ 650/1251 ( 52%)]  Loss: 6.549 (6.62)  Time: 0.324s, 3157.93/s  (0.314s, 3259.03/s)  LR: 7.694e-05  Data: 0.025 (0.021)
Train: 1 [ 700/1251 ( 56%)]  Loss: 6.631 (6.62)  Time: 0.325s, 3153.23/s  (0.314s, 3256.42/s)  LR: 7.894e-05  Data: 0.021 (0.021)
Train: 1 [ 750/1251 ( 60%)]  Loss: 6.496 (6.62)  Time: 0.326s, 3145.55/s  (0.315s, 3253.14/s)  LR: 8.094e-05  Data: 0.015 (0.021)
Train: 1 [ 800/1251 ( 64%)]  Loss: 6.453 (6.61)  Time: 0.322s, 3180.34/s  (0.315s, 3250.00/s)  LR: 8.293e-05  Data: 0.023 (0.021)
Train: 1 [ 850/1251 ( 68%)]  Loss: 6.590 (6.61)  Time: 0.312s, 3284.90/s  (0.315s, 3247.34/s)  LR: 8.493e-05  Data: 0.019 (0.021)
Train: 1 [ 900/1251 ( 72%)]  Loss: 6.548 (6.60)  Time: 0.321s, 3192.53/s  (0.316s, 3244.82/s)  LR: 8.693e-05  Data: 0.019 (0.021)
Train: 1 [ 950/1251 ( 76%)]  Loss: 6.569 (6.60)  Time: 0.320s, 3198.42/s  (0.316s, 3242.19/s)  LR: 8.892e-05  Data: 0.018 (0.021)
Train: 1 [1000/1251 ( 80%)]  Loss: 6.423 (6.59)  Time: 0.323s, 3167.90/s  (0.316s, 3240.18/s)  LR: 9.092e-05  Data: 0.021 (0.021)
Train: 1 [1050/1251 ( 84%)]  Loss: 6.562 (6.59)  Time: 0.326s, 3142.31/s  (0.316s, 3238.08/s)  LR: 9.291e-05  Data: 0.021 (0.020)
Train: 1 [1100/1251 ( 88%)]  Loss: 6.426 (6.58)  Time: 0.321s, 3194.01/s  (0.316s, 3235.75/s)  LR: 9.491e-05  Data: 0.021 (0.020)
Train: 1 [1150/1251 ( 92%)]  Loss: 6.593 (6.58)  Time: 0.322s, 3184.41/s  (0.317s, 3232.54/s)  LR: 9.691e-05  Data: 0.012 (0.020)
Train: 1 [1200/1251 ( 96%)]  Loss: 6.518 (6.58)  Time: 0.330s, 3100.95/s  (0.317s, 3229.79/s)  LR: 9.890e-05  Data: 0.025 (0.020)
Train: 1 [1250/1251 (100%)]  Loss: 6.379 (6.57)  Time: 0.300s, 3413.43/s  (0.317s, 3229.28/s)  LR: 1.009e-04  Data: 0.000 (0.020)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.041 (2.041)  Loss:  5.5469 (5.5469)  Acc@1:  5.3711 ( 5.3711)  Acc@5: 24.7070 (24.7070)
Test: [  48/48]  Time: 0.069 (0.239)  Loss:  5.0000 (5.8122)  Acc@1: 22.8774 ( 5.1840)  Acc@5: 38.6792 (15.2420)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-1.pth.tar', 5.183999982910156)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-0.pth.tar', 2.091999998168945)

Train: 2 [   0/1251 (  0%)]  Loss: 6.459 (6.46)  Time: 2.218s,  461.69/s  (2.218s,  461.69/s)  LR: 1.009e-04  Data: 1.983 (1.983)
Train: 2 [  50/1251 (  4%)]  Loss: 6.423 (6.44)  Time: 0.310s, 3300.26/s  (0.346s, 2958.98/s)  LR: 1.029e-04  Data: 0.026 (0.063)
Train: 2 [ 100/1251 (  8%)]  Loss: 6.483 (6.45)  Time: 0.317s, 3234.74/s  (0.330s, 3100.14/s)  LR: 1.049e-04  Data: 0.023 (0.043)
Train: 2 [ 150/1251 ( 12%)]  Loss: 6.536 (6.48)  Time: 0.317s, 3226.90/s  (0.326s, 3138.43/s)  LR: 1.069e-04  Data: 0.022 (0.036)
Train: 2 [ 200/1251 ( 16%)]  Loss: 6.497 (6.48)  Time: 0.319s, 3208.01/s  (0.325s, 3154.02/s)  LR: 1.089e-04  Data: 0.022 (0.033)
Train: 2 [ 250/1251 ( 20%)]  Loss: 6.483 (6.48)  Time: 0.323s, 3174.02/s  (0.324s, 3162.39/s)  LR: 1.109e-04  Data: 0.024 (0.030)
Train: 2 [ 300/1251 ( 24%)]  Loss: 6.402 (6.47)  Time: 0.324s, 3156.98/s  (0.323s, 3167.14/s)  LR: 1.129e-04  Data: 0.023 (0.029)
Train: 2 [ 350/1251 ( 28%)]  Loss: 6.426 (6.46)  Time: 0.316s, 3235.63/s  (0.323s, 3168.50/s)  LR: 1.149e-04  Data: 0.022 (0.028)
Train: 2 [ 400/1251 ( 32%)]  Loss: 6.461 (6.46)  Time: 0.322s, 3183.44/s  (0.323s, 3171.00/s)  LR: 1.169e-04  Data: 0.022 (0.027)
Train: 2 [ 450/1251 ( 36%)]  Loss: 6.401 (6.46)  Time: 0.324s, 3161.55/s  (0.323s, 3173.97/s)  LR: 1.189e-04  Data: 0.024 (0.027)
Train: 2 [ 500/1251 ( 40%)]  Loss: 6.461 (6.46)  Time: 0.320s, 3203.59/s  (0.323s, 3174.72/s)  LR: 1.209e-04  Data: 0.019 (0.026)
Train: 2 [ 550/1251 ( 44%)]  Loss: 6.523 (6.46)  Time: 0.322s, 3183.51/s  (0.323s, 3174.93/s)  LR: 1.229e-04  Data: 0.023 (0.026)
Train: 2 [ 600/1251 ( 48%)]  Loss: 6.359 (6.45)  Time: 0.325s, 3153.06/s  (0.323s, 3174.41/s)  LR: 1.249e-04  Data: 0.022 (0.026)
Train: 2 [ 650/1251 ( 52%)]  Loss: 6.389 (6.45)  Time: 0.317s, 3225.77/s  (0.322s, 3176.05/s)  LR: 1.269e-04  Data: 0.023 (0.025)
Train: 2 [ 700/1251 ( 56%)]  Loss: 6.433 (6.45)  Time: 0.325s, 3154.97/s  (0.322s, 3176.93/s)  LR: 1.289e-04  Data: 0.020 (0.025)
Train: 2 [ 750/1251 ( 60%)]  Loss: 6.419 (6.45)  Time: 0.318s, 3218.33/s  (0.322s, 3177.29/s)  LR: 1.309e-04  Data: 0.023 (0.025)
Train: 2 [ 800/1251 ( 64%)]  Loss: 6.440 (6.45)  Time: 0.323s, 3172.10/s  (0.322s, 3177.26/s)  LR: 1.329e-04  Data: 0.022 (0.025)
Train: 2 [ 850/1251 ( 68%)]  Loss: 6.545 (6.45)  Time: 0.320s, 3201.08/s  (0.322s, 3177.23/s)  LR: 1.349e-04  Data: 0.021 (0.025)
Train: 2 [ 900/1251 ( 72%)]  Loss: 6.312 (6.44)  Time: 0.330s, 3100.18/s  (0.322s, 3177.09/s)  LR: 1.369e-04  Data: 0.024 (0.024)
Train: 2 [ 950/1251 ( 76%)]  Loss: 6.190 (6.43)  Time: 0.323s, 3174.72/s  (0.322s, 3177.48/s)  LR: 1.389e-04  Data: 0.021 (0.024)
Train: 2 [1000/1251 ( 80%)]  Loss: 6.461 (6.43)  Time: 0.318s, 3215.66/s  (0.322s, 3177.71/s)  LR: 1.409e-04  Data: 0.024 (0.024)
Train: 2 [1050/1251 ( 84%)]  Loss: 6.486 (6.44)  Time: 0.318s, 3216.65/s  (0.322s, 3177.87/s)  LR: 1.429e-04  Data: 0.024 (0.024)
Train: 2 [1100/1251 ( 88%)]  Loss: 6.460 (6.44)  Time: 0.322s, 3183.37/s  (0.322s, 3177.50/s)  LR: 1.449e-04  Data: 0.022 (0.024)
Train: 2 [1150/1251 ( 92%)]  Loss: 6.180 (6.43)  Time: 0.323s, 3169.57/s  (0.322s, 3177.43/s)  LR: 1.469e-04  Data: 0.023 (0.024)
Train: 2 [1200/1251 ( 96%)]  Loss: 6.213 (6.42)  Time: 0.322s, 3181.84/s  (0.322s, 3177.59/s)  LR: 1.489e-04  Data: 0.023 (0.024)
Train: 2 [1250/1251 (100%)]  Loss: 6.345 (6.41)  Time: 0.299s, 3428.86/s  (0.322s, 3179.02/s)  LR: 1.509e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.036 (2.036)  Loss:  5.0195 (5.0195)  Acc@1: 12.4023 (12.4023)  Acc@5: 32.2266 (32.2266)
Test: [  48/48]  Time: 0.070 (0.239)  Loss:  4.5195 (5.4254)  Acc@1: 28.1840 ( 8.1160)  Acc@5: 43.5142 (21.1180)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-2.pth.tar', 8.115999993286133)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-1.pth.tar', 5.183999982910156)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-0.pth.tar', 2.091999998168945)

Train: 3 [   0/1251 (  0%)]  Loss: 6.230 (6.23)  Time: 2.532s,  404.36/s  (2.532s,  404.36/s)  LR: 1.509e-04  Data: 2.300 (2.300)
Train: 3 [  50/1251 (  4%)]  Loss: 6.296 (6.26)  Time: 0.309s, 3311.09/s  (0.349s, 2934.43/s)  LR: 1.529e-04  Data: 0.024 (0.068)
Train: 3 [ 100/1251 (  8%)]  Loss: 6.115 (6.21)  Time: 0.320s, 3202.48/s  (0.332s, 3086.18/s)  LR: 1.549e-04  Data: 0.026 (0.046)
Train: 3 [ 150/1251 ( 12%)]  Loss: 6.265 (6.23)  Time: 0.317s, 3235.32/s  (0.327s, 3130.29/s)  LR: 1.569e-04  Data: 0.020 (0.038)
Train: 3 [ 200/1251 ( 16%)]  Loss: 6.367 (6.25)  Time: 0.324s, 3159.24/s  (0.325s, 3148.69/s)  LR: 1.589e-04  Data: 0.025 (0.034)
Train: 3 [ 250/1251 ( 20%)]  Loss: 6.295 (6.26)  Time: 0.319s, 3207.84/s  (0.324s, 3159.90/s)  LR: 1.609e-04  Data: 0.022 (0.032)
Train: 3 [ 300/1251 ( 24%)]  Loss: 6.270 (6.26)  Time: 0.320s, 3197.38/s  (0.323s, 3165.67/s)  LR: 1.629e-04  Data: 0.022 (0.030)
Train: 3 [ 350/1251 ( 28%)]  Loss: 6.392 (6.28)  Time: 0.315s, 3255.59/s  (0.323s, 3170.31/s)  LR: 1.649e-04  Data: 0.021 (0.029)
Train: 3 [ 400/1251 ( 32%)]  Loss: 6.323 (6.28)  Time: 0.321s, 3185.27/s  (0.323s, 3172.54/s)  LR: 1.669e-04  Data: 0.025 (0.029)
Train: 3 [ 450/1251 ( 36%)]  Loss: 6.375 (6.29)  Time: 0.322s, 3183.97/s  (0.323s, 3173.64/s)  LR: 1.689e-04  Data: 0.023 (0.028)
Train: 3 [ 500/1251 ( 40%)]  Loss: 6.338 (6.30)  Time: 0.320s, 3202.42/s  (0.322s, 3175.37/s)  LR: 1.709e-04  Data: 0.021 (0.027)
Train: 3 [ 550/1251 ( 44%)]  Loss: 6.334 (6.30)  Time: 0.321s, 3191.27/s  (0.322s, 3177.14/s)  LR: 1.729e-04  Data: 0.021 (0.027)
Train: 3 [ 600/1251 ( 48%)]  Loss: 6.276 (6.30)  Time: 0.322s, 3177.31/s  (0.322s, 3177.59/s)  LR: 1.748e-04  Data: 0.024 (0.027)
Train: 3 [ 650/1251 ( 52%)]  Loss: 6.384 (6.30)  Time: 0.322s, 3180.82/s  (0.322s, 3178.02/s)  LR: 1.768e-04  Data: 0.019 (0.026)
Train: 3 [ 700/1251 ( 56%)]  Loss: 6.281 (6.30)  Time: 0.320s, 3197.83/s  (0.322s, 3178.22/s)  LR: 1.788e-04  Data: 0.021 (0.026)
Train: 3 [ 750/1251 ( 60%)]  Loss: 6.372 (6.31)  Time: 0.312s, 3285.18/s  (0.322s, 3178.22/s)  LR: 1.808e-04  Data: 0.022 (0.026)
Train: 3 [ 800/1251 ( 64%)]  Loss: 6.331 (6.31)  Time: 0.322s, 3178.22/s  (0.322s, 3178.20/s)  LR: 1.828e-04  Data: 0.022 (0.026)
Train: 3 [ 850/1251 ( 68%)]  Loss: 6.367 (6.31)  Time: 0.323s, 3170.84/s  (0.322s, 3178.34/s)  LR: 1.848e-04  Data: 0.022 (0.025)
Train: 3 [ 900/1251 ( 72%)]  Loss: 6.323 (6.31)  Time: 0.325s, 3146.13/s  (0.322s, 3178.39/s)  LR: 1.868e-04  Data: 0.024 (0.025)
Train: 3 [ 950/1251 ( 76%)]  Loss: 6.305 (6.31)  Time: 0.317s, 3234.32/s  (0.322s, 3178.69/s)  LR: 1.888e-04  Data: 0.021 (0.025)
Train: 3 [1000/1251 ( 80%)]  Loss: 6.189 (6.31)  Time: 0.325s, 3146.81/s  (0.322s, 3178.06/s)  LR: 1.908e-04  Data: 0.024 (0.025)
Train: 3 [1050/1251 ( 84%)]  Loss: 6.304 (6.31)  Time: 0.323s, 3166.52/s  (0.322s, 3177.51/s)  LR: 1.928e-04  Data: 0.026 (0.025)
Train: 3 [1100/1251 ( 88%)]  Loss: 6.308 (6.31)  Time: 0.324s, 3159.03/s  (0.322s, 3176.97/s)  LR: 1.948e-04  Data: 0.020 (0.025)
Train: 3 [1150/1251 ( 92%)]  Loss: 6.282 (6.31)  Time: 0.329s, 3117.02/s  (0.322s, 3176.25/s)  LR: 1.968e-04  Data: 0.024 (0.025)
Train: 3 [1200/1251 ( 96%)]  Loss: 6.199 (6.30)  Time: 0.320s, 3203.98/s  (0.322s, 3176.15/s)  LR: 1.988e-04  Data: 0.023 (0.025)
Train: 3 [1250/1251 (100%)]  Loss: 6.180 (6.30)  Time: 0.303s, 3379.99/s  (0.322s, 3177.56/s)  LR: 2.008e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.014 (2.014)  Loss:  4.4570 (4.4570)  Acc@1: 18.0664 (18.0664)  Acc@5: 43.6523 (43.6523)
Test: [  48/48]  Time: 0.070 (0.236)  Loss:  3.7988 (5.0415)  Acc@1: 37.0283 (11.9120)  Acc@5: 56.4858 (28.3380)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-3.pth.tar', 11.911999924316406)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-2.pth.tar', 8.115999993286133)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-1.pth.tar', 5.183999982910156)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-0.pth.tar', 2.091999998168945)

Train: 4 [   0/1251 (  0%)]  Loss: 6.195 (6.19)  Time: 2.254s,  454.24/s  (2.254s,  454.24/s)  LR: 2.008e-04  Data: 2.024 (2.024)
Train: 4 [  50/1251 (  4%)]  Loss: 6.229 (6.21)  Time: 0.310s, 3298.33/s  (0.345s, 2971.86/s)  LR: 2.028e-04  Data: 0.022 (0.063)
Train: 4 [ 100/1251 (  8%)]  Loss: 6.306 (6.24)  Time: 0.311s, 3295.67/s  (0.329s, 3115.65/s)  LR: 2.048e-04  Data: 0.022 (0.043)
Train: 4 [ 150/1251 ( 12%)]  Loss: 6.215 (6.24)  Time: 0.308s, 3328.73/s  (0.325s, 3153.52/s)  LR: 2.068e-04  Data: 0.024 (0.037)
Train: 4 [ 200/1251 ( 16%)]  Loss: 6.198 (6.23)  Time: 0.320s, 3198.23/s  (0.323s, 3170.55/s)  LR: 2.088e-04  Data: 0.027 (0.033)
Train: 4 [ 250/1251 ( 20%)]  Loss: 6.314 (6.24)  Time: 0.312s, 3286.57/s  (0.322s, 3179.02/s)  LR: 2.108e-04  Data: 0.019 (0.031)
Train: 4 [ 300/1251 ( 24%)]  Loss: 6.126 (6.23)  Time: 0.313s, 3268.27/s  (0.322s, 3183.51/s)  LR: 2.128e-04  Data: 0.024 (0.030)
Train: 4 [ 350/1251 ( 28%)]  Loss: 6.442 (6.25)  Time: 0.320s, 3199.94/s  (0.321s, 3187.58/s)  LR: 2.148e-04  Data: 0.021 (0.029)
Train: 4 [ 400/1251 ( 32%)]  Loss: 6.173 (6.24)  Time: 0.321s, 3194.55/s  (0.321s, 3189.24/s)  LR: 2.168e-04  Data: 0.030 (0.028)
Train: 4 [ 450/1251 ( 36%)]  Loss: 6.090 (6.23)  Time: 0.326s, 3138.20/s  (0.321s, 3190.40/s)  LR: 2.188e-04  Data: 0.022 (0.027)
Train: 4 [ 500/1251 ( 40%)]  Loss: 6.066 (6.21)  Time: 0.323s, 3169.61/s  (0.321s, 3190.70/s)  LR: 2.208e-04  Data: 0.022 (0.027)
Train: 4 [ 550/1251 ( 44%)]  Loss: 6.185 (6.21)  Time: 0.315s, 3248.17/s  (0.321s, 3192.39/s)  LR: 2.228e-04  Data: 0.023 (0.027)
Train: 4 [ 600/1251 ( 48%)]  Loss: 6.259 (6.22)  Time: 0.316s, 3241.52/s  (0.321s, 3193.52/s)  LR: 2.248e-04  Data: 0.026 (0.026)
Train: 4 [ 650/1251 ( 52%)]  Loss: 6.264 (6.22)  Time: 0.324s, 3156.87/s  (0.321s, 3194.74/s)  LR: 2.268e-04  Data: 0.024 (0.026)
Train: 4 [ 700/1251 ( 56%)]  Loss: 6.294 (6.22)  Time: 0.320s, 3203.97/s  (0.321s, 3194.97/s)  LR: 2.288e-04  Data: 0.024 (0.026)
Train: 4 [ 750/1251 ( 60%)]  Loss: 6.221 (6.22)  Time: 0.323s, 3174.67/s  (0.320s, 3195.29/s)  LR: 2.308e-04  Data: 0.022 (0.026)
Train: 4 [ 800/1251 ( 64%)]  Loss: 6.058 (6.21)  Time: 0.316s, 3242.12/s  (0.320s, 3195.28/s)  LR: 2.328e-04  Data: 0.021 (0.025)
Train: 4 [ 850/1251 ( 68%)]  Loss: 6.039 (6.20)  Time: 0.315s, 3249.64/s  (0.320s, 3195.78/s)  LR: 2.348e-04  Data: 0.024 (0.025)
Train: 4 [ 900/1251 ( 72%)]  Loss: 6.275 (6.21)  Time: 0.321s, 3191.23/s  (0.320s, 3195.82/s)  LR: 2.368e-04  Data: 0.024 (0.025)
Train: 4 [ 950/1251 ( 76%)]  Loss: 5.955 (6.20)  Time: 0.314s, 3264.28/s  (0.320s, 3195.91/s)  LR: 2.388e-04  Data: 0.021 (0.025)
Train: 4 [1000/1251 ( 80%)]  Loss: 6.211 (6.20)  Time: 0.319s, 3214.49/s  (0.320s, 3196.58/s)  LR: 2.408e-04  Data: 0.025 (0.025)
Train: 4 [1050/1251 ( 84%)]  Loss: 6.048 (6.19)  Time: 0.321s, 3192.31/s  (0.320s, 3197.54/s)  LR: 2.428e-04  Data: 0.022 (0.025)
Train: 4 [1100/1251 ( 88%)]  Loss: 5.842 (6.17)  Time: 0.324s, 3156.89/s  (0.320s, 3198.22/s)  LR: 2.448e-04  Data: 0.023 (0.025)
Train: 4 [1150/1251 ( 92%)]  Loss: 6.151 (6.17)  Time: 0.316s, 3237.25/s  (0.320s, 3198.77/s)  LR: 2.468e-04  Data: 0.023 (0.025)
Train: 4 [1200/1251 ( 96%)]  Loss: 6.163 (6.17)  Time: 0.324s, 3164.32/s  (0.320s, 3198.78/s)  LR: 2.488e-04  Data: 0.022 (0.025)
Train: 4 [1250/1251 (100%)]  Loss: 5.794 (6.16)  Time: 0.297s, 3447.75/s  (0.320s, 3200.55/s)  LR: 2.508e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.026 (2.026)  Loss:  4.0781 (4.0781)  Acc@1: 22.9492 (22.9492)  Acc@5: 50.0000 (50.0000)
Test: [  48/48]  Time: 0.071 (0.237)  Loss:  3.4277 (4.6240)  Acc@1: 41.5094 (16.4500)  Acc@5: 60.4953 (35.9920)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-4.pth.tar', 16.44999999633789)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-3.pth.tar', 11.911999924316406)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-2.pth.tar', 8.115999993286133)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-1.pth.tar', 5.183999982910156)

Train: 5 [   0/1251 (  0%)]  Loss: 5.924 (5.92)  Time: 2.249s,  455.40/s  (2.249s,  455.40/s)  LR: 2.508e-04  Data: 2.025 (2.025)
Train: 5 [  50/1251 (  4%)]  Loss: 6.090 (6.01)  Time: 0.311s, 3288.96/s  (0.343s, 2984.87/s)  LR: 2.528e-04  Data: 0.024 (0.063)
Train: 5 [ 100/1251 (  8%)]  Loss: 6.030 (6.01)  Time: 0.311s, 3296.92/s  (0.328s, 3125.12/s)  LR: 2.548e-04  Data: 0.025 (0.043)
Train: 5 [ 150/1251 ( 12%)]  Loss: 6.054 (6.02)  Time: 0.316s, 3235.99/s  (0.323s, 3169.51/s)  LR: 2.568e-04  Data: 0.022 (0.036)
Train: 5 [ 200/1251 ( 16%)]  Loss: 6.047 (6.03)  Time: 0.319s, 3211.65/s  (0.321s, 3186.89/s)  LR: 2.588e-04  Data: 0.023 (0.033)
Train: 5 [ 250/1251 ( 20%)]  Loss: 5.925 (6.01)  Time: 0.317s, 3229.97/s  (0.320s, 3198.12/s)  LR: 2.608e-04  Data: 0.028 (0.031)
Train: 5 [ 300/1251 ( 24%)]  Loss: 5.868 (5.99)  Time: 0.322s, 3177.83/s  (0.320s, 3203.99/s)  LR: 2.628e-04  Data: 0.025 (0.030)
Train: 5 [ 350/1251 ( 28%)]  Loss: 5.975 (5.99)  Time: 0.317s, 3231.61/s  (0.319s, 3209.13/s)  LR: 2.648e-04  Data: 0.023 (0.029)
Train: 5 [ 400/1251 ( 32%)]  Loss: 5.996 (5.99)  Time: 0.321s, 3185.49/s  (0.319s, 3211.83/s)  LR: 2.668e-04  Data: 0.023 (0.028)
Train: 5 [ 450/1251 ( 36%)]  Loss: 5.898 (5.98)  Time: 0.311s, 3296.65/s  (0.319s, 3213.82/s)  LR: 2.688e-04  Data: 0.023 (0.027)
Train: 5 [ 500/1251 ( 40%)]  Loss: 6.081 (5.99)  Time: 0.319s, 3209.03/s  (0.319s, 3213.93/s)  LR: 2.708e-04  Data: 0.021 (0.027)
Train: 5 [ 550/1251 ( 44%)]  Loss: 5.791 (5.97)  Time: 0.319s, 3205.24/s  (0.319s, 3214.67/s)  LR: 2.728e-04  Data: 0.022 (0.026)
Train: 5 [ 600/1251 ( 48%)]  Loss: 5.977 (5.97)  Time: 0.322s, 3176.63/s  (0.319s, 3214.32/s)  LR: 2.747e-04  Data: 0.021 (0.026)
Train: 5 [ 650/1251 ( 52%)]  Loss: 6.253 (5.99)  Time: 0.319s, 3213.67/s  (0.319s, 3214.65/s)  LR: 2.767e-04  Data: 0.023 (0.026)
Train: 5 [ 700/1251 ( 56%)]  Loss: 5.817 (5.98)  Time: 0.316s, 3242.09/s  (0.319s, 3214.43/s)  LR: 2.787e-04  Data: 0.021 (0.026)
Train: 5 [ 750/1251 ( 60%)]  Loss: 5.955 (5.98)  Time: 0.319s, 3208.26/s  (0.319s, 3214.94/s)  LR: 2.807e-04  Data: 0.020 (0.026)
Train: 5 [ 800/1251 ( 64%)]  Loss: 6.070 (5.99)  Time: 0.312s, 3282.57/s  (0.319s, 3215.06/s)  LR: 2.827e-04  Data: 0.021 (0.025)
Train: 5 [ 850/1251 ( 68%)]  Loss: 5.971 (5.98)  Time: 0.316s, 3237.09/s  (0.318s, 3215.23/s)  LR: 2.847e-04  Data: 0.023 (0.025)
Train: 5 [ 900/1251 ( 72%)]  Loss: 6.042 (5.99)  Time: 0.317s, 3233.14/s  (0.318s, 3215.80/s)  LR: 2.867e-04  Data: 0.023 (0.025)
Train: 5 [ 950/1251 ( 76%)]  Loss: 5.940 (5.99)  Time: 0.323s, 3174.42/s  (0.318s, 3216.55/s)  LR: 2.887e-04  Data: 0.026 (0.025)
Train: 5 [1000/1251 ( 80%)]  Loss: 6.164 (5.99)  Time: 0.316s, 3241.91/s  (0.318s, 3217.82/s)  LR: 2.907e-04  Data: 0.022 (0.025)
Train: 5 [1050/1251 ( 84%)]  Loss: 6.051 (6.00)  Time: 0.314s, 3258.34/s  (0.318s, 3218.04/s)  LR: 2.927e-04  Data: 0.019 (0.025)
Train: 5 [1100/1251 ( 88%)]  Loss: 5.862 (5.99)  Time: 0.320s, 3200.86/s  (0.318s, 3218.06/s)  LR: 2.947e-04  Data: 0.025 (0.025)
Train: 5 [1150/1251 ( 92%)]  Loss: 5.979 (5.99)  Time: 0.319s, 3212.67/s  (0.318s, 3218.60/s)  LR: 2.967e-04  Data: 0.026 (0.025)
Train: 5 [1200/1251 ( 96%)]  Loss: 5.780 (5.98)  Time: 0.314s, 3262.38/s  (0.318s, 3218.81/s)  LR: 2.987e-04  Data: 0.021 (0.025)
Train: 5 [1250/1251 (100%)]  Loss: 5.967 (5.98)  Time: 0.285s, 3596.34/s  (0.318s, 3220.54/s)  LR: 3.007e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.979 (1.979)  Loss:  3.4688 (3.4688)  Acc@1: 32.6172 (32.6172)  Acc@5: 62.3047 (62.3047)
Test: [  48/48]  Time: 0.059 (0.236)  Loss:  2.9805 (4.1927)  Acc@1: 48.9387 (21.4400)  Acc@5: 67.6887 (43.5820)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-5.pth.tar', 21.43999999145508)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-4.pth.tar', 16.44999999633789)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-3.pth.tar', 11.911999924316406)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-2.pth.tar', 8.115999993286133)

Train: 6 [   0/1251 (  0%)]  Loss: 5.989 (5.99)  Time: 2.148s,  476.81/s  (2.148s,  476.81/s)  LR: 3.007e-04  Data: 1.918 (1.918)
Train: 6 [  50/1251 (  4%)]  Loss: 5.823 (5.91)  Time: 0.307s, 3333.47/s  (0.339s, 3021.79/s)  LR: 3.027e-04  Data: 0.026 (0.062)
Train: 6 [ 100/1251 (  8%)]  Loss: 5.931 (5.91)  Time: 0.309s, 3316.09/s  (0.324s, 3157.23/s)  LR: 3.047e-04  Data: 0.022 (0.043)
Train: 6 [ 150/1251 ( 12%)]  Loss: 6.018 (5.94)  Time: 0.309s, 3317.08/s  (0.320s, 3198.98/s)  LR: 3.067e-04  Data: 0.022 (0.036)
Train: 6 [ 200/1251 ( 16%)]  Loss: 6.000 (5.95)  Time: 0.316s, 3239.96/s  (0.318s, 3217.90/s)  LR: 3.087e-04  Data: 0.023 (0.033)
Train: 6 [ 250/1251 ( 20%)]  Loss: 5.853 (5.94)  Time: 0.314s, 3259.06/s  (0.317s, 3228.61/s)  LR: 3.107e-04  Data: 0.024 (0.031)
Train: 6 [ 300/1251 ( 24%)]  Loss: 5.741 (5.91)  Time: 0.311s, 3291.24/s  (0.316s, 3235.88/s)  LR: 3.127e-04  Data: 0.025 (0.030)
Train: 6 [ 350/1251 ( 28%)]  Loss: 5.949 (5.91)  Time: 0.319s, 3206.08/s  (0.316s, 3239.22/s)  LR: 3.147e-04  Data: 0.025 (0.029)
Train: 6 [ 400/1251 ( 32%)]  Loss: 5.825 (5.90)  Time: 0.323s, 3170.39/s  (0.316s, 3240.86/s)  LR: 3.167e-04  Data: 0.025 (0.028)
Train: 6 [ 450/1251 ( 36%)]  Loss: 5.764 (5.89)  Time: 0.314s, 3264.72/s  (0.316s, 3242.07/s)  LR: 3.187e-04  Data: 0.025 (0.027)
Train: 6 [ 500/1251 ( 40%)]  Loss: 5.891 (5.89)  Time: 0.317s, 3232.74/s  (0.316s, 3242.91/s)  LR: 3.207e-04  Data: 0.023 (0.027)
Train: 6 [ 550/1251 ( 44%)]  Loss: 5.618 (5.87)  Time: 0.317s, 3228.02/s  (0.316s, 3242.81/s)  LR: 3.227e-04  Data: 0.022 (0.027)
Train: 6 [ 600/1251 ( 48%)]  Loss: 5.707 (5.85)  Time: 0.315s, 3250.49/s  (0.316s, 3244.30/s)  LR: 3.247e-04  Data: 0.023 (0.026)
Train: 6 [ 650/1251 ( 52%)]  Loss: 6.098 (5.87)  Time: 0.321s, 3187.99/s  (0.316s, 3244.76/s)  LR: 3.267e-04  Data: 0.025 (0.026)
Train: 6 [ 700/1251 ( 56%)]  Loss: 6.038 (5.88)  Time: 0.320s, 3201.17/s  (0.316s, 3245.34/s)  LR: 3.287e-04  Data: 0.022 (0.026)
Train: 6 [ 750/1251 ( 60%)]  Loss: 5.732 (5.87)  Time: 0.310s, 3299.47/s  (0.316s, 3245.26/s)  LR: 3.307e-04  Data: 0.024 (0.026)
Train: 6 [ 800/1251 ( 64%)]  Loss: 6.105 (5.89)  Time: 0.314s, 3256.54/s  (0.316s, 3245.42/s)  LR: 3.327e-04  Data: 0.022 (0.026)
Train: 6 [ 850/1251 ( 68%)]  Loss: 5.755 (5.88)  Time: 0.315s, 3252.51/s  (0.316s, 3245.04/s)  LR: 3.347e-04  Data: 0.022 (0.025)
Train: 6 [ 900/1251 ( 72%)]  Loss: 6.164 (5.89)  Time: 0.313s, 3275.74/s  (0.316s, 3244.54/s)  LR: 3.367e-04  Data: 0.021 (0.025)
Train: 6 [ 950/1251 ( 76%)]  Loss: 5.751 (5.89)  Time: 0.314s, 3261.79/s  (0.316s, 3243.84/s)  LR: 3.387e-04  Data: 0.022 (0.025)
Train: 6 [1000/1251 ( 80%)]  Loss: 5.684 (5.88)  Time: 0.312s, 3282.23/s  (0.316s, 3242.87/s)  LR: 3.407e-04  Data: 0.022 (0.025)
Train: 6 [1050/1251 ( 84%)]  Loss: 5.670 (5.87)  Time: 0.317s, 3235.09/s  (0.316s, 3241.97/s)  LR: 3.427e-04  Data: 0.022 (0.025)
Train: 6 [1100/1251 ( 88%)]  Loss: 5.483 (5.85)  Time: 0.328s, 3123.10/s  (0.316s, 3240.72/s)  LR: 3.447e-04  Data: 0.023 (0.025)
Train: 6 [1150/1251 ( 92%)]  Loss: 5.552 (5.84)  Time: 0.316s, 3237.70/s  (0.316s, 3239.88/s)  LR: 3.467e-04  Data: 0.024 (0.025)
Train: 6 [1200/1251 ( 96%)]  Loss: 5.813 (5.84)  Time: 0.314s, 3263.45/s  (0.316s, 3240.00/s)  LR: 3.487e-04  Data: 0.023 (0.025)
Train: 6 [1250/1251 (100%)]  Loss: 5.912 (5.84)  Time: 0.293s, 3495.81/s  (0.316s, 3241.76/s)  LR: 3.507e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.058 (2.058)  Loss:  2.7988 (2.7988)  Acc@1: 42.6758 (42.6758)  Acc@5: 73.2422 (73.2422)
Test: [  48/48]  Time: 0.067 (0.239)  Loss:  2.2617 (3.7884)  Acc@1: 59.6698 (27.0100)  Acc@5: 75.4717 (50.6480)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-6.pth.tar', 27.010000063476564)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-5.pth.tar', 21.43999999145508)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-4.pth.tar', 16.44999999633789)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-3.pth.tar', 11.911999924316406)

Train: 7 [   0/1251 (  0%)]  Loss: 5.671 (5.67)  Time: 2.348s,  436.11/s  (2.348s,  436.11/s)  LR: 3.507e-04  Data: 2.119 (2.119)
Train: 7 [  50/1251 (  4%)]  Loss: 5.857 (5.76)  Time: 0.305s, 3352.64/s  (0.340s, 3015.37/s)  LR: 3.527e-04  Data: 0.023 (0.064)
Train: 7 [ 100/1251 (  8%)]  Loss: 5.773 (5.77)  Time: 0.310s, 3302.58/s  (0.325s, 3155.52/s)  LR: 3.547e-04  Data: 0.022 (0.044)
Train: 7 [ 150/1251 ( 12%)]  Loss: 5.657 (5.74)  Time: 0.312s, 3284.45/s  (0.320s, 3198.67/s)  LR: 3.567e-04  Data: 0.023 (0.037)
Train: 7 [ 200/1251 ( 16%)]  Loss: 5.752 (5.74)  Time: 0.312s, 3285.75/s  (0.318s, 3218.66/s)  LR: 3.587e-04  Data: 0.025 (0.034)
Train: 7 [ 250/1251 ( 20%)]  Loss: 5.705 (5.74)  Time: 0.310s, 3301.76/s  (0.317s, 3229.51/s)  LR: 3.607e-04  Data: 0.024 (0.032)
Train: 7 [ 300/1251 ( 24%)]  Loss: 5.873 (5.76)  Time: 0.315s, 3246.39/s  (0.316s, 3236.29/s)  LR: 3.627e-04  Data: 0.025 (0.030)
Train: 7 [ 350/1251 ( 28%)]  Loss: 5.412 (5.71)  Time: 0.324s, 3158.28/s  (0.316s, 3241.55/s)  LR: 3.647e-04  Data: 0.024 (0.029)
Train: 7 [ 400/1251 ( 32%)]  Loss: 5.597 (5.70)  Time: 0.315s, 3249.44/s  (0.316s, 3244.65/s)  LR: 3.667e-04  Data: 0.022 (0.028)
Train: 7 [ 450/1251 ( 36%)]  Loss: 5.480 (5.68)  Time: 0.313s, 3273.94/s  (0.315s, 3247.63/s)  LR: 3.687e-04  Data: 0.024 (0.028)
Train: 7 [ 500/1251 ( 40%)]  Loss: 5.655 (5.68)  Time: 0.319s, 3205.59/s  (0.315s, 3248.75/s)  LR: 3.707e-04  Data: 0.025 (0.027)
Train: 7 [ 550/1251 ( 44%)]  Loss: 5.656 (5.67)  Time: 0.308s, 3323.28/s  (0.315s, 3250.26/s)  LR: 3.727e-04  Data: 0.022 (0.027)
Train: 7 [ 600/1251 ( 48%)]  Loss: 5.584 (5.67)  Time: 0.324s, 3163.13/s  (0.315s, 3251.26/s)  LR: 3.746e-04  Data: 0.020 (0.027)
Train: 7 [ 650/1251 ( 52%)]  Loss: 5.485 (5.65)  Time: 0.314s, 3257.87/s  (0.315s, 3251.88/s)  LR: 3.766e-04  Data: 0.022 (0.026)
Train: 7 [ 700/1251 ( 56%)]  Loss: 5.549 (5.65)  Time: 0.321s, 3192.56/s  (0.315s, 3252.54/s)  LR: 3.786e-04  Data: 0.027 (0.026)
Train: 7 [ 750/1251 ( 60%)]  Loss: 5.729 (5.65)  Time: 0.316s, 3241.72/s  (0.315s, 3252.88/s)  LR: 3.806e-04  Data: 0.024 (0.026)
Train: 7 [ 800/1251 ( 64%)]  Loss: 5.704 (5.66)  Time: 0.327s, 3135.12/s  (0.315s, 3253.22/s)  LR: 3.826e-04  Data: 0.026 (0.026)
Train: 7 [ 850/1251 ( 68%)]  Loss: 5.579 (5.65)  Time: 0.312s, 3283.60/s  (0.315s, 3254.11/s)  LR: 3.846e-04  Data: 0.023 (0.026)
Train: 7 [ 900/1251 ( 72%)]  Loss: 5.499 (5.64)  Time: 0.315s, 3245.66/s  (0.315s, 3253.31/s)  LR: 3.866e-04  Data: 0.022 (0.025)
Train: 7 [ 950/1251 ( 76%)]  Loss: 5.431 (5.63)  Time: 0.316s, 3241.12/s  (0.315s, 3253.27/s)  LR: 3.886e-04  Data: 0.025 (0.025)
Train: 7 [1000/1251 ( 80%)]  Loss: 5.576 (5.63)  Time: 0.317s, 3233.40/s  (0.315s, 3253.81/s)  LR: 3.906e-04  Data: 0.024 (0.025)
Train: 7 [1050/1251 ( 84%)]  Loss: 5.567 (5.63)  Time: 0.310s, 3306.92/s  (0.315s, 3254.03/s)  LR: 3.926e-04  Data: 0.023 (0.025)
Train: 7 [1100/1251 ( 88%)]  Loss: 5.505 (5.62)  Time: 0.313s, 3267.39/s  (0.315s, 3254.69/s)  LR: 3.946e-04  Data: 0.024 (0.025)
Train: 7 [1150/1251 ( 92%)]  Loss: 5.575 (5.62)  Time: 0.311s, 3289.77/s  (0.315s, 3254.96/s)  LR: 3.966e-04  Data: 0.022 (0.025)
Train: 7 [1200/1251 ( 96%)]  Loss: 5.405 (5.61)  Time: 0.310s, 3304.57/s  (0.315s, 3255.20/s)  LR: 3.986e-04  Data: 0.022 (0.025)
Train: 7 [1250/1251 (100%)]  Loss: 5.834 (5.62)  Time: 0.285s, 3597.16/s  (0.314s, 3257.52/s)  LR: 4.006e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.164 (2.164)  Loss:  2.7656 (2.7656)  Acc@1: 42.6758 (42.6758)  Acc@5: 71.3867 (71.3867)
Test: [  48/48]  Time: 0.065 (0.236)  Loss:  2.2070 (3.5414)  Acc@1: 59.5519 (30.6280)  Acc@5: 77.2406 (55.2840)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-7.pth.tar', 30.628000012207032)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-6.pth.tar', 27.010000063476564)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-5.pth.tar', 21.43999999145508)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-4.pth.tar', 16.44999999633789)

Train: 8 [   0/1251 (  0%)]  Loss: 5.527 (5.53)  Time: 2.341s,  437.35/s  (2.341s,  437.35/s)  LR: 4.006e-04  Data: 2.105 (2.105)
Train: 8 [  50/1251 (  4%)]  Loss: 5.570 (5.55)  Time: 0.303s, 3374.86/s  (0.338s, 3033.92/s)  LR: 4.026e-04  Data: 0.025 (0.064)
Train: 8 [ 100/1251 (  8%)]  Loss: 5.652 (5.58)  Time: 0.311s, 3290.59/s  (0.323s, 3173.62/s)  LR: 4.046e-04  Data: 0.022 (0.044)
Train: 8 [ 150/1251 ( 12%)]  Loss: 5.696 (5.61)  Time: 0.310s, 3302.50/s  (0.319s, 3214.68/s)  LR: 4.066e-04  Data: 0.024 (0.037)
Train: 8 [ 200/1251 ( 16%)]  Loss: 5.313 (5.55)  Time: 0.314s, 3265.05/s  (0.317s, 3234.13/s)  LR: 4.086e-04  Data: 0.025 (0.033)
Train: 8 [ 250/1251 ( 20%)]  Loss: 5.345 (5.52)  Time: 0.315s, 3253.28/s  (0.315s, 3245.86/s)  LR: 4.106e-04  Data: 0.023 (0.031)
Train: 8 [ 300/1251 ( 24%)]  Loss: 5.760 (5.55)  Time: 0.312s, 3283.49/s  (0.315s, 3253.09/s)  LR: 4.126e-04  Data: 0.022 (0.030)
Train: 8 [ 350/1251 ( 28%)]  Loss: 5.540 (5.55)  Time: 0.310s, 3308.48/s  (0.314s, 3258.40/s)  LR: 4.146e-04  Data: 0.022 (0.029)
Train: 8 [ 400/1251 ( 32%)]  Loss: 5.677 (5.56)  Time: 0.309s, 3313.85/s  (0.314s, 3262.97/s)  LR: 4.166e-04  Data: 0.022 (0.028)
Train: 8 [ 450/1251 ( 36%)]  Loss: 5.565 (5.56)  Time: 0.307s, 3336.99/s  (0.313s, 3266.35/s)  LR: 4.186e-04  Data: 0.026 (0.028)
Train: 8 [ 500/1251 ( 40%)]  Loss: 5.462 (5.56)  Time: 0.309s, 3311.52/s  (0.313s, 3268.06/s)  LR: 4.206e-04  Data: 0.023 (0.027)
Train: 8 [ 550/1251 ( 44%)]  Loss: 5.802 (5.58)  Time: 0.308s, 3329.37/s  (0.313s, 3268.60/s)  LR: 4.226e-04  Data: 0.022 (0.027)
Train: 8 [ 600/1251 ( 48%)]  Loss: 5.384 (5.56)  Time: 0.307s, 3332.76/s  (0.313s, 3269.98/s)  LR: 4.246e-04  Data: 0.022 (0.027)
Train: 8 [ 650/1251 ( 52%)]  Loss: 5.654 (5.57)  Time: 0.313s, 3270.65/s  (0.313s, 3270.78/s)  LR: 4.266e-04  Data: 0.026 (0.026)
Train: 8 [ 700/1251 ( 56%)]  Loss: 5.544 (5.57)  Time: 0.311s, 3292.16/s  (0.313s, 3271.55/s)  LR: 4.286e-04  Data: 0.023 (0.026)
Train: 8 [ 750/1251 ( 60%)]  Loss: 5.755 (5.58)  Time: 0.310s, 3299.66/s  (0.313s, 3271.82/s)  LR: 4.306e-04  Data: 0.022 (0.026)
Train: 8 [ 800/1251 ( 64%)]  Loss: 5.192 (5.56)  Time: 0.308s, 3327.28/s  (0.313s, 3272.01/s)  LR: 4.326e-04  Data: 0.025 (0.026)
Train: 8 [ 850/1251 ( 68%)]  Loss: 5.684 (5.56)  Time: 0.321s, 3187.47/s  (0.313s, 3271.38/s)  LR: 4.346e-04  Data: 0.022 (0.026)
Train: 8 [ 900/1251 ( 72%)]  Loss: 5.268 (5.55)  Time: 0.310s, 3299.82/s  (0.313s, 3271.75/s)  LR: 4.366e-04  Data: 0.023 (0.026)
Train: 8 [ 950/1251 ( 76%)]  Loss: 5.513 (5.55)  Time: 0.316s, 3236.91/s  (0.313s, 3271.09/s)  LR: 4.386e-04  Data: 0.023 (0.025)
Train: 8 [1000/1251 ( 80%)]  Loss: 5.254 (5.53)  Time: 0.315s, 3246.12/s  (0.313s, 3271.68/s)  LR: 4.406e-04  Data: 0.022 (0.025)
Train: 8 [1050/1251 ( 84%)]  Loss: 5.331 (5.52)  Time: 0.318s, 3217.52/s  (0.313s, 3271.11/s)  LR: 4.426e-04  Data: 0.021 (0.025)
Train: 8 [1100/1251 ( 88%)]  Loss: 5.493 (5.52)  Time: 0.315s, 3254.84/s  (0.313s, 3270.84/s)  LR: 4.446e-04  Data: 0.023 (0.025)
Train: 8 [1150/1251 ( 92%)]  Loss: 5.535 (5.52)  Time: 0.325s, 3146.70/s  (0.313s, 3270.08/s)  LR: 4.466e-04  Data: 0.022 (0.025)
Train: 8 [1200/1251 ( 96%)]  Loss: 5.673 (5.53)  Time: 0.312s, 3284.52/s  (0.313s, 3269.64/s)  LR: 4.486e-04  Data: 0.026 (0.025)
Train: 8 [1250/1251 (100%)]  Loss: 5.585 (5.53)  Time: 0.285s, 3590.64/s  (0.313s, 3270.60/s)  LR: 4.506e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.073 (2.073)  Loss:  2.2617 (2.2617)  Acc@1: 54.9805 (54.9805)  Acc@5: 80.6641 (80.6641)
Test: [  48/48]  Time: 0.065 (0.239)  Loss:  1.8965 (3.2596)  Acc@1: 64.1509 (34.3960)  Acc@5: 80.5425 (59.8360)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-8.pth.tar', 34.39599994140625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-7.pth.tar', 30.628000012207032)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-6.pth.tar', 27.010000063476564)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-5.pth.tar', 21.43999999145508)

Train: 9 [   0/1251 (  0%)]  Loss: 5.336 (5.34)  Time: 2.207s,  463.98/s  (2.207s,  463.98/s)  LR: 4.506e-04  Data: 1.975 (1.975)
Train: 9 [  50/1251 (  4%)]  Loss: 5.207 (5.27)  Time: 0.308s, 3327.53/s  (0.339s, 3017.99/s)  LR: 4.526e-04  Data: 0.024 (0.063)
Train: 9 [ 100/1251 (  8%)]  Loss: 5.407 (5.32)  Time: 0.312s, 3278.37/s  (0.323s, 3168.86/s)  LR: 4.546e-04  Data: 0.024 (0.043)
Train: 9 [ 150/1251 ( 12%)]  Loss: 5.410 (5.34)  Time: 0.312s, 3278.18/s  (0.319s, 3211.97/s)  LR: 4.566e-04  Data: 0.023 (0.037)
Train: 9 [ 200/1251 ( 16%)]  Loss: 5.393 (5.35)  Time: 0.309s, 3314.43/s  (0.317s, 3234.95/s)  LR: 4.586e-04  Data: 0.026 (0.033)
Train: 9 [ 250/1251 ( 20%)]  Loss: 5.334 (5.35)  Time: 0.308s, 3323.24/s  (0.315s, 3245.70/s)  LR: 4.606e-04  Data: 0.022 (0.031)
Train: 9 [ 300/1251 ( 24%)]  Loss: 5.510 (5.37)  Time: 0.306s, 3351.74/s  (0.315s, 3253.81/s)  LR: 4.626e-04  Data: 0.020 (0.030)
Train: 9 [ 350/1251 ( 28%)]  Loss: 5.494 (5.39)  Time: 0.316s, 3240.21/s  (0.314s, 3259.00/s)  LR: 4.646e-04  Data: 0.021 (0.029)
Train: 9 [ 400/1251 ( 32%)]  Loss: 5.460 (5.39)  Time: 0.312s, 3286.48/s  (0.314s, 3263.10/s)  LR: 4.666e-04  Data: 0.026 (0.028)
Train: 9 [ 450/1251 ( 36%)]  Loss: 5.451 (5.40)  Time: 0.309s, 3313.44/s  (0.314s, 3265.25/s)  LR: 4.686e-04  Data: 0.027 (0.028)
Train: 9 [ 500/1251 ( 40%)]  Loss: 5.423 (5.40)  Time: 0.310s, 3302.72/s  (0.313s, 3266.45/s)  LR: 4.706e-04  Data: 0.020 (0.027)
Train: 9 [ 550/1251 ( 44%)]  Loss: 4.973 (5.37)  Time: 0.307s, 3337.50/s  (0.313s, 3267.63/s)  LR: 4.726e-04  Data: 0.023 (0.027)
Train: 9 [ 600/1251 ( 48%)]  Loss: 5.475 (5.37)  Time: 0.309s, 3309.89/s  (0.313s, 3268.61/s)  LR: 4.745e-04  Data: 0.020 (0.027)
Train: 9 [ 650/1251 ( 52%)]  Loss: 5.324 (5.37)  Time: 0.315s, 3251.44/s  (0.313s, 3268.80/s)  LR: 4.765e-04  Data: 0.023 (0.026)
Train: 9 [ 700/1251 ( 56%)]  Loss: 5.728 (5.40)  Time: 0.313s, 3271.21/s  (0.313s, 3269.62/s)  LR: 4.785e-04  Data: 0.024 (0.026)
Train: 9 [ 750/1251 ( 60%)]  Loss: 5.438 (5.40)  Time: 0.311s, 3292.95/s  (0.313s, 3269.83/s)  LR: 4.805e-04  Data: 0.022 (0.026)
Train: 9 [ 800/1251 ( 64%)]  Loss: 5.413 (5.40)  Time: 0.314s, 3264.69/s  (0.313s, 3269.39/s)  LR: 4.825e-04  Data: 0.024 (0.026)
Train: 9 [ 850/1251 ( 68%)]  Loss: 5.739 (5.42)  Time: 0.321s, 3189.46/s  (0.313s, 3268.87/s)  LR: 4.845e-04  Data: 0.023 (0.026)
Train: 9 [ 900/1251 ( 72%)]  Loss: 5.327 (5.41)  Time: 0.307s, 3330.44/s  (0.313s, 3268.66/s)  LR: 4.865e-04  Data: 0.022 (0.025)
Train: 9 [ 950/1251 ( 76%)]  Loss: 5.468 (5.42)  Time: 0.308s, 3329.11/s  (0.313s, 3268.60/s)  LR: 4.885e-04  Data: 0.022 (0.025)
Train: 9 [1000/1251 ( 80%)]  Loss: 4.937 (5.39)  Time: 0.311s, 3296.08/s  (0.313s, 3267.67/s)  LR: 4.905e-04  Data: 0.027 (0.025)
Train: 9 [1050/1251 ( 84%)]  Loss: 5.570 (5.40)  Time: 0.313s, 3270.35/s  (0.313s, 3267.03/s)  LR: 4.925e-04  Data: 0.024 (0.025)
Train: 9 [1100/1251 ( 88%)]  Loss: 5.219 (5.39)  Time: 0.311s, 3287.57/s  (0.313s, 3266.74/s)  LR: 4.945e-04  Data: 0.024 (0.025)
Train: 9 [1150/1251 ( 92%)]  Loss: 5.390 (5.39)  Time: 0.317s, 3233.59/s  (0.314s, 3266.25/s)  LR: 4.965e-04  Data: 0.021 (0.025)
Train: 9 [1200/1251 ( 96%)]  Loss: 5.264 (5.39)  Time: 0.313s, 3270.50/s  (0.314s, 3265.65/s)  LR: 4.985e-04  Data: 0.027 (0.025)
Train: 9 [1250/1251 (100%)]  Loss: 5.258 (5.38)  Time: 0.291s, 3520.64/s  (0.313s, 3267.23/s)  LR: 5.005e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.070 (2.070)  Loss:  2.0996 (2.0996)  Acc@1: 58.1055 (58.1055)  Acc@5: 81.3477 (81.3477)
Test: [  48/48]  Time: 0.049 (0.231)  Loss:  1.7314 (3.0550)  Acc@1: 64.3868 (37.8340)  Acc@5: 83.3726 (63.2780)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-9.pth.tar', 37.83400004394531)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-8.pth.tar', 34.39599994140625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-7.pth.tar', 30.628000012207032)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-6.pth.tar', 27.010000063476564)

Train: 10 [   0/1251 (  0%)]  Loss: 5.162 (5.16)  Time: 2.323s,  440.79/s  (2.323s,  440.79/s)  LR: 5.005e-04  Data: 2.095 (2.095)
Train: 10 [  50/1251 (  4%)]  Loss: 5.452 (5.31)  Time: 0.318s, 3224.43/s  (0.339s, 3020.04/s)  LR: 5.025e-04  Data: 0.022 (0.065)
Train: 10 [ 100/1251 (  8%)]  Loss: 5.217 (5.28)  Time: 0.305s, 3354.30/s  (0.324s, 3163.21/s)  LR: 5.045e-04  Data: 0.020 (0.044)
Train: 10 [ 150/1251 ( 12%)]  Loss: 5.434 (5.32)  Time: 0.313s, 3274.99/s  (0.319s, 3208.96/s)  LR: 5.065e-04  Data: 0.025 (0.037)
Train: 10 [ 200/1251 ( 16%)]  Loss: 5.189 (5.29)  Time: 0.311s, 3287.72/s  (0.317s, 3229.53/s)  LR: 5.085e-04  Data: 0.026 (0.034)
Train: 10 [ 250/1251 ( 20%)]  Loss: 5.470 (5.32)  Time: 0.309s, 3310.35/s  (0.316s, 3242.17/s)  LR: 5.105e-04  Data: 0.025 (0.032)
Train: 10 [ 300/1251 ( 24%)]  Loss: 5.517 (5.35)  Time: 0.310s, 3307.10/s  (0.315s, 3250.00/s)  LR: 5.125e-04  Data: 0.022 (0.030)
Train: 10 [ 350/1251 ( 28%)]  Loss: 5.094 (5.32)  Time: 0.313s, 3271.15/s  (0.315s, 3254.82/s)  LR: 5.145e-04  Data: 0.026 (0.029)
Train: 10 [ 400/1251 ( 32%)]  Loss: 5.564 (5.34)  Time: 0.319s, 3212.13/s  (0.314s, 3258.53/s)  LR: 5.165e-04  Data: 0.026 (0.029)
Train: 10 [ 450/1251 ( 36%)]  Loss: 5.466 (5.36)  Time: 0.317s, 3231.24/s  (0.314s, 3260.24/s)  LR: 5.185e-04  Data: 0.024 (0.028)
Train: 10 [ 500/1251 ( 40%)]  Loss: 5.300 (5.35)  Time: 0.311s, 3290.39/s  (0.314s, 3260.76/s)  LR: 5.205e-04  Data: 0.025 (0.028)
Train: 10 [ 550/1251 ( 44%)]  Loss: 5.205 (5.34)  Time: 0.308s, 3323.74/s  (0.314s, 3262.02/s)  LR: 5.225e-04  Data: 0.023 (0.027)
Train: 10 [ 600/1251 ( 48%)]  Loss: 5.239 (5.33)  Time: 0.317s, 3230.00/s  (0.314s, 3263.99/s)  LR: 5.245e-04  Data: 0.023 (0.027)
Train: 10 [ 650/1251 ( 52%)]  Loss: 4.763 (5.29)  Time: 0.314s, 3259.46/s  (0.314s, 3265.60/s)  LR: 5.265e-04  Data: 0.027 (0.026)
Train: 10 [ 700/1251 ( 56%)]  Loss: 5.384 (5.30)  Time: 0.320s, 3196.29/s  (0.313s, 3266.60/s)  LR: 5.285e-04  Data: 0.028 (0.026)
Train: 10 [ 750/1251 ( 60%)]  Loss: 5.241 (5.29)  Time: 0.317s, 3231.90/s  (0.313s, 3267.17/s)  LR: 5.305e-04  Data: 0.023 (0.026)
Train: 10 [ 800/1251 ( 64%)]  Loss: 5.132 (5.28)  Time: 0.311s, 3293.62/s  (0.313s, 3267.90/s)  LR: 5.325e-04  Data: 0.024 (0.026)
Train: 10 [ 850/1251 ( 68%)]  Loss: 5.145 (5.28)  Time: 0.317s, 3233.03/s  (0.313s, 3268.45/s)  LR: 5.345e-04  Data: 0.022 (0.026)
Train: 10 [ 900/1251 ( 72%)]  Loss: 5.536 (5.29)  Time: 0.318s, 3221.96/s  (0.313s, 3268.58/s)  LR: 5.365e-04  Data: 0.023 (0.026)
Train: 10 [ 950/1251 ( 76%)]  Loss: 5.570 (5.30)  Time: 0.317s, 3234.72/s  (0.313s, 3268.99/s)  LR: 5.385e-04  Data: 0.028 (0.025)
Train: 10 [1000/1251 ( 80%)]  Loss: 5.428 (5.31)  Time: 0.313s, 3269.21/s  (0.313s, 3269.91/s)  LR: 5.405e-04  Data: 0.023 (0.025)
Train: 10 [1050/1251 ( 84%)]  Loss: 5.000 (5.30)  Time: 0.311s, 3293.88/s  (0.313s, 3270.11/s)  LR: 5.425e-04  Data: 0.021 (0.025)
Train: 10 [1100/1251 ( 88%)]  Loss: 5.335 (5.30)  Time: 0.318s, 3219.57/s  (0.313s, 3269.73/s)  LR: 5.445e-04  Data: 0.025 (0.025)
Train: 10 [1150/1251 ( 92%)]  Loss: 5.190 (5.29)  Time: 0.309s, 3312.59/s  (0.313s, 3268.90/s)  LR: 5.465e-04  Data: 0.020 (0.025)
Train: 10 [1200/1251 ( 96%)]  Loss: 5.519 (5.30)  Time: 0.308s, 3320.70/s  (0.313s, 3268.54/s)  LR: 5.485e-04  Data: 0.020 (0.025)
Train: 10 [1250/1251 (100%)]  Loss: 5.079 (5.29)  Time: 0.287s, 3569.18/s  (0.313s, 3270.09/s)  LR: 5.505e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.026 (2.026)  Loss:  1.9395 (1.9395)  Acc@1: 58.9844 (58.9844)  Acc@5: 83.6914 (83.6914)
Test: [  48/48]  Time: 0.053 (0.234)  Loss:  1.7500 (2.9093)  Acc@1: 64.7406 (39.9860)  Acc@5: 82.4292 (65.5620)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-10.pth.tar', 39.985999938964845)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-9.pth.tar', 37.83400004394531)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-8.pth.tar', 34.39599994140625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-7.pth.tar', 30.628000012207032)

Train: 11 [   0/1251 (  0%)]  Loss: 4.979 (4.98)  Time: 2.852s,  359.02/s  (2.852s,  359.02/s)  LR: 5.505e-04  Data: 2.643 (2.643)
Train: 11 [  50/1251 (  4%)]  Loss: 4.744 (4.86)  Time: 0.301s, 3399.30/s  (0.340s, 3015.49/s)  LR: 5.525e-04  Data: 0.022 (0.074)
Train: 11 [ 100/1251 (  8%)]  Loss: 5.227 (4.98)  Time: 0.308s, 3320.80/s  (0.324s, 3163.90/s)  LR: 5.545e-04  Data: 0.025 (0.049)
Train: 11 [ 150/1251 ( 12%)]  Loss: 5.083 (5.01)  Time: 0.313s, 3272.48/s  (0.319s, 3209.41/s)  LR: 5.565e-04  Data: 0.023 (0.041)
Train: 11 [ 200/1251 ( 16%)]  Loss: 4.976 (5.00)  Time: 0.311s, 3291.85/s  (0.317s, 3230.54/s)  LR: 5.585e-04  Data: 0.027 (0.036)
Train: 11 [ 250/1251 ( 20%)]  Loss: 5.643 (5.11)  Time: 0.308s, 3322.91/s  (0.316s, 3241.17/s)  LR: 5.605e-04  Data: 0.024 (0.034)
Train: 11 [ 300/1251 ( 24%)]  Loss: 5.498 (5.16)  Time: 0.317s, 3226.43/s  (0.315s, 3248.85/s)  LR: 5.625e-04  Data: 0.022 (0.032)
Train: 11 [ 350/1251 ( 28%)]  Loss: 5.147 (5.16)  Time: 0.310s, 3301.67/s  (0.315s, 3253.39/s)  LR: 5.645e-04  Data: 0.024 (0.031)
Train: 11 [ 400/1251 ( 32%)]  Loss: 5.087 (5.15)  Time: 0.316s, 3245.42/s  (0.314s, 3257.10/s)  LR: 5.665e-04  Data: 0.022 (0.030)
Train: 11 [ 450/1251 ( 36%)]  Loss: 5.199 (5.16)  Time: 0.312s, 3284.45/s  (0.314s, 3258.53/s)  LR: 5.685e-04  Data: 0.026 (0.029)
Train: 11 [ 500/1251 ( 40%)]  Loss: 5.426 (5.18)  Time: 0.313s, 3273.94/s  (0.314s, 3259.97/s)  LR: 5.705e-04  Data: 0.022 (0.028)
Train: 11 [ 550/1251 ( 44%)]  Loss: 5.488 (5.21)  Time: 0.317s, 3232.01/s  (0.314s, 3262.07/s)  LR: 5.725e-04  Data: 0.020 (0.028)
Train: 11 [ 600/1251 ( 48%)]  Loss: 5.354 (5.22)  Time: 0.310s, 3306.43/s  (0.314s, 3263.30/s)  LR: 5.744e-04  Data: 0.026 (0.028)
Train: 11 [ 650/1251 ( 52%)]  Loss: 5.118 (5.21)  Time: 0.312s, 3282.81/s  (0.314s, 3264.00/s)  LR: 5.764e-04  Data: 0.022 (0.027)
Train: 11 [ 700/1251 ( 56%)]  Loss: 5.005 (5.20)  Time: 0.314s, 3258.20/s  (0.314s, 3263.95/s)  LR: 5.784e-04  Data: 0.023 (0.027)
Train: 11 [ 750/1251 ( 60%)]  Loss: 5.309 (5.21)  Time: 0.317s, 3231.97/s  (0.314s, 3264.01/s)  LR: 5.804e-04  Data: 0.023 (0.027)
Train: 11 [ 800/1251 ( 64%)]  Loss: 5.346 (5.21)  Time: 0.312s, 3279.29/s  (0.314s, 3264.35/s)  LR: 5.824e-04  Data: 0.026 (0.027)
Train: 11 [ 850/1251 ( 68%)]  Loss: 5.264 (5.22)  Time: 0.315s, 3255.04/s  (0.314s, 3264.65/s)  LR: 5.844e-04  Data: 0.022 (0.026)
Train: 11 [ 900/1251 ( 72%)]  Loss: 4.967 (5.20)  Time: 0.322s, 3176.62/s  (0.314s, 3264.76/s)  LR: 5.864e-04  Data: 0.024 (0.026)
Train: 11 [ 950/1251 ( 76%)]  Loss: 5.310 (5.21)  Time: 0.317s, 3225.78/s  (0.314s, 3264.45/s)  LR: 5.884e-04  Data: 0.022 (0.026)
Train: 11 [1000/1251 ( 80%)]  Loss: 5.238 (5.21)  Time: 0.314s, 3261.08/s  (0.314s, 3264.31/s)  LR: 5.904e-04  Data: 0.024 (0.026)
Train: 11 [1050/1251 ( 84%)]  Loss: 4.970 (5.20)  Time: 0.312s, 3282.90/s  (0.314s, 3264.71/s)  LR: 5.924e-04  Data: 0.022 (0.026)
Train: 11 [1100/1251 ( 88%)]  Loss: 5.351 (5.21)  Time: 0.317s, 3235.03/s  (0.314s, 3264.45/s)  LR: 5.944e-04  Data: 0.021 (0.026)
Train: 11 [1150/1251 ( 92%)]  Loss: 5.361 (5.21)  Time: 0.313s, 3267.63/s  (0.314s, 3264.46/s)  LR: 5.964e-04  Data: 0.023 (0.025)
Train: 11 [1200/1251 ( 96%)]  Loss: 4.832 (5.20)  Time: 0.317s, 3232.65/s  (0.314s, 3264.10/s)  LR: 5.984e-04  Data: 0.023 (0.025)
Train: 11 [1250/1251 (100%)]  Loss: 5.297 (5.20)  Time: 0.288s, 3550.39/s  (0.314s, 3266.02/s)  LR: 6.004e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.019 (2.019)  Loss:  1.7988 (1.7988)  Acc@1: 63.9648 (63.9648)  Acc@5: 84.4727 (84.4727)
Test: [  48/48]  Time: 0.063 (0.234)  Loss:  1.6738 (2.8100)  Acc@1: 67.0991 (42.1520)  Acc@5: 84.4340 (67.5980)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-11.pth.tar', 42.15200005859375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-10.pth.tar', 39.985999938964845)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-9.pth.tar', 37.83400004394531)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-8.pth.tar', 34.39599994140625)

Train: 12 [   0/1251 (  0%)]  Loss: 5.159 (5.16)  Time: 2.518s,  406.73/s  (2.518s,  406.73/s)  LR: 6.004e-04  Data: 2.306 (2.306)
Train: 12 [  50/1251 (  4%)]  Loss: 5.158 (5.16)  Time: 0.304s, 3365.68/s  (0.339s, 3017.14/s)  LR: 6.024e-04  Data: 0.025 (0.068)
Train: 12 [ 100/1251 (  8%)]  Loss: 4.893 (5.07)  Time: 0.305s, 3354.89/s  (0.323s, 3168.21/s)  LR: 6.044e-04  Data: 0.025 (0.046)
Train: 12 [ 150/1251 ( 12%)]  Loss: 5.297 (5.13)  Time: 0.313s, 3267.71/s  (0.319s, 3213.22/s)  LR: 6.064e-04  Data: 0.026 (0.039)
Train: 12 [ 200/1251 ( 16%)]  Loss: 5.378 (5.18)  Time: 0.310s, 3305.86/s  (0.317s, 3235.12/s)  LR: 6.084e-04  Data: 0.022 (0.035)
Train: 12 [ 250/1251 ( 20%)]  Loss: 4.905 (5.13)  Time: 0.310s, 3302.26/s  (0.315s, 3247.65/s)  LR: 6.104e-04  Data: 0.023 (0.032)
Train: 12 [ 300/1251 ( 24%)]  Loss: 5.102 (5.13)  Time: 0.306s, 3342.16/s  (0.315s, 3255.59/s)  LR: 6.124e-04  Data: 0.022 (0.031)
Train: 12 [ 350/1251 ( 28%)]  Loss: 5.477 (5.17)  Time: 0.312s, 3282.67/s  (0.314s, 3260.18/s)  LR: 6.144e-04  Data: 0.022 (0.030)
Train: 12 [ 400/1251 ( 32%)]  Loss: 5.028 (5.16)  Time: 0.306s, 3345.26/s  (0.314s, 3263.40/s)  LR: 6.164e-04  Data: 0.020 (0.029)
Train: 12 [ 450/1251 ( 36%)]  Loss: 4.880 (5.13)  Time: 0.310s, 3303.51/s  (0.314s, 3266.04/s)  LR: 6.184e-04  Data: 0.024 (0.028)
Train: 12 [ 500/1251 ( 40%)]  Loss: 5.428 (5.15)  Time: 0.314s, 3256.69/s  (0.313s, 3267.45/s)  LR: 6.204e-04  Data: 0.024 (0.028)
Train: 12 [ 550/1251 ( 44%)]  Loss: 5.530 (5.19)  Time: 0.308s, 3322.13/s  (0.313s, 3269.27/s)  LR: 6.224e-04  Data: 0.026 (0.027)
Train: 12 [ 600/1251 ( 48%)]  Loss: 4.935 (5.17)  Time: 0.311s, 3290.57/s  (0.313s, 3269.67/s)  LR: 6.244e-04  Data: 0.022 (0.027)
Train: 12 [ 650/1251 ( 52%)]  Loss: 4.973 (5.15)  Time: 0.314s, 3264.98/s  (0.313s, 3270.41/s)  LR: 6.264e-04  Data: 0.025 (0.027)
Train: 12 [ 700/1251 ( 56%)]  Loss: 5.479 (5.17)  Time: 0.314s, 3264.96/s  (0.313s, 3270.30/s)  LR: 6.284e-04  Data: 0.020 (0.027)
Train: 12 [ 750/1251 ( 60%)]  Loss: 5.101 (5.17)  Time: 0.310s, 3298.68/s  (0.313s, 3269.94/s)  LR: 6.304e-04  Data: 0.022 (0.026)
Train: 12 [ 800/1251 ( 64%)]  Loss: 4.941 (5.16)  Time: 0.324s, 3158.86/s  (0.313s, 3269.87/s)  LR: 6.324e-04  Data: 0.019 (0.026)
Train: 12 [ 850/1251 ( 68%)]  Loss: 4.974 (5.15)  Time: 0.307s, 3332.62/s  (0.313s, 3270.20/s)  LR: 6.344e-04  Data: 0.022 (0.026)
Train: 12 [ 900/1251 ( 72%)]  Loss: 5.391 (5.16)  Time: 0.313s, 3274.68/s  (0.313s, 3269.94/s)  LR: 6.364e-04  Data: 0.023 (0.026)
Train: 12 [ 950/1251 ( 76%)]  Loss: 5.078 (5.16)  Time: 0.311s, 3292.96/s  (0.313s, 3269.92/s)  LR: 6.384e-04  Data: 0.022 (0.026)
Train: 12 [1000/1251 ( 80%)]  Loss: 5.093 (5.15)  Time: 0.309s, 3313.51/s  (0.313s, 3270.24/s)  LR: 6.404e-04  Data: 0.024 (0.026)
Train: 12 [1050/1251 ( 84%)]  Loss: 4.977 (5.14)  Time: 0.313s, 3271.70/s  (0.313s, 3269.69/s)  LR: 6.424e-04  Data: 0.021 (0.025)
Train: 12 [1100/1251 ( 88%)]  Loss: 4.917 (5.13)  Time: 0.314s, 3263.23/s  (0.313s, 3269.64/s)  LR: 6.444e-04  Data: 0.024 (0.025)
Train: 12 [1150/1251 ( 92%)]  Loss: 5.109 (5.13)  Time: 0.316s, 3244.50/s  (0.313s, 3269.39/s)  LR: 6.464e-04  Data: 0.026 (0.025)
Train: 12 [1200/1251 ( 96%)]  Loss: 5.068 (5.13)  Time: 0.318s, 3218.71/s  (0.313s, 3269.20/s)  LR: 6.484e-04  Data: 0.024 (0.025)
Train: 12 [1250/1251 (100%)]  Loss: 4.806 (5.12)  Time: 0.279s, 3670.83/s  (0.313s, 3271.40/s)  LR: 6.504e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.008 (2.008)  Loss:  1.5596 (1.5596)  Acc@1: 66.7969 (66.7969)  Acc@5: 86.9141 (86.9141)
Test: [  48/48]  Time: 0.059 (0.231)  Loss:  1.5166 (2.6251)  Acc@1: 68.1604 (44.7600)  Acc@5: 86.0849 (70.4420)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-12.pth.tar', 44.76000013183594)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-11.pth.tar', 42.15200005859375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-10.pth.tar', 39.985999938964845)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-9.pth.tar', 37.83400004394531)

Train: 13 [   0/1251 (  0%)]  Loss: 5.550 (5.55)  Time: 2.023s,  506.08/s  (2.023s,  506.08/s)  LR: 6.504e-04  Data: 1.787 (1.787)
Train: 13 [  50/1251 (  4%)]  Loss: 5.145 (5.35)  Time: 0.308s, 3321.48/s  (0.335s, 3052.75/s)  LR: 6.524e-04  Data: 0.027 (0.061)
Train: 13 [ 100/1251 (  8%)]  Loss: 5.299 (5.33)  Time: 0.309s, 3315.87/s  (0.322s, 3181.00/s)  LR: 6.544e-04  Data: 0.024 (0.042)
Train: 13 [ 150/1251 ( 12%)]  Loss: 5.126 (5.28)  Time: 0.308s, 3320.48/s  (0.318s, 3221.45/s)  LR: 6.564e-04  Data: 0.024 (0.036)
Train: 13 [ 200/1251 ( 16%)]  Loss: 5.091 (5.24)  Time: 0.311s, 3294.37/s  (0.316s, 3238.95/s)  LR: 6.584e-04  Data: 0.026 (0.032)
Train: 13 [ 250/1251 ( 20%)]  Loss: 5.383 (5.27)  Time: 0.312s, 3284.11/s  (0.315s, 3247.93/s)  LR: 6.604e-04  Data: 0.023 (0.031)
Train: 13 [ 300/1251 ( 24%)]  Loss: 4.865 (5.21)  Time: 0.314s, 3261.06/s  (0.315s, 3253.41/s)  LR: 6.624e-04  Data: 0.024 (0.029)
Train: 13 [ 350/1251 ( 28%)]  Loss: 5.125 (5.20)  Time: 0.314s, 3262.20/s  (0.315s, 3255.11/s)  LR: 6.644e-04  Data: 0.022 (0.028)
Train: 13 [ 400/1251 ( 32%)]  Loss: 5.353 (5.22)  Time: 0.319s, 3214.32/s  (0.314s, 3257.57/s)  LR: 6.664e-04  Data: 0.021 (0.028)
Train: 13 [ 450/1251 ( 36%)]  Loss: 4.846 (5.18)  Time: 0.314s, 3264.26/s  (0.314s, 3258.79/s)  LR: 6.684e-04  Data: 0.022 (0.027)
Train: 13 [ 500/1251 ( 40%)]  Loss: 5.098 (5.17)  Time: 0.321s, 3189.89/s  (0.314s, 3259.55/s)  LR: 6.704e-04  Data: 0.026 (0.027)
Train: 13 [ 550/1251 ( 44%)]  Loss: 4.806 (5.14)  Time: 0.318s, 3215.89/s  (0.314s, 3260.40/s)  LR: 6.724e-04  Data: 0.025 (0.026)
Train: 13 [ 600/1251 ( 48%)]  Loss: 5.032 (5.13)  Time: 0.311s, 3289.21/s  (0.314s, 3261.19/s)  LR: 6.743e-04  Data: 0.023 (0.026)
Train: 13 [ 650/1251 ( 52%)]  Loss: 5.084 (5.13)  Time: 0.313s, 3268.10/s  (0.314s, 3262.49/s)  LR: 6.763e-04  Data: 0.024 (0.026)
Train: 13 [ 700/1251 ( 56%)]  Loss: 4.776 (5.11)  Time: 0.310s, 3300.05/s  (0.314s, 3263.43/s)  LR: 6.783e-04  Data: 0.021 (0.026)
Train: 13 [ 750/1251 ( 60%)]  Loss: 4.931 (5.09)  Time: 0.311s, 3290.01/s  (0.314s, 3263.76/s)  LR: 6.803e-04  Data: 0.023 (0.026)
Train: 13 [ 800/1251 ( 64%)]  Loss: 4.880 (5.08)  Time: 0.313s, 3272.96/s  (0.314s, 3264.24/s)  LR: 6.823e-04  Data: 0.023 (0.025)
Train: 13 [ 850/1251 ( 68%)]  Loss: 4.680 (5.06)  Time: 0.317s, 3230.08/s  (0.314s, 3263.70/s)  LR: 6.843e-04  Data: 0.023 (0.025)
Train: 13 [ 900/1251 ( 72%)]  Loss: 5.121 (5.06)  Time: 0.314s, 3261.57/s  (0.314s, 3263.47/s)  LR: 6.863e-04  Data: 0.022 (0.025)
Train: 13 [ 950/1251 ( 76%)]  Loss: 5.290 (5.07)  Time: 0.316s, 3244.04/s  (0.314s, 3263.41/s)  LR: 6.883e-04  Data: 0.021 (0.025)
Train: 13 [1000/1251 ( 80%)]  Loss: 5.063 (5.07)  Time: 0.320s, 3197.57/s  (0.314s, 3263.46/s)  LR: 6.903e-04  Data: 0.023 (0.025)
Train: 13 [1050/1251 ( 84%)]  Loss: 5.273 (5.08)  Time: 0.308s, 3320.49/s  (0.314s, 3263.34/s)  LR: 6.923e-04  Data: 0.023 (0.025)
Train: 13 [1100/1251 ( 88%)]  Loss: 4.988 (5.08)  Time: 0.310s, 3305.21/s  (0.314s, 3263.64/s)  LR: 6.943e-04  Data: 0.025 (0.025)
Train: 13 [1150/1251 ( 92%)]  Loss: 4.720 (5.06)  Time: 0.311s, 3297.45/s  (0.314s, 3263.62/s)  LR: 6.963e-04  Data: 0.023 (0.025)
Train: 13 [1200/1251 ( 96%)]  Loss: 5.397 (5.08)  Time: 0.321s, 3193.78/s  (0.314s, 3263.89/s)  LR: 6.983e-04  Data: 0.024 (0.025)
Train: 13 [1250/1251 (100%)]  Loss: 4.787 (5.07)  Time: 0.284s, 3604.76/s  (0.313s, 3266.55/s)  LR: 7.003e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.019 (2.019)  Loss:  1.5234 (1.5234)  Acc@1: 67.9688 (67.9688)  Acc@5: 86.4258 (86.4258)
Test: [  48/48]  Time: 0.044 (0.236)  Loss:  1.5752 (2.5294)  Acc@1: 67.6887 (46.0960)  Acc@5: 84.3160 (71.5700)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-13.pth.tar', 46.09599992675781)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-12.pth.tar', 44.76000013183594)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-11.pth.tar', 42.15200005859375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-10.pth.tar', 39.985999938964845)

Train: 14 [   0/1251 (  0%)]  Loss: 5.222 (5.22)  Time: 2.402s,  426.32/s  (2.402s,  426.32/s)  LR: 7.003e-04  Data: 2.179 (2.179)
Train: 14 [  50/1251 (  4%)]  Loss: 4.833 (5.03)  Time: 0.306s, 3348.11/s  (0.341s, 3006.89/s)  LR: 7.023e-04  Data: 0.023 (0.066)
Train: 14 [ 100/1251 (  8%)]  Loss: 5.116 (5.06)  Time: 0.310s, 3301.36/s  (0.324s, 3159.40/s)  LR: 7.043e-04  Data: 0.024 (0.045)
Train: 14 [ 150/1251 ( 12%)]  Loss: 4.925 (5.02)  Time: 0.312s, 3283.76/s  (0.319s, 3208.54/s)  LR: 7.063e-04  Data: 0.028 (0.038)
Train: 14 [ 200/1251 ( 16%)]  Loss: 5.073 (5.03)  Time: 0.308s, 3324.12/s  (0.317s, 3229.28/s)  LR: 7.083e-04  Data: 0.027 (0.034)
Train: 14 [ 250/1251 ( 20%)]  Loss: 4.917 (5.01)  Time: 0.307s, 3335.04/s  (0.316s, 3242.13/s)  LR: 7.103e-04  Data: 0.021 (0.032)
Train: 14 [ 300/1251 ( 24%)]  Loss: 5.244 (5.05)  Time: 0.311s, 3296.95/s  (0.315s, 3249.43/s)  LR: 7.123e-04  Data: 0.025 (0.030)
Train: 14 [ 350/1251 ( 28%)]  Loss: 5.144 (5.06)  Time: 0.309s, 3311.38/s  (0.315s, 3255.05/s)  LR: 7.143e-04  Data: 0.023 (0.029)
Train: 14 [ 400/1251 ( 32%)]  Loss: 4.818 (5.03)  Time: 0.308s, 3322.38/s  (0.314s, 3258.22/s)  LR: 7.163e-04  Data: 0.021 (0.029)
Train: 14 [ 450/1251 ( 36%)]  Loss: 4.818 (5.01)  Time: 0.313s, 3271.27/s  (0.314s, 3259.90/s)  LR: 7.183e-04  Data: 0.023 (0.028)
Train: 14 [ 500/1251 ( 40%)]  Loss: 4.646 (4.98)  Time: 0.311s, 3297.11/s  (0.314s, 3262.05/s)  LR: 7.203e-04  Data: 0.026 (0.028)
Train: 14 [ 550/1251 ( 44%)]  Loss: 4.686 (4.95)  Time: 0.312s, 3280.27/s  (0.314s, 3262.61/s)  LR: 7.223e-04  Data: 0.024 (0.027)
Train: 14 [ 600/1251 ( 48%)]  Loss: 5.115 (4.97)  Time: 0.317s, 3230.79/s  (0.314s, 3262.97/s)  LR: 7.243e-04  Data: 0.026 (0.027)
Train: 14 [ 650/1251 ( 52%)]  Loss: 5.212 (4.98)  Time: 0.313s, 3270.70/s  (0.314s, 3263.82/s)  LR: 7.263e-04  Data: 0.024 (0.026)
Train: 14 [ 700/1251 ( 56%)]  Loss: 5.077 (4.99)  Time: 0.316s, 3239.40/s  (0.314s, 3264.45/s)  LR: 7.283e-04  Data: 0.023 (0.026)
Train: 14 [ 750/1251 ( 60%)]  Loss: 4.657 (4.97)  Time: 0.309s, 3314.14/s  (0.314s, 3264.41/s)  LR: 7.303e-04  Data: 0.023 (0.026)
Train: 14 [ 800/1251 ( 64%)]  Loss: 4.918 (4.97)  Time: 0.311s, 3289.83/s  (0.314s, 3263.88/s)  LR: 7.323e-04  Data: 0.024 (0.026)
Train: 14 [ 850/1251 ( 68%)]  Loss: 5.061 (4.97)  Time: 0.318s, 3224.39/s  (0.314s, 3263.42/s)  LR: 7.343e-04  Data: 0.023 (0.026)
Train: 14 [ 900/1251 ( 72%)]  Loss: 5.116 (4.98)  Time: 0.314s, 3261.21/s  (0.314s, 3262.83/s)  LR: 7.363e-04  Data: 0.021 (0.025)
Train: 14 [ 950/1251 ( 76%)]  Loss: 5.290 (4.99)  Time: 0.315s, 3247.98/s  (0.314s, 3262.55/s)  LR: 7.383e-04  Data: 0.021 (0.025)
Train: 14 [1000/1251 ( 80%)]  Loss: 5.361 (5.01)  Time: 0.314s, 3260.38/s  (0.314s, 3262.61/s)  LR: 7.403e-04  Data: 0.025 (0.025)
Train: 14 [1050/1251 ( 84%)]  Loss: 4.919 (5.01)  Time: 0.313s, 3267.41/s  (0.314s, 3262.39/s)  LR: 7.423e-04  Data: 0.023 (0.025)
Train: 14 [1100/1251 ( 88%)]  Loss: 4.962 (5.01)  Time: 0.320s, 3204.62/s  (0.314s, 3261.92/s)  LR: 7.443e-04  Data: 0.022 (0.025)
Train: 14 [1150/1251 ( 92%)]  Loss: 5.038 (5.01)  Time: 0.316s, 3243.98/s  (0.314s, 3261.82/s)  LR: 7.463e-04  Data: 0.021 (0.025)
Train: 14 [1200/1251 ( 96%)]  Loss: 4.837 (5.00)  Time: 0.316s, 3245.55/s  (0.314s, 3261.47/s)  LR: 7.483e-04  Data: 0.022 (0.025)
Train: 14 [1250/1251 (100%)]  Loss: 5.309 (5.01)  Time: 0.288s, 3559.11/s  (0.314s, 3263.51/s)  LR: 7.503e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.063 (2.063)  Loss:  1.5127 (1.5127)  Acc@1: 68.0664 (68.0664)  Acc@5: 87.5977 (87.5977)
Test: [  48/48]  Time: 0.066 (0.236)  Loss:  1.5068 (2.4632)  Acc@1: 68.3962 (48.0820)  Acc@5: 87.1462 (73.2620)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-14.pth.tar', 48.08199997558594)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-13.pth.tar', 46.09599992675781)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-12.pth.tar', 44.76000013183594)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-11.pth.tar', 42.15200005859375)

Train: 15 [   0/1251 (  0%)]  Loss: 5.098 (5.10)  Time: 2.105s,  486.40/s  (2.105s,  486.40/s)  LR: 7.503e-04  Data: 1.871 (1.871)
Train: 15 [  50/1251 (  4%)]  Loss: 4.844 (4.97)  Time: 0.305s, 3360.44/s  (0.335s, 3053.13/s)  LR: 7.523e-04  Data: 0.024 (0.061)
Train: 15 [ 100/1251 (  8%)]  Loss: 4.847 (4.93)  Time: 0.316s, 3243.85/s  (0.322s, 3180.92/s)  LR: 7.543e-04  Data: 0.025 (0.043)
Train: 15 [ 150/1251 ( 12%)]  Loss: 5.386 (5.04)  Time: 0.313s, 3275.81/s  (0.318s, 3217.23/s)  LR: 7.563e-04  Data: 0.023 (0.036)
Train: 15 [ 200/1251 ( 16%)]  Loss: 5.096 (5.05)  Time: 0.310s, 3297.94/s  (0.316s, 3236.24/s)  LR: 7.583e-04  Data: 0.027 (0.033)
Train: 15 [ 250/1251 ( 20%)]  Loss: 4.536 (4.97)  Time: 0.315s, 3247.88/s  (0.315s, 3245.80/s)  LR: 7.603e-04  Data: 0.022 (0.031)
Train: 15 [ 300/1251 ( 24%)]  Loss: 4.901 (4.96)  Time: 0.318s, 3219.96/s  (0.315s, 3251.75/s)  LR: 7.623e-04  Data: 0.025 (0.030)
Train: 15 [ 350/1251 ( 28%)]  Loss: 4.837 (4.94)  Time: 0.312s, 3279.21/s  (0.314s, 3256.36/s)  LR: 7.643e-04  Data: 0.022 (0.029)
Train: 15 [ 400/1251 ( 32%)]  Loss: 4.784 (4.93)  Time: 0.307s, 3330.38/s  (0.314s, 3258.92/s)  LR: 7.663e-04  Data: 0.026 (0.028)
Train: 15 [ 450/1251 ( 36%)]  Loss: 4.992 (4.93)  Time: 0.319s, 3214.02/s  (0.314s, 3260.64/s)  LR: 7.683e-04  Data: 0.024 (0.028)
Train: 15 [ 500/1251 ( 40%)]  Loss: 4.978 (4.94)  Time: 0.323s, 3170.64/s  (0.314s, 3260.92/s)  LR: 7.703e-04  Data: 0.026 (0.027)
Train: 15 [ 550/1251 ( 44%)]  Loss: 5.047 (4.95)  Time: 0.310s, 3299.78/s  (0.314s, 3261.31/s)  LR: 7.723e-04  Data: 0.021 (0.027)
Train: 15 [ 600/1251 ( 48%)]  Loss: 4.885 (4.94)  Time: 0.308s, 3328.66/s  (0.314s, 3261.64/s)  LR: 7.742e-04  Data: 0.023 (0.027)
Train: 15 [ 650/1251 ( 52%)]  Loss: 5.122 (4.95)  Time: 0.320s, 3203.97/s  (0.314s, 3261.30/s)  LR: 7.762e-04  Data: 0.026 (0.026)
Train: 15 [ 700/1251 ( 56%)]  Loss: 5.331 (4.98)  Time: 0.317s, 3227.70/s  (0.314s, 3261.30/s)  LR: 7.782e-04  Data: 0.022 (0.026)
Train: 15 [ 750/1251 ( 60%)]  Loss: 5.206 (4.99)  Time: 0.314s, 3258.17/s  (0.314s, 3260.36/s)  LR: 7.802e-04  Data: 0.025 (0.026)
Train: 15 [ 800/1251 ( 64%)]  Loss: 5.123 (5.00)  Time: 0.307s, 3333.83/s  (0.314s, 3260.10/s)  LR: 7.822e-04  Data: 0.025 (0.026)
Train: 15 [ 850/1251 ( 68%)]  Loss: 4.281 (4.96)  Time: 0.309s, 3314.39/s  (0.314s, 3260.21/s)  LR: 7.842e-04  Data: 0.024 (0.026)
Train: 15 [ 900/1251 ( 72%)]  Loss: 4.849 (4.95)  Time: 0.317s, 3233.45/s  (0.314s, 3259.94/s)  LR: 7.862e-04  Data: 0.023 (0.025)
Train: 15 [ 950/1251 ( 76%)]  Loss: 5.104 (4.96)  Time: 0.317s, 3228.54/s  (0.314s, 3259.41/s)  LR: 7.882e-04  Data: 0.025 (0.025)
Train: 15 [1000/1251 ( 80%)]  Loss: 5.084 (4.97)  Time: 0.318s, 3215.59/s  (0.314s, 3259.01/s)  LR: 7.902e-04  Data: 0.021 (0.025)
Train: 15 [1050/1251 ( 84%)]  Loss: 4.867 (4.96)  Time: 0.316s, 3238.39/s  (0.314s, 3258.43/s)  LR: 7.922e-04  Data: 0.025 (0.025)
Train: 15 [1100/1251 ( 88%)]  Loss: 4.802 (4.96)  Time: 0.315s, 3248.20/s  (0.314s, 3258.04/s)  LR: 7.942e-04  Data: 0.023 (0.025)
Train: 15 [1150/1251 ( 92%)]  Loss: 4.602 (4.94)  Time: 0.320s, 3196.75/s  (0.314s, 3256.54/s)  LR: 7.962e-04  Data: 0.021 (0.025)
Train: 15 [1200/1251 ( 96%)]  Loss: 5.049 (4.95)  Time: 0.317s, 3225.21/s  (0.315s, 3255.61/s)  LR: 7.982e-04  Data: 0.023 (0.025)
Train: 15 [1250/1251 (100%)]  Loss: 5.056 (4.95)  Time: 0.294s, 3484.88/s  (0.314s, 3256.19/s)  LR: 8.002e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.225 (2.225)  Loss:  1.4873 (1.4873)  Acc@1: 68.7500 (68.7500)  Acc@5: 87.3047 (87.3047)
Test: [  48/48]  Time: 0.070 (0.239)  Loss:  1.4023 (2.4151)  Acc@1: 71.8160 (49.0440)  Acc@5: 86.6745 (74.1220)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-15.pth.tar', 49.04400016845703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-14.pth.tar', 48.08199997558594)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-13.pth.tar', 46.09599992675781)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-12.pth.tar', 44.76000013183594)

Train: 16 [   0/1251 (  0%)]  Loss: 4.927 (4.93)  Time: 2.218s,  461.72/s  (2.218s,  461.72/s)  LR: 8.002e-04  Data: 1.979 (1.979)
Train: 16 [  50/1251 (  4%)]  Loss: 4.842 (4.88)  Time: 0.310s, 3301.81/s  (0.339s, 3019.36/s)  LR: 8.022e-04  Data: 0.023 (0.063)
Train: 16 [ 100/1251 (  8%)]  Loss: 4.959 (4.91)  Time: 0.312s, 3286.26/s  (0.324s, 3155.87/s)  LR: 8.042e-04  Data: 0.024 (0.043)
Train: 16 [ 150/1251 ( 12%)]  Loss: 5.129 (4.96)  Time: 0.308s, 3326.39/s  (0.320s, 3200.42/s)  LR: 8.062e-04  Data: 0.021 (0.036)
Train: 16 [ 200/1251 ( 16%)]  Loss: 4.808 (4.93)  Time: 0.314s, 3263.84/s  (0.318s, 3218.61/s)  LR: 8.082e-04  Data: 0.023 (0.033)
Train: 16 [ 250/1251 ( 20%)]  Loss: 5.069 (4.96)  Time: 0.311s, 3297.56/s  (0.317s, 3230.00/s)  LR: 8.102e-04  Data: 0.024 (0.031)
Train: 16 [ 300/1251 ( 24%)]  Loss: 4.759 (4.93)  Time: 0.317s, 3229.57/s  (0.317s, 3233.61/s)  LR: 8.122e-04  Data: 0.023 (0.030)
Train: 16 [ 350/1251 ( 28%)]  Loss: 5.126 (4.95)  Time: 0.313s, 3267.97/s  (0.316s, 3236.33/s)  LR: 8.142e-04  Data: 0.022 (0.029)
Train: 16 [ 400/1251 ( 32%)]  Loss: 4.789 (4.93)  Time: 0.309s, 3318.65/s  (0.316s, 3238.38/s)  LR: 8.162e-04  Data: 0.025 (0.028)
Train: 16 [ 450/1251 ( 36%)]  Loss: 4.664 (4.91)  Time: 0.308s, 3321.69/s  (0.316s, 3240.17/s)  LR: 8.182e-04  Data: 0.021 (0.028)
Train: 16 [ 500/1251 ( 40%)]  Loss: 4.840 (4.90)  Time: 0.322s, 3183.24/s  (0.316s, 3241.15/s)  LR: 8.202e-04  Data: 0.025 (0.027)
Train: 16 [ 550/1251 ( 44%)]  Loss: 4.530 (4.87)  Time: 0.316s, 3242.66/s  (0.316s, 3242.60/s)  LR: 8.222e-04  Data: 0.022 (0.027)
Train: 16 [ 600/1251 ( 48%)]  Loss: 4.442 (4.84)  Time: 0.312s, 3286.61/s  (0.316s, 3243.99/s)  LR: 8.242e-04  Data: 0.023 (0.027)
Train: 16 [ 650/1251 ( 52%)]  Loss: 5.181 (4.86)  Time: 0.318s, 3218.77/s  (0.316s, 3244.05/s)  LR: 8.262e-04  Data: 0.024 (0.026)
Train: 16 [ 700/1251 ( 56%)]  Loss: 4.725 (4.85)  Time: 0.318s, 3222.56/s  (0.316s, 3244.80/s)  LR: 8.282e-04  Data: 0.024 (0.026)
Train: 16 [ 750/1251 ( 60%)]  Loss: 4.431 (4.83)  Time: 0.310s, 3301.83/s  (0.316s, 3245.55/s)  LR: 8.302e-04  Data: 0.022 (0.026)
Train: 16 [ 800/1251 ( 64%)]  Loss: 4.934 (4.83)  Time: 0.308s, 3327.52/s  (0.315s, 3246.19/s)  LR: 8.322e-04  Data: 0.022 (0.026)
Train: 16 [ 850/1251 ( 68%)]  Loss: 4.789 (4.83)  Time: 0.317s, 3228.68/s  (0.315s, 3246.34/s)  LR: 8.342e-04  Data: 0.022 (0.025)
Train: 16 [ 900/1251 ( 72%)]  Loss: 4.923 (4.84)  Time: 0.309s, 3313.79/s  (0.315s, 3246.96/s)  LR: 8.362e-04  Data: 0.020 (0.025)
Train: 16 [ 950/1251 ( 76%)]  Loss: 4.369 (4.81)  Time: 0.312s, 3287.02/s  (0.315s, 3247.62/s)  LR: 8.382e-04  Data: 0.023 (0.025)
Train: 16 [1000/1251 ( 80%)]  Loss: 4.498 (4.80)  Time: 0.310s, 3308.35/s  (0.315s, 3248.20/s)  LR: 8.402e-04  Data: 0.023 (0.025)
Train: 16 [1050/1251 ( 84%)]  Loss: 4.879 (4.80)  Time: 0.313s, 3269.79/s  (0.315s, 3248.42/s)  LR: 8.422e-04  Data: 0.024 (0.025)
Train: 16 [1100/1251 ( 88%)]  Loss: 5.132 (4.81)  Time: 0.326s, 3140.49/s  (0.315s, 3248.74/s)  LR: 8.442e-04  Data: 0.025 (0.025)
Train: 16 [1150/1251 ( 92%)]  Loss: 4.845 (4.82)  Time: 0.314s, 3256.05/s  (0.315s, 3248.71/s)  LR: 8.462e-04  Data: 0.024 (0.025)
Train: 16 [1200/1251 ( 96%)]  Loss: 4.635 (4.81)  Time: 0.316s, 3238.52/s  (0.315s, 3248.82/s)  LR: 8.482e-04  Data: 0.023 (0.025)
Train: 16 [1250/1251 (100%)]  Loss: 5.086 (4.82)  Time: 0.291s, 3513.79/s  (0.315s, 3251.19/s)  LR: 8.502e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.059 (2.059)  Loss:  1.4141 (1.4141)  Acc@1: 72.5586 (72.5586)  Acc@5: 88.2812 (88.2812)
Test: [  48/48]  Time: 0.041 (0.235)  Loss:  1.3613 (2.3247)  Acc@1: 72.9953 (50.5240)  Acc@5: 87.9717 (75.3420)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-16.pth.tar', 50.524000034179686)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-15.pth.tar', 49.04400016845703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-14.pth.tar', 48.08199997558594)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-13.pth.tar', 46.09599992675781)

Train: 17 [   0/1251 (  0%)]  Loss: 4.583 (4.58)  Time: 2.063s,  496.29/s  (2.063s,  496.29/s)  LR: 8.502e-04  Data: 1.836 (1.836)
Train: 17 [  50/1251 (  4%)]  Loss: 4.617 (4.60)  Time: 0.310s, 3304.41/s  (0.335s, 3056.23/s)  LR: 8.522e-04  Data: 0.024 (0.062)
Train: 17 [ 100/1251 (  8%)]  Loss: 5.038 (4.75)  Time: 0.311s, 3293.09/s  (0.321s, 3189.51/s)  LR: 8.542e-04  Data: 0.020 (0.043)
Train: 17 [ 150/1251 ( 12%)]  Loss: 5.034 (4.82)  Time: 0.312s, 3283.98/s  (0.317s, 3225.39/s)  LR: 8.562e-04  Data: 0.021 (0.036)
Train: 17 [ 200/1251 ( 16%)]  Loss: 4.783 (4.81)  Time: 0.308s, 3328.94/s  (0.316s, 3242.28/s)  LR: 8.582e-04  Data: 0.022 (0.033)
Train: 17 [ 250/1251 ( 20%)]  Loss: 4.617 (4.78)  Time: 0.308s, 3324.24/s  (0.315s, 3252.62/s)  LR: 8.602e-04  Data: 0.023 (0.031)
Train: 17 [ 300/1251 ( 24%)]  Loss: 4.973 (4.81)  Time: 0.313s, 3269.28/s  (0.314s, 3257.58/s)  LR: 8.622e-04  Data: 0.024 (0.030)
Train: 17 [ 350/1251 ( 28%)]  Loss: 4.703 (4.79)  Time: 0.310s, 3305.01/s  (0.314s, 3260.69/s)  LR: 8.642e-04  Data: 0.022 (0.029)
Train: 17 [ 400/1251 ( 32%)]  Loss: 4.820 (4.80)  Time: 0.306s, 3345.33/s  (0.314s, 3263.33/s)  LR: 8.662e-04  Data: 0.022 (0.028)
Train: 17 [ 450/1251 ( 36%)]  Loss: 4.883 (4.81)  Time: 0.309s, 3311.75/s  (0.314s, 3266.23/s)  LR: 8.682e-04  Data: 0.026 (0.028)
Train: 17 [ 500/1251 ( 40%)]  Loss: 4.932 (4.82)  Time: 0.315s, 3248.37/s  (0.313s, 3266.72/s)  LR: 8.702e-04  Data: 0.025 (0.027)
Train: 17 [ 550/1251 ( 44%)]  Loss: 4.810 (4.82)  Time: 0.311s, 3291.91/s  (0.313s, 3267.31/s)  LR: 8.722e-04  Data: 0.028 (0.027)
Train: 17 [ 600/1251 ( 48%)]  Loss: 5.104 (4.84)  Time: 0.314s, 3257.58/s  (0.313s, 3267.54/s)  LR: 8.741e-04  Data: 0.023 (0.027)
Train: 17 [ 650/1251 ( 52%)]  Loss: 4.611 (4.82)  Time: 0.310s, 3301.09/s  (0.313s, 3267.87/s)  LR: 8.761e-04  Data: 0.022 (0.026)
Train: 17 [ 700/1251 ( 56%)]  Loss: 4.902 (4.83)  Time: 0.320s, 3201.95/s  (0.313s, 3268.98/s)  LR: 8.781e-04  Data: 0.021 (0.026)
Train: 17 [ 750/1251 ( 60%)]  Loss: 4.904 (4.83)  Time: 0.316s, 3235.47/s  (0.313s, 3268.32/s)  LR: 8.801e-04  Data: 0.023 (0.026)
Train: 17 [ 800/1251 ( 64%)]  Loss: 4.952 (4.84)  Time: 0.312s, 3281.61/s  (0.313s, 3267.56/s)  LR: 8.821e-04  Data: 0.025 (0.026)
Train: 17 [ 850/1251 ( 68%)]  Loss: 4.607 (4.83)  Time: 0.319s, 3208.80/s  (0.313s, 3266.77/s)  LR: 8.841e-04  Data: 0.023 (0.026)
Train: 17 [ 900/1251 ( 72%)]  Loss: 4.243 (4.80)  Time: 0.314s, 3265.16/s  (0.314s, 3266.16/s)  LR: 8.861e-04  Data: 0.026 (0.025)
Train: 17 [ 950/1251 ( 76%)]  Loss: 4.824 (4.80)  Time: 0.323s, 3169.84/s  (0.314s, 3265.87/s)  LR: 8.881e-04  Data: 0.023 (0.025)
Train: 17 [1000/1251 ( 80%)]  Loss: 4.807 (4.80)  Time: 0.316s, 3243.55/s  (0.314s, 3266.18/s)  LR: 8.901e-04  Data: 0.022 (0.025)
Train: 17 [1050/1251 ( 84%)]  Loss: 4.677 (4.79)  Time: 0.314s, 3266.18/s  (0.314s, 3265.54/s)  LR: 8.921e-04  Data: 0.022 (0.025)
Train: 17 [1100/1251 ( 88%)]  Loss: 4.747 (4.79)  Time: 0.319s, 3207.72/s  (0.314s, 3264.96/s)  LR: 8.941e-04  Data: 0.022 (0.025)
Train: 17 [1150/1251 ( 92%)]  Loss: 5.177 (4.81)  Time: 0.309s, 3312.50/s  (0.314s, 3264.57/s)  LR: 8.961e-04  Data: 0.021 (0.025)
Train: 17 [1200/1251 ( 96%)]  Loss: 4.589 (4.80)  Time: 0.317s, 3229.72/s  (0.314s, 3264.51/s)  LR: 8.981e-04  Data: 0.021 (0.025)
Train: 17 [1250/1251 (100%)]  Loss: 4.994 (4.81)  Time: 0.280s, 3658.93/s  (0.314s, 3266.00/s)  LR: 9.001e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.023 (2.023)  Loss:  1.3740 (1.3740)  Acc@1: 71.3867 (71.3867)  Acc@5: 89.4531 (89.4531)
Test: [  48/48]  Time: 0.070 (0.237)  Loss:  1.2939 (2.2644)  Acc@1: 74.6462 (51.7100)  Acc@5: 88.2075 (76.2460)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-17.pth.tar', 51.70999997558594)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-16.pth.tar', 50.524000034179686)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-15.pth.tar', 49.04400016845703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-14.pth.tar', 48.08199997558594)

Train: 18 [   0/1251 (  0%)]  Loss: 5.007 (5.01)  Time: 2.377s,  430.77/s  (2.377s,  430.77/s)  LR: 9.001e-04  Data: 2.155 (2.155)
Train: 18 [  50/1251 (  4%)]  Loss: 4.768 (4.89)  Time: 0.302s, 3392.72/s  (0.334s, 3062.69/s)  LR: 9.021e-04  Data: 0.021 (0.065)
Train: 18 [ 100/1251 (  8%)]  Loss: 4.783 (4.85)  Time: 0.307s, 3340.34/s  (0.321s, 3193.16/s)  LR: 9.041e-04  Data: 0.023 (0.045)
Train: 18 [ 150/1251 ( 12%)]  Loss: 4.945 (4.88)  Time: 0.313s, 3273.63/s  (0.317s, 3232.88/s)  LR: 9.061e-04  Data: 0.024 (0.038)
Train: 18 [ 200/1251 ( 16%)]  Loss: 4.839 (4.87)  Time: 0.309s, 3309.20/s  (0.315s, 3249.06/s)  LR: 9.081e-04  Data: 0.025 (0.034)
Train: 18 [ 250/1251 ( 20%)]  Loss: 4.249 (4.77)  Time: 0.312s, 3281.48/s  (0.314s, 3258.43/s)  LR: 9.101e-04  Data: 0.022 (0.032)
Train: 18 [ 300/1251 ( 24%)]  Loss: 4.885 (4.78)  Time: 0.317s, 3226.13/s  (0.314s, 3263.56/s)  LR: 9.121e-04  Data: 0.026 (0.030)
Train: 18 [ 350/1251 ( 28%)]  Loss: 4.864 (4.79)  Time: 0.313s, 3270.44/s  (0.313s, 3267.46/s)  LR: 9.141e-04  Data: 0.023 (0.029)
Train: 18 [ 400/1251 ( 32%)]  Loss: 4.629 (4.77)  Time: 0.312s, 3279.23/s  (0.313s, 3270.14/s)  LR: 9.161e-04  Data: 0.030 (0.029)
Train: 18 [ 450/1251 ( 36%)]  Loss: 4.616 (4.76)  Time: 0.306s, 3340.98/s  (0.313s, 3270.78/s)  LR: 9.181e-04  Data: 0.021 (0.028)
Train: 18 [ 500/1251 ( 40%)]  Loss: 4.858 (4.77)  Time: 0.320s, 3199.40/s  (0.313s, 3271.09/s)  LR: 9.201e-04  Data: 0.024 (0.028)
Train: 18 [ 550/1251 ( 44%)]  Loss: 4.524 (4.75)  Time: 0.311s, 3295.16/s  (0.313s, 3271.92/s)  LR: 9.221e-04  Data: 0.024 (0.027)
Train: 18 [ 600/1251 ( 48%)]  Loss: 4.814 (4.75)  Time: 0.308s, 3319.86/s  (0.313s, 3272.96/s)  LR: 9.241e-04  Data: 0.025 (0.027)
Train: 18 [ 650/1251 ( 52%)]  Loss: 4.634 (4.74)  Time: 0.309s, 3316.99/s  (0.313s, 3273.36/s)  LR: 9.261e-04  Data: 0.021 (0.027)
Train: 18 [ 700/1251 ( 56%)]  Loss: 5.040 (4.76)  Time: 0.314s, 3260.30/s  (0.313s, 3272.88/s)  LR: 9.281e-04  Data: 0.022 (0.026)
Train: 18 [ 750/1251 ( 60%)]  Loss: 4.542 (4.75)  Time: 0.323s, 3169.87/s  (0.313s, 3272.93/s)  LR: 9.301e-04  Data: 0.026 (0.026)
Train: 18 [ 800/1251 ( 64%)]  Loss: 4.959 (4.76)  Time: 0.318s, 3223.52/s  (0.313s, 3273.05/s)  LR: 9.321e-04  Data: 0.022 (0.026)
Train: 18 [ 850/1251 ( 68%)]  Loss: 5.048 (4.78)  Time: 0.314s, 3257.78/s  (0.313s, 3273.03/s)  LR: 9.341e-04  Data: 0.023 (0.026)
Train: 18 [ 900/1251 ( 72%)]  Loss: 4.750 (4.78)  Time: 0.312s, 3279.76/s  (0.313s, 3272.98/s)  LR: 9.361e-04  Data: 0.022 (0.026)
Train: 18 [ 950/1251 ( 76%)]  Loss: 4.596 (4.77)  Time: 0.310s, 3302.24/s  (0.313s, 3273.01/s)  LR: 9.381e-04  Data: 0.025 (0.025)
Train: 18 [1000/1251 ( 80%)]  Loss: 5.049 (4.78)  Time: 0.315s, 3249.06/s  (0.313s, 3272.75/s)  LR: 9.401e-04  Data: 0.027 (0.025)
Train: 18 [1050/1251 ( 84%)]  Loss: 4.622 (4.77)  Time: 0.310s, 3300.74/s  (0.313s, 3272.14/s)  LR: 9.421e-04  Data: 0.023 (0.025)
Train: 18 [1100/1251 ( 88%)]  Loss: 4.831 (4.78)  Time: 0.318s, 3215.71/s  (0.313s, 3271.66/s)  LR: 9.441e-04  Data: 0.023 (0.025)
Train: 18 [1150/1251 ( 92%)]  Loss: 4.822 (4.78)  Time: 0.323s, 3175.13/s  (0.313s, 3271.16/s)  LR: 9.461e-04  Data: 0.023 (0.025)
Train: 18 [1200/1251 ( 96%)]  Loss: 5.010 (4.79)  Time: 0.309s, 3310.86/s  (0.313s, 3270.96/s)  LR: 9.481e-04  Data: 0.022 (0.025)
Train: 18 [1250/1251 (100%)]  Loss: 4.676 (4.78)  Time: 0.284s, 3604.11/s  (0.313s, 3272.58/s)  LR: 9.501e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.023 (2.023)  Loss:  1.3291 (1.3291)  Acc@1: 73.8281 (73.8281)  Acc@5: 89.8438 (89.8438)
Test: [  48/48]  Time: 0.045 (0.239)  Loss:  1.3232 (2.2265)  Acc@1: 73.4670 (52.4020)  Acc@5: 87.9717 (77.0120)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-18.pth.tar', 52.40199998046875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-17.pth.tar', 51.70999997558594)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-16.pth.tar', 50.524000034179686)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-15.pth.tar', 49.04400016845703)

Train: 19 [   0/1251 (  0%)]  Loss: 4.856 (4.86)  Time: 2.250s,  455.09/s  (2.250s,  455.09/s)  LR: 9.501e-04  Data: 2.031 (2.031)
Train: 19 [  50/1251 (  4%)]  Loss: 4.868 (4.86)  Time: 0.303s, 3381.64/s  (0.337s, 3041.50/s)  LR: 9.521e-04  Data: 0.022 (0.063)
Train: 19 [ 100/1251 (  8%)]  Loss: 4.700 (4.81)  Time: 0.314s, 3262.42/s  (0.322s, 3175.85/s)  LR: 9.541e-04  Data: 0.022 (0.043)
Train: 19 [ 150/1251 ( 12%)]  Loss: 4.876 (4.82)  Time: 0.308s, 3324.98/s  (0.318s, 3217.15/s)  LR: 9.561e-04  Data: 0.023 (0.037)
Train: 19 [ 200/1251 ( 16%)]  Loss: 5.093 (4.88)  Time: 0.310s, 3305.57/s  (0.316s, 3236.57/s)  LR: 9.581e-04  Data: 0.024 (0.033)
Train: 19 [ 250/1251 ( 20%)]  Loss: 4.333 (4.79)  Time: 0.310s, 3298.33/s  (0.316s, 3244.31/s)  LR: 9.601e-04  Data: 0.024 (0.031)
Train: 19 [ 300/1251 ( 24%)]  Loss: 4.388 (4.73)  Time: 0.319s, 3212.45/s  (0.315s, 3247.10/s)  LR: 9.621e-04  Data: 0.023 (0.030)
Train: 19 [ 350/1251 ( 28%)]  Loss: 4.744 (4.73)  Time: 0.310s, 3303.53/s  (0.315s, 3251.55/s)  LR: 9.641e-04  Data: 0.023 (0.029)
Train: 19 [ 400/1251 ( 32%)]  Loss: 4.629 (4.72)  Time: 0.309s, 3313.29/s  (0.315s, 3253.76/s)  LR: 9.661e-04  Data: 0.026 (0.028)
Train: 19 [ 450/1251 ( 36%)]  Loss: 4.683 (4.72)  Time: 0.309s, 3311.92/s  (0.315s, 3254.49/s)  LR: 9.681e-04  Data: 0.021 (0.028)
Train: 19 [ 500/1251 ( 40%)]  Loss: 4.411 (4.69)  Time: 0.331s, 3098.01/s  (0.315s, 3253.64/s)  LR: 9.701e-04  Data: 0.023 (0.027)
Train: 19 [ 550/1251 ( 44%)]  Loss: 4.649 (4.69)  Time: 0.317s, 3232.49/s  (0.315s, 3255.29/s)  LR: 9.721e-04  Data: 0.025 (0.027)
Train: 19 [ 600/1251 ( 48%)]  Loss: 5.024 (4.71)  Time: 0.306s, 3341.83/s  (0.314s, 3256.49/s)  LR: 9.740e-04  Data: 0.022 (0.026)
Train: 19 [ 650/1251 ( 52%)]  Loss: 4.589 (4.70)  Time: 0.314s, 3259.32/s  (0.314s, 3256.92/s)  LR: 9.760e-04  Data: 0.021 (0.026)
Train: 19 [ 700/1251 ( 56%)]  Loss: 5.024 (4.72)  Time: 0.311s, 3292.34/s  (0.314s, 3257.65/s)  LR: 9.780e-04  Data: 0.024 (0.026)
Train: 19 [ 750/1251 ( 60%)]  Loss: 4.756 (4.73)  Time: 0.310s, 3304.37/s  (0.314s, 3258.00/s)  LR: 9.800e-04  Data: 0.023 (0.026)
Train: 19 [ 800/1251 ( 64%)]  Loss: 4.726 (4.73)  Time: 0.320s, 3200.66/s  (0.314s, 3258.46/s)  LR: 9.820e-04  Data: 0.025 (0.026)
Train: 19 [ 850/1251 ( 68%)]  Loss: 4.501 (4.71)  Time: 0.309s, 3318.12/s  (0.314s, 3259.03/s)  LR: 9.840e-04  Data: 0.026 (0.026)
Train: 19 [ 900/1251 ( 72%)]  Loss: 4.899 (4.72)  Time: 0.314s, 3264.02/s  (0.314s, 3259.66/s)  LR: 9.860e-04  Data: 0.023 (0.025)
Train: 19 [ 950/1251 ( 76%)]  Loss: 5.050 (4.74)  Time: 0.313s, 3267.59/s  (0.314s, 3260.13/s)  LR: 9.880e-04  Data: 0.022 (0.025)
Train: 19 [1000/1251 ( 80%)]  Loss: 4.949 (4.75)  Time: 0.313s, 3276.74/s  (0.314s, 3260.40/s)  LR: 9.900e-04  Data: 0.024 (0.025)
Train: 19 [1050/1251 ( 84%)]  Loss: 4.693 (4.75)  Time: 0.311s, 3291.95/s  (0.314s, 3260.65/s)  LR: 9.920e-04  Data: 0.023 (0.025)
Train: 19 [1100/1251 ( 88%)]  Loss: 4.215 (4.72)  Time: 0.312s, 3277.90/s  (0.314s, 3261.12/s)  LR: 9.940e-04  Data: 0.022 (0.025)
Train: 19 [1150/1251 ( 92%)]  Loss: 4.886 (4.73)  Time: 0.316s, 3242.41/s  (0.314s, 3261.17/s)  LR: 9.960e-04  Data: 0.023 (0.025)
Train: 19 [1200/1251 ( 96%)]  Loss: 4.809 (4.73)  Time: 0.314s, 3257.65/s  (0.314s, 3261.69/s)  LR: 9.980e-04  Data: 0.021 (0.025)
Train: 19 [1250/1251 (100%)]  Loss: 4.740 (4.73)  Time: 0.288s, 3555.15/s  (0.314s, 3263.54/s)  LR: 9.891e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.042 (2.042)  Loss:  1.2383 (1.2383)  Acc@1: 75.2930 (75.2930)  Acc@5: 90.7227 (90.7227)
Test: [  48/48]  Time: 0.045 (0.238)  Loss:  1.3076 (2.1549)  Acc@1: 74.6462 (53.1380)  Acc@5: 87.5000 (77.9480)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-19.pth.tar', 53.137999975585934)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-18.pth.tar', 52.40199998046875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-17.pth.tar', 51.70999997558594)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-16.pth.tar', 50.524000034179686)

Train: 20 [   0/1251 (  0%)]  Loss: 4.785 (4.79)  Time: 2.328s,  439.80/s  (2.328s,  439.80/s)  LR: 9.891e-04  Data: 2.105 (2.105)
Train: 20 [  50/1251 (  4%)]  Loss: 4.423 (4.60)  Time: 0.302s, 3392.35/s  (0.336s, 3046.31/s)  LR: 9.891e-04  Data: 0.022 (0.065)
Train: 20 [ 100/1251 (  8%)]  Loss: 4.698 (4.64)  Time: 0.311s, 3296.84/s  (0.321s, 3185.49/s)  LR: 9.890e-04  Data: 0.021 (0.044)
Train: 20 [ 150/1251 ( 12%)]  Loss: 4.791 (4.67)  Time: 0.307s, 3340.11/s  (0.317s, 3228.89/s)  LR: 9.890e-04  Data: 0.023 (0.037)
Train: 20 [ 200/1251 ( 16%)]  Loss: 4.957 (4.73)  Time: 0.314s, 3258.80/s  (0.315s, 3246.25/s)  LR: 9.890e-04  Data: 0.024 (0.034)
Train: 20 [ 250/1251 ( 20%)]  Loss: 4.524 (4.70)  Time: 0.308s, 3321.04/s  (0.314s, 3257.03/s)  LR: 9.889e-04  Data: 0.021 (0.031)
Train: 20 [ 300/1251 ( 24%)]  Loss: 4.464 (4.66)  Time: 0.314s, 3260.63/s  (0.314s, 3262.63/s)  LR: 9.889e-04  Data: 0.022 (0.030)
Train: 20 [ 350/1251 ( 28%)]  Loss: 4.617 (4.66)  Time: 0.313s, 3273.85/s  (0.313s, 3266.95/s)  LR: 9.888e-04  Data: 0.022 (0.029)
Train: 20 [ 400/1251 ( 32%)]  Loss: 4.829 (4.68)  Time: 0.312s, 3282.63/s  (0.313s, 3269.18/s)  LR: 9.888e-04  Data: 0.024 (0.028)
Train: 20 [ 450/1251 ( 36%)]  Loss: 4.753 (4.68)  Time: 0.313s, 3274.18/s  (0.313s, 3270.20/s)  LR: 9.887e-04  Data: 0.022 (0.028)
Train: 20 [ 500/1251 ( 40%)]  Loss: 4.713 (4.69)  Time: 0.315s, 3249.36/s  (0.313s, 3270.30/s)  LR: 9.887e-04  Data: 0.024 (0.027)
Train: 20 [ 550/1251 ( 44%)]  Loss: 4.844 (4.70)  Time: 0.314s, 3261.46/s  (0.313s, 3269.95/s)  LR: 9.886e-04  Data: 0.022 (0.027)
Train: 20 [ 600/1251 ( 48%)]  Loss: 4.655 (4.70)  Time: 0.309s, 3310.98/s  (0.313s, 3270.77/s)  LR: 9.886e-04  Data: 0.025 (0.027)
Train: 20 [ 650/1251 ( 52%)]  Loss: 4.843 (4.71)  Time: 0.315s, 3247.70/s  (0.313s, 3270.82/s)  LR: 9.886e-04  Data: 0.022 (0.026)
Train: 20 [ 700/1251 ( 56%)]  Loss: 4.273 (4.68)  Time: 0.313s, 3273.60/s  (0.313s, 3270.67/s)  LR: 9.885e-04  Data: 0.022 (0.026)
Train: 20 [ 750/1251 ( 60%)]  Loss: 4.743 (4.68)  Time: 0.315s, 3249.25/s  (0.313s, 3271.19/s)  LR: 9.885e-04  Data: 0.021 (0.026)
Train: 20 [ 800/1251 ( 64%)]  Loss: 4.609 (4.68)  Time: 0.313s, 3271.65/s  (0.313s, 3270.91/s)  LR: 9.884e-04  Data: 0.026 (0.026)
Train: 20 [ 850/1251 ( 68%)]  Loss: 4.396 (4.66)  Time: 0.319s, 3209.55/s  (0.313s, 3271.24/s)  LR: 9.884e-04  Data: 0.022 (0.026)
Train: 20 [ 900/1251 ( 72%)]  Loss: 4.286 (4.64)  Time: 0.318s, 3220.17/s  (0.313s, 3271.39/s)  LR: 9.883e-04  Data: 0.026 (0.026)
Train: 20 [ 950/1251 ( 76%)]  Loss: 4.783 (4.65)  Time: 0.315s, 3254.93/s  (0.313s, 3271.17/s)  LR: 9.883e-04  Data: 0.028 (0.025)
Train: 20 [1000/1251 ( 80%)]  Loss: 4.774 (4.66)  Time: 0.317s, 3231.56/s  (0.313s, 3271.80/s)  LR: 9.882e-04  Data: 0.024 (0.025)
Train: 20 [1050/1251 ( 84%)]  Loss: 4.428 (4.64)  Time: 0.319s, 3210.41/s  (0.313s, 3271.59/s)  LR: 9.882e-04  Data: 0.022 (0.025)
Train: 20 [1100/1251 ( 88%)]  Loss: 4.851 (4.65)  Time: 0.311s, 3293.74/s  (0.313s, 3271.70/s)  LR: 9.882e-04  Data: 0.022 (0.025)
Train: 20 [1150/1251 ( 92%)]  Loss: 4.515 (4.65)  Time: 0.309s, 3317.10/s  (0.313s, 3271.93/s)  LR: 9.881e-04  Data: 0.023 (0.025)
Train: 20 [1200/1251 ( 96%)]  Loss: 4.537 (4.64)  Time: 0.308s, 3321.78/s  (0.313s, 3271.70/s)  LR: 9.881e-04  Data: 0.024 (0.025)
Train: 20 [1250/1251 (100%)]  Loss: 5.019 (4.66)  Time: 0.284s, 3610.99/s  (0.313s, 3273.83/s)  LR: 9.880e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.365 (2.365)  Loss:  1.2607 (1.2607)  Acc@1: 75.0977 (75.0977)  Acc@5: 90.1367 (90.1367)
Test: [  48/48]  Time: 0.044 (0.239)  Loss:  1.2383 (2.0797)  Acc@1: 74.6462 (54.5040)  Acc@5: 88.3255 (78.7300)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-20.pth.tar', 54.503999975585934)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-19.pth.tar', 53.137999975585934)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-18.pth.tar', 52.40199998046875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-17.pth.tar', 51.70999997558594)

Train: 21 [   0/1251 (  0%)]  Loss: 4.280 (4.28)  Time: 2.145s,  477.28/s  (2.145s,  477.28/s)  LR: 9.880e-04  Data: 1.923 (1.923)
Train: 21 [  50/1251 (  4%)]  Loss: 4.780 (4.53)  Time: 0.303s, 3378.53/s  (0.341s, 3006.02/s)  LR: 9.880e-04  Data: 0.023 (0.072)
Train: 21 [ 100/1251 (  8%)]  Loss: 4.271 (4.44)  Time: 0.308s, 3319.99/s  (0.323s, 3169.29/s)  LR: 9.879e-04  Data: 0.024 (0.048)
Train: 21 [ 150/1251 ( 12%)]  Loss: 4.641 (4.49)  Time: 0.311s, 3292.52/s  (0.318s, 3217.30/s)  LR: 9.879e-04  Data: 0.025 (0.040)
Train: 21 [ 200/1251 ( 16%)]  Loss: 4.524 (4.50)  Time: 0.311s, 3290.55/s  (0.316s, 3238.39/s)  LR: 9.878e-04  Data: 0.024 (0.036)
Train: 21 [ 250/1251 ( 20%)]  Loss: 4.491 (4.50)  Time: 0.310s, 3298.42/s  (0.315s, 3250.64/s)  LR: 9.878e-04  Data: 0.023 (0.033)
Train: 21 [ 300/1251 ( 24%)]  Loss: 4.276 (4.47)  Time: 0.312s, 3279.97/s  (0.314s, 3257.58/s)  LR: 9.877e-04  Data: 0.025 (0.032)
Train: 21 [ 350/1251 ( 28%)]  Loss: 4.139 (4.43)  Time: 0.315s, 3250.75/s  (0.314s, 3263.87/s)  LR: 9.877e-04  Data: 0.023 (0.030)
Train: 21 [ 400/1251 ( 32%)]  Loss: 4.514 (4.44)  Time: 0.309s, 3312.96/s  (0.314s, 3266.13/s)  LR: 9.877e-04  Data: 0.024 (0.029)
Train: 21 [ 450/1251 ( 36%)]  Loss: 4.258 (4.42)  Time: 0.314s, 3259.39/s  (0.313s, 3268.22/s)  LR: 9.876e-04  Data: 0.025 (0.029)
Train: 21 [ 500/1251 ( 40%)]  Loss: 4.385 (4.41)  Time: 0.310s, 3300.36/s  (0.313s, 3270.25/s)  LR: 9.876e-04  Data: 0.026 (0.028)
Train: 21 [ 550/1251 ( 44%)]  Loss: 4.756 (4.44)  Time: 0.311s, 3294.90/s  (0.313s, 3272.03/s)  LR: 9.875e-04  Data: 0.024 (0.028)
Train: 21 [ 600/1251 ( 48%)]  Loss: 4.675 (4.46)  Time: 0.310s, 3302.57/s  (0.313s, 3273.57/s)  LR: 9.875e-04  Data: 0.021 (0.027)
Train: 21 [ 650/1251 ( 52%)]  Loss: 4.652 (4.47)  Time: 0.311s, 3293.05/s  (0.313s, 3274.17/s)  LR: 9.874e-04  Data: 0.022 (0.027)
Train: 21 [ 700/1251 ( 56%)]  Loss: 4.823 (4.50)  Time: 0.312s, 3287.25/s  (0.313s, 3275.14/s)  LR: 9.874e-04  Data: 0.026 (0.027)
Train: 21 [ 750/1251 ( 60%)]  Loss: 4.231 (4.48)  Time: 0.308s, 3321.62/s  (0.313s, 3275.75/s)  LR: 9.873e-04  Data: 0.024 (0.027)
Train: 21 [ 800/1251 ( 64%)]  Loss: 4.651 (4.49)  Time: 0.312s, 3281.30/s  (0.313s, 3276.43/s)  LR: 9.873e-04  Data: 0.022 (0.026)
Train: 21 [ 850/1251 ( 68%)]  Loss: 4.091 (4.47)  Time: 0.313s, 3266.60/s  (0.312s, 3277.10/s)  LR: 9.872e-04  Data: 0.022 (0.026)
Train: 21 [ 900/1251 ( 72%)]  Loss: 4.601 (4.48)  Time: 0.310s, 3299.93/s  (0.312s, 3278.11/s)  LR: 9.872e-04  Data: 0.021 (0.026)
Train: 21 [ 950/1251 ( 76%)]  Loss: 4.640 (4.48)  Time: 0.313s, 3272.91/s  (0.312s, 3278.66/s)  LR: 9.871e-04  Data: 0.024 (0.026)
Train: 21 [1000/1251 ( 80%)]  Loss: 4.444 (4.48)  Time: 0.309s, 3317.64/s  (0.312s, 3279.29/s)  LR: 9.871e-04  Data: 0.025 (0.026)
Train: 21 [1050/1251 ( 84%)]  Loss: 4.593 (4.49)  Time: 0.313s, 3275.40/s  (0.312s, 3280.07/s)  LR: 9.870e-04  Data: 0.020 (0.026)
Train: 21 [1100/1251 ( 88%)]  Loss: 4.693 (4.50)  Time: 0.311s, 3287.95/s  (0.312s, 3280.82/s)  LR: 9.870e-04  Data: 0.023 (0.025)
Train: 21 [1150/1251 ( 92%)]  Loss: 4.626 (4.50)  Time: 0.308s, 3326.59/s  (0.312s, 3281.54/s)  LR: 9.870e-04  Data: 0.023 (0.025)
Train: 21 [1200/1251 ( 96%)]  Loss: 4.891 (4.52)  Time: 0.310s, 3305.30/s  (0.312s, 3281.93/s)  LR: 9.869e-04  Data: 0.022 (0.025)
Train: 21 [1250/1251 (100%)]  Loss: 4.658 (4.52)  Time: 0.281s, 3644.60/s  (0.312s, 3284.32/s)  LR: 9.869e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.019 (2.019)  Loss:  1.1943 (1.1943)  Acc@1: 75.4883 (75.4883)  Acc@5: 91.3086 (91.3086)
Test: [  48/48]  Time: 0.065 (0.234)  Loss:  1.2061 (2.0490)  Acc@1: 76.5330 (55.5280)  Acc@5: 89.6227 (79.8560)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-21.pth.tar', 55.52800001953125)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-20.pth.tar', 54.503999975585934)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-19.pth.tar', 53.137999975585934)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-18.pth.tar', 52.40199998046875)

Train: 22 [   0/1251 (  0%)]  Loss: 4.773 (4.77)  Time: 2.258s,  453.59/s  (2.258s,  453.59/s)  LR: 9.869e-04  Data: 2.031 (2.031)
Train: 22 [  50/1251 (  4%)]  Loss: 4.684 (4.73)  Time: 0.300s, 3413.27/s  (0.334s, 3062.89/s)  LR: 9.868e-04  Data: 0.021 (0.063)
Train: 22 [ 100/1251 (  8%)]  Loss: 4.331 (4.60)  Time: 0.313s, 3272.39/s  (0.320s, 3201.71/s)  LR: 9.868e-04  Data: 0.022 (0.043)
Train: 22 [ 150/1251 ( 12%)]  Loss: 4.669 (4.61)  Time: 0.303s, 3376.55/s  (0.316s, 3245.62/s)  LR: 9.867e-04  Data: 0.022 (0.037)
Train: 22 [ 200/1251 ( 16%)]  Loss: 4.137 (4.52)  Time: 0.308s, 3329.46/s  (0.314s, 3262.57/s)  LR: 9.867e-04  Data: 0.025 (0.033)
Train: 22 [ 250/1251 ( 20%)]  Loss: 4.365 (4.49)  Time: 0.310s, 3302.01/s  (0.313s, 3271.86/s)  LR: 9.866e-04  Data: 0.024 (0.031)
Train: 22 [ 300/1251 ( 24%)]  Loss: 4.749 (4.53)  Time: 0.308s, 3324.92/s  (0.312s, 3278.16/s)  LR: 9.866e-04  Data: 0.024 (0.030)
Train: 22 [ 350/1251 ( 28%)]  Loss: 4.611 (4.54)  Time: 0.311s, 3290.00/s  (0.312s, 3281.08/s)  LR: 9.865e-04  Data: 0.025 (0.029)
Train: 22 [ 400/1251 ( 32%)]  Loss: 4.151 (4.50)  Time: 0.313s, 3266.74/s  (0.312s, 3283.29/s)  LR: 9.865e-04  Data: 0.025 (0.028)
Train: 22 [ 450/1251 ( 36%)]  Loss: 4.648 (4.51)  Time: 0.307s, 3335.21/s  (0.312s, 3284.74/s)  LR: 9.864e-04  Data: 0.023 (0.028)
Train: 22 [ 500/1251 ( 40%)]  Loss: 4.475 (4.51)  Time: 0.309s, 3309.88/s  (0.312s, 3286.13/s)  LR: 9.864e-04  Data: 0.022 (0.027)
Train: 22 [ 550/1251 ( 44%)]  Loss: 4.560 (4.51)  Time: 0.312s, 3278.27/s  (0.311s, 3287.36/s)  LR: 9.863e-04  Data: 0.026 (0.027)
Train: 22 [ 600/1251 ( 48%)]  Loss: 4.899 (4.54)  Time: 0.311s, 3293.31/s  (0.311s, 3287.96/s)  LR: 9.863e-04  Data: 0.023 (0.027)
Train: 22 [ 650/1251 ( 52%)]  Loss: 4.538 (4.54)  Time: 0.314s, 3262.37/s  (0.311s, 3288.81/s)  LR: 9.862e-04  Data: 0.021 (0.026)
Train: 22 [ 700/1251 ( 56%)]  Loss: 4.751 (4.56)  Time: 0.314s, 3261.44/s  (0.311s, 3289.30/s)  LR: 9.862e-04  Data: 0.023 (0.026)
Train: 22 [ 750/1251 ( 60%)]  Loss: 4.597 (4.56)  Time: 0.308s, 3325.54/s  (0.311s, 3289.63/s)  LR: 9.861e-04  Data: 0.018 (0.026)
Train: 22 [ 800/1251 ( 64%)]  Loss: 4.373 (4.55)  Time: 0.309s, 3308.94/s  (0.311s, 3289.99/s)  LR: 9.861e-04  Data: 0.022 (0.026)
Train: 22 [ 850/1251 ( 68%)]  Loss: 4.524 (4.55)  Time: 0.313s, 3267.26/s  (0.311s, 3290.12/s)  LR: 9.860e-04  Data: 0.022 (0.026)
Train: 22 [ 900/1251 ( 72%)]  Loss: 4.641 (4.55)  Time: 0.310s, 3305.17/s  (0.311s, 3290.07/s)  LR: 9.860e-04  Data: 0.023 (0.025)
Train: 22 [ 950/1251 ( 76%)]  Loss: 4.623 (4.55)  Time: 0.317s, 3226.42/s  (0.311s, 3290.13/s)  LR: 9.859e-04  Data: 0.021 (0.025)
Train: 22 [1000/1251 ( 80%)]  Loss: 4.623 (4.56)  Time: 0.306s, 3347.80/s  (0.311s, 3289.95/s)  LR: 9.859e-04  Data: 0.023 (0.025)
Train: 22 [1050/1251 ( 84%)]  Loss: 4.426 (4.55)  Time: 0.312s, 3277.16/s  (0.311s, 3289.91/s)  LR: 9.858e-04  Data: 0.025 (0.025)
Train: 22 [1100/1251 ( 88%)]  Loss: 4.734 (4.56)  Time: 0.311s, 3292.16/s  (0.311s, 3289.83/s)  LR: 9.858e-04  Data: 0.023 (0.025)
Train: 22 [1150/1251 ( 92%)]  Loss: 4.567 (4.56)  Time: 0.313s, 3269.89/s  (0.311s, 3289.41/s)  LR: 9.857e-04  Data: 0.021 (0.025)
Train: 22 [1200/1251 ( 96%)]  Loss: 4.363 (4.55)  Time: 0.314s, 3261.56/s  (0.311s, 3289.16/s)  LR: 9.857e-04  Data: 0.026 (0.025)
Train: 22 [1250/1251 (100%)]  Loss: 4.539 (4.55)  Time: 0.283s, 3613.44/s  (0.311s, 3290.77/s)  LR: 9.856e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.988 (1.988)  Loss:  1.1328 (1.1328)  Acc@1: 76.0742 (76.0742)  Acc@5: 91.6016 (91.6016)
Test: [  48/48]  Time: 0.043 (0.239)  Loss:  1.2061 (2.0046)  Acc@1: 75.3538 (56.1340)  Acc@5: 89.0330 (79.9880)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-22.pth.tar', 56.13399989501953)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-21.pth.tar', 55.52800001953125)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-20.pth.tar', 54.503999975585934)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-19.pth.tar', 53.137999975585934)

Train: 23 [   0/1251 (  0%)]  Loss: 4.607 (4.61)  Time: 2.417s,  423.69/s  (2.417s,  423.69/s)  LR: 9.856e-04  Data: 2.195 (2.195)
Train: 23 [  50/1251 (  4%)]  Loss: 4.795 (4.70)  Time: 0.303s, 3379.69/s  (0.334s, 3067.67/s)  LR: 9.856e-04  Data: 0.024 (0.065)
Train: 23 [ 100/1251 (  8%)]  Loss: 4.558 (4.65)  Time: 0.303s, 3377.54/s  (0.319s, 3205.78/s)  LR: 9.855e-04  Data: 0.021 (0.044)
Train: 23 [ 150/1251 ( 12%)]  Loss: 4.921 (4.72)  Time: 0.312s, 3286.67/s  (0.315s, 3249.20/s)  LR: 9.855e-04  Data: 0.019 (0.037)
Train: 23 [ 200/1251 ( 16%)]  Loss: 4.993 (4.77)  Time: 0.310s, 3305.94/s  (0.313s, 3267.43/s)  LR: 9.854e-04  Data: 0.023 (0.034)
Train: 23 [ 250/1251 ( 20%)]  Loss: 4.824 (4.78)  Time: 0.315s, 3250.35/s  (0.312s, 3277.13/s)  LR: 9.854e-04  Data: 0.028 (0.031)
Train: 23 [ 300/1251 ( 24%)]  Loss: 4.522 (4.75)  Time: 0.310s, 3304.40/s  (0.312s, 3282.67/s)  LR: 9.853e-04  Data: 0.022 (0.030)
Train: 23 [ 350/1251 ( 28%)]  Loss: 4.348 (4.70)  Time: 0.312s, 3278.26/s  (0.312s, 3286.50/s)  LR: 9.853e-04  Data: 0.023 (0.029)
Train: 23 [ 400/1251 ( 32%)]  Loss: 4.673 (4.69)  Time: 0.307s, 3332.67/s  (0.311s, 3288.75/s)  LR: 9.852e-04  Data: 0.026 (0.028)
Train: 23 [ 450/1251 ( 36%)]  Loss: 4.481 (4.67)  Time: 0.313s, 3269.40/s  (0.311s, 3290.91/s)  LR: 9.852e-04  Data: 0.024 (0.028)
Train: 23 [ 500/1251 ( 40%)]  Loss: 4.374 (4.65)  Time: 0.313s, 3267.55/s  (0.311s, 3291.79/s)  LR: 9.851e-04  Data: 0.023 (0.027)
Train: 23 [ 550/1251 ( 44%)]  Loss: 4.517 (4.63)  Time: 0.310s, 3301.25/s  (0.311s, 3292.36/s)  LR: 9.851e-04  Data: 0.024 (0.027)
Train: 23 [ 600/1251 ( 48%)]  Loss: 4.398 (4.62)  Time: 0.312s, 3281.10/s  (0.311s, 3292.98/s)  LR: 9.850e-04  Data: 0.026 (0.027)
Train: 23 [ 650/1251 ( 52%)]  Loss: 4.549 (4.61)  Time: 0.310s, 3302.11/s  (0.311s, 3294.10/s)  LR: 9.850e-04  Data: 0.025 (0.026)
Train: 23 [ 700/1251 ( 56%)]  Loss: 4.861 (4.63)  Time: 0.310s, 3302.19/s  (0.311s, 3294.20/s)  LR: 9.849e-04  Data: 0.020 (0.026)
Train: 23 [ 750/1251 ( 60%)]  Loss: 4.718 (4.63)  Time: 0.313s, 3270.75/s  (0.311s, 3294.77/s)  LR: 9.849e-04  Data: 0.021 (0.026)
Train: 23 [ 800/1251 ( 64%)]  Loss: 4.283 (4.61)  Time: 0.311s, 3289.88/s  (0.311s, 3295.45/s)  LR: 9.848e-04  Data: 0.024 (0.026)
Train: 23 [ 850/1251 ( 68%)]  Loss: 4.292 (4.60)  Time: 0.314s, 3260.66/s  (0.311s, 3295.35/s)  LR: 9.848e-04  Data: 0.022 (0.026)
Train: 23 [ 900/1251 ( 72%)]  Loss: 4.726 (4.60)  Time: 0.315s, 3253.52/s  (0.311s, 3295.01/s)  LR: 9.847e-04  Data: 0.024 (0.026)
Train: 23 [ 950/1251 ( 76%)]  Loss: 4.789 (4.61)  Time: 0.310s, 3301.32/s  (0.311s, 3294.88/s)  LR: 9.847e-04  Data: 0.023 (0.025)
Train: 23 [1000/1251 ( 80%)]  Loss: 4.617 (4.61)  Time: 0.311s, 3295.42/s  (0.311s, 3294.77/s)  LR: 9.846e-04  Data: 0.026 (0.025)
Train: 23 [1050/1251 ( 84%)]  Loss: 4.405 (4.60)  Time: 0.318s, 3224.16/s  (0.311s, 3294.49/s)  LR: 9.846e-04  Data: 0.026 (0.025)
Train: 23 [1100/1251 ( 88%)]  Loss: 4.350 (4.59)  Time: 0.313s, 3271.29/s  (0.311s, 3294.18/s)  LR: 9.845e-04  Data: 0.022 (0.025)
Train: 23 [1150/1251 ( 92%)]  Loss: 4.540 (4.59)  Time: 0.307s, 3335.80/s  (0.311s, 3294.42/s)  LR: 9.845e-04  Data: 0.021 (0.025)
Train: 23 [1200/1251 ( 96%)]  Loss: 4.403 (4.58)  Time: 0.307s, 3331.09/s  (0.311s, 3294.83/s)  LR: 9.844e-04  Data: 0.022 (0.025)
Train: 23 [1250/1251 (100%)]  Loss: 4.379 (4.57)  Time: 0.284s, 3600.06/s  (0.311s, 3297.08/s)  LR: 9.844e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.007 (2.007)  Loss:  1.0762 (1.0762)  Acc@1: 78.2227 (78.2227)  Acc@5: 92.9688 (92.9688)
Test: [  48/48]  Time: 0.043 (0.241)  Loss:  1.2197 (1.9803)  Acc@1: 76.2972 (57.1580)  Acc@5: 89.2689 (80.8480)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-23.pth.tar', 57.15800004638672)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-22.pth.tar', 56.13399989501953)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-21.pth.tar', 55.52800001953125)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-20.pth.tar', 54.503999975585934)

Train: 24 [   0/1251 (  0%)]  Loss: 4.509 (4.51)  Time: 2.127s,  481.53/s  (2.127s,  481.53/s)  LR: 9.844e-04  Data: 1.902 (1.902)
Train: 24 [  50/1251 (  4%)]  Loss: 4.517 (4.51)  Time: 0.298s, 3436.05/s  (0.331s, 3095.75/s)  LR: 9.843e-04  Data: 0.023 (0.060)
Train: 24 [ 100/1251 (  8%)]  Loss: 4.160 (4.40)  Time: 0.305s, 3356.81/s  (0.318s, 3221.98/s)  LR: 9.843e-04  Data: 0.020 (0.042)
Train: 24 [ 150/1251 ( 12%)]  Loss: 4.883 (4.52)  Time: 0.308s, 3321.17/s  (0.314s, 3260.92/s)  LR: 9.842e-04  Data: 0.020 (0.036)
Train: 24 [ 200/1251 ( 16%)]  Loss: 4.450 (4.50)  Time: 0.313s, 3276.23/s  (0.313s, 3276.52/s)  LR: 9.842e-04  Data: 0.021 (0.033)
Train: 24 [ 250/1251 ( 20%)]  Loss: 5.009 (4.59)  Time: 0.311s, 3287.88/s  (0.312s, 3285.74/s)  LR: 9.841e-04  Data: 0.025 (0.031)
Train: 24 [ 300/1251 ( 24%)]  Loss: 4.746 (4.61)  Time: 0.314s, 3260.95/s  (0.311s, 3289.26/s)  LR: 9.841e-04  Data: 0.022 (0.029)
Train: 24 [ 350/1251 ( 28%)]  Loss: 4.389 (4.58)  Time: 0.310s, 3305.22/s  (0.311s, 3292.16/s)  LR: 9.840e-04  Data: 0.027 (0.029)
Train: 24 [ 400/1251 ( 32%)]  Loss: 4.773 (4.60)  Time: 0.308s, 3325.64/s  (0.311s, 3293.61/s)  LR: 9.840e-04  Data: 0.023 (0.028)
Train: 24 [ 450/1251 ( 36%)]  Loss: 4.470 (4.59)  Time: 0.307s, 3340.55/s  (0.311s, 3294.86/s)  LR: 9.839e-04  Data: 0.021 (0.027)
Train: 24 [ 500/1251 ( 40%)]  Loss: 3.921 (4.53)  Time: 0.301s, 3398.64/s  (0.311s, 3296.43/s)  LR: 9.838e-04  Data: 0.019 (0.027)
Train: 24 [ 550/1251 ( 44%)]  Loss: 4.607 (4.54)  Time: 0.312s, 3286.12/s  (0.311s, 3296.95/s)  LR: 9.838e-04  Data: 0.022 (0.027)
Train: 24 [ 600/1251 ( 48%)]  Loss: 4.349 (4.52)  Time: 0.308s, 3321.07/s  (0.311s, 3297.44/s)  LR: 9.837e-04  Data: 0.023 (0.026)
Train: 24 [ 650/1251 ( 52%)]  Loss: 4.523 (4.52)  Time: 0.306s, 3345.34/s  (0.311s, 3297.85/s)  LR: 9.837e-04  Data: 0.020 (0.026)
Train: 24 [ 700/1251 ( 56%)]  Loss: 4.295 (4.51)  Time: 0.306s, 3350.70/s  (0.310s, 3298.16/s)  LR: 9.836e-04  Data: 0.020 (0.026)
Train: 24 [ 750/1251 ( 60%)]  Loss: 4.181 (4.49)  Time: 0.306s, 3347.95/s  (0.310s, 3298.41/s)  LR: 9.836e-04  Data: 0.023 (0.026)
Train: 24 [ 800/1251 ( 64%)]  Loss: 4.443 (4.48)  Time: 0.308s, 3328.50/s  (0.310s, 3298.46/s)  LR: 9.835e-04  Data: 0.023 (0.026)
Train: 24 [ 850/1251 ( 68%)]  Loss: 4.367 (4.48)  Time: 0.311s, 3289.17/s  (0.310s, 3297.99/s)  LR: 9.835e-04  Data: 0.025 (0.025)
Train: 24 [ 900/1251 ( 72%)]  Loss: 4.653 (4.49)  Time: 0.313s, 3275.44/s  (0.310s, 3298.25/s)  LR: 9.834e-04  Data: 0.024 (0.025)
Train: 24 [ 950/1251 ( 76%)]  Loss: 4.200 (4.47)  Time: 0.310s, 3302.47/s  (0.310s, 3298.54/s)  LR: 9.834e-04  Data: 0.022 (0.025)
Train: 24 [1000/1251 ( 80%)]  Loss: 4.421 (4.47)  Time: 0.312s, 3285.79/s  (0.310s, 3298.66/s)  LR: 9.833e-04  Data: 0.023 (0.025)
Train: 24 [1050/1251 ( 84%)]  Loss: 4.719 (4.48)  Time: 0.312s, 3281.53/s  (0.310s, 3298.61/s)  LR: 9.833e-04  Data: 0.027 (0.025)
Train: 24 [1100/1251 ( 88%)]  Loss: 4.434 (4.48)  Time: 0.314s, 3264.46/s  (0.310s, 3298.44/s)  LR: 9.832e-04  Data: 0.023 (0.025)
Train: 24 [1150/1251 ( 92%)]  Loss: 4.127 (4.46)  Time: 0.311s, 3297.83/s  (0.310s, 3298.28/s)  LR: 9.832e-04  Data: 0.023 (0.025)
Train: 24 [1200/1251 ( 96%)]  Loss: 4.179 (4.45)  Time: 0.308s, 3327.28/s  (0.311s, 3297.89/s)  LR: 9.831e-04  Data: 0.025 (0.025)
Train: 24 [1250/1251 (100%)]  Loss: 4.571 (4.46)  Time: 0.278s, 3686.54/s  (0.310s, 3299.59/s)  LR: 9.830e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.968 (1.968)  Loss:  1.1133 (1.1133)  Acc@1: 76.7578 (76.7578)  Acc@5: 92.0898 (92.0898)
Test: [  48/48]  Time: 0.065 (0.232)  Loss:  1.1377 (1.9248)  Acc@1: 75.9434 (58.2720)  Acc@5: 91.0377 (81.7780)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-24.pth.tar', 58.27200002197266)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-23.pth.tar', 57.15800004638672)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-22.pth.tar', 56.13399989501953)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-21.pth.tar', 55.52800001953125)

Train: 25 [   0/1251 (  0%)]  Loss: 4.540 (4.54)  Time: 2.276s,  449.97/s  (2.276s,  449.97/s)  LR: 9.830e-04  Data: 2.052 (2.052)
Train: 25 [  50/1251 (  4%)]  Loss: 3.913 (4.23)  Time: 0.300s, 3414.52/s  (0.333s, 3072.64/s)  LR: 9.830e-04  Data: 0.022 (0.064)
Train: 25 [ 100/1251 (  8%)]  Loss: 4.765 (4.41)  Time: 0.307s, 3340.03/s  (0.319s, 3209.84/s)  LR: 9.829e-04  Data: 0.026 (0.044)
Train: 25 [ 150/1251 ( 12%)]  Loss: 4.564 (4.45)  Time: 0.311s, 3288.72/s  (0.315s, 3250.04/s)  LR: 9.829e-04  Data: 0.023 (0.037)
Train: 25 [ 200/1251 ( 16%)]  Loss: 4.647 (4.49)  Time: 0.311s, 3288.52/s  (0.313s, 3269.58/s)  LR: 9.828e-04  Data: 0.022 (0.034)
Train: 25 [ 250/1251 ( 20%)]  Loss: 4.085 (4.42)  Time: 0.307s, 3335.06/s  (0.312s, 3278.43/s)  LR: 9.828e-04  Data: 0.022 (0.032)
Train: 25 [ 300/1251 ( 24%)]  Loss: 4.371 (4.41)  Time: 0.313s, 3268.94/s  (0.312s, 3284.14/s)  LR: 9.827e-04  Data: 0.023 (0.030)
Train: 25 [ 350/1251 ( 28%)]  Loss: 4.691 (4.45)  Time: 0.312s, 3282.74/s  (0.311s, 3289.52/s)  LR: 9.827e-04  Data: 0.023 (0.029)
Train: 25 [ 400/1251 ( 32%)]  Loss: 4.346 (4.44)  Time: 0.309s, 3317.74/s  (0.311s, 3292.15/s)  LR: 9.826e-04  Data: 0.024 (0.028)
Train: 25 [ 450/1251 ( 36%)]  Loss: 4.503 (4.44)  Time: 0.306s, 3340.95/s  (0.311s, 3293.36/s)  LR: 9.826e-04  Data: 0.020 (0.028)
Train: 25 [ 500/1251 ( 40%)]  Loss: 4.091 (4.41)  Time: 0.312s, 3286.28/s  (0.311s, 3294.09/s)  LR: 9.825e-04  Data: 0.021 (0.027)
Train: 25 [ 550/1251 ( 44%)]  Loss: 4.385 (4.41)  Time: 0.312s, 3279.73/s  (0.311s, 3294.74/s)  LR: 9.824e-04  Data: 0.023 (0.027)
Train: 25 [ 600/1251 ( 48%)]  Loss: 4.275 (4.40)  Time: 0.309s, 3312.24/s  (0.311s, 3294.95/s)  LR: 9.824e-04  Data: 0.024 (0.027)
Train: 25 [ 650/1251 ( 52%)]  Loss: 4.359 (4.40)  Time: 0.309s, 3309.75/s  (0.311s, 3295.58/s)  LR: 9.823e-04  Data: 0.022 (0.026)
Train: 25 [ 700/1251 ( 56%)]  Loss: 4.279 (4.39)  Time: 0.315s, 3255.22/s  (0.311s, 3296.13/s)  LR: 9.823e-04  Data: 0.024 (0.026)
Train: 25 [ 750/1251 ( 60%)]  Loss: 4.499 (4.39)  Time: 0.312s, 3286.80/s  (0.311s, 3296.07/s)  LR: 9.822e-04  Data: 0.023 (0.026)
Train: 25 [ 800/1251 ( 64%)]  Loss: 4.752 (4.42)  Time: 0.310s, 3299.93/s  (0.311s, 3295.97/s)  LR: 9.822e-04  Data: 0.022 (0.026)
Train: 25 [ 850/1251 ( 68%)]  Loss: 4.337 (4.41)  Time: 0.314s, 3258.13/s  (0.311s, 3295.60/s)  LR: 9.821e-04  Data: 0.022 (0.026)
Train: 25 [ 900/1251 ( 72%)]  Loss: 4.448 (4.41)  Time: 0.322s, 3175.55/s  (0.311s, 3295.27/s)  LR: 9.821e-04  Data: 0.022 (0.025)
Train: 25 [ 950/1251 ( 76%)]  Loss: 4.532 (4.42)  Time: 0.313s, 3268.53/s  (0.311s, 3294.67/s)  LR: 9.820e-04  Data: 0.023 (0.025)
Train: 25 [1000/1251 ( 80%)]  Loss: 4.137 (4.41)  Time: 0.311s, 3293.36/s  (0.311s, 3294.65/s)  LR: 9.820e-04  Data: 0.024 (0.025)
Train: 25 [1050/1251 ( 84%)]  Loss: 4.477 (4.41)  Time: 0.310s, 3298.55/s  (0.311s, 3295.01/s)  LR: 9.819e-04  Data: 0.021 (0.025)
Train: 25 [1100/1251 ( 88%)]  Loss: 4.535 (4.41)  Time: 0.311s, 3291.13/s  (0.311s, 3294.87/s)  LR: 9.818e-04  Data: 0.025 (0.025)
Train: 25 [1150/1251 ( 92%)]  Loss: 4.010 (4.40)  Time: 0.309s, 3315.96/s  (0.311s, 3294.73/s)  LR: 9.818e-04  Data: 0.021 (0.025)
Train: 25 [1200/1251 ( 96%)]  Loss: 3.973 (4.38)  Time: 0.310s, 3308.26/s  (0.311s, 3294.45/s)  LR: 9.817e-04  Data: 0.020 (0.025)
Train: 25 [1250/1251 (100%)]  Loss: 4.457 (4.38)  Time: 0.285s, 3589.29/s  (0.311s, 3296.10/s)  LR: 9.817e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.020 (2.020)  Loss:  1.1016 (1.1016)  Acc@1: 78.8086 (78.8086)  Acc@5: 92.2852 (92.2852)
Test: [  48/48]  Time: 0.067 (0.235)  Loss:  1.1094 (1.9077)  Acc@1: 77.8302 (58.6980)  Acc@5: 91.1557 (82.0860)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-25.pth.tar', 58.69800006591797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-24.pth.tar', 58.27200002197266)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-23.pth.tar', 57.15800004638672)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-22.pth.tar', 56.13399989501953)

Train: 26 [   0/1251 (  0%)]  Loss: 4.590 (4.59)  Time: 2.210s,  463.44/s  (2.210s,  463.44/s)  LR: 9.817e-04  Data: 1.985 (1.985)
Train: 26 [  50/1251 (  4%)]  Loss: 4.314 (4.45)  Time: 0.301s, 3401.15/s  (0.330s, 3101.06/s)  LR: 9.816e-04  Data: 0.021 (0.064)
Train: 26 [ 100/1251 (  8%)]  Loss: 4.633 (4.51)  Time: 0.307s, 3334.95/s  (0.318s, 3221.11/s)  LR: 9.816e-04  Data: 0.021 (0.044)
Train: 26 [ 150/1251 ( 12%)]  Loss: 4.493 (4.51)  Time: 0.306s, 3351.37/s  (0.315s, 3253.82/s)  LR: 9.815e-04  Data: 0.022 (0.037)
Train: 26 [ 200/1251 ( 16%)]  Loss: 4.000 (4.41)  Time: 0.308s, 3324.16/s  (0.313s, 3267.23/s)  LR: 9.814e-04  Data: 0.025 (0.033)
Train: 26 [ 250/1251 ( 20%)]  Loss: 4.497 (4.42)  Time: 0.315s, 3254.31/s  (0.313s, 3274.44/s)  LR: 9.814e-04  Data: 0.025 (0.031)
Train: 26 [ 300/1251 ( 24%)]  Loss: 4.282 (4.40)  Time: 0.320s, 3201.40/s  (0.312s, 3279.99/s)  LR: 9.813e-04  Data: 0.026 (0.030)
Train: 26 [ 350/1251 ( 28%)]  Loss: 4.914 (4.47)  Time: 0.312s, 3280.97/s  (0.312s, 3282.92/s)  LR: 9.813e-04  Data: 0.025 (0.029)
Train: 26 [ 400/1251 ( 32%)]  Loss: 4.406 (4.46)  Time: 0.306s, 3342.50/s  (0.312s, 3284.91/s)  LR: 9.812e-04  Data: 0.027 (0.028)
Train: 26 [ 450/1251 ( 36%)]  Loss: 4.272 (4.44)  Time: 0.309s, 3313.00/s  (0.312s, 3286.17/s)  LR: 9.812e-04  Data: 0.022 (0.028)
Train: 26 [ 500/1251 ( 40%)]  Loss: 3.944 (4.40)  Time: 0.309s, 3312.99/s  (0.312s, 3286.95/s)  LR: 9.811e-04  Data: 0.022 (0.027)
Train: 26 [ 550/1251 ( 44%)]  Loss: 4.244 (4.38)  Time: 0.311s, 3289.56/s  (0.311s, 3287.56/s)  LR: 9.811e-04  Data: 0.022 (0.027)
Train: 26 [ 600/1251 ( 48%)]  Loss: 4.000 (4.35)  Time: 0.312s, 3280.45/s  (0.311s, 3288.32/s)  LR: 9.810e-04  Data: 0.022 (0.027)
Train: 26 [ 650/1251 ( 52%)]  Loss: 4.362 (4.35)  Time: 0.314s, 3263.74/s  (0.311s, 3289.25/s)  LR: 9.809e-04  Data: 0.025 (0.026)
Train: 26 [ 700/1251 ( 56%)]  Loss: 4.906 (4.39)  Time: 0.310s, 3298.10/s  (0.311s, 3289.04/s)  LR: 9.809e-04  Data: 0.021 (0.026)
Train: 26 [ 750/1251 ( 60%)]  Loss: 4.189 (4.38)  Time: 0.311s, 3296.30/s  (0.311s, 3289.24/s)  LR: 9.808e-04  Data: 0.023 (0.026)
Train: 26 [ 800/1251 ( 64%)]  Loss: 4.438 (4.38)  Time: 0.310s, 3302.36/s  (0.311s, 3289.95/s)  LR: 9.808e-04  Data: 0.022 (0.026)
Train: 26 [ 850/1251 ( 68%)]  Loss: 4.362 (4.38)  Time: 0.312s, 3280.67/s  (0.311s, 3290.30/s)  LR: 9.807e-04  Data: 0.024 (0.026)
Train: 26 [ 900/1251 ( 72%)]  Loss: 4.362 (4.38)  Time: 0.310s, 3305.50/s  (0.311s, 3290.48/s)  LR: 9.807e-04  Data: 0.020 (0.026)
Train: 26 [ 950/1251 ( 76%)]  Loss: 4.732 (4.40)  Time: 0.314s, 3256.41/s  (0.311s, 3291.01/s)  LR: 9.806e-04  Data: 0.026 (0.025)
Train: 26 [1000/1251 ( 80%)]  Loss: 4.359 (4.40)  Time: 0.311s, 3291.58/s  (0.311s, 3291.26/s)  LR: 9.805e-04  Data: 0.023 (0.025)
Train: 26 [1050/1251 ( 84%)]  Loss: 4.475 (4.40)  Time: 0.316s, 3239.13/s  (0.311s, 3291.25/s)  LR: 9.805e-04  Data: 0.022 (0.025)
Train: 26 [1100/1251 ( 88%)]  Loss: 4.040 (4.38)  Time: 0.314s, 3260.50/s  (0.311s, 3291.17/s)  LR: 9.804e-04  Data: 0.027 (0.025)
Train: 26 [1150/1251 ( 92%)]  Loss: 4.361 (4.38)  Time: 0.313s, 3274.65/s  (0.311s, 3291.19/s)  LR: 9.804e-04  Data: 0.023 (0.025)
Train: 26 [1200/1251 ( 96%)]  Loss: 4.384 (4.38)  Time: 0.307s, 3339.26/s  (0.311s, 3291.56/s)  LR: 9.803e-04  Data: 0.022 (0.025)
Train: 26 [1250/1251 (100%)]  Loss: 4.518 (4.39)  Time: 0.280s, 3654.92/s  (0.311s, 3293.79/s)  LR: 9.802e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.056 (2.056)  Loss:  1.0068 (1.0068)  Acc@1: 79.1992 (79.1992)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.053 (0.231)  Loss:  1.0234 (1.8328)  Acc@1: 79.7170 (59.5680)  Acc@5: 91.2736 (82.7360)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-26.pth.tar', 59.568000109863284)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-25.pth.tar', 58.69800006591797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-24.pth.tar', 58.27200002197266)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-23.pth.tar', 57.15800004638672)

Train: 27 [   0/1251 (  0%)]  Loss: 4.225 (4.23)  Time: 2.306s,  444.06/s  (2.306s,  444.06/s)  LR: 9.802e-04  Data: 2.070 (2.070)
Train: 27 [  50/1251 (  4%)]  Loss: 4.488 (4.36)  Time: 0.299s, 3419.08/s  (0.334s, 3065.37/s)  LR: 9.802e-04  Data: 0.022 (0.063)
Train: 27 [ 100/1251 (  8%)]  Loss: 4.037 (4.25)  Time: 0.306s, 3345.72/s  (0.320s, 3200.80/s)  LR: 9.801e-04  Data: 0.021 (0.044)
Train: 27 [ 150/1251 ( 12%)]  Loss: 4.638 (4.35)  Time: 0.316s, 3236.16/s  (0.316s, 3240.53/s)  LR: 9.801e-04  Data: 0.021 (0.037)
Train: 27 [ 200/1251 ( 16%)]  Loss: 4.753 (4.43)  Time: 0.303s, 3376.40/s  (0.314s, 3259.54/s)  LR: 9.800e-04  Data: 0.018 (0.033)
Train: 27 [ 250/1251 ( 20%)]  Loss: 4.653 (4.47)  Time: 0.312s, 3286.46/s  (0.313s, 3268.98/s)  LR: 9.800e-04  Data: 0.023 (0.031)
Train: 27 [ 300/1251 ( 24%)]  Loss: 4.302 (4.44)  Time: 0.310s, 3300.54/s  (0.313s, 3275.37/s)  LR: 9.799e-04  Data: 0.021 (0.030)
Train: 27 [ 350/1251 ( 28%)]  Loss: 4.666 (4.47)  Time: 0.310s, 3307.31/s  (0.312s, 3278.94/s)  LR: 9.798e-04  Data: 0.021 (0.029)
Train: 27 [ 400/1251 ( 32%)]  Loss: 4.332 (4.46)  Time: 0.308s, 3327.54/s  (0.312s, 3281.30/s)  LR: 9.798e-04  Data: 0.025 (0.028)
Train: 27 [ 450/1251 ( 36%)]  Loss: 4.481 (4.46)  Time: 0.309s, 3310.30/s  (0.312s, 3283.60/s)  LR: 9.797e-04  Data: 0.025 (0.028)
Train: 27 [ 500/1251 ( 40%)]  Loss: 4.323 (4.45)  Time: 0.311s, 3296.53/s  (0.312s, 3285.25/s)  LR: 9.797e-04  Data: 0.022 (0.027)
Train: 27 [ 550/1251 ( 44%)]  Loss: 4.399 (4.44)  Time: 0.311s, 3288.03/s  (0.312s, 3286.42/s)  LR: 9.796e-04  Data: 0.023 (0.027)
Train: 27 [ 600/1251 ( 48%)]  Loss: 4.315 (4.43)  Time: 0.313s, 3269.87/s  (0.312s, 3287.28/s)  LR: 9.795e-04  Data: 0.024 (0.027)
Train: 27 [ 650/1251 ( 52%)]  Loss: 4.141 (4.41)  Time: 0.312s, 3286.09/s  (0.311s, 3287.50/s)  LR: 9.795e-04  Data: 0.022 (0.026)
Train: 27 [ 700/1251 ( 56%)]  Loss: 4.200 (4.40)  Time: 0.312s, 3277.89/s  (0.311s, 3287.64/s)  LR: 9.794e-04  Data: 0.023 (0.026)
Train: 27 [ 750/1251 ( 60%)]  Loss: 4.376 (4.40)  Time: 0.311s, 3290.80/s  (0.312s, 3286.95/s)  LR: 9.794e-04  Data: 0.022 (0.026)
Train: 27 [ 800/1251 ( 64%)]  Loss: 4.325 (4.39)  Time: 0.317s, 3227.16/s  (0.312s, 3286.97/s)  LR: 9.793e-04  Data: 0.022 (0.026)
Train: 27 [ 850/1251 ( 68%)]  Loss: 4.750 (4.41)  Time: 0.311s, 3289.67/s  (0.311s, 3287.46/s)  LR: 9.792e-04  Data: 0.023 (0.026)
Train: 27 [ 900/1251 ( 72%)]  Loss: 4.402 (4.41)  Time: 0.315s, 3253.19/s  (0.312s, 3286.85/s)  LR: 9.792e-04  Data: 0.023 (0.026)
Train: 27 [ 950/1251 ( 76%)]  Loss: 4.230 (4.40)  Time: 0.309s, 3314.71/s  (0.312s, 3287.02/s)  LR: 9.791e-04  Data: 0.022 (0.025)
Train: 27 [1000/1251 ( 80%)]  Loss: 4.150 (4.39)  Time: 0.309s, 3313.73/s  (0.312s, 3287.31/s)  LR: 9.791e-04  Data: 0.021 (0.025)
Train: 27 [1050/1251 ( 84%)]  Loss: 4.582 (4.40)  Time: 0.314s, 3260.70/s  (0.312s, 3287.30/s)  LR: 9.790e-04  Data: 0.022 (0.025)
Train: 27 [1100/1251 ( 88%)]  Loss: 4.496 (4.40)  Time: 0.315s, 3247.17/s  (0.312s, 3287.03/s)  LR: 9.789e-04  Data: 0.027 (0.025)
Train: 27 [1150/1251 ( 92%)]  Loss: 4.401 (4.40)  Time: 0.311s, 3293.53/s  (0.312s, 3286.91/s)  LR: 9.789e-04  Data: 0.024 (0.025)
Train: 27 [1200/1251 ( 96%)]  Loss: 4.519 (4.41)  Time: 0.310s, 3308.42/s  (0.312s, 3286.71/s)  LR: 9.788e-04  Data: 0.027 (0.025)
Train: 27 [1250/1251 (100%)]  Loss: 4.511 (4.41)  Time: 0.283s, 3617.33/s  (0.311s, 3288.44/s)  LR: 9.788e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.021 (2.021)  Loss:  1.0166 (1.0166)  Acc@1: 79.1992 (79.1992)  Acc@5: 93.0664 (93.0664)
Test: [  48/48]  Time: 0.058 (0.234)  Loss:  1.0918 (1.8430)  Acc@1: 76.8868 (59.6160)  Acc@5: 91.8632 (82.6880)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-27.pth.tar', 59.61600004394531)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-26.pth.tar', 59.568000109863284)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-25.pth.tar', 58.69800006591797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-24.pth.tar', 58.27200002197266)

Train: 28 [   0/1251 (  0%)]  Loss: 4.696 (4.70)  Time: 2.132s,  480.33/s  (2.132s,  480.33/s)  LR: 9.788e-04  Data: 1.905 (1.905)
Train: 28 [  50/1251 (  4%)]  Loss: 4.070 (4.38)  Time: 0.306s, 3343.89/s  (0.332s, 3085.36/s)  LR: 9.787e-04  Data: 0.025 (0.061)
Train: 28 [ 100/1251 (  8%)]  Loss: 4.846 (4.54)  Time: 0.303s, 3384.24/s  (0.319s, 3214.42/s)  LR: 9.786e-04  Data: 0.023 (0.042)
Train: 28 [ 150/1251 ( 12%)]  Loss: 4.589 (4.55)  Time: 0.310s, 3300.73/s  (0.315s, 3249.39/s)  LR: 9.786e-04  Data: 0.020 (0.036)
Train: 28 [ 200/1251 ( 16%)]  Loss: 4.370 (4.51)  Time: 0.307s, 3336.68/s  (0.314s, 3262.95/s)  LR: 9.785e-04  Data: 0.025 (0.033)
Train: 28 [ 250/1251 ( 20%)]  Loss: 3.908 (4.41)  Time: 0.311s, 3296.98/s  (0.313s, 3271.67/s)  LR: 9.785e-04  Data: 0.022 (0.031)
Train: 28 [ 300/1251 ( 24%)]  Loss: 4.246 (4.39)  Time: 0.306s, 3345.10/s  (0.313s, 3275.67/s)  LR: 9.784e-04  Data: 0.021 (0.030)
Train: 28 [ 350/1251 ( 28%)]  Loss: 4.342 (4.38)  Time: 0.308s, 3319.51/s  (0.312s, 3280.03/s)  LR: 9.783e-04  Data: 0.022 (0.029)
Train: 28 [ 400/1251 ( 32%)]  Loss: 4.644 (4.41)  Time: 0.306s, 3349.49/s  (0.312s, 3282.97/s)  LR: 9.783e-04  Data: 0.027 (0.028)
Train: 28 [ 450/1251 ( 36%)]  Loss: 4.329 (4.40)  Time: 0.311s, 3289.43/s  (0.312s, 3284.70/s)  LR: 9.782e-04  Data: 0.021 (0.028)
Train: 28 [ 500/1251 ( 40%)]  Loss: 4.572 (4.42)  Time: 0.319s, 3213.37/s  (0.312s, 3286.47/s)  LR: 9.782e-04  Data: 0.027 (0.027)
Train: 28 [ 550/1251 ( 44%)]  Loss: 4.229 (4.40)  Time: 0.310s, 3302.13/s  (0.311s, 3287.58/s)  LR: 9.781e-04  Data: 0.024 (0.027)
Train: 28 [ 600/1251 ( 48%)]  Loss: 4.375 (4.40)  Time: 0.313s, 3273.65/s  (0.311s, 3288.32/s)  LR: 9.780e-04  Data: 0.024 (0.026)
Train: 28 [ 650/1251 ( 52%)]  Loss: 4.458 (4.41)  Time: 0.311s, 3294.06/s  (0.311s, 3289.25/s)  LR: 9.780e-04  Data: 0.025 (0.026)
Train: 28 [ 700/1251 ( 56%)]  Loss: 4.333 (4.40)  Time: 0.309s, 3313.65/s  (0.311s, 3289.78/s)  LR: 9.779e-04  Data: 0.020 (0.026)
Train: 28 [ 750/1251 ( 60%)]  Loss: 4.321 (4.40)  Time: 0.305s, 3355.09/s  (0.311s, 3289.93/s)  LR: 9.779e-04  Data: 0.021 (0.026)
Train: 28 [ 800/1251 ( 64%)]  Loss: 4.308 (4.39)  Time: 0.308s, 3325.73/s  (0.311s, 3289.93/s)  LR: 9.778e-04  Data: 0.022 (0.026)
Train: 28 [ 850/1251 ( 68%)]  Loss: 4.115 (4.38)  Time: 0.318s, 3218.57/s  (0.311s, 3290.00/s)  LR: 9.777e-04  Data: 0.021 (0.026)
Train: 28 [ 900/1251 ( 72%)]  Loss: 4.015 (4.36)  Time: 0.309s, 3309.91/s  (0.311s, 3289.53/s)  LR: 9.777e-04  Data: 0.022 (0.025)
Train: 28 [ 950/1251 ( 76%)]  Loss: 4.386 (4.36)  Time: 0.317s, 3229.46/s  (0.311s, 3289.42/s)  LR: 9.776e-04  Data: 0.023 (0.025)
Train: 28 [1000/1251 ( 80%)]  Loss: 3.960 (4.34)  Time: 0.313s, 3274.70/s  (0.311s, 3289.40/s)  LR: 9.775e-04  Data: 0.024 (0.025)
Train: 28 [1050/1251 ( 84%)]  Loss: 4.113 (4.33)  Time: 0.315s, 3252.73/s  (0.311s, 3289.44/s)  LR: 9.775e-04  Data: 0.024 (0.025)
Train: 28 [1100/1251 ( 88%)]  Loss: 4.286 (4.33)  Time: 0.315s, 3254.71/s  (0.311s, 3289.15/s)  LR: 9.774e-04  Data: 0.022 (0.025)
Train: 28 [1150/1251 ( 92%)]  Loss: 4.069 (4.32)  Time: 0.308s, 3327.83/s  (0.311s, 3289.32/s)  LR: 9.774e-04  Data: 0.024 (0.025)
Train: 28 [1200/1251 ( 96%)]  Loss: 4.765 (4.33)  Time: 0.317s, 3230.07/s  (0.311s, 3289.13/s)  LR: 9.773e-04  Data: 0.025 (0.025)
Train: 28 [1250/1251 (100%)]  Loss: 4.189 (4.33)  Time: 0.289s, 3548.13/s  (0.311s, 3291.15/s)  LR: 9.772e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.231 (2.231)  Loss:  0.9512 (0.9512)  Acc@1: 80.4688 (80.4688)  Acc@5: 93.5547 (93.5547)
Test: [  48/48]  Time: 0.070 (0.236)  Loss:  1.1357 (1.8022)  Acc@1: 77.3585 (60.7120)  Acc@5: 90.9198 (83.4160)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-28.pth.tar', 60.711999990234375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-27.pth.tar', 59.61600004394531)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-26.pth.tar', 59.568000109863284)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-25.pth.tar', 58.69800006591797)

Train: 29 [   0/1251 (  0%)]  Loss: 4.295 (4.29)  Time: 2.148s,  476.75/s  (2.148s,  476.75/s)  LR: 9.772e-04  Data: 1.922 (1.922)
Train: 29 [  50/1251 (  4%)]  Loss: 4.206 (4.25)  Time: 0.304s, 3363.41/s  (0.332s, 3081.65/s)  LR: 9.772e-04  Data: 0.025 (0.060)
Train: 29 [ 100/1251 (  8%)]  Loss: 4.520 (4.34)  Time: 0.304s, 3362.93/s  (0.318s, 3220.21/s)  LR: 9.771e-04  Data: 0.020 (0.042)
Train: 29 [ 150/1251 ( 12%)]  Loss: 4.579 (4.40)  Time: 0.310s, 3306.59/s  (0.314s, 3258.00/s)  LR: 9.770e-04  Data: 0.024 (0.036)
Train: 29 [ 200/1251 ( 16%)]  Loss: 4.409 (4.40)  Time: 0.306s, 3348.48/s  (0.313s, 3273.35/s)  LR: 9.770e-04  Data: 0.022 (0.033)
Train: 29 [ 250/1251 ( 20%)]  Loss: 4.462 (4.41)  Time: 0.308s, 3322.99/s  (0.312s, 3281.54/s)  LR: 9.769e-04  Data: 0.022 (0.031)
Train: 29 [ 300/1251 ( 24%)]  Loss: 4.456 (4.42)  Time: 0.312s, 3276.82/s  (0.312s, 3285.57/s)  LR: 9.769e-04  Data: 0.023 (0.030)
Train: 29 [ 350/1251 ( 28%)]  Loss: 4.449 (4.42)  Time: 0.310s, 3299.95/s  (0.311s, 3287.43/s)  LR: 9.768e-04  Data: 0.021 (0.029)
Train: 29 [ 400/1251 ( 32%)]  Loss: 4.384 (4.42)  Time: 0.308s, 3325.69/s  (0.311s, 3289.15/s)  LR: 9.767e-04  Data: 0.025 (0.028)
Train: 29 [ 450/1251 ( 36%)]  Loss: 4.520 (4.43)  Time: 0.309s, 3315.90/s  (0.311s, 3290.09/s)  LR: 9.767e-04  Data: 0.023 (0.027)
Train: 29 [ 500/1251 ( 40%)]  Loss: 4.768 (4.46)  Time: 0.311s, 3296.33/s  (0.311s, 3290.58/s)  LR: 9.766e-04  Data: 0.027 (0.027)
Train: 29 [ 550/1251 ( 44%)]  Loss: 3.794 (4.40)  Time: 0.309s, 3314.90/s  (0.311s, 3291.00/s)  LR: 9.765e-04  Data: 0.023 (0.027)
Train: 29 [ 600/1251 ( 48%)]  Loss: 4.855 (4.44)  Time: 0.307s, 3331.51/s  (0.311s, 3291.65/s)  LR: 9.765e-04  Data: 0.021 (0.026)
Train: 29 [ 650/1251 ( 52%)]  Loss: 4.478 (4.44)  Time: 0.313s, 3272.52/s  (0.311s, 3291.79/s)  LR: 9.764e-04  Data: 0.023 (0.026)
Train: 29 [ 700/1251 ( 56%)]  Loss: 4.063 (4.42)  Time: 0.316s, 3242.74/s  (0.311s, 3291.95/s)  LR: 9.764e-04  Data: 0.027 (0.026)
Train: 29 [ 750/1251 ( 60%)]  Loss: 4.176 (4.40)  Time: 0.311s, 3290.84/s  (0.311s, 3292.23/s)  LR: 9.763e-04  Data: 0.024 (0.026)
Train: 29 [ 800/1251 ( 64%)]  Loss: 4.333 (4.40)  Time: 0.307s, 3335.80/s  (0.311s, 3292.59/s)  LR: 9.762e-04  Data: 0.025 (0.026)
Train: 29 [ 850/1251 ( 68%)]  Loss: 4.102 (4.38)  Time: 0.314s, 3265.57/s  (0.311s, 3292.53/s)  LR: 9.762e-04  Data: 0.024 (0.025)
Train: 29 [ 900/1251 ( 72%)]  Loss: 4.361 (4.38)  Time: 0.308s, 3320.11/s  (0.311s, 3292.71/s)  LR: 9.761e-04  Data: 0.024 (0.025)
Train: 29 [ 950/1251 ( 76%)]  Loss: 4.619 (4.39)  Time: 0.313s, 3272.91/s  (0.311s, 3292.89/s)  LR: 9.760e-04  Data: 0.023 (0.025)
Train: 29 [1000/1251 ( 80%)]  Loss: 4.377 (4.39)  Time: 0.308s, 3327.59/s  (0.311s, 3293.16/s)  LR: 9.760e-04  Data: 0.026 (0.025)
Train: 29 [1050/1251 ( 84%)]  Loss: 4.578 (4.40)  Time: 0.316s, 3236.34/s  (0.311s, 3293.07/s)  LR: 9.759e-04  Data: 0.025 (0.025)
Train: 29 [1100/1251 ( 88%)]  Loss: 3.921 (4.38)  Time: 0.322s, 3176.88/s  (0.311s, 3292.66/s)  LR: 9.758e-04  Data: 0.023 (0.025)
Train: 29 [1150/1251 ( 92%)]  Loss: 4.226 (4.37)  Time: 0.313s, 3274.83/s  (0.311s, 3292.16/s)  LR: 9.758e-04  Data: 0.022 (0.025)
Train: 29 [1200/1251 ( 96%)]  Loss: 4.220 (4.37)  Time: 0.307s, 3339.75/s  (0.311s, 3291.87/s)  LR: 9.757e-04  Data: 0.024 (0.025)
Train: 29 [1250/1251 (100%)]  Loss: 4.009 (4.35)  Time: 0.279s, 3675.73/s  (0.311s, 3293.87/s)  LR: 9.757e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.080 (2.080)  Loss:  1.0850 (1.0850)  Acc@1: 78.4180 (78.4180)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.042 (0.240)  Loss:  1.0244 (1.7720)  Acc@1: 79.2453 (60.9520)  Acc@5: 90.9198 (83.7960)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-29.pth.tar', 60.951999904785154)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-28.pth.tar', 60.711999990234375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-27.pth.tar', 59.61600004394531)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-26.pth.tar', 59.568000109863284)

Train: 30 [   0/1251 (  0%)]  Loss: 4.367 (4.37)  Time: 2.764s,  370.50/s  (2.764s,  370.50/s)  LR: 9.756e-04  Data: 2.554 (2.554)
Train: 30 [  50/1251 (  4%)]  Loss: 4.190 (4.28)  Time: 0.297s, 3448.86/s  (0.336s, 3048.24/s)  LR: 9.756e-04  Data: 0.026 (0.072)
Train: 30 [ 100/1251 (  8%)]  Loss: 4.503 (4.35)  Time: 0.307s, 3336.69/s  (0.320s, 3203.24/s)  LR: 9.755e-04  Data: 0.023 (0.048)
Train: 30 [ 150/1251 ( 12%)]  Loss: 4.200 (4.31)  Time: 0.310s, 3300.91/s  (0.315s, 3247.83/s)  LR: 9.755e-04  Data: 0.025 (0.040)
Train: 30 [ 200/1251 ( 16%)]  Loss: 4.609 (4.37)  Time: 0.308s, 3329.25/s  (0.313s, 3267.17/s)  LR: 9.754e-04  Data: 0.024 (0.036)
Train: 30 [ 250/1251 ( 20%)]  Loss: 4.174 (4.34)  Time: 0.315s, 3253.18/s  (0.313s, 3276.49/s)  LR: 9.753e-04  Data: 0.021 (0.033)
Train: 30 [ 300/1251 ( 24%)]  Loss: 4.087 (4.30)  Time: 0.317s, 3233.06/s  (0.312s, 3282.89/s)  LR: 9.753e-04  Data: 0.023 (0.032)
Train: 30 [ 350/1251 ( 28%)]  Loss: 4.052 (4.27)  Time: 0.312s, 3285.24/s  (0.312s, 3285.82/s)  LR: 9.752e-04  Data: 0.026 (0.030)
Train: 30 [ 400/1251 ( 32%)]  Loss: 4.008 (4.24)  Time: 0.309s, 3312.62/s  (0.311s, 3289.07/s)  LR: 9.751e-04  Data: 0.026 (0.030)
Train: 30 [ 450/1251 ( 36%)]  Loss: 4.354 (4.25)  Time: 0.308s, 3323.92/s  (0.311s, 3291.76/s)  LR: 9.751e-04  Data: 0.021 (0.029)
Train: 30 [ 500/1251 ( 40%)]  Loss: 4.290 (4.26)  Time: 0.311s, 3287.90/s  (0.311s, 3293.72/s)  LR: 9.750e-04  Data: 0.022 (0.028)
Train: 30 [ 550/1251 ( 44%)]  Loss: 4.512 (4.28)  Time: 0.313s, 3272.09/s  (0.311s, 3294.83/s)  LR: 9.749e-04  Data: 0.023 (0.028)
Train: 30 [ 600/1251 ( 48%)]  Loss: 4.092 (4.26)  Time: 0.309s, 3313.61/s  (0.311s, 3295.28/s)  LR: 9.749e-04  Data: 0.022 (0.027)
Train: 30 [ 650/1251 ( 52%)]  Loss: 4.608 (4.29)  Time: 0.312s, 3285.16/s  (0.311s, 3295.47/s)  LR: 9.748e-04  Data: 0.021 (0.027)
Train: 30 [ 700/1251 ( 56%)]  Loss: 4.121 (4.28)  Time: 0.309s, 3316.00/s  (0.311s, 3295.41/s)  LR: 9.747e-04  Data: 0.015 (0.027)
Train: 30 [ 750/1251 ( 60%)]  Loss: 4.262 (4.28)  Time: 0.307s, 3331.41/s  (0.311s, 3295.75/s)  LR: 9.747e-04  Data: 0.022 (0.027)
Train: 30 [ 800/1251 ( 64%)]  Loss: 3.654 (4.24)  Time: 0.308s, 3325.07/s  (0.311s, 3295.76/s)  LR: 9.746e-04  Data: 0.022 (0.026)
Train: 30 [ 850/1251 ( 68%)]  Loss: 4.288 (4.24)  Time: 0.310s, 3302.12/s  (0.311s, 3295.75/s)  LR: 9.745e-04  Data: 0.024 (0.026)
Train: 30 [ 900/1251 ( 72%)]  Loss: 4.293 (4.25)  Time: 0.311s, 3293.62/s  (0.311s, 3295.65/s)  LR: 9.745e-04  Data: 0.023 (0.026)
Train: 30 [ 950/1251 ( 76%)]  Loss: 4.583 (4.26)  Time: 0.305s, 3361.15/s  (0.311s, 3296.12/s)  LR: 9.744e-04  Data: 0.020 (0.026)
Train: 30 [1000/1251 ( 80%)]  Loss: 4.503 (4.27)  Time: 0.308s, 3322.26/s  (0.311s, 3296.19/s)  LR: 9.743e-04  Data: 0.023 (0.026)
Train: 30 [1050/1251 ( 84%)]  Loss: 4.362 (4.28)  Time: 0.317s, 3230.66/s  (0.311s, 3296.29/s)  LR: 9.743e-04  Data: 0.022 (0.026)
Train: 30 [1100/1251 ( 88%)]  Loss: 4.229 (4.28)  Time: 0.312s, 3286.54/s  (0.311s, 3296.53/s)  LR: 9.742e-04  Data: 0.023 (0.026)
Train: 30 [1150/1251 ( 92%)]  Loss: 4.431 (4.28)  Time: 0.311s, 3296.51/s  (0.311s, 3296.60/s)  LR: 9.741e-04  Data: 0.022 (0.025)
Train: 30 [1200/1251 ( 96%)]  Loss: 4.614 (4.30)  Time: 0.312s, 3282.44/s  (0.311s, 3296.42/s)  LR: 9.741e-04  Data: 0.022 (0.025)
Train: 30 [1250/1251 (100%)]  Loss: 4.199 (4.29)  Time: 0.277s, 3699.43/s  (0.310s, 3298.93/s)  LR: 9.740e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.290 (2.290)  Loss:  1.0381 (1.0381)  Acc@1: 79.6875 (79.6875)  Acc@5: 92.4805 (92.4805)
Test: [  48/48]  Time: 0.058 (0.242)  Loss:  0.9976 (1.7652)  Acc@1: 79.2453 (61.3560)  Acc@5: 92.0991 (83.8360)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-30.pth.tar', 61.35600003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-29.pth.tar', 60.951999904785154)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-28.pth.tar', 60.711999990234375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-27.pth.tar', 59.61600004394531)

Train: 31 [   0/1251 (  0%)]  Loss: 4.212 (4.21)  Time: 2.097s,  488.37/s  (2.097s,  488.37/s)  LR: 9.740e-04  Data: 1.868 (1.868)
Train: 31 [  50/1251 (  4%)]  Loss: 4.286 (4.25)  Time: 0.299s, 3429.12/s  (0.342s, 2992.10/s)  LR: 9.739e-04  Data: 0.025 (0.075)
Train: 31 [ 100/1251 (  8%)]  Loss: 4.526 (4.34)  Time: 0.310s, 3308.53/s  (0.323s, 3167.69/s)  LR: 9.739e-04  Data: 0.026 (0.049)
Train: 31 [ 150/1251 ( 12%)]  Loss: 4.207 (4.31)  Time: 0.305s, 3361.86/s  (0.318s, 3224.72/s)  LR: 9.738e-04  Data: 0.020 (0.041)
Train: 31 [ 200/1251 ( 16%)]  Loss: 4.595 (4.37)  Time: 0.308s, 3327.85/s  (0.315s, 3250.56/s)  LR: 9.737e-04  Data: 0.026 (0.036)
Train: 31 [ 250/1251 ( 20%)]  Loss: 4.270 (4.35)  Time: 0.309s, 3309.44/s  (0.314s, 3264.14/s)  LR: 9.737e-04  Data: 0.024 (0.034)
Train: 31 [ 300/1251 ( 24%)]  Loss: 3.946 (4.29)  Time: 0.310s, 3302.67/s  (0.313s, 3272.51/s)  LR: 9.736e-04  Data: 0.022 (0.032)
Train: 31 [ 350/1251 ( 28%)]  Loss: 4.446 (4.31)  Time: 0.312s, 3281.01/s  (0.312s, 3278.71/s)  LR: 9.735e-04  Data: 0.024 (0.031)
Train: 31 [ 400/1251 ( 32%)]  Loss: 4.305 (4.31)  Time: 0.307s, 3330.78/s  (0.312s, 3280.84/s)  LR: 9.735e-04  Data: 0.022 (0.030)
Train: 31 [ 450/1251 ( 36%)]  Loss: 4.477 (4.33)  Time: 0.310s, 3304.74/s  (0.312s, 3284.47/s)  LR: 9.734e-04  Data: 0.023 (0.029)
Train: 31 [ 500/1251 ( 40%)]  Loss: 4.257 (4.32)  Time: 0.308s, 3329.35/s  (0.311s, 3287.65/s)  LR: 9.733e-04  Data: 0.023 (0.029)
Train: 31 [ 550/1251 ( 44%)]  Loss: 4.318 (4.32)  Time: 0.307s, 3337.70/s  (0.311s, 3290.02/s)  LR: 9.733e-04  Data: 0.023 (0.028)
Train: 31 [ 600/1251 ( 48%)]  Loss: 4.337 (4.32)  Time: 0.306s, 3345.15/s  (0.311s, 3291.46/s)  LR: 9.732e-04  Data: 0.022 (0.028)
Train: 31 [ 650/1251 ( 52%)]  Loss: 4.103 (4.31)  Time: 0.311s, 3296.08/s  (0.311s, 3291.80/s)  LR: 9.731e-04  Data: 0.023 (0.027)
Train: 31 [ 700/1251 ( 56%)]  Loss: 4.079 (4.29)  Time: 0.319s, 3212.58/s  (0.311s, 3292.16/s)  LR: 9.731e-04  Data: 0.025 (0.027)
Train: 31 [ 750/1251 ( 60%)]  Loss: 4.264 (4.29)  Time: 0.307s, 3337.40/s  (0.311s, 3292.51/s)  LR: 9.730e-04  Data: 0.023 (0.027)
Train: 31 [ 800/1251 ( 64%)]  Loss: 4.405 (4.30)  Time: 0.306s, 3347.17/s  (0.311s, 3293.14/s)  LR: 9.729e-04  Data: 0.023 (0.027)
Train: 31 [ 850/1251 ( 68%)]  Loss: 3.993 (4.28)  Time: 0.307s, 3338.79/s  (0.311s, 3293.86/s)  LR: 9.729e-04  Data: 0.020 (0.026)
Train: 31 [ 900/1251 ( 72%)]  Loss: 4.089 (4.27)  Time: 0.314s, 3256.31/s  (0.311s, 3293.81/s)  LR: 9.728e-04  Data: 0.023 (0.026)
Train: 31 [ 950/1251 ( 76%)]  Loss: 3.951 (4.25)  Time: 0.312s, 3280.26/s  (0.311s, 3293.85/s)  LR: 9.727e-04  Data: 0.021 (0.026)
Train: 31 [1000/1251 ( 80%)]  Loss: 4.000 (4.24)  Time: 0.308s, 3328.78/s  (0.311s, 3294.25/s)  LR: 9.727e-04  Data: 0.025 (0.026)
Train: 31 [1050/1251 ( 84%)]  Loss: 3.993 (4.23)  Time: 0.312s, 3278.74/s  (0.311s, 3294.57/s)  LR: 9.726e-04  Data: 0.023 (0.026)
Train: 31 [1100/1251 ( 88%)]  Loss: 4.034 (4.22)  Time: 0.311s, 3295.05/s  (0.311s, 3294.64/s)  LR: 9.725e-04  Data: 0.024 (0.026)
Train: 31 [1150/1251 ( 92%)]  Loss: 4.325 (4.23)  Time: 0.317s, 3226.78/s  (0.311s, 3294.17/s)  LR: 9.725e-04  Data: 0.023 (0.026)
Train: 31 [1200/1251 ( 96%)]  Loss: 4.153 (4.22)  Time: 0.310s, 3298.63/s  (0.311s, 3293.85/s)  LR: 9.724e-04  Data: 0.022 (0.025)
Train: 31 [1250/1251 (100%)]  Loss: 4.076 (4.22)  Time: 0.282s, 3631.17/s  (0.311s, 3295.20/s)  LR: 9.723e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.977 (1.977)  Loss:  1.0303 (1.0303)  Acc@1: 78.9062 (78.9062)  Acc@5: 93.7500 (93.7500)
Test: [  48/48]  Time: 0.061 (0.236)  Loss:  0.9697 (1.7258)  Acc@1: 79.3632 (62.0380)  Acc@5: 92.0991 (84.4660)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-31.pth.tar', 62.03800008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-30.pth.tar', 61.35600003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-29.pth.tar', 60.951999904785154)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-28.pth.tar', 60.711999990234375)

Train: 32 [   0/1251 (  0%)]  Loss: 4.109 (4.11)  Time: 2.215s,  462.22/s  (2.215s,  462.22/s)  LR: 9.723e-04  Data: 1.975 (1.975)
Train: 32 [  50/1251 (  4%)]  Loss: 4.859 (4.48)  Time: 0.310s, 3308.02/s  (0.336s, 3049.53/s)  LR: 9.723e-04  Data: 0.023 (0.063)
Train: 32 [ 100/1251 (  8%)]  Loss: 4.368 (4.45)  Time: 0.309s, 3315.42/s  (0.321s, 3189.42/s)  LR: 9.722e-04  Data: 0.021 (0.043)
Train: 32 [ 150/1251 ( 12%)]  Loss: 3.954 (4.32)  Time: 0.308s, 3323.31/s  (0.317s, 3231.47/s)  LR: 9.721e-04  Data: 0.022 (0.037)
Train: 32 [ 200/1251 ( 16%)]  Loss: 4.434 (4.34)  Time: 0.307s, 3333.92/s  (0.315s, 3250.63/s)  LR: 9.721e-04  Data: 0.027 (0.033)
Train: 32 [ 250/1251 ( 20%)]  Loss: 4.309 (4.34)  Time: 0.311s, 3290.86/s  (0.314s, 3260.69/s)  LR: 9.720e-04  Data: 0.023 (0.031)
Train: 32 [ 300/1251 ( 24%)]  Loss: 4.385 (4.35)  Time: 0.305s, 3353.06/s  (0.313s, 3266.41/s)  LR: 9.719e-04  Data: 0.020 (0.030)
Train: 32 [ 350/1251 ( 28%)]  Loss: 4.582 (4.37)  Time: 0.313s, 3269.55/s  (0.313s, 3270.67/s)  LR: 9.718e-04  Data: 0.023 (0.029)
Train: 32 [ 400/1251 ( 32%)]  Loss: 4.381 (4.38)  Time: 0.315s, 3251.91/s  (0.313s, 3274.34/s)  LR: 9.718e-04  Data: 0.024 (0.028)
Train: 32 [ 450/1251 ( 36%)]  Loss: 4.004 (4.34)  Time: 0.313s, 3267.73/s  (0.313s, 3275.95/s)  LR: 9.717e-04  Data: 0.023 (0.028)
Train: 32 [ 500/1251 ( 40%)]  Loss: 4.617 (4.36)  Time: 0.313s, 3267.35/s  (0.312s, 3277.66/s)  LR: 9.716e-04  Data: 0.024 (0.027)
Train: 32 [ 550/1251 ( 44%)]  Loss: 4.505 (4.38)  Time: 0.312s, 3281.92/s  (0.312s, 3278.83/s)  LR: 9.716e-04  Data: 0.022 (0.027)
Train: 32 [ 600/1251 ( 48%)]  Loss: 3.758 (4.33)  Time: 0.313s, 3276.06/s  (0.312s, 3279.82/s)  LR: 9.715e-04  Data: 0.023 (0.027)
Train: 32 [ 650/1251 ( 52%)]  Loss: 4.686 (4.35)  Time: 0.313s, 3274.46/s  (0.312s, 3280.73/s)  LR: 9.714e-04  Data: 0.026 (0.026)
Train: 32 [ 700/1251 ( 56%)]  Loss: 4.390 (4.36)  Time: 0.312s, 3281.54/s  (0.312s, 3281.51/s)  LR: 9.714e-04  Data: 0.025 (0.026)
Train: 32 [ 750/1251 ( 60%)]  Loss: 4.216 (4.35)  Time: 0.312s, 3278.31/s  (0.312s, 3282.03/s)  LR: 9.713e-04  Data: 0.023 (0.026)
Train: 32 [ 800/1251 ( 64%)]  Loss: 4.476 (4.35)  Time: 0.308s, 3321.40/s  (0.312s, 3282.71/s)  LR: 9.712e-04  Data: 0.023 (0.026)
Train: 32 [ 850/1251 ( 68%)]  Loss: 3.918 (4.33)  Time: 0.311s, 3297.63/s  (0.312s, 3283.27/s)  LR: 9.711e-04  Data: 0.024 (0.026)
Train: 32 [ 900/1251 ( 72%)]  Loss: 4.601 (4.34)  Time: 0.310s, 3302.67/s  (0.312s, 3283.49/s)  LR: 9.711e-04  Data: 0.022 (0.025)
Train: 32 [ 950/1251 ( 76%)]  Loss: 4.230 (4.34)  Time: 0.309s, 3315.12/s  (0.312s, 3284.01/s)  LR: 9.710e-04  Data: 0.022 (0.025)
Train: 32 [1000/1251 ( 80%)]  Loss: 4.318 (4.34)  Time: 0.306s, 3351.35/s  (0.312s, 3284.65/s)  LR: 9.709e-04  Data: 0.022 (0.025)
Train: 32 [1050/1251 ( 84%)]  Loss: 3.941 (4.32)  Time: 0.308s, 3320.37/s  (0.312s, 3284.63/s)  LR: 9.709e-04  Data: 0.020 (0.025)
Train: 32 [1100/1251 ( 88%)]  Loss: 4.119 (4.31)  Time: 0.310s, 3305.30/s  (0.312s, 3284.58/s)  LR: 9.708e-04  Data: 0.021 (0.025)
Train: 32 [1150/1251 ( 92%)]  Loss: 3.965 (4.30)  Time: 0.309s, 3311.90/s  (0.312s, 3284.52/s)  LR: 9.707e-04  Data: 0.023 (0.025)
Train: 32 [1200/1251 ( 96%)]  Loss: 4.114 (4.29)  Time: 0.311s, 3295.87/s  (0.312s, 3284.79/s)  LR: 9.707e-04  Data: 0.016 (0.025)
Train: 32 [1250/1251 (100%)]  Loss: 4.201 (4.29)  Time: 0.282s, 3634.06/s  (0.312s, 3286.84/s)  LR: 9.706e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.957 (1.957)  Loss:  0.9888 (0.9888)  Acc@1: 79.9805 (79.9805)  Acc@5: 93.6523 (93.6523)
Test: [  48/48]  Time: 0.042 (0.235)  Loss:  0.9707 (1.6844)  Acc@1: 79.4811 (62.5440)  Acc@5: 93.7500 (84.9080)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-32.pth.tar', 62.54400000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-31.pth.tar', 62.03800008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-30.pth.tar', 61.35600003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-29.pth.tar', 60.951999904785154)

Train: 33 [   0/1251 (  0%)]  Loss: 4.460 (4.46)  Time: 2.007s,  510.15/s  (2.007s,  510.15/s)  LR: 9.706e-04  Data: 1.766 (1.766)
Train: 33 [  50/1251 (  4%)]  Loss: 4.198 (4.33)  Time: 0.308s, 3323.25/s  (0.331s, 3097.88/s)  LR: 9.705e-04  Data: 0.022 (0.060)
Train: 33 [ 100/1251 (  8%)]  Loss: 4.295 (4.32)  Time: 0.311s, 3291.68/s  (0.318s, 3215.84/s)  LR: 9.704e-04  Data: 0.024 (0.041)
Train: 33 [ 150/1251 ( 12%)]  Loss: 4.480 (4.36)  Time: 0.307s, 3335.54/s  (0.315s, 3252.38/s)  LR: 9.704e-04  Data: 0.023 (0.035)
Train: 33 [ 200/1251 ( 16%)]  Loss: 3.960 (4.28)  Time: 0.307s, 3340.55/s  (0.314s, 3265.84/s)  LR: 9.703e-04  Data: 0.026 (0.032)
Train: 33 [ 250/1251 ( 20%)]  Loss: 4.562 (4.33)  Time: 0.318s, 3222.87/s  (0.313s, 3272.56/s)  LR: 9.702e-04  Data: 0.022 (0.030)
Train: 33 [ 300/1251 ( 24%)]  Loss: 4.050 (4.29)  Time: 0.310s, 3303.01/s  (0.313s, 3276.78/s)  LR: 9.702e-04  Data: 0.022 (0.029)
Train: 33 [ 350/1251 ( 28%)]  Loss: 4.008 (4.25)  Time: 0.309s, 3310.77/s  (0.312s, 3279.43/s)  LR: 9.701e-04  Data: 0.022 (0.028)
Train: 33 [ 400/1251 ( 32%)]  Loss: 4.286 (4.26)  Time: 0.308s, 3327.15/s  (0.312s, 3281.96/s)  LR: 9.700e-04  Data: 0.021 (0.027)
Train: 33 [ 450/1251 ( 36%)]  Loss: 4.239 (4.25)  Time: 0.312s, 3286.65/s  (0.312s, 3283.17/s)  LR: 9.699e-04  Data: 0.023 (0.027)
Train: 33 [ 500/1251 ( 40%)]  Loss: 3.731 (4.21)  Time: 0.312s, 3283.68/s  (0.312s, 3284.09/s)  LR: 9.699e-04  Data: 0.025 (0.027)
Train: 33 [ 550/1251 ( 44%)]  Loss: 4.549 (4.23)  Time: 0.312s, 3278.87/s  (0.312s, 3285.52/s)  LR: 9.698e-04  Data: 0.022 (0.026)
Train: 33 [ 600/1251 ( 48%)]  Loss: 4.251 (4.24)  Time: 0.307s, 3339.49/s  (0.312s, 3285.89/s)  LR: 9.697e-04  Data: 0.020 (0.026)
Train: 33 [ 650/1251 ( 52%)]  Loss: 4.276 (4.24)  Time: 0.313s, 3275.77/s  (0.312s, 3285.73/s)  LR: 9.697e-04  Data: 0.027 (0.026)
Train: 33 [ 700/1251 ( 56%)]  Loss: 4.445 (4.25)  Time: 0.314s, 3258.17/s  (0.312s, 3285.99/s)  LR: 9.696e-04  Data: 0.021 (0.026)
Train: 33 [ 750/1251 ( 60%)]  Loss: 4.100 (4.24)  Time: 0.310s, 3306.70/s  (0.312s, 3286.47/s)  LR: 9.695e-04  Data: 0.022 (0.025)
Train: 33 [ 800/1251 ( 64%)]  Loss: 4.475 (4.26)  Time: 0.311s, 3294.38/s  (0.312s, 3286.37/s)  LR: 9.694e-04  Data: 0.021 (0.025)
Train: 33 [ 850/1251 ( 68%)]  Loss: 4.063 (4.25)  Time: 0.310s, 3300.51/s  (0.312s, 3286.59/s)  LR: 9.694e-04  Data: 0.022 (0.025)
Train: 33 [ 900/1251 ( 72%)]  Loss: 4.055 (4.24)  Time: 0.316s, 3238.56/s  (0.312s, 3286.26/s)  LR: 9.693e-04  Data: 0.023 (0.025)
Train: 33 [ 950/1251 ( 76%)]  Loss: 4.336 (4.24)  Time: 0.311s, 3293.29/s  (0.312s, 3286.34/s)  LR: 9.692e-04  Data: 0.020 (0.025)
Train: 33 [1000/1251 ( 80%)]  Loss: 4.303 (4.24)  Time: 0.310s, 3307.54/s  (0.312s, 3285.96/s)  LR: 9.692e-04  Data: 0.023 (0.025)
Train: 33 [1050/1251 ( 84%)]  Loss: 3.980 (4.23)  Time: 0.316s, 3243.09/s  (0.312s, 3285.90/s)  LR: 9.691e-04  Data: 0.021 (0.025)
Train: 33 [1100/1251 ( 88%)]  Loss: 4.328 (4.24)  Time: 0.314s, 3257.06/s  (0.312s, 3285.99/s)  LR: 9.690e-04  Data: 0.022 (0.025)
Train: 33 [1150/1251 ( 92%)]  Loss: 4.022 (4.23)  Time: 0.313s, 3269.58/s  (0.312s, 3285.39/s)  LR: 9.689e-04  Data: 0.024 (0.024)
Train: 33 [1200/1251 ( 96%)]  Loss: 3.849 (4.21)  Time: 0.306s, 3348.10/s  (0.312s, 3285.50/s)  LR: 9.689e-04  Data: 0.025 (0.024)
Train: 33 [1250/1251 (100%)]  Loss: 4.048 (4.21)  Time: 0.278s, 3679.08/s  (0.311s, 3287.66/s)  LR: 9.688e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.970 (1.970)  Loss:  0.9844 (0.9844)  Acc@1: 79.8828 (79.8828)  Acc@5: 93.6523 (93.6523)
Test: [  48/48]  Time: 0.052 (0.233)  Loss:  0.9888 (1.6906)  Acc@1: 80.0708 (62.7980)  Acc@5: 93.8679 (85.0420)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-33.pth.tar', 62.798000004882816)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-32.pth.tar', 62.54400000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-31.pth.tar', 62.03800008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-30.pth.tar', 61.35600003417969)

Train: 34 [   0/1251 (  0%)]  Loss: 4.228 (4.23)  Time: 2.245s,  456.06/s  (2.245s,  456.06/s)  LR: 9.688e-04  Data: 2.019 (2.019)
Train: 34 [  50/1251 (  4%)]  Loss: 4.412 (4.32)  Time: 0.305s, 3356.77/s  (0.334s, 3064.25/s)  LR: 9.687e-04  Data: 0.025 (0.063)
Train: 34 [ 100/1251 (  8%)]  Loss: 4.052 (4.23)  Time: 0.315s, 3246.95/s  (0.320s, 3197.09/s)  LR: 9.687e-04  Data: 0.026 (0.043)
Train: 34 [ 150/1251 ( 12%)]  Loss: 4.017 (4.18)  Time: 0.309s, 3309.02/s  (0.317s, 3233.71/s)  LR: 9.686e-04  Data: 0.020 (0.037)
Train: 34 [ 200/1251 ( 16%)]  Loss: 4.226 (4.19)  Time: 0.308s, 3320.20/s  (0.315s, 3250.04/s)  LR: 9.685e-04  Data: 0.023 (0.033)
Train: 34 [ 250/1251 ( 20%)]  Loss: 4.378 (4.22)  Time: 0.313s, 3268.25/s  (0.314s, 3259.38/s)  LR: 9.684e-04  Data: 0.024 (0.031)
Train: 34 [ 300/1251 ( 24%)]  Loss: 3.893 (4.17)  Time: 0.305s, 3353.32/s  (0.314s, 3264.78/s)  LR: 9.684e-04  Data: 0.021 (0.030)
Train: 34 [ 350/1251 ( 28%)]  Loss: 3.786 (4.12)  Time: 0.312s, 3285.91/s  (0.313s, 3268.20/s)  LR: 9.683e-04  Data: 0.023 (0.029)
Train: 34 [ 400/1251 ( 32%)]  Loss: 3.982 (4.11)  Time: 0.309s, 3308.58/s  (0.313s, 3270.68/s)  LR: 9.682e-04  Data: 0.025 (0.028)
Train: 34 [ 450/1251 ( 36%)]  Loss: 4.108 (4.11)  Time: 0.311s, 3292.28/s  (0.313s, 3272.44/s)  LR: 9.681e-04  Data: 0.026 (0.027)
Train: 34 [ 500/1251 ( 40%)]  Loss: 4.231 (4.12)  Time: 0.310s, 3303.04/s  (0.313s, 3274.53/s)  LR: 9.681e-04  Data: 0.021 (0.027)
Train: 34 [ 550/1251 ( 44%)]  Loss: 4.140 (4.12)  Time: 0.305s, 3353.10/s  (0.313s, 3276.62/s)  LR: 9.680e-04  Data: 0.020 (0.026)
Train: 34 [ 600/1251 ( 48%)]  Loss: 4.348 (4.14)  Time: 0.310s, 3299.47/s  (0.312s, 3277.43/s)  LR: 9.679e-04  Data: 0.020 (0.026)
Train: 34 [ 650/1251 ( 52%)]  Loss: 3.842 (4.12)  Time: 0.309s, 3309.65/s  (0.312s, 3278.81/s)  LR: 9.678e-04  Data: 0.022 (0.026)
Train: 34 [ 700/1251 ( 56%)]  Loss: 4.249 (4.13)  Time: 0.310s, 3303.16/s  (0.312s, 3280.06/s)  LR: 9.678e-04  Data: 0.023 (0.026)
Train: 34 [ 750/1251 ( 60%)]  Loss: 3.853 (4.11)  Time: 0.309s, 3309.68/s  (0.312s, 3280.91/s)  LR: 9.677e-04  Data: 0.020 (0.025)
Train: 34 [ 800/1251 ( 64%)]  Loss: 4.214 (4.12)  Time: 0.314s, 3262.09/s  (0.312s, 3281.28/s)  LR: 9.676e-04  Data: 0.023 (0.025)
Train: 34 [ 850/1251 ( 68%)]  Loss: 4.407 (4.13)  Time: 0.314s, 3264.67/s  (0.312s, 3282.23/s)  LR: 9.676e-04  Data: 0.019 (0.025)
Train: 34 [ 900/1251 ( 72%)]  Loss: 4.157 (4.13)  Time: 0.309s, 3318.61/s  (0.312s, 3282.70/s)  LR: 9.675e-04  Data: 0.024 (0.025)
Train: 34 [ 950/1251 ( 76%)]  Loss: 3.904 (4.12)  Time: 0.313s, 3270.37/s  (0.312s, 3283.24/s)  LR: 9.674e-04  Data: 0.020 (0.025)
Train: 34 [1000/1251 ( 80%)]  Loss: 4.318 (4.13)  Time: 0.310s, 3305.74/s  (0.312s, 3283.89/s)  LR: 9.673e-04  Data: 0.023 (0.025)
Train: 34 [1050/1251 ( 84%)]  Loss: 4.521 (4.15)  Time: 0.314s, 3266.18/s  (0.312s, 3284.69/s)  LR: 9.673e-04  Data: 0.024 (0.025)
Train: 34 [1100/1251 ( 88%)]  Loss: 4.366 (4.16)  Time: 0.312s, 3282.17/s  (0.312s, 3284.75/s)  LR: 9.672e-04  Data: 0.024 (0.025)
Train: 34 [1150/1251 ( 92%)]  Loss: 4.235 (4.16)  Time: 0.308s, 3323.77/s  (0.312s, 3285.04/s)  LR: 9.671e-04  Data: 0.023 (0.025)
Train: 34 [1200/1251 ( 96%)]  Loss: 4.034 (4.16)  Time: 0.313s, 3267.07/s  (0.312s, 3285.51/s)  LR: 9.670e-04  Data: 0.019 (0.024)
Train: 34 [1250/1251 (100%)]  Loss: 4.163 (4.16)  Time: 0.278s, 3687.87/s  (0.311s, 3288.13/s)  LR: 9.670e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.077 (2.077)  Loss:  0.9761 (0.9761)  Acc@1: 81.2500 (81.2500)  Acc@5: 94.1406 (94.1406)
Test: [  48/48]  Time: 0.042 (0.236)  Loss:  0.9990 (1.6849)  Acc@1: 77.8302 (63.1400)  Acc@5: 93.6321 (85.3760)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-34.pth.tar', 63.13999993652344)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-33.pth.tar', 62.798000004882816)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-32.pth.tar', 62.54400000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-31.pth.tar', 62.03800008544922)

Train: 35 [   0/1251 (  0%)]  Loss: 4.210 (4.21)  Time: 2.431s,  421.27/s  (2.431s,  421.27/s)  LR: 9.670e-04  Data: 2.198 (2.198)
Train: 35 [  50/1251 (  4%)]  Loss: 4.143 (4.18)  Time: 0.306s, 3347.48/s  (0.334s, 3067.63/s)  LR: 9.669e-04  Data: 0.024 (0.065)
Train: 35 [ 100/1251 (  8%)]  Loss: 4.108 (4.15)  Time: 0.305s, 3362.06/s  (0.319s, 3206.21/s)  LR: 9.668e-04  Data: 0.022 (0.044)
Train: 35 [ 150/1251 ( 12%)]  Loss: 4.114 (4.14)  Time: 0.310s, 3305.50/s  (0.315s, 3245.80/s)  LR: 9.667e-04  Data: 0.023 (0.037)
Train: 35 [ 200/1251 ( 16%)]  Loss: 4.400 (4.20)  Time: 0.304s, 3371.70/s  (0.314s, 3265.07/s)  LR: 9.667e-04  Data: 0.025 (0.033)
Train: 35 [ 250/1251 ( 20%)]  Loss: 4.111 (4.18)  Time: 0.308s, 3328.98/s  (0.313s, 3273.78/s)  LR: 9.666e-04  Data: 0.021 (0.031)
Train: 35 [ 300/1251 ( 24%)]  Loss: 4.399 (4.21)  Time: 0.310s, 3304.56/s  (0.312s, 3278.63/s)  LR: 9.665e-04  Data: 0.026 (0.030)
Train: 35 [ 350/1251 ( 28%)]  Loss: 4.166 (4.21)  Time: 0.308s, 3326.59/s  (0.312s, 3280.78/s)  LR: 9.664e-04  Data: 0.021 (0.029)
Train: 35 [ 400/1251 ( 32%)]  Loss: 4.466 (4.24)  Time: 0.310s, 3306.91/s  (0.312s, 3282.56/s)  LR: 9.664e-04  Data: 0.021 (0.028)
Train: 35 [ 450/1251 ( 36%)]  Loss: 4.019 (4.21)  Time: 0.308s, 3321.70/s  (0.312s, 3284.63/s)  LR: 9.663e-04  Data: 0.021 (0.027)
Train: 35 [ 500/1251 ( 40%)]  Loss: 3.745 (4.17)  Time: 0.308s, 3327.53/s  (0.312s, 3284.98/s)  LR: 9.662e-04  Data: 0.024 (0.027)
Train: 35 [ 550/1251 ( 44%)]  Loss: 4.461 (4.20)  Time: 0.313s, 3271.06/s  (0.312s, 3285.83/s)  LR: 9.661e-04  Data: 0.023 (0.027)
Train: 35 [ 600/1251 ( 48%)]  Loss: 4.534 (4.22)  Time: 0.310s, 3298.42/s  (0.312s, 3286.43/s)  LR: 9.661e-04  Data: 0.023 (0.026)
Train: 35 [ 650/1251 ( 52%)]  Loss: 4.220 (4.22)  Time: 0.310s, 3304.56/s  (0.312s, 3287.06/s)  LR: 9.660e-04  Data: 0.021 (0.026)
Train: 35 [ 700/1251 ( 56%)]  Loss: 4.119 (4.21)  Time: 0.312s, 3287.28/s  (0.312s, 3286.83/s)  LR: 9.659e-04  Data: 0.023 (0.026)
Train: 35 [ 750/1251 ( 60%)]  Loss: 4.102 (4.21)  Time: 0.312s, 3285.38/s  (0.312s, 3286.21/s)  LR: 9.658e-04  Data: 0.020 (0.026)
Train: 35 [ 800/1251 ( 64%)]  Loss: 4.140 (4.20)  Time: 0.306s, 3342.50/s  (0.312s, 3285.65/s)  LR: 9.658e-04  Data: 0.021 (0.025)
Train: 35 [ 850/1251 ( 68%)]  Loss: 4.189 (4.20)  Time: 0.311s, 3289.96/s  (0.312s, 3284.75/s)  LR: 9.657e-04  Data: 0.022 (0.025)
Train: 35 [ 900/1251 ( 72%)]  Loss: 4.263 (4.21)  Time: 0.313s, 3267.06/s  (0.312s, 3284.64/s)  LR: 9.656e-04  Data: 0.024 (0.025)
Train: 35 [ 950/1251 ( 76%)]  Loss: 4.123 (4.20)  Time: 0.314s, 3257.81/s  (0.312s, 3284.45/s)  LR: 9.655e-04  Data: 0.021 (0.025)
Train: 35 [1000/1251 ( 80%)]  Loss: 4.541 (4.22)  Time: 0.309s, 3309.62/s  (0.312s, 3284.13/s)  LR: 9.654e-04  Data: 0.021 (0.025)
Train: 35 [1050/1251 ( 84%)]  Loss: 4.371 (4.22)  Time: 0.317s, 3232.66/s  (0.312s, 3283.73/s)  LR: 9.654e-04  Data: 0.021 (0.025)
Train: 35 [1100/1251 ( 88%)]  Loss: 4.360 (4.23)  Time: 0.312s, 3277.95/s  (0.312s, 3283.33/s)  LR: 9.653e-04  Data: 0.026 (0.025)
Train: 35 [1150/1251 ( 92%)]  Loss: 4.265 (4.23)  Time: 0.308s, 3322.63/s  (0.312s, 3283.10/s)  LR: 9.652e-04  Data: 0.022 (0.025)
Train: 35 [1200/1251 ( 96%)]  Loss: 4.306 (4.24)  Time: 0.311s, 3289.77/s  (0.312s, 3282.44/s)  LR: 9.651e-04  Data: 0.022 (0.025)
Train: 35 [1250/1251 (100%)]  Loss: 4.201 (4.23)  Time: 0.282s, 3630.02/s  (0.312s, 3284.02/s)  LR: 9.651e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.044 (2.044)  Loss:  0.9858 (0.9858)  Acc@1: 80.6641 (80.6641)  Acc@5: 93.7500 (93.7500)
Test: [  48/48]  Time: 0.046 (0.240)  Loss:  0.9844 (1.6588)  Acc@1: 79.5991 (63.7160)  Acc@5: 92.5707 (85.5820)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-35.pth.tar', 63.71600005859375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-34.pth.tar', 63.13999993652344)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-33.pth.tar', 62.798000004882816)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-32.pth.tar', 62.54400000732422)

Train: 36 [   0/1251 (  0%)]  Loss: 4.219 (4.22)  Time: 2.178s,  470.23/s  (2.178s,  470.23/s)  LR: 9.651e-04  Data: 1.954 (1.954)
Train: 36 [  50/1251 (  4%)]  Loss: 4.524 (4.37)  Time: 0.304s, 3365.89/s  (0.333s, 3079.45/s)  LR: 9.650e-04  Data: 0.018 (0.062)
Train: 36 [ 100/1251 (  8%)]  Loss: 3.924 (4.22)  Time: 0.310s, 3300.90/s  (0.319s, 3208.21/s)  LR: 9.649e-04  Data: 0.023 (0.042)
Train: 36 [ 150/1251 ( 12%)]  Loss: 4.307 (4.24)  Time: 0.311s, 3289.17/s  (0.316s, 3239.92/s)  LR: 9.648e-04  Data: 0.026 (0.036)
Train: 36 [ 200/1251 ( 16%)]  Loss: 3.894 (4.17)  Time: 0.310s, 3301.67/s  (0.314s, 3256.31/s)  LR: 9.648e-04  Data: 0.021 (0.032)
Train: 36 [ 250/1251 ( 20%)]  Loss: 3.929 (4.13)  Time: 0.311s, 3290.76/s  (0.314s, 3265.74/s)  LR: 9.647e-04  Data: 0.021 (0.030)
Train: 36 [ 300/1251 ( 24%)]  Loss: 4.098 (4.13)  Time: 0.314s, 3265.47/s  (0.313s, 3269.44/s)  LR: 9.646e-04  Data: 0.024 (0.029)
Train: 36 [ 350/1251 ( 28%)]  Loss: 4.046 (4.12)  Time: 0.311s, 3290.49/s  (0.313s, 3272.45/s)  LR: 9.645e-04  Data: 0.024 (0.028)
Train: 36 [ 400/1251 ( 32%)]  Loss: 4.416 (4.15)  Time: 0.310s, 3305.84/s  (0.313s, 3274.96/s)  LR: 9.644e-04  Data: 0.023 (0.028)
Train: 36 [ 450/1251 ( 36%)]  Loss: 3.918 (4.13)  Time: 0.309s, 3309.78/s  (0.312s, 3276.88/s)  LR: 9.644e-04  Data: 0.023 (0.027)
Train: 36 [ 500/1251 ( 40%)]  Loss: 4.202 (4.13)  Time: 0.316s, 3235.79/s  (0.312s, 3277.19/s)  LR: 9.643e-04  Data: 0.025 (0.027)
Train: 36 [ 550/1251 ( 44%)]  Loss: 3.924 (4.12)  Time: 0.313s, 3274.83/s  (0.312s, 3277.39/s)  LR: 9.642e-04  Data: 0.021 (0.026)
Train: 36 [ 600/1251 ( 48%)]  Loss: 4.242 (4.13)  Time: 0.310s, 3301.79/s  (0.312s, 3278.46/s)  LR: 9.641e-04  Data: 0.021 (0.026)
Train: 36 [ 650/1251 ( 52%)]  Loss: 4.308 (4.14)  Time: 0.314s, 3260.19/s  (0.312s, 3278.70/s)  LR: 9.641e-04  Data: 0.022 (0.026)
Train: 36 [ 700/1251 ( 56%)]  Loss: 4.269 (4.15)  Time: 0.319s, 3205.19/s  (0.312s, 3278.52/s)  LR: 9.640e-04  Data: 0.021 (0.025)
Train: 36 [ 750/1251 ( 60%)]  Loss: 3.981 (4.14)  Time: 0.317s, 3229.71/s  (0.312s, 3278.50/s)  LR: 9.639e-04  Data: 0.025 (0.025)
Train: 36 [ 800/1251 ( 64%)]  Loss: 4.280 (4.15)  Time: 0.315s, 3249.42/s  (0.312s, 3278.73/s)  LR: 9.638e-04  Data: 0.022 (0.025)
Train: 36 [ 850/1251 ( 68%)]  Loss: 4.308 (4.15)  Time: 0.309s, 3315.29/s  (0.312s, 3278.97/s)  LR: 9.637e-04  Data: 0.022 (0.025)
Train: 36 [ 900/1251 ( 72%)]  Loss: 4.209 (4.16)  Time: 0.319s, 3212.75/s  (0.312s, 3279.49/s)  LR: 9.637e-04  Data: 0.018 (0.025)
Train: 36 [ 950/1251 ( 76%)]  Loss: 3.974 (4.15)  Time: 0.310s, 3304.34/s  (0.312s, 3279.68/s)  LR: 9.636e-04  Data: 0.020 (0.025)
Train: 36 [1000/1251 ( 80%)]  Loss: 3.933 (4.14)  Time: 0.313s, 3266.47/s  (0.312s, 3280.00/s)  LR: 9.635e-04  Data: 0.022 (0.025)
Train: 36 [1050/1251 ( 84%)]  Loss: 4.249 (4.14)  Time: 0.311s, 3293.60/s  (0.312s, 3279.85/s)  LR: 9.634e-04  Data: 0.021 (0.025)
Train: 36 [1100/1251 ( 88%)]  Loss: 3.725 (4.13)  Time: 0.314s, 3262.75/s  (0.312s, 3279.88/s)  LR: 9.634e-04  Data: 0.023 (0.025)
Train: 36 [1150/1251 ( 92%)]  Loss: 4.011 (4.12)  Time: 0.314s, 3256.07/s  (0.312s, 3279.83/s)  LR: 9.633e-04  Data: 0.024 (0.024)
Train: 36 [1200/1251 ( 96%)]  Loss: 4.670 (4.14)  Time: 0.324s, 3158.55/s  (0.312s, 3279.92/s)  LR: 9.632e-04  Data: 0.026 (0.024)
Train: 36 [1250/1251 (100%)]  Loss: 4.229 (4.15)  Time: 0.283s, 3621.67/s  (0.312s, 3282.24/s)  LR: 9.631e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.061 (2.061)  Loss:  0.9644 (0.9644)  Acc@1: 80.2734 (80.2734)  Acc@5: 93.9453 (93.9453)
Test: [  48/48]  Time: 0.070 (0.236)  Loss:  0.9136 (1.6289)  Acc@1: 80.4245 (63.9940)  Acc@5: 93.5142 (85.6260)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-36.pth.tar', 63.99399989990234)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-35.pth.tar', 63.71600005859375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-34.pth.tar', 63.13999993652344)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-33.pth.tar', 62.798000004882816)

Train: 37 [   0/1251 (  0%)]  Loss: 3.954 (3.95)  Time: 2.009s,  509.82/s  (2.009s,  509.82/s)  LR: 9.631e-04  Data: 1.782 (1.782)
Train: 37 [  50/1251 (  4%)]  Loss: 3.933 (3.94)  Time: 0.303s, 3376.96/s  (0.332s, 3081.24/s)  LR: 9.630e-04  Data: 0.022 (0.060)
Train: 37 [ 100/1251 (  8%)]  Loss: 4.215 (4.03)  Time: 0.308s, 3320.13/s  (0.319s, 3207.05/s)  LR: 9.630e-04  Data: 0.025 (0.042)
Train: 37 [ 150/1251 ( 12%)]  Loss: 4.177 (4.07)  Time: 0.310s, 3301.62/s  (0.316s, 3243.11/s)  LR: 9.629e-04  Data: 0.023 (0.035)
Train: 37 [ 200/1251 ( 16%)]  Loss: 4.228 (4.10)  Time: 0.313s, 3269.06/s  (0.314s, 3258.32/s)  LR: 9.628e-04  Data: 0.020 (0.032)
Train: 37 [ 250/1251 ( 20%)]  Loss: 4.250 (4.13)  Time: 0.310s, 3303.76/s  (0.313s, 3267.79/s)  LR: 9.627e-04  Data: 0.024 (0.030)
Train: 37 [ 300/1251 ( 24%)]  Loss: 4.167 (4.13)  Time: 0.309s, 3317.85/s  (0.313s, 3273.33/s)  LR: 9.626e-04  Data: 0.022 (0.029)
Train: 37 [ 350/1251 ( 28%)]  Loss: 4.020 (4.12)  Time: 0.314s, 3265.90/s  (0.313s, 3275.84/s)  LR: 9.626e-04  Data: 0.022 (0.028)
Train: 37 [ 400/1251 ( 32%)]  Loss: 3.959 (4.10)  Time: 0.312s, 3281.62/s  (0.312s, 3277.95/s)  LR: 9.625e-04  Data: 0.023 (0.027)
Train: 37 [ 450/1251 ( 36%)]  Loss: 4.231 (4.11)  Time: 0.308s, 3320.09/s  (0.312s, 3279.94/s)  LR: 9.624e-04  Data: 0.023 (0.027)
Train: 37 [ 500/1251 ( 40%)]  Loss: 3.908 (4.09)  Time: 0.310s, 3306.33/s  (0.312s, 3280.82/s)  LR: 9.623e-04  Data: 0.019 (0.027)
Train: 37 [ 550/1251 ( 44%)]  Loss: 4.401 (4.12)  Time: 0.308s, 3329.24/s  (0.312s, 3281.08/s)  LR: 9.622e-04  Data: 0.019 (0.026)
Train: 37 [ 600/1251 ( 48%)]  Loss: 4.057 (4.12)  Time: 0.310s, 3300.94/s  (0.312s, 3280.88/s)  LR: 9.622e-04  Data: 0.021 (0.026)
Train: 37 [ 650/1251 ( 52%)]  Loss: 4.139 (4.12)  Time: 0.312s, 3280.94/s  (0.312s, 3281.16/s)  LR: 9.621e-04  Data: 0.025 (0.026)
Train: 37 [ 700/1251 ( 56%)]  Loss: 4.148 (4.12)  Time: 0.316s, 3244.39/s  (0.312s, 3281.61/s)  LR: 9.620e-04  Data: 0.023 (0.025)
Train: 37 [ 750/1251 ( 60%)]  Loss: 4.055 (4.12)  Time: 0.319s, 3213.53/s  (0.312s, 3281.77/s)  LR: 9.619e-04  Data: 0.023 (0.025)
Train: 37 [ 800/1251 ( 64%)]  Loss: 4.369 (4.13)  Time: 0.312s, 3282.15/s  (0.312s, 3282.02/s)  LR: 9.618e-04  Data: 0.023 (0.025)
Train: 37 [ 850/1251 ( 68%)]  Loss: 4.540 (4.15)  Time: 0.318s, 3216.78/s  (0.312s, 3281.77/s)  LR: 9.618e-04  Data: 0.022 (0.025)
Train: 37 [ 900/1251 ( 72%)]  Loss: 4.155 (4.15)  Time: 0.320s, 3199.71/s  (0.312s, 3281.28/s)  LR: 9.617e-04  Data: 0.025 (0.025)
Train: 37 [ 950/1251 ( 76%)]  Loss: 4.319 (4.16)  Time: 0.310s, 3302.34/s  (0.312s, 3280.92/s)  LR: 9.616e-04  Data: 0.025 (0.025)
Train: 37 [1000/1251 ( 80%)]  Loss: 4.271 (4.17)  Time: 0.312s, 3279.87/s  (0.312s, 3281.10/s)  LR: 9.615e-04  Data: 0.023 (0.025)
Train: 37 [1050/1251 ( 84%)]  Loss: 4.213 (4.17)  Time: 0.313s, 3271.14/s  (0.312s, 3280.57/s)  LR: 9.614e-04  Data: 0.026 (0.025)
Train: 37 [1100/1251 ( 88%)]  Loss: 4.641 (4.19)  Time: 0.314s, 3261.57/s  (0.312s, 3279.75/s)  LR: 9.614e-04  Data: 0.022 (0.024)
Train: 37 [1150/1251 ( 92%)]  Loss: 3.868 (4.18)  Time: 0.311s, 3297.01/s  (0.312s, 3279.69/s)  LR: 9.613e-04  Data: 0.022 (0.024)
Train: 37 [1200/1251 ( 96%)]  Loss: 4.127 (4.17)  Time: 0.310s, 3297.98/s  (0.312s, 3279.33/s)  LR: 9.612e-04  Data: 0.023 (0.024)
Train: 37 [1250/1251 (100%)]  Loss: 4.123 (4.17)  Time: 0.278s, 3679.86/s  (0.312s, 3281.05/s)  LR: 9.611e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.992 (1.992)  Loss:  0.9458 (0.9458)  Acc@1: 80.2734 (80.2734)  Acc@5: 93.6523 (93.6523)
Test: [  48/48]  Time: 0.053 (0.237)  Loss:  0.9624 (1.6270)  Acc@1: 79.3632 (63.8340)  Acc@5: 93.8679 (85.8060)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-36.pth.tar', 63.99399989990234)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-37.pth.tar', 63.83399995605469)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-35.pth.tar', 63.71600005859375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-34.pth.tar', 63.13999993652344)

Train: 38 [   0/1251 (  0%)]  Loss: 4.209 (4.21)  Time: 2.116s,  483.97/s  (2.116s,  483.97/s)  LR: 9.611e-04  Data: 1.891 (1.891)
Train: 38 [  50/1251 (  4%)]  Loss: 4.154 (4.18)  Time: 0.306s, 3347.14/s  (0.335s, 3059.73/s)  LR: 9.610e-04  Data: 0.017 (0.059)
Train: 38 [ 100/1251 (  8%)]  Loss: 4.236 (4.20)  Time: 0.309s, 3319.21/s  (0.321s, 3190.11/s)  LR: 9.610e-04  Data: 0.022 (0.041)
Train: 38 [ 150/1251 ( 12%)]  Loss: 4.347 (4.24)  Time: 0.310s, 3299.75/s  (0.317s, 3229.54/s)  LR: 9.609e-04  Data: 0.024 (0.035)
Train: 38 [ 200/1251 ( 16%)]  Loss: 4.177 (4.22)  Time: 0.312s, 3283.99/s  (0.316s, 3245.56/s)  LR: 9.608e-04  Data: 0.021 (0.032)
Train: 38 [ 250/1251 ( 20%)]  Loss: 3.920 (4.17)  Time: 0.309s, 3311.92/s  (0.315s, 3253.81/s)  LR: 9.607e-04  Data: 0.022 (0.030)
Train: 38 [ 300/1251 ( 24%)]  Loss: 4.039 (4.15)  Time: 0.311s, 3293.84/s  (0.314s, 3259.71/s)  LR: 9.606e-04  Data: 0.019 (0.029)
Train: 38 [ 350/1251 ( 28%)]  Loss: 4.030 (4.14)  Time: 0.317s, 3233.69/s  (0.314s, 3264.38/s)  LR: 9.606e-04  Data: 0.022 (0.028)
Train: 38 [ 400/1251 ( 32%)]  Loss: 4.247 (4.15)  Time: 0.308s, 3319.63/s  (0.313s, 3266.68/s)  LR: 9.605e-04  Data: 0.025 (0.027)
Train: 38 [ 450/1251 ( 36%)]  Loss: 3.944 (4.13)  Time: 0.311s, 3292.87/s  (0.313s, 3268.93/s)  LR: 9.604e-04  Data: 0.019 (0.027)
Train: 38 [ 500/1251 ( 40%)]  Loss: 4.375 (4.15)  Time: 0.311s, 3292.98/s  (0.313s, 3270.66/s)  LR: 9.603e-04  Data: 0.024 (0.026)
Train: 38 [ 550/1251 ( 44%)]  Loss: 4.263 (4.16)  Time: 0.320s, 3202.96/s  (0.313s, 3271.64/s)  LR: 9.602e-04  Data: 0.026 (0.026)
Train: 38 [ 600/1251 ( 48%)]  Loss: 4.289 (4.17)  Time: 0.310s, 3302.76/s  (0.313s, 3272.28/s)  LR: 9.602e-04  Data: 0.022 (0.026)
Train: 38 [ 650/1251 ( 52%)]  Loss: 3.849 (4.15)  Time: 0.316s, 3236.44/s  (0.313s, 3272.89/s)  LR: 9.601e-04  Data: 0.024 (0.025)
Train: 38 [ 700/1251 ( 56%)]  Loss: 4.081 (4.14)  Time: 0.318s, 3215.30/s  (0.313s, 3274.08/s)  LR: 9.600e-04  Data: 0.024 (0.025)
Train: 38 [ 750/1251 ( 60%)]  Loss: 4.167 (4.15)  Time: 0.314s, 3263.90/s  (0.313s, 3274.32/s)  LR: 9.599e-04  Data: 0.021 (0.025)
Train: 38 [ 800/1251 ( 64%)]  Loss: 3.919 (4.13)  Time: 0.313s, 3271.22/s  (0.313s, 3274.57/s)  LR: 9.598e-04  Data: 0.024 (0.025)
Train: 38 [ 850/1251 ( 68%)]  Loss: 4.043 (4.13)  Time: 0.315s, 3248.02/s  (0.313s, 3275.24/s)  LR: 9.597e-04  Data: 0.024 (0.025)
Train: 38 [ 900/1251 ( 72%)]  Loss: 3.679 (4.10)  Time: 0.310s, 3300.28/s  (0.313s, 3275.79/s)  LR: 9.597e-04  Data: 0.022 (0.025)
Train: 38 [ 950/1251 ( 76%)]  Loss: 3.990 (4.10)  Time: 0.309s, 3316.46/s  (0.313s, 3276.18/s)  LR: 9.596e-04  Data: 0.024 (0.025)
Train: 38 [1000/1251 ( 80%)]  Loss: 4.339 (4.11)  Time: 0.317s, 3232.82/s  (0.313s, 3276.50/s)  LR: 9.595e-04  Data: 0.022 (0.025)
Train: 38 [1050/1251 ( 84%)]  Loss: 4.463 (4.13)  Time: 0.312s, 3279.60/s  (0.313s, 3276.28/s)  LR: 9.594e-04  Data: 0.021 (0.024)
Train: 38 [1100/1251 ( 88%)]  Loss: 4.577 (4.15)  Time: 0.316s, 3235.77/s  (0.313s, 3276.47/s)  LR: 9.593e-04  Data: 0.025 (0.024)
Train: 38 [1150/1251 ( 92%)]  Loss: 4.061 (4.14)  Time: 0.313s, 3270.50/s  (0.313s, 3276.62/s)  LR: 9.592e-04  Data: 0.022 (0.024)
Train: 38 [1200/1251 ( 96%)]  Loss: 4.352 (4.15)  Time: 0.311s, 3290.05/s  (0.312s, 3277.06/s)  LR: 9.592e-04  Data: 0.027 (0.024)
Train: 38 [1250/1251 (100%)]  Loss: 4.422 (4.16)  Time: 0.278s, 3685.94/s  (0.312s, 3279.13/s)  LR: 9.591e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.982 (1.982)  Loss:  0.9585 (0.9585)  Acc@1: 81.7383 (81.7383)  Acc@5: 94.3359 (94.3359)
Test: [  48/48]  Time: 0.047 (0.236)  Loss:  0.9492 (1.6527)  Acc@1: 79.8349 (63.9920)  Acc@5: 93.2783 (85.8100)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-36.pth.tar', 63.99399989990234)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-38.pth.tar', 63.992000161132815)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-37.pth.tar', 63.83399995605469)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-35.pth.tar', 63.71600005859375)

Train: 39 [   0/1251 (  0%)]  Loss: 4.115 (4.11)  Time: 2.028s,  505.04/s  (2.028s,  505.04/s)  LR: 9.591e-04  Data: 1.788 (1.788)
Train: 39 [  50/1251 (  4%)]  Loss: 3.887 (4.00)  Time: 0.311s, 3295.26/s  (0.333s, 3079.21/s)  LR: 9.590e-04  Data: 0.022 (0.061)
Train: 39 [ 100/1251 (  8%)]  Loss: 4.456 (4.15)  Time: 0.309s, 3311.35/s  (0.319s, 3211.08/s)  LR: 9.589e-04  Data: 0.023 (0.042)
Train: 39 [ 150/1251 ( 12%)]  Loss: 4.268 (4.18)  Time: 0.308s, 3329.14/s  (0.315s, 3248.68/s)  LR: 9.588e-04  Data: 0.026 (0.036)
Train: 39 [ 200/1251 ( 16%)]  Loss: 3.952 (4.14)  Time: 0.313s, 3274.05/s  (0.314s, 3264.74/s)  LR: 9.587e-04  Data: 0.023 (0.032)
Train: 39 [ 250/1251 ( 20%)]  Loss: 4.235 (4.15)  Time: 0.313s, 3271.24/s  (0.313s, 3271.66/s)  LR: 9.587e-04  Data: 0.024 (0.030)
Train: 39 [ 300/1251 ( 24%)]  Loss: 4.127 (4.15)  Time: 0.313s, 3275.19/s  (0.313s, 3276.58/s)  LR: 9.586e-04  Data: 0.022 (0.029)
Train: 39 [ 350/1251 ( 28%)]  Loss: 3.862 (4.11)  Time: 0.312s, 3282.16/s  (0.312s, 3278.79/s)  LR: 9.585e-04  Data: 0.022 (0.028)
Train: 39 [ 400/1251 ( 32%)]  Loss: 3.936 (4.09)  Time: 0.307s, 3338.54/s  (0.312s, 3282.14/s)  LR: 9.584e-04  Data: 0.022 (0.028)
Train: 39 [ 450/1251 ( 36%)]  Loss: 4.211 (4.10)  Time: 0.313s, 3271.68/s  (0.312s, 3283.29/s)  LR: 9.583e-04  Data: 0.022 (0.027)
Train: 39 [ 500/1251 ( 40%)]  Loss: 4.342 (4.13)  Time: 0.312s, 3286.94/s  (0.312s, 3283.70/s)  LR: 9.583e-04  Data: 0.022 (0.027)
Train: 39 [ 550/1251 ( 44%)]  Loss: 4.243 (4.14)  Time: 0.310s, 3301.79/s  (0.312s, 3284.19/s)  LR: 9.582e-04  Data: 0.023 (0.026)
Train: 39 [ 600/1251 ( 48%)]  Loss: 3.850 (4.11)  Time: 0.312s, 3283.21/s  (0.312s, 3283.82/s)  LR: 9.581e-04  Data: 0.024 (0.026)
Train: 39 [ 650/1251 ( 52%)]  Loss: 4.375 (4.13)  Time: 0.309s, 3308.73/s  (0.312s, 3284.29/s)  LR: 9.580e-04  Data: 0.025 (0.026)
Train: 39 [ 700/1251 ( 56%)]  Loss: 3.998 (4.12)  Time: 0.310s, 3299.11/s  (0.312s, 3284.09/s)  LR: 9.579e-04  Data: 0.021 (0.025)
Train: 39 [ 750/1251 ( 60%)]  Loss: 4.486 (4.15)  Time: 0.316s, 3241.40/s  (0.312s, 3284.38/s)  LR: 9.578e-04  Data: 0.023 (0.025)
Train: 39 [ 800/1251 ( 64%)]  Loss: 4.236 (4.15)  Time: 0.309s, 3314.10/s  (0.312s, 3283.98/s)  LR: 9.577e-04  Data: 0.017 (0.025)
Train: 39 [ 850/1251 ( 68%)]  Loss: 4.093 (4.15)  Time: 0.310s, 3298.94/s  (0.312s, 3283.92/s)  LR: 9.577e-04  Data: 0.022 (0.025)
Train: 39 [ 900/1251 ( 72%)]  Loss: 4.060 (4.14)  Time: 0.313s, 3271.12/s  (0.312s, 3283.81/s)  LR: 9.576e-04  Data: 0.024 (0.025)
Train: 39 [ 950/1251 ( 76%)]  Loss: 4.008 (4.14)  Time: 0.313s, 3276.51/s  (0.312s, 3283.83/s)  LR: 9.575e-04  Data: 0.021 (0.025)
Train: 39 [1000/1251 ( 80%)]  Loss: 3.997 (4.13)  Time: 0.306s, 3346.11/s  (0.312s, 3283.65/s)  LR: 9.574e-04  Data: 0.022 (0.025)
Train: 39 [1050/1251 ( 84%)]  Loss: 4.410 (4.14)  Time: 0.308s, 3324.86/s  (0.312s, 3283.67/s)  LR: 9.573e-04  Data: 0.019 (0.025)
Train: 39 [1100/1251 ( 88%)]  Loss: 4.525 (4.16)  Time: 0.315s, 3252.64/s  (0.312s, 3283.82/s)  LR: 9.572e-04  Data: 0.025 (0.024)
Train: 39 [1150/1251 ( 92%)]  Loss: 4.292 (4.17)  Time: 0.314s, 3258.44/s  (0.312s, 3283.67/s)  LR: 9.572e-04  Data: 0.023 (0.024)
Train: 39 [1200/1251 ( 96%)]  Loss: 4.208 (4.17)  Time: 0.310s, 3304.21/s  (0.312s, 3283.87/s)  LR: 9.571e-04  Data: 0.024 (0.024)
Train: 39 [1250/1251 (100%)]  Loss: 4.150 (4.17)  Time: 0.282s, 3631.61/s  (0.312s, 3285.90/s)  LR: 9.570e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.034 (2.034)  Loss:  0.8604 (0.8604)  Acc@1: 81.1523 (81.1523)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.061 (0.236)  Loss:  0.9346 (1.5686)  Acc@1: 80.0708 (64.9720)  Acc@5: 92.4528 (86.5680)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-39.pth.tar', 64.97200000488282)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-36.pth.tar', 63.99399989990234)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-38.pth.tar', 63.992000161132815)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-37.pth.tar', 63.83399995605469)

Train: 40 [   0/1251 (  0%)]  Loss: 3.930 (3.93)  Time: 2.081s,  492.05/s  (2.081s,  492.05/s)  LR: 9.570e-04  Data: 1.842 (1.842)
Train: 40 [  50/1251 (  4%)]  Loss: 4.140 (4.04)  Time: 0.306s, 3344.36/s  (0.331s, 3092.40/s)  LR: 9.569e-04  Data: 0.022 (0.058)
Train: 40 [ 100/1251 (  8%)]  Loss: 4.438 (4.17)  Time: 0.302s, 3389.65/s  (0.318s, 3219.11/s)  LR: 9.568e-04  Data: 0.020 (0.041)
Train: 40 [ 150/1251 ( 12%)]  Loss: 3.823 (4.08)  Time: 0.313s, 3272.09/s  (0.314s, 3256.04/s)  LR: 9.567e-04  Data: 0.025 (0.035)
Train: 40 [ 200/1251 ( 16%)]  Loss: 4.045 (4.08)  Time: 0.307s, 3335.62/s  (0.313s, 3274.22/s)  LR: 9.566e-04  Data: 0.024 (0.032)
Train: 40 [ 250/1251 ( 20%)]  Loss: 4.298 (4.11)  Time: 0.313s, 3267.22/s  (0.312s, 3280.54/s)  LR: 9.566e-04  Data: 0.021 (0.030)
Train: 40 [ 300/1251 ( 24%)]  Loss: 4.095 (4.11)  Time: 0.312s, 3279.57/s  (0.312s, 3285.24/s)  LR: 9.565e-04  Data: 0.022 (0.029)
Train: 40 [ 350/1251 ( 28%)]  Loss: 4.105 (4.11)  Time: 0.314s, 3263.57/s  (0.312s, 3286.96/s)  LR: 9.564e-04  Data: 0.019 (0.028)
Train: 40 [ 400/1251 ( 32%)]  Loss: 3.935 (4.09)  Time: 0.308s, 3323.70/s  (0.311s, 3289.39/s)  LR: 9.563e-04  Data: 0.030 (0.027)
Train: 40 [ 450/1251 ( 36%)]  Loss: 4.185 (4.10)  Time: 0.312s, 3279.86/s  (0.311s, 3289.66/s)  LR: 9.562e-04  Data: 0.021 (0.027)
Train: 40 [ 500/1251 ( 40%)]  Loss: 3.700 (4.06)  Time: 0.314s, 3259.35/s  (0.311s, 3290.56/s)  LR: 9.561e-04  Data: 0.022 (0.026)
Train: 40 [ 550/1251 ( 44%)]  Loss: 3.650 (4.03)  Time: 0.307s, 3338.10/s  (0.311s, 3292.34/s)  LR: 9.561e-04  Data: 0.026 (0.026)
Train: 40 [ 600/1251 ( 48%)]  Loss: 3.980 (4.02)  Time: 0.307s, 3335.39/s  (0.311s, 3293.03/s)  LR: 9.560e-04  Data: 0.025 (0.026)
Train: 40 [ 650/1251 ( 52%)]  Loss: 4.156 (4.03)  Time: 0.309s, 3318.89/s  (0.311s, 3293.78/s)  LR: 9.559e-04  Data: 0.021 (0.026)
Train: 40 [ 700/1251 ( 56%)]  Loss: 4.126 (4.04)  Time: 0.316s, 3243.88/s  (0.311s, 3294.47/s)  LR: 9.558e-04  Data: 0.023 (0.025)
Train: 40 [ 750/1251 ( 60%)]  Loss: 4.040 (4.04)  Time: 0.309s, 3315.76/s  (0.311s, 3295.21/s)  LR: 9.557e-04  Data: 0.024 (0.025)
Train: 40 [ 800/1251 ( 64%)]  Loss: 4.372 (4.06)  Time: 0.309s, 3316.94/s  (0.311s, 3295.71/s)  LR: 9.556e-04  Data: 0.019 (0.025)
Train: 40 [ 850/1251 ( 68%)]  Loss: 4.029 (4.06)  Time: 0.309s, 3315.65/s  (0.311s, 3296.18/s)  LR: 9.555e-04  Data: 0.027 (0.025)
Train: 40 [ 900/1251 ( 72%)]  Loss: 4.065 (4.06)  Time: 0.312s, 3280.33/s  (0.311s, 3296.67/s)  LR: 9.554e-04  Data: 0.021 (0.025)
Train: 40 [ 950/1251 ( 76%)]  Loss: 3.807 (4.05)  Time: 0.311s, 3295.52/s  (0.311s, 3296.74/s)  LR: 9.554e-04  Data: 0.021 (0.025)
Train: 40 [1000/1251 ( 80%)]  Loss: 4.393 (4.06)  Time: 0.308s, 3328.56/s  (0.311s, 3296.81/s)  LR: 9.553e-04  Data: 0.022 (0.025)
Train: 40 [1050/1251 ( 84%)]  Loss: 3.841 (4.05)  Time: 0.308s, 3320.39/s  (0.311s, 3297.37/s)  LR: 9.552e-04  Data: 0.022 (0.024)
Train: 40 [1100/1251 ( 88%)]  Loss: 3.664 (4.04)  Time: 0.311s, 3295.20/s  (0.311s, 3297.84/s)  LR: 9.551e-04  Data: 0.022 (0.024)
Train: 40 [1150/1251 ( 92%)]  Loss: 4.001 (4.03)  Time: 0.312s, 3285.74/s  (0.311s, 3297.81/s)  LR: 9.550e-04  Data: 0.023 (0.024)
Train: 40 [1200/1251 ( 96%)]  Loss: 4.269 (4.04)  Time: 0.310s, 3306.60/s  (0.310s, 3297.94/s)  LR: 9.549e-04  Data: 0.021 (0.024)
Train: 40 [1250/1251 (100%)]  Loss: 3.981 (4.04)  Time: 0.277s, 3692.41/s  (0.310s, 3300.31/s)  LR: 9.548e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.264 (2.264)  Loss:  0.9038 (0.9038)  Acc@1: 81.6406 (81.6406)  Acc@5: 94.6289 (94.6289)
Test: [  48/48]  Time: 0.062 (0.232)  Loss:  0.9902 (1.5892)  Acc@1: 79.1274 (64.9860)  Acc@5: 93.3962 (86.4780)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-40.pth.tar', 64.98599998291016)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-39.pth.tar', 64.97200000488282)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-36.pth.tar', 63.99399989990234)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-38.pth.tar', 63.992000161132815)

Train: 41 [   0/1251 (  0%)]  Loss: 4.155 (4.16)  Time: 2.546s,  402.22/s  (2.546s,  402.22/s)  LR: 9.548e-04  Data: 2.316 (2.316)
Train: 41 [  50/1251 (  4%)]  Loss: 4.039 (4.10)  Time: 0.300s, 3409.05/s  (0.344s, 2973.39/s)  LR: 9.548e-04  Data: 0.022 (0.078)
Train: 41 [ 100/1251 (  8%)]  Loss: 3.855 (4.02)  Time: 0.304s, 3368.25/s  (0.324s, 3157.30/s)  LR: 9.547e-04  Data: 0.022 (0.051)
Train: 41 [ 150/1251 ( 12%)]  Loss: 4.017 (4.02)  Time: 0.316s, 3244.09/s  (0.318s, 3217.51/s)  LR: 9.546e-04  Data: 0.022 (0.042)
Train: 41 [ 200/1251 ( 16%)]  Loss: 4.084 (4.03)  Time: 0.308s, 3319.96/s  (0.316s, 3244.80/s)  LR: 9.545e-04  Data: 0.022 (0.037)
Train: 41 [ 250/1251 ( 20%)]  Loss: 4.278 (4.07)  Time: 0.313s, 3273.84/s  (0.314s, 3257.64/s)  LR: 9.544e-04  Data: 0.023 (0.034)
Train: 41 [ 300/1251 ( 24%)]  Loss: 4.150 (4.08)  Time: 0.308s, 3321.53/s  (0.314s, 3264.27/s)  LR: 9.543e-04  Data: 0.022 (0.032)
Train: 41 [ 350/1251 ( 28%)]  Loss: 4.120 (4.09)  Time: 0.314s, 3265.65/s  (0.313s, 3270.81/s)  LR: 9.542e-04  Data: 0.025 (0.031)
Train: 41 [ 400/1251 ( 32%)]  Loss: 4.446 (4.13)  Time: 0.312s, 3283.84/s  (0.313s, 3274.74/s)  LR: 9.541e-04  Data: 0.025 (0.030)
Train: 41 [ 450/1251 ( 36%)]  Loss: 4.113 (4.13)  Time: 0.309s, 3311.90/s  (0.312s, 3277.92/s)  LR: 9.541e-04  Data: 0.024 (0.029)
Train: 41 [ 500/1251 ( 40%)]  Loss: 4.180 (4.13)  Time: 0.317s, 3233.67/s  (0.312s, 3280.40/s)  LR: 9.540e-04  Data: 0.021 (0.029)
Train: 41 [ 550/1251 ( 44%)]  Loss: 3.694 (4.09)  Time: 0.312s, 3286.99/s  (0.312s, 3282.31/s)  LR: 9.539e-04  Data: 0.022 (0.028)
Train: 41 [ 600/1251 ( 48%)]  Loss: 3.816 (4.07)  Time: 0.312s, 3286.17/s  (0.312s, 3283.51/s)  LR: 9.538e-04  Data: 0.021 (0.028)
Train: 41 [ 650/1251 ( 52%)]  Loss: 4.200 (4.08)  Time: 0.314s, 3264.81/s  (0.312s, 3284.43/s)  LR: 9.537e-04  Data: 0.022 (0.027)
Train: 41 [ 700/1251 ( 56%)]  Loss: 3.671 (4.05)  Time: 0.308s, 3324.73/s  (0.312s, 3285.10/s)  LR: 9.536e-04  Data: 0.022 (0.027)
Train: 41 [ 750/1251 ( 60%)]  Loss: 4.219 (4.06)  Time: 0.308s, 3320.96/s  (0.312s, 3285.78/s)  LR: 9.535e-04  Data: 0.025 (0.027)
Train: 41 [ 800/1251 ( 64%)]  Loss: 3.978 (4.06)  Time: 0.310s, 3298.57/s  (0.312s, 3285.78/s)  LR: 9.534e-04  Data: 0.025 (0.027)
Train: 41 [ 850/1251 ( 68%)]  Loss: 4.092 (4.06)  Time: 0.313s, 3269.44/s  (0.312s, 3286.03/s)  LR: 9.534e-04  Data: 0.021 (0.026)
Train: 41 [ 900/1251 ( 72%)]  Loss: 4.471 (4.08)  Time: 0.312s, 3284.17/s  (0.312s, 3286.47/s)  LR: 9.533e-04  Data: 0.022 (0.026)
Train: 41 [ 950/1251 ( 76%)]  Loss: 3.976 (4.08)  Time: 0.310s, 3298.11/s  (0.312s, 3286.25/s)  LR: 9.532e-04  Data: 0.023 (0.026)
Train: 41 [1000/1251 ( 80%)]  Loss: 3.952 (4.07)  Time: 0.311s, 3289.58/s  (0.312s, 3286.05/s)  LR: 9.531e-04  Data: 0.022 (0.026)
Train: 41 [1050/1251 ( 84%)]  Loss: 3.989 (4.07)  Time: 0.313s, 3267.26/s  (0.312s, 3286.46/s)  LR: 9.530e-04  Data: 0.019 (0.026)
Train: 41 [1100/1251 ( 88%)]  Loss: 4.415 (4.08)  Time: 0.313s, 3270.88/s  (0.312s, 3286.25/s)  LR: 9.529e-04  Data: 0.023 (0.025)
Train: 41 [1150/1251 ( 92%)]  Loss: 4.584 (4.10)  Time: 0.312s, 3283.81/s  (0.312s, 3286.33/s)  LR: 9.528e-04  Data: 0.025 (0.025)
Train: 41 [1200/1251 ( 96%)]  Loss: 3.708 (4.09)  Time: 0.308s, 3323.78/s  (0.312s, 3286.17/s)  LR: 9.527e-04  Data: 0.022 (0.025)
Train: 41 [1250/1251 (100%)]  Loss: 3.776 (4.08)  Time: 0.281s, 3642.96/s  (0.311s, 3288.06/s)  LR: 9.527e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.036 (2.036)  Loss:  0.9004 (0.9004)  Acc@1: 82.6172 (82.6172)  Acc@5: 94.6289 (94.6289)
Test: [  48/48]  Time: 0.069 (0.239)  Loss:  0.9912 (1.5742)  Acc@1: 79.7170 (65.0460)  Acc@5: 93.1604 (86.7120)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-41.pth.tar', 65.04599998046875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-40.pth.tar', 64.98599998291016)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-39.pth.tar', 64.97200000488282)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-36.pth.tar', 63.99399989990234)

Train: 42 [   0/1251 (  0%)]  Loss: 3.981 (3.98)  Time: 2.391s,  428.36/s  (2.391s,  428.36/s)  LR: 9.526e-04  Data: 2.171 (2.171)
Train: 42 [  50/1251 (  4%)]  Loss: 3.834 (3.91)  Time: 0.307s, 3340.17/s  (0.337s, 3040.44/s)  LR: 9.526e-04  Data: 0.022 (0.065)
Train: 42 [ 100/1251 (  8%)]  Loss: 4.396 (4.07)  Time: 0.307s, 3340.84/s  (0.322s, 3185.05/s)  LR: 9.525e-04  Data: 0.023 (0.044)
Train: 42 [ 150/1251 ( 12%)]  Loss: 4.110 (4.08)  Time: 0.309s, 3311.08/s  (0.317s, 3231.74/s)  LR: 9.524e-04  Data: 0.023 (0.037)
Train: 42 [ 200/1251 ( 16%)]  Loss: 4.541 (4.17)  Time: 0.305s, 3354.79/s  (0.315s, 3250.60/s)  LR: 9.523e-04  Data: 0.025 (0.034)
Train: 42 [ 250/1251 ( 20%)]  Loss: 4.088 (4.16)  Time: 0.310s, 3300.82/s  (0.314s, 3261.18/s)  LR: 9.522e-04  Data: 0.023 (0.032)
Train: 42 [ 300/1251 ( 24%)]  Loss: 4.321 (4.18)  Time: 0.312s, 3279.08/s  (0.313s, 3268.58/s)  LR: 9.521e-04  Data: 0.021 (0.030)
Train: 42 [ 350/1251 ( 28%)]  Loss: 3.862 (4.14)  Time: 0.310s, 3307.57/s  (0.313s, 3273.98/s)  LR: 9.520e-04  Data: 0.025 (0.029)
Train: 42 [ 400/1251 ( 32%)]  Loss: 4.310 (4.16)  Time: 0.307s, 3340.70/s  (0.312s, 3277.58/s)  LR: 9.519e-04  Data: 0.027 (0.028)
Train: 42 [ 450/1251 ( 36%)]  Loss: 4.144 (4.16)  Time: 0.313s, 3268.53/s  (0.312s, 3279.39/s)  LR: 9.518e-04  Data: 0.024 (0.028)
Train: 42 [ 500/1251 ( 40%)]  Loss: 4.141 (4.16)  Time: 0.310s, 3301.32/s  (0.312s, 3280.74/s)  LR: 9.518e-04  Data: 0.023 (0.027)
Train: 42 [ 550/1251 ( 44%)]  Loss: 4.011 (4.14)  Time: 0.307s, 3339.76/s  (0.312s, 3281.53/s)  LR: 9.517e-04  Data: 0.022 (0.027)
Train: 42 [ 600/1251 ( 48%)]  Loss: 4.069 (4.14)  Time: 0.311s, 3291.75/s  (0.312s, 3282.61/s)  LR: 9.516e-04  Data: 0.019 (0.026)
Train: 42 [ 650/1251 ( 52%)]  Loss: 4.004 (4.13)  Time: 0.311s, 3295.16/s  (0.312s, 3283.35/s)  LR: 9.515e-04  Data: 0.022 (0.026)
Train: 42 [ 700/1251 ( 56%)]  Loss: 4.150 (4.13)  Time: 0.304s, 3363.08/s  (0.312s, 3283.98/s)  LR: 9.514e-04  Data: 0.022 (0.026)
Train: 42 [ 750/1251 ( 60%)]  Loss: 4.332 (4.14)  Time: 0.306s, 3342.12/s  (0.312s, 3283.96/s)  LR: 9.513e-04  Data: 0.025 (0.026)
Train: 42 [ 800/1251 ( 64%)]  Loss: 4.098 (4.14)  Time: 0.311s, 3290.98/s  (0.312s, 3283.80/s)  LR: 9.512e-04  Data: 0.024 (0.025)
Train: 42 [ 850/1251 ( 68%)]  Loss: 4.146 (4.14)  Time: 0.312s, 3283.79/s  (0.312s, 3283.93/s)  LR: 9.511e-04  Data: 0.022 (0.025)
Train: 42 [ 900/1251 ( 72%)]  Loss: 3.904 (4.13)  Time: 0.314s, 3260.16/s  (0.312s, 3284.02/s)  LR: 9.510e-04  Data: 0.021 (0.025)
Train: 42 [ 950/1251 ( 76%)]  Loss: 3.775 (4.11)  Time: 0.320s, 3201.72/s  (0.312s, 3284.08/s)  LR: 9.510e-04  Data: 0.024 (0.025)
Train: 42 [1000/1251 ( 80%)]  Loss: 4.438 (4.13)  Time: 0.313s, 3273.72/s  (0.312s, 3283.66/s)  LR: 9.509e-04  Data: 0.022 (0.025)
Train: 42 [1050/1251 ( 84%)]  Loss: 3.503 (4.10)  Time: 0.314s, 3258.22/s  (0.312s, 3283.13/s)  LR: 9.508e-04  Data: 0.021 (0.025)
Train: 42 [1100/1251 ( 88%)]  Loss: 4.120 (4.10)  Time: 0.315s, 3246.87/s  (0.312s, 3282.56/s)  LR: 9.507e-04  Data: 0.023 (0.025)
Train: 42 [1150/1251 ( 92%)]  Loss: 4.447 (4.11)  Time: 0.315s, 3246.70/s  (0.312s, 3281.98/s)  LR: 9.506e-04  Data: 0.024 (0.025)
Train: 42 [1200/1251 ( 96%)]  Loss: 4.069 (4.11)  Time: 0.307s, 3335.10/s  (0.312s, 3281.77/s)  LR: 9.505e-04  Data: 0.020 (0.025)
Train: 42 [1250/1251 (100%)]  Loss: 3.799 (4.10)  Time: 0.281s, 3641.58/s  (0.312s, 3283.96/s)  LR: 9.504e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.000 (2.000)  Loss:  0.8379 (0.8379)  Acc@1: 82.6172 (82.6172)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.064 (0.235)  Loss:  0.9478 (1.5665)  Acc@1: 79.4811 (64.9620)  Acc@5: 93.3962 (86.5660)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-41.pth.tar', 65.04599998046875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-40.pth.tar', 64.98599998291016)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-39.pth.tar', 64.97200000488282)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-42.pth.tar', 64.96199987792968)

Train: 43 [   0/1251 (  0%)]  Loss: 3.904 (3.90)  Time: 2.504s,  408.95/s  (2.504s,  408.95/s)  LR: 9.504e-04  Data: 2.266 (2.266)
Train: 43 [  50/1251 (  4%)]  Loss: 4.040 (3.97)  Time: 0.307s, 3334.92/s  (0.338s, 3030.60/s)  LR: 9.503e-04  Data: 0.023 (0.067)
Train: 43 [ 100/1251 (  8%)]  Loss: 3.628 (3.86)  Time: 0.309s, 3318.20/s  (0.322s, 3177.82/s)  LR: 9.502e-04  Data: 0.025 (0.045)
Train: 43 [ 150/1251 ( 12%)]  Loss: 4.136 (3.93)  Time: 0.312s, 3283.15/s  (0.318s, 3221.69/s)  LR: 9.501e-04  Data: 0.022 (0.038)
Train: 43 [ 200/1251 ( 16%)]  Loss: 4.601 (4.06)  Time: 0.310s, 3305.42/s  (0.316s, 3243.17/s)  LR: 9.500e-04  Data: 0.017 (0.034)
Train: 43 [ 250/1251 ( 20%)]  Loss: 4.482 (4.13)  Time: 0.311s, 3292.28/s  (0.315s, 3253.75/s)  LR: 9.500e-04  Data: 0.023 (0.032)
Train: 43 [ 300/1251 ( 24%)]  Loss: 4.058 (4.12)  Time: 0.311s, 3292.07/s  (0.314s, 3259.88/s)  LR: 9.499e-04  Data: 0.020 (0.030)
Train: 43 [ 350/1251 ( 28%)]  Loss: 4.062 (4.11)  Time: 0.307s, 3333.39/s  (0.314s, 3264.90/s)  LR: 9.498e-04  Data: 0.021 (0.029)
Train: 43 [ 400/1251 ( 32%)]  Loss: 4.013 (4.10)  Time: 0.312s, 3285.72/s  (0.313s, 3269.01/s)  LR: 9.497e-04  Data: 0.023 (0.028)
Train: 43 [ 450/1251 ( 36%)]  Loss: 3.916 (4.08)  Time: 0.310s, 3303.72/s  (0.313s, 3271.41/s)  LR: 9.496e-04  Data: 0.023 (0.028)
Train: 43 [ 500/1251 ( 40%)]  Loss: 4.189 (4.09)  Time: 0.314s, 3260.40/s  (0.313s, 3273.08/s)  LR: 9.495e-04  Data: 0.026 (0.027)
Train: 43 [ 550/1251 ( 44%)]  Loss: 3.349 (4.03)  Time: 0.316s, 3244.52/s  (0.313s, 3274.43/s)  LR: 9.494e-04  Data: 0.023 (0.027)
Train: 43 [ 600/1251 ( 48%)]  Loss: 4.220 (4.05)  Time: 0.309s, 3314.53/s  (0.313s, 3274.37/s)  LR: 9.493e-04  Data: 0.023 (0.027)
Train: 43 [ 650/1251 ( 52%)]  Loss: 4.254 (4.06)  Time: 0.314s, 3259.08/s  (0.313s, 3274.59/s)  LR: 9.492e-04  Data: 0.022 (0.026)
Train: 43 [ 700/1251 ( 56%)]  Loss: 4.283 (4.08)  Time: 0.313s, 3273.46/s  (0.313s, 3274.94/s)  LR: 9.491e-04  Data: 0.028 (0.026)
Train: 43 [ 750/1251 ( 60%)]  Loss: 4.102 (4.08)  Time: 0.312s, 3279.59/s  (0.313s, 3274.89/s)  LR: 9.490e-04  Data: 0.020 (0.026)
Train: 43 [ 800/1251 ( 64%)]  Loss: 4.220 (4.09)  Time: 0.309s, 3313.00/s  (0.313s, 3274.60/s)  LR: 9.489e-04  Data: 0.022 (0.026)
Train: 43 [ 850/1251 ( 68%)]  Loss: 4.158 (4.09)  Time: 0.316s, 3238.94/s  (0.313s, 3274.65/s)  LR: 9.489e-04  Data: 0.028 (0.026)
Train: 43 [ 900/1251 ( 72%)]  Loss: 3.957 (4.08)  Time: 0.314s, 3256.02/s  (0.313s, 3274.30/s)  LR: 9.488e-04  Data: 0.025 (0.025)
Train: 43 [ 950/1251 ( 76%)]  Loss: 3.780 (4.07)  Time: 0.307s, 3333.54/s  (0.313s, 3274.12/s)  LR: 9.487e-04  Data: 0.020 (0.025)
Train: 43 [1000/1251 ( 80%)]  Loss: 4.201 (4.07)  Time: 0.314s, 3265.73/s  (0.313s, 3274.20/s)  LR: 9.486e-04  Data: 0.021 (0.025)
Train: 43 [1050/1251 ( 84%)]  Loss: 3.986 (4.07)  Time: 0.311s, 3295.04/s  (0.313s, 3274.37/s)  LR: 9.485e-04  Data: 0.021 (0.025)
Train: 43 [1100/1251 ( 88%)]  Loss: 4.284 (4.08)  Time: 0.321s, 3191.46/s  (0.313s, 3273.85/s)  LR: 9.484e-04  Data: 0.022 (0.025)
Train: 43 [1150/1251 ( 92%)]  Loss: 3.996 (4.08)  Time: 0.315s, 3245.88/s  (0.313s, 3273.12/s)  LR: 9.483e-04  Data: 0.023 (0.025)
Train: 43 [1200/1251 ( 96%)]  Loss: 4.041 (4.07)  Time: 0.308s, 3324.78/s  (0.313s, 3273.19/s)  LR: 9.482e-04  Data: 0.021 (0.025)
Train: 43 [1250/1251 (100%)]  Loss: 4.221 (4.08)  Time: 0.282s, 3636.67/s  (0.313s, 3274.99/s)  LR: 9.481e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.112 (2.112)  Loss:  0.8672 (0.8672)  Acc@1: 83.0078 (83.0078)  Acc@5: 94.9219 (94.9219)
Test: [  48/48]  Time: 0.061 (0.238)  Loss:  0.9570 (1.5872)  Acc@1: 80.6604 (65.1300)  Acc@5: 93.8679 (86.8220)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-43.pth.tar', 65.1300000024414)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-41.pth.tar', 65.04599998046875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-40.pth.tar', 64.98599998291016)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-39.pth.tar', 64.97200000488282)

Train: 44 [   0/1251 (  0%)]  Loss: 4.028 (4.03)  Time: 2.278s,  449.57/s  (2.278s,  449.57/s)  LR: 9.481e-04  Data: 2.050 (2.050)
Train: 44 [  50/1251 (  4%)]  Loss: 4.294 (4.16)  Time: 0.305s, 3362.26/s  (0.334s, 3064.36/s)  LR: 9.480e-04  Data: 0.021 (0.063)
Train: 44 [ 100/1251 (  8%)]  Loss: 4.354 (4.23)  Time: 0.308s, 3320.87/s  (0.321s, 3193.46/s)  LR: 9.479e-04  Data: 0.022 (0.043)
Train: 44 [ 150/1251 ( 12%)]  Loss: 3.518 (4.05)  Time: 0.312s, 3286.37/s  (0.317s, 3230.01/s)  LR: 9.478e-04  Data: 0.023 (0.036)
Train: 44 [ 200/1251 ( 16%)]  Loss: 4.418 (4.12)  Time: 0.308s, 3320.90/s  (0.315s, 3246.02/s)  LR: 9.477e-04  Data: 0.021 (0.033)
Train: 44 [ 250/1251 ( 20%)]  Loss: 4.303 (4.15)  Time: 0.309s, 3316.18/s  (0.315s, 3255.22/s)  LR: 9.477e-04  Data: 0.021 (0.031)
Train: 44 [ 300/1251 ( 24%)]  Loss: 3.964 (4.13)  Time: 0.315s, 3247.51/s  (0.314s, 3259.38/s)  LR: 9.476e-04  Data: 0.022 (0.030)
Train: 44 [ 350/1251 ( 28%)]  Loss: 4.104 (4.12)  Time: 0.314s, 3257.48/s  (0.314s, 3262.60/s)  LR: 9.475e-04  Data: 0.024 (0.029)
Train: 44 [ 400/1251 ( 32%)]  Loss: 3.660 (4.07)  Time: 0.309s, 3315.78/s  (0.314s, 3264.29/s)  LR: 9.474e-04  Data: 0.022 (0.028)
Train: 44 [ 450/1251 ( 36%)]  Loss: 4.225 (4.09)  Time: 0.311s, 3290.42/s  (0.313s, 3266.74/s)  LR: 9.473e-04  Data: 0.025 (0.027)
Train: 44 [ 500/1251 ( 40%)]  Loss: 4.203 (4.10)  Time: 0.310s, 3306.70/s  (0.313s, 3268.44/s)  LR: 9.472e-04  Data: 0.022 (0.027)
Train: 44 [ 550/1251 ( 44%)]  Loss: 4.086 (4.10)  Time: 0.317s, 3234.31/s  (0.313s, 3269.10/s)  LR: 9.471e-04  Data: 0.023 (0.027)
Train: 44 [ 600/1251 ( 48%)]  Loss: 4.406 (4.12)  Time: 0.308s, 3329.59/s  (0.313s, 3269.20/s)  LR: 9.470e-04  Data: 0.022 (0.026)
Train: 44 [ 650/1251 ( 52%)]  Loss: 4.077 (4.12)  Time: 0.312s, 3286.94/s  (0.313s, 3269.69/s)  LR: 9.469e-04  Data: 0.021 (0.026)
Train: 44 [ 700/1251 ( 56%)]  Loss: 4.198 (4.12)  Time: 0.316s, 3236.65/s  (0.313s, 3268.95/s)  LR: 9.468e-04  Data: 0.027 (0.026)
Train: 44 [ 750/1251 ( 60%)]  Loss: 3.930 (4.11)  Time: 0.317s, 3225.78/s  (0.313s, 3268.86/s)  LR: 9.467e-04  Data: 0.020 (0.026)
Train: 44 [ 800/1251 ( 64%)]  Loss: 4.239 (4.12)  Time: 0.314s, 3263.39/s  (0.313s, 3268.25/s)  LR: 9.466e-04  Data: 0.029 (0.025)
Train: 44 [ 850/1251 ( 68%)]  Loss: 4.019 (4.11)  Time: 0.314s, 3259.07/s  (0.313s, 3267.84/s)  LR: 9.465e-04  Data: 0.022 (0.025)
Train: 44 [ 900/1251 ( 72%)]  Loss: 4.017 (4.11)  Time: 0.321s, 3187.97/s  (0.313s, 3267.53/s)  LR: 9.464e-04  Data: 0.022 (0.025)
Train: 44 [ 950/1251 ( 76%)]  Loss: 3.873 (4.10)  Time: 0.324s, 3160.92/s  (0.313s, 3267.40/s)  LR: 9.463e-04  Data: 0.023 (0.025)
Train: 44 [1000/1251 ( 80%)]  Loss: 3.880 (4.09)  Time: 0.310s, 3306.94/s  (0.313s, 3267.34/s)  LR: 9.462e-04  Data: 0.021 (0.025)
Train: 44 [1050/1251 ( 84%)]  Loss: 4.055 (4.08)  Time: 0.319s, 3210.14/s  (0.313s, 3266.76/s)  LR: 9.462e-04  Data: 0.021 (0.025)
Train: 44 [1100/1251 ( 88%)]  Loss: 4.112 (4.09)  Time: 0.314s, 3261.43/s  (0.313s, 3266.77/s)  LR: 9.461e-04  Data: 0.026 (0.025)
Train: 44 [1150/1251 ( 92%)]  Loss: 4.177 (4.09)  Time: 0.312s, 3283.76/s  (0.313s, 3266.86/s)  LR: 9.460e-04  Data: 0.021 (0.025)
Train: 44 [1200/1251 ( 96%)]  Loss: 4.405 (4.10)  Time: 0.312s, 3286.42/s  (0.313s, 3266.50/s)  LR: 9.459e-04  Data: 0.024 (0.025)
Train: 44 [1250/1251 (100%)]  Loss: 3.722 (4.09)  Time: 0.289s, 3543.63/s  (0.313s, 3267.76/s)  LR: 9.458e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.090 (2.090)  Loss:  0.8447 (0.8447)  Acc@1: 82.8125 (82.8125)  Acc@5: 94.9219 (94.9219)
Test: [  48/48]  Time: 0.069 (0.234)  Loss:  0.9229 (1.5145)  Acc@1: 81.1321 (66.1820)  Acc@5: 93.5141 (87.2400)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-44.pth.tar', 66.18199994873046)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-43.pth.tar', 65.1300000024414)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-41.pth.tar', 65.04599998046875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-40.pth.tar', 64.98599998291016)

Train: 45 [   0/1251 (  0%)]  Loss: 4.248 (4.25)  Time: 2.416s,  423.90/s  (2.416s,  423.90/s)  LR: 9.458e-04  Data: 2.194 (2.194)
Train: 45 [  50/1251 (  4%)]  Loss: 3.582 (3.91)  Time: 0.309s, 3317.64/s  (0.340s, 3010.00/s)  LR: 9.457e-04  Data: 0.025 (0.065)
Train: 45 [ 100/1251 (  8%)]  Loss: 4.308 (4.05)  Time: 0.308s, 3327.61/s  (0.324s, 3156.54/s)  LR: 9.456e-04  Data: 0.021 (0.044)
Train: 45 [ 150/1251 ( 12%)]  Loss: 3.650 (3.95)  Time: 0.312s, 3287.00/s  (0.320s, 3201.29/s)  LR: 9.455e-04  Data: 0.023 (0.037)
Train: 45 [ 200/1251 ( 16%)]  Loss: 4.279 (4.01)  Time: 0.307s, 3335.31/s  (0.317s, 3226.55/s)  LR: 9.454e-04  Data: 0.021 (0.034)
Train: 45 [ 250/1251 ( 20%)]  Loss: 4.063 (4.02)  Time: 0.310s, 3298.52/s  (0.316s, 3238.75/s)  LR: 9.453e-04  Data: 0.024 (0.031)
Train: 45 [ 300/1251 ( 24%)]  Loss: 3.945 (4.01)  Time: 0.311s, 3287.92/s  (0.316s, 3245.61/s)  LR: 9.452e-04  Data: 0.021 (0.030)
Train: 45 [ 350/1251 ( 28%)]  Loss: 3.958 (4.00)  Time: 0.317s, 3232.96/s  (0.315s, 3249.14/s)  LR: 9.451e-04  Data: 0.022 (0.029)
Train: 45 [ 400/1251 ( 32%)]  Loss: 3.991 (4.00)  Time: 0.311s, 3296.80/s  (0.315s, 3250.33/s)  LR: 9.450e-04  Data: 0.021 (0.028)
Train: 45 [ 450/1251 ( 36%)]  Loss: 3.869 (3.99)  Time: 0.308s, 3322.18/s  (0.315s, 3252.84/s)  LR: 9.449e-04  Data: 0.022 (0.028)
Train: 45 [ 500/1251 ( 40%)]  Loss: 4.011 (3.99)  Time: 0.323s, 3167.33/s  (0.315s, 3254.41/s)  LR: 9.448e-04  Data: 0.025 (0.027)
Train: 45 [ 550/1251 ( 44%)]  Loss: 4.370 (4.02)  Time: 0.307s, 3334.87/s  (0.315s, 3255.05/s)  LR: 9.447e-04  Data: 0.022 (0.027)
Train: 45 [ 600/1251 ( 48%)]  Loss: 3.804 (4.01)  Time: 0.308s, 3321.93/s  (0.314s, 3256.66/s)  LR: 9.446e-04  Data: 0.023 (0.026)
Train: 45 [ 650/1251 ( 52%)]  Loss: 4.001 (4.01)  Time: 0.311s, 3290.05/s  (0.314s, 3257.43/s)  LR: 9.445e-04  Data: 0.021 (0.026)
Train: 45 [ 700/1251 ( 56%)]  Loss: 4.278 (4.02)  Time: 0.313s, 3266.84/s  (0.314s, 3258.05/s)  LR: 9.444e-04  Data: 0.023 (0.026)
Train: 45 [ 750/1251 ( 60%)]  Loss: 3.522 (3.99)  Time: 0.317s, 3231.29/s  (0.314s, 3258.52/s)  LR: 9.443e-04  Data: 0.026 (0.026)
Train: 45 [ 800/1251 ( 64%)]  Loss: 4.336 (4.01)  Time: 0.308s, 3320.08/s  (0.314s, 3258.99/s)  LR: 9.443e-04  Data: 0.023 (0.026)
Train: 45 [ 850/1251 ( 68%)]  Loss: 4.013 (4.01)  Time: 0.316s, 3244.30/s  (0.314s, 3258.88/s)  LR: 9.442e-04  Data: 0.022 (0.025)
Train: 45 [ 900/1251 ( 72%)]  Loss: 4.021 (4.01)  Time: 0.312s, 3281.84/s  (0.314s, 3259.45/s)  LR: 9.441e-04  Data: 0.021 (0.025)
Train: 45 [ 950/1251 ( 76%)]  Loss: 4.069 (4.02)  Time: 0.312s, 3281.07/s  (0.314s, 3259.61/s)  LR: 9.440e-04  Data: 0.023 (0.025)
Train: 45 [1000/1251 ( 80%)]  Loss: 4.217 (4.03)  Time: 0.310s, 3303.38/s  (0.314s, 3259.89/s)  LR: 9.439e-04  Data: 0.022 (0.025)
Train: 45 [1050/1251 ( 84%)]  Loss: 3.875 (4.02)  Time: 0.318s, 3221.05/s  (0.314s, 3259.68/s)  LR: 9.438e-04  Data: 0.023 (0.025)
Train: 45 [1100/1251 ( 88%)]  Loss: 4.007 (4.02)  Time: 0.312s, 3283.25/s  (0.314s, 3259.67/s)  LR: 9.437e-04  Data: 0.023 (0.025)
Train: 45 [1150/1251 ( 92%)]  Loss: 4.366 (4.03)  Time: 0.311s, 3289.61/s  (0.314s, 3260.31/s)  LR: 9.436e-04  Data: 0.022 (0.025)
Train: 45 [1200/1251 ( 96%)]  Loss: 3.940 (4.03)  Time: 0.316s, 3237.60/s  (0.314s, 3260.57/s)  LR: 9.435e-04  Data: 0.023 (0.025)
Train: 45 [1250/1251 (100%)]  Loss: 3.675 (4.02)  Time: 0.290s, 3533.24/s  (0.314s, 3262.54/s)  LR: 9.434e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.266 (2.266)  Loss:  0.7988 (0.7988)  Acc@1: 83.6914 (83.6914)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.049 (0.238)  Loss:  0.9434 (1.5366)  Acc@1: 79.8349 (65.8740)  Acc@5: 93.2783 (87.2220)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-44.pth.tar', 66.18199994873046)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-45.pth.tar', 65.87400003173828)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-43.pth.tar', 65.1300000024414)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-41.pth.tar', 65.04599998046875)

Train: 46 [   0/1251 (  0%)]  Loss: 4.270 (4.27)  Time: 2.489s,  411.39/s  (2.489s,  411.39/s)  LR: 9.434e-04  Data: 2.264 (2.264)
Train: 46 [  50/1251 (  4%)]  Loss: 4.102 (4.19)  Time: 0.307s, 3340.41/s  (0.338s, 3031.41/s)  LR: 9.433e-04  Data: 0.022 (0.067)
Train: 46 [ 100/1251 (  8%)]  Loss: 4.005 (4.13)  Time: 0.304s, 3368.91/s  (0.323s, 3171.86/s)  LR: 9.432e-04  Data: 0.025 (0.045)
Train: 46 [ 150/1251 ( 12%)]  Loss: 3.969 (4.09)  Time: 0.310s, 3302.35/s  (0.319s, 3212.43/s)  LR: 9.431e-04  Data: 0.021 (0.038)
Train: 46 [ 200/1251 ( 16%)]  Loss: 4.047 (4.08)  Time: 0.311s, 3292.88/s  (0.317s, 3234.70/s)  LR: 9.430e-04  Data: 0.022 (0.034)
Train: 46 [ 250/1251 ( 20%)]  Loss: 4.013 (4.07)  Time: 0.309s, 3315.47/s  (0.315s, 3246.17/s)  LR: 9.429e-04  Data: 0.022 (0.032)
Train: 46 [ 300/1251 ( 24%)]  Loss: 4.162 (4.08)  Time: 0.308s, 3324.37/s  (0.315s, 3252.83/s)  LR: 9.428e-04  Data: 0.022 (0.030)
Train: 46 [ 350/1251 ( 28%)]  Loss: 3.873 (4.06)  Time: 0.317s, 3228.19/s  (0.314s, 3256.88/s)  LR: 9.427e-04  Data: 0.021 (0.029)
Train: 46 [ 400/1251 ( 32%)]  Loss: 3.930 (4.04)  Time: 0.307s, 3334.95/s  (0.314s, 3259.93/s)  LR: 9.426e-04  Data: 0.022 (0.029)
Train: 46 [ 450/1251 ( 36%)]  Loss: 4.164 (4.05)  Time: 0.310s, 3303.87/s  (0.314s, 3262.61/s)  LR: 9.425e-04  Data: 0.018 (0.028)
Train: 46 [ 500/1251 ( 40%)]  Loss: 4.111 (4.06)  Time: 0.306s, 3343.03/s  (0.314s, 3263.75/s)  LR: 9.424e-04  Data: 0.022 (0.027)
Train: 46 [ 550/1251 ( 44%)]  Loss: 3.777 (4.04)  Time: 0.311s, 3293.48/s  (0.314s, 3264.48/s)  LR: 9.423e-04  Data: 0.022 (0.027)
Train: 46 [ 600/1251 ( 48%)]  Loss: 4.074 (4.04)  Time: 0.309s, 3317.79/s  (0.314s, 3265.91/s)  LR: 9.422e-04  Data: 0.021 (0.027)
Train: 46 [ 650/1251 ( 52%)]  Loss: 4.367 (4.06)  Time: 0.315s, 3253.74/s  (0.314s, 3266.28/s)  LR: 9.421e-04  Data: 0.022 (0.026)
Train: 46 [ 700/1251 ( 56%)]  Loss: 3.823 (4.05)  Time: 0.314s, 3263.14/s  (0.313s, 3267.15/s)  LR: 9.420e-04  Data: 0.023 (0.026)
Train: 46 [ 750/1251 ( 60%)]  Loss: 3.621 (4.02)  Time: 0.307s, 3340.32/s  (0.313s, 3267.66/s)  LR: 9.419e-04  Data: 0.021 (0.026)
Train: 46 [ 800/1251 ( 64%)]  Loss: 4.285 (4.04)  Time: 0.308s, 3319.94/s  (0.313s, 3268.19/s)  LR: 9.418e-04  Data: 0.025 (0.026)
Train: 46 [ 850/1251 ( 68%)]  Loss: 4.072 (4.04)  Time: 0.319s, 3209.29/s  (0.313s, 3268.87/s)  LR: 9.417e-04  Data: 0.026 (0.025)
Train: 46 [ 900/1251 ( 72%)]  Loss: 3.978 (4.03)  Time: 0.325s, 3154.53/s  (0.313s, 3268.82/s)  LR: 9.416e-04  Data: 0.026 (0.025)
Train: 46 [ 950/1251 ( 76%)]  Loss: 4.289 (4.05)  Time: 0.315s, 3252.11/s  (0.313s, 3268.39/s)  LR: 9.415e-04  Data: 0.022 (0.025)
Train: 46 [1000/1251 ( 80%)]  Loss: 4.028 (4.05)  Time: 0.313s, 3271.45/s  (0.313s, 3268.20/s)  LR: 9.414e-04  Data: 0.019 (0.025)
Train: 46 [1050/1251 ( 84%)]  Loss: 4.388 (4.06)  Time: 0.313s, 3271.23/s  (0.313s, 3267.26/s)  LR: 9.413e-04  Data: 0.022 (0.025)
Train: 46 [1100/1251 ( 88%)]  Loss: 4.118 (4.06)  Time: 0.311s, 3291.22/s  (0.313s, 3266.54/s)  LR: 9.412e-04  Data: 0.021 (0.025)
Train: 46 [1150/1251 ( 92%)]  Loss: 4.152 (4.07)  Time: 0.311s, 3297.06/s  (0.314s, 3266.20/s)  LR: 9.411e-04  Data: 0.026 (0.025)
Train: 46 [1200/1251 ( 96%)]  Loss: 3.932 (4.06)  Time: 0.315s, 3246.40/s  (0.314s, 3265.54/s)  LR: 9.410e-04  Data: 0.023 (0.025)
Train: 46 [1250/1251 (100%)]  Loss: 3.830 (4.05)  Time: 0.289s, 3539.66/s  (0.313s, 3267.19/s)  LR: 9.409e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.072 (2.072)  Loss:  0.8379 (0.8379)  Acc@1: 82.0312 (82.0312)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.061 (0.239)  Loss:  0.9370 (1.5344)  Acc@1: 81.2500 (65.9600)  Acc@5: 94.1038 (87.3080)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-44.pth.tar', 66.18199994873046)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-46.pth.tar', 65.96)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-45.pth.tar', 65.87400003173828)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-43.pth.tar', 65.1300000024414)

Train: 47 [   0/1251 (  0%)]  Loss: 4.445 (4.44)  Time: 2.182s,  469.39/s  (2.182s,  469.39/s)  LR: 9.409e-04  Data: 1.955 (1.955)
Train: 47 [  50/1251 (  4%)]  Loss: 4.082 (4.26)  Time: 0.303s, 3374.86/s  (0.333s, 3074.28/s)  LR: 9.408e-04  Data: 0.023 (0.062)
Train: 47 [ 100/1251 (  8%)]  Loss: 3.837 (4.12)  Time: 0.315s, 3252.05/s  (0.321s, 3193.84/s)  LR: 9.407e-04  Data: 0.023 (0.042)
Train: 47 [ 150/1251 ( 12%)]  Loss: 4.255 (4.15)  Time: 0.309s, 3310.05/s  (0.317s, 3227.69/s)  LR: 9.407e-04  Data: 0.023 (0.036)
Train: 47 [ 200/1251 ( 16%)]  Loss: 3.831 (4.09)  Time: 0.308s, 3320.90/s  (0.316s, 3243.76/s)  LR: 9.406e-04  Data: 0.021 (0.032)
Train: 47 [ 250/1251 ( 20%)]  Loss: 3.888 (4.06)  Time: 0.312s, 3282.61/s  (0.315s, 3251.72/s)  LR: 9.405e-04  Data: 0.022 (0.030)
Train: 47 [ 300/1251 ( 24%)]  Loss: 4.046 (4.05)  Time: 0.309s, 3309.01/s  (0.314s, 3257.18/s)  LR: 9.404e-04  Data: 0.023 (0.029)
Train: 47 [ 350/1251 ( 28%)]  Loss: 4.266 (4.08)  Time: 0.312s, 3282.75/s  (0.314s, 3260.81/s)  LR: 9.403e-04  Data: 0.025 (0.028)
Train: 47 [ 400/1251 ( 32%)]  Loss: 3.878 (4.06)  Time: 0.309s, 3314.19/s  (0.314s, 3263.97/s)  LR: 9.402e-04  Data: 0.022 (0.028)
Train: 47 [ 450/1251 ( 36%)]  Loss: 3.755 (4.03)  Time: 0.316s, 3244.03/s  (0.314s, 3265.55/s)  LR: 9.401e-04  Data: 0.023 (0.027)
Train: 47 [ 500/1251 ( 40%)]  Loss: 3.989 (4.02)  Time: 0.315s, 3251.33/s  (0.313s, 3267.14/s)  LR: 9.400e-04  Data: 0.023 (0.027)
Train: 47 [ 550/1251 ( 44%)]  Loss: 3.855 (4.01)  Time: 0.319s, 3214.52/s  (0.313s, 3267.56/s)  LR: 9.399e-04  Data: 0.027 (0.026)
Train: 47 [ 600/1251 ( 48%)]  Loss: 3.943 (4.01)  Time: 0.319s, 3211.88/s  (0.313s, 3268.20/s)  LR: 9.398e-04  Data: 0.023 (0.026)
Train: 47 [ 650/1251 ( 52%)]  Loss: 3.838 (3.99)  Time: 0.313s, 3267.31/s  (0.313s, 3268.45/s)  LR: 9.397e-04  Data: 0.027 (0.026)
Train: 47 [ 700/1251 ( 56%)]  Loss: 4.218 (4.01)  Time: 0.319s, 3215.06/s  (0.313s, 3268.45/s)  LR: 9.396e-04  Data: 0.019 (0.026)
Train: 47 [ 750/1251 ( 60%)]  Loss: 4.190 (4.02)  Time: 0.314s, 3262.61/s  (0.313s, 3268.71/s)  LR: 9.395e-04  Data: 0.024 (0.025)
Train: 47 [ 800/1251 ( 64%)]  Loss: 4.143 (4.03)  Time: 0.311s, 3291.69/s  (0.313s, 3269.42/s)  LR: 9.394e-04  Data: 0.021 (0.025)
Train: 47 [ 850/1251 ( 68%)]  Loss: 4.248 (4.04)  Time: 0.314s, 3261.32/s  (0.313s, 3270.42/s)  LR: 9.393e-04  Data: 0.024 (0.025)
Train: 47 [ 900/1251 ( 72%)]  Loss: 3.955 (4.03)  Time: 0.316s, 3240.72/s  (0.313s, 3271.32/s)  LR: 9.392e-04  Data: 0.023 (0.025)
Train: 47 [ 950/1251 ( 76%)]  Loss: 4.201 (4.04)  Time: 0.312s, 3277.97/s  (0.313s, 3271.67/s)  LR: 9.391e-04  Data: 0.022 (0.025)
Train: 47 [1000/1251 ( 80%)]  Loss: 4.050 (4.04)  Time: 0.315s, 3248.41/s  (0.313s, 3272.28/s)  LR: 9.390e-04  Data: 0.024 (0.025)
Train: 47 [1050/1251 ( 84%)]  Loss: 4.039 (4.04)  Time: 0.312s, 3278.18/s  (0.313s, 3272.32/s)  LR: 9.389e-04  Data: 0.024 (0.025)
Train: 47 [1100/1251 ( 88%)]  Loss: 4.429 (4.06)  Time: 0.321s, 3186.87/s  (0.313s, 3272.35/s)  LR: 9.388e-04  Data: 0.023 (0.025)
Train: 47 [1150/1251 ( 92%)]  Loss: 3.930 (4.05)  Time: 0.312s, 3283.20/s  (0.313s, 3272.57/s)  LR: 9.387e-04  Data: 0.026 (0.024)
Train: 47 [1200/1251 ( 96%)]  Loss: 4.154 (4.06)  Time: 0.307s, 3336.18/s  (0.313s, 3273.29/s)  LR: 9.386e-04  Data: 0.023 (0.024)
Train: 47 [1250/1251 (100%)]  Loss: 4.060 (4.06)  Time: 0.282s, 3633.79/s  (0.313s, 3275.44/s)  LR: 9.385e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.064 (2.064)  Loss:  0.8218 (0.8218)  Acc@1: 83.3008 (83.3008)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.061 (0.237)  Loss:  0.8672 (1.5384)  Acc@1: 80.5424 (65.8860)  Acc@5: 94.2217 (87.2560)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-44.pth.tar', 66.18199994873046)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-46.pth.tar', 65.96)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-47.pth.tar', 65.88599995117187)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-45.pth.tar', 65.87400003173828)

Train: 48 [   0/1251 (  0%)]  Loss: 4.079 (4.08)  Time: 2.377s,  430.86/s  (2.377s,  430.86/s)  LR: 9.385e-04  Data: 2.145 (2.145)
Train: 48 [  50/1251 (  4%)]  Loss: 4.199 (4.14)  Time: 0.301s, 3402.14/s  (0.336s, 3051.12/s)  LR: 9.384e-04  Data: 0.022 (0.064)
Train: 48 [ 100/1251 (  8%)]  Loss: 3.785 (4.02)  Time: 0.303s, 3374.59/s  (0.321s, 3192.98/s)  LR: 9.383e-04  Data: 0.024 (0.043)
Train: 48 [ 150/1251 ( 12%)]  Loss: 3.842 (3.98)  Time: 0.311s, 3297.74/s  (0.317s, 3234.78/s)  LR: 9.382e-04  Data: 0.022 (0.036)
Train: 48 [ 200/1251 ( 16%)]  Loss: 4.170 (4.01)  Time: 0.308s, 3320.27/s  (0.315s, 3252.94/s)  LR: 9.381e-04  Data: 0.022 (0.033)
Train: 48 [ 250/1251 ( 20%)]  Loss: 3.820 (3.98)  Time: 0.315s, 3249.88/s  (0.314s, 3260.84/s)  LR: 9.380e-04  Data: 0.028 (0.031)
Train: 48 [ 300/1251 ( 24%)]  Loss: 3.886 (3.97)  Time: 0.309s, 3311.16/s  (0.314s, 3265.52/s)  LR: 9.379e-04  Data: 0.022 (0.030)
Train: 48 [ 350/1251 ( 28%)]  Loss: 4.045 (3.98)  Time: 0.312s, 3276.82/s  (0.313s, 3269.18/s)  LR: 9.378e-04  Data: 0.023 (0.029)
Train: 48 [ 400/1251 ( 32%)]  Loss: 4.082 (3.99)  Time: 0.313s, 3269.58/s  (0.313s, 3271.26/s)  LR: 9.377e-04  Data: 0.021 (0.028)
Train: 48 [ 450/1251 ( 36%)]  Loss: 4.127 (4.00)  Time: 0.312s, 3280.46/s  (0.313s, 3273.88/s)  LR: 9.376e-04  Data: 0.024 (0.028)
Train: 48 [ 500/1251 ( 40%)]  Loss: 4.350 (4.04)  Time: 0.315s, 3254.77/s  (0.313s, 3275.31/s)  LR: 9.375e-04  Data: 0.026 (0.027)
Train: 48 [ 550/1251 ( 44%)]  Loss: 4.266 (4.05)  Time: 0.313s, 3275.15/s  (0.313s, 3276.28/s)  LR: 9.374e-04  Data: 0.026 (0.027)
Train: 48 [ 600/1251 ( 48%)]  Loss: 2.977 (3.97)  Time: 0.309s, 3308.66/s  (0.312s, 3276.90/s)  LR: 9.373e-04  Data: 0.023 (0.026)
Train: 48 [ 650/1251 ( 52%)]  Loss: 3.679 (3.95)  Time: 0.315s, 3250.89/s  (0.312s, 3277.20/s)  LR: 9.372e-04  Data: 0.023 (0.026)
Train: 48 [ 700/1251 ( 56%)]  Loss: 3.971 (3.95)  Time: 0.310s, 3305.78/s  (0.312s, 3277.17/s)  LR: 9.370e-04  Data: 0.023 (0.026)
Train: 48 [ 750/1251 ( 60%)]  Loss: 3.585 (3.93)  Time: 0.315s, 3255.60/s  (0.312s, 3277.39/s)  LR: 9.369e-04  Data: 0.022 (0.026)
Train: 48 [ 800/1251 ( 64%)]  Loss: 3.762 (3.92)  Time: 0.310s, 3299.82/s  (0.312s, 3277.69/s)  LR: 9.368e-04  Data: 0.023 (0.025)
Train: 48 [ 850/1251 ( 68%)]  Loss: 3.880 (3.92)  Time: 0.311s, 3290.70/s  (0.312s, 3278.01/s)  LR: 9.367e-04  Data: 0.022 (0.025)
Train: 48 [ 900/1251 ( 72%)]  Loss: 4.090 (3.93)  Time: 0.316s, 3236.32/s  (0.312s, 3277.86/s)  LR: 9.366e-04  Data: 0.025 (0.025)
Train: 48 [ 950/1251 ( 76%)]  Loss: 4.101 (3.93)  Time: 0.313s, 3274.99/s  (0.312s, 3278.23/s)  LR: 9.365e-04  Data: 0.024 (0.025)
Train: 48 [1000/1251 ( 80%)]  Loss: 4.102 (3.94)  Time: 0.309s, 3311.55/s  (0.312s, 3278.37/s)  LR: 9.364e-04  Data: 0.023 (0.025)
Train: 48 [1050/1251 ( 84%)]  Loss: 4.175 (3.95)  Time: 0.313s, 3274.50/s  (0.312s, 3278.56/s)  LR: 9.363e-04  Data: 0.024 (0.025)
Train: 48 [1100/1251 ( 88%)]  Loss: 3.588 (3.94)  Time: 0.314s, 3261.29/s  (0.312s, 3278.67/s)  LR: 9.362e-04  Data: 0.023 (0.025)
Train: 48 [1150/1251 ( 92%)]  Loss: 3.891 (3.94)  Time: 0.310s, 3305.27/s  (0.312s, 3278.61/s)  LR: 9.361e-04  Data: 0.021 (0.025)
Train: 48 [1200/1251 ( 96%)]  Loss: 3.651 (3.92)  Time: 0.308s, 3324.21/s  (0.312s, 3278.66/s)  LR: 9.360e-04  Data: 0.025 (0.025)
Train: 48 [1250/1251 (100%)]  Loss: 4.200 (3.93)  Time: 0.283s, 3618.51/s  (0.312s, 3280.58/s)  LR: 9.359e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.158 (2.158)  Loss:  0.8823 (0.8823)  Acc@1: 82.0312 (82.0312)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.066 (0.236)  Loss:  0.9102 (1.5180)  Acc@1: 79.8349 (66.3680)  Acc@5: 94.2217 (87.5380)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-48.pth.tar', 66.36800003173828)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-44.pth.tar', 66.18199994873046)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-46.pth.tar', 65.96)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-47.pth.tar', 65.88599995117187)

Train: 49 [   0/1251 (  0%)]  Loss: 3.631 (3.63)  Time: 2.409s,  425.01/s  (2.409s,  425.01/s)  LR: 9.359e-04  Data: 2.181 (2.181)
Train: 49 [  50/1251 (  4%)]  Loss: 4.085 (3.86)  Time: 0.305s, 3360.08/s  (0.337s, 3038.33/s)  LR: 9.358e-04  Data: 0.021 (0.066)
Train: 49 [ 100/1251 (  8%)]  Loss: 3.994 (3.90)  Time: 0.307s, 3336.92/s  (0.322s, 3180.89/s)  LR: 9.357e-04  Data: 0.028 (0.045)
Train: 49 [ 150/1251 ( 12%)]  Loss: 3.980 (3.92)  Time: 0.313s, 3275.67/s  (0.318s, 3222.40/s)  LR: 9.356e-04  Data: 0.023 (0.037)
Train: 49 [ 200/1251 ( 16%)]  Loss: 3.700 (3.88)  Time: 0.310s, 3308.53/s  (0.316s, 3239.46/s)  LR: 9.355e-04  Data: 0.015 (0.034)
Train: 49 [ 250/1251 ( 20%)]  Loss: 3.947 (3.89)  Time: 0.312s, 3280.62/s  (0.315s, 3249.24/s)  LR: 9.354e-04  Data: 0.023 (0.032)
Train: 49 [ 300/1251 ( 24%)]  Loss: 4.149 (3.93)  Time: 0.310s, 3300.43/s  (0.314s, 3255.99/s)  LR: 9.353e-04  Data: 0.023 (0.030)
Train: 49 [ 350/1251 ( 28%)]  Loss: 3.870 (3.92)  Time: 0.313s, 3276.26/s  (0.314s, 3259.21/s)  LR: 9.352e-04  Data: 0.022 (0.029)
Train: 49 [ 400/1251 ( 32%)]  Loss: 4.127 (3.94)  Time: 0.311s, 3297.65/s  (0.314s, 3262.90/s)  LR: 9.351e-04  Data: 0.018 (0.028)
Train: 49 [ 450/1251 ( 36%)]  Loss: 3.731 (3.92)  Time: 0.321s, 3189.71/s  (0.314s, 3265.20/s)  LR: 9.350e-04  Data: 0.022 (0.027)
Train: 49 [ 500/1251 ( 40%)]  Loss: 3.693 (3.90)  Time: 0.321s, 3188.25/s  (0.313s, 3266.48/s)  LR: 9.349e-04  Data: 0.023 (0.027)
Train: 49 [ 550/1251 ( 44%)]  Loss: 4.363 (3.94)  Time: 0.317s, 3227.66/s  (0.313s, 3268.60/s)  LR: 9.348e-04  Data: 0.024 (0.027)
Train: 49 [ 600/1251 ( 48%)]  Loss: 4.080 (3.95)  Time: 0.313s, 3270.58/s  (0.313s, 3270.04/s)  LR: 9.347e-04  Data: 0.022 (0.026)
Train: 49 [ 650/1251 ( 52%)]  Loss: 3.752 (3.94)  Time: 0.310s, 3300.29/s  (0.313s, 3271.63/s)  LR: 9.346e-04  Data: 0.022 (0.026)
Train: 49 [ 700/1251 ( 56%)]  Loss: 4.142 (3.95)  Time: 0.316s, 3237.86/s  (0.313s, 3272.30/s)  LR: 9.345e-04  Data: 0.023 (0.026)
Train: 49 [ 750/1251 ( 60%)]  Loss: 4.082 (3.96)  Time: 0.309s, 3319.26/s  (0.313s, 3273.28/s)  LR: 9.344e-04  Data: 0.022 (0.026)
Train: 49 [ 800/1251 ( 64%)]  Loss: 4.290 (3.98)  Time: 0.309s, 3316.88/s  (0.313s, 3273.84/s)  LR: 9.343e-04  Data: 0.021 (0.026)
Train: 49 [ 850/1251 ( 68%)]  Loss: 4.270 (3.99)  Time: 0.314s, 3257.96/s  (0.313s, 3274.09/s)  LR: 9.342e-04  Data: 0.023 (0.025)
Train: 49 [ 900/1251 ( 72%)]  Loss: 3.667 (3.98)  Time: 0.306s, 3350.99/s  (0.313s, 3275.07/s)  LR: 9.341e-04  Data: 0.025 (0.025)
Train: 49 [ 950/1251 ( 76%)]  Loss: 3.776 (3.97)  Time: 0.314s, 3260.39/s  (0.313s, 3275.46/s)  LR: 9.340e-04  Data: 0.022 (0.025)
Train: 49 [1000/1251 ( 80%)]  Loss: 4.199 (3.98)  Time: 0.313s, 3267.24/s  (0.313s, 3276.01/s)  LR: 9.339e-04  Data: 0.023 (0.025)
Train: 49 [1050/1251 ( 84%)]  Loss: 3.790 (3.97)  Time: 0.315s, 3248.70/s  (0.313s, 3276.20/s)  LR: 9.338e-04  Data: 0.022 (0.025)
Train: 49 [1100/1251 ( 88%)]  Loss: 4.271 (3.98)  Time: 0.306s, 3345.32/s  (0.313s, 3276.50/s)  LR: 9.337e-04  Data: 0.023 (0.025)
Train: 49 [1150/1251 ( 92%)]  Loss: 3.850 (3.98)  Time: 0.322s, 3182.79/s  (0.312s, 3276.85/s)  LR: 9.336e-04  Data: 0.021 (0.025)
Train: 49 [1200/1251 ( 96%)]  Loss: 3.773 (3.97)  Time: 0.312s, 3281.27/s  (0.312s, 3277.59/s)  LR: 9.335e-04  Data: 0.021 (0.025)
Train: 49 [1250/1251 (100%)]  Loss: 4.062 (3.97)  Time: 0.282s, 3625.92/s  (0.312s, 3279.79/s)  LR: 9.333e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.060 (2.060)  Loss:  0.7832 (0.7832)  Acc@1: 83.5938 (83.5938)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.058 (0.230)  Loss:  0.8594 (1.4681)  Acc@1: 80.6604 (67.0360)  Acc@5: 94.3396 (87.9680)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-49.pth.tar', 67.03600000244141)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-48.pth.tar', 66.36800003173828)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-44.pth.tar', 66.18199994873046)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-46.pth.tar', 65.96)

Train: 50 [   0/1251 (  0%)]  Loss: 4.135 (4.14)  Time: 2.222s,  460.88/s  (2.222s,  460.88/s)  LR: 9.333e-04  Data: 1.994 (1.994)
Train: 50 [  50/1251 (  4%)]  Loss: 4.135 (4.14)  Time: 0.305s, 3356.06/s  (0.329s, 3110.14/s)  LR: 9.332e-04  Data: 0.023 (0.061)
Train: 50 [ 100/1251 (  8%)]  Loss: 4.226 (4.17)  Time: 0.305s, 3360.21/s  (0.316s, 3235.91/s)  LR: 9.331e-04  Data: 0.024 (0.042)
Train: 50 [ 150/1251 ( 12%)]  Loss: 3.875 (4.09)  Time: 0.306s, 3343.31/s  (0.313s, 3270.50/s)  LR: 9.330e-04  Data: 0.019 (0.036)
Train: 50 [ 200/1251 ( 16%)]  Loss: 4.143 (4.10)  Time: 0.310s, 3306.10/s  (0.311s, 3287.54/s)  LR: 9.329e-04  Data: 0.025 (0.032)
Train: 50 [ 250/1251 ( 20%)]  Loss: 4.309 (4.14)  Time: 0.306s, 3351.51/s  (0.311s, 3296.05/s)  LR: 9.328e-04  Data: 0.023 (0.031)
Train: 50 [ 300/1251 ( 24%)]  Loss: 3.797 (4.09)  Time: 0.305s, 3355.62/s  (0.310s, 3300.41/s)  LR: 9.327e-04  Data: 0.021 (0.029)
Train: 50 [ 350/1251 ( 28%)]  Loss: 4.144 (4.10)  Time: 0.308s, 3319.47/s  (0.310s, 3303.42/s)  LR: 9.326e-04  Data: 0.020 (0.028)
Train: 50 [ 400/1251 ( 32%)]  Loss: 4.301 (4.12)  Time: 0.303s, 3374.42/s  (0.310s, 3304.59/s)  LR: 9.325e-04  Data: 0.023 (0.028)
Train: 50 [ 450/1251 ( 36%)]  Loss: 4.158 (4.12)  Time: 0.310s, 3299.15/s  (0.310s, 3306.43/s)  LR: 9.324e-04  Data: 0.026 (0.027)
Train: 50 [ 500/1251 ( 40%)]  Loss: 4.121 (4.12)  Time: 0.307s, 3333.42/s  (0.310s, 3306.63/s)  LR: 9.323e-04  Data: 0.024 (0.027)
Train: 50 [ 550/1251 ( 44%)]  Loss: 3.732 (4.09)  Time: 0.311s, 3292.34/s  (0.310s, 3307.81/s)  LR: 9.322e-04  Data: 0.021 (0.026)
Train: 50 [ 600/1251 ( 48%)]  Loss: 3.478 (4.04)  Time: 0.301s, 3404.89/s  (0.309s, 3308.57/s)  LR: 9.321e-04  Data: 0.023 (0.026)
Train: 50 [ 650/1251 ( 52%)]  Loss: 4.121 (4.05)  Time: 0.311s, 3296.88/s  (0.310s, 3308.42/s)  LR: 9.320e-04  Data: 0.021 (0.026)
Train: 50 [ 700/1251 ( 56%)]  Loss: 4.192 (4.06)  Time: 0.310s, 3305.92/s  (0.309s, 3308.85/s)  LR: 9.319e-04  Data: 0.024 (0.026)
Train: 50 [ 750/1251 ( 60%)]  Loss: 4.202 (4.07)  Time: 0.312s, 3283.91/s  (0.309s, 3308.57/s)  LR: 9.318e-04  Data: 0.022 (0.025)
Train: 50 [ 800/1251 ( 64%)]  Loss: 4.251 (4.08)  Time: 0.308s, 3324.20/s  (0.310s, 3308.09/s)  LR: 9.317e-04  Data: 0.021 (0.025)
Train: 50 [ 850/1251 ( 68%)]  Loss: 4.027 (4.07)  Time: 0.311s, 3287.89/s  (0.310s, 3307.86/s)  LR: 9.316e-04  Data: 0.023 (0.025)
Train: 50 [ 900/1251 ( 72%)]  Loss: 4.177 (4.08)  Time: 0.313s, 3272.63/s  (0.310s, 3307.95/s)  LR: 9.315e-04  Data: 0.022 (0.025)
Train: 50 [ 950/1251 ( 76%)]  Loss: 4.340 (4.09)  Time: 0.306s, 3347.79/s  (0.310s, 3307.88/s)  LR: 9.314e-04  Data: 0.022 (0.025)
Train: 50 [1000/1251 ( 80%)]  Loss: 4.065 (4.09)  Time: 0.307s, 3330.88/s  (0.310s, 3307.70/s)  LR: 9.312e-04  Data: 0.025 (0.025)
Train: 50 [1050/1251 ( 84%)]  Loss: 4.027 (4.09)  Time: 0.309s, 3314.53/s  (0.310s, 3308.10/s)  LR: 9.311e-04  Data: 0.022 (0.025)
Train: 50 [1100/1251 ( 88%)]  Loss: 4.192 (4.09)  Time: 0.308s, 3326.37/s  (0.310s, 3307.95/s)  LR: 9.310e-04  Data: 0.023 (0.025)
Train: 50 [1150/1251 ( 92%)]  Loss: 4.265 (4.10)  Time: 0.318s, 3224.39/s  (0.310s, 3308.40/s)  LR: 9.309e-04  Data: 0.025 (0.024)
Train: 50 [1200/1251 ( 96%)]  Loss: 3.875 (4.09)  Time: 0.312s, 3280.20/s  (0.310s, 3308.13/s)  LR: 9.308e-04  Data: 0.024 (0.024)
Train: 50 [1250/1251 (100%)]  Loss: 4.439 (4.10)  Time: 0.277s, 3698.33/s  (0.309s, 3310.34/s)  LR: 9.307e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.364 (2.364)  Loss:  0.9082 (0.9082)  Acc@1: 82.6172 (82.6172)  Acc@5: 94.9219 (94.9219)
Test: [  48/48]  Time: 0.062 (0.236)  Loss:  0.9570 (1.5647)  Acc@1: 79.7170 (66.0280)  Acc@5: 93.8679 (87.3160)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-49.pth.tar', 67.03600000244141)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-48.pth.tar', 66.36800003173828)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-44.pth.tar', 66.18199994873046)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-50.pth.tar', 66.02799998046875)

Train: 51 [   0/1251 (  0%)]  Loss: 4.196 (4.20)  Time: 2.680s,  382.06/s  (2.680s,  382.06/s)  LR: 9.307e-04  Data: 2.471 (2.471)
Train: 51 [  50/1251 (  4%)]  Loss: 4.206 (4.20)  Time: 0.297s, 3450.49/s  (0.343s, 2982.98/s)  LR: 9.306e-04  Data: 0.017 (0.081)
Train: 51 [ 100/1251 (  8%)]  Loss: 4.291 (4.23)  Time: 0.311s, 3294.48/s  (0.324s, 3161.90/s)  LR: 9.305e-04  Data: 0.024 (0.052)
Train: 51 [ 150/1251 ( 12%)]  Loss: 3.774 (4.12)  Time: 0.304s, 3371.54/s  (0.318s, 3220.52/s)  LR: 9.304e-04  Data: 0.019 (0.042)
Train: 51 [ 200/1251 ( 16%)]  Loss: 4.132 (4.12)  Time: 0.313s, 3271.90/s  (0.315s, 3247.20/s)  LR: 9.303e-04  Data: 0.026 (0.037)
Train: 51 [ 250/1251 ( 20%)]  Loss: 3.898 (4.08)  Time: 0.311s, 3294.79/s  (0.314s, 3260.31/s)  LR: 9.302e-04  Data: 0.019 (0.034)
Train: 51 [ 300/1251 ( 24%)]  Loss: 3.946 (4.06)  Time: 0.311s, 3288.52/s  (0.313s, 3269.12/s)  LR: 9.301e-04  Data: 0.020 (0.032)
Train: 51 [ 350/1251 ( 28%)]  Loss: 4.020 (4.06)  Time: 0.307s, 3338.82/s  (0.313s, 3276.11/s)  LR: 9.300e-04  Data: 0.021 (0.031)
Train: 51 [ 400/1251 ( 32%)]  Loss: 4.217 (4.08)  Time: 0.311s, 3290.71/s  (0.312s, 3280.76/s)  LR: 9.299e-04  Data: 0.017 (0.030)
Train: 51 [ 450/1251 ( 36%)]  Loss: 4.121 (4.08)  Time: 0.309s, 3309.98/s  (0.312s, 3283.71/s)  LR: 9.298e-04  Data: 0.022 (0.029)
Train: 51 [ 500/1251 ( 40%)]  Loss: 4.031 (4.08)  Time: 0.313s, 3271.31/s  (0.312s, 3286.20/s)  LR: 9.297e-04  Data: 0.024 (0.029)
Train: 51 [ 550/1251 ( 44%)]  Loss: 4.412 (4.10)  Time: 0.310s, 3299.06/s  (0.311s, 3288.28/s)  LR: 9.295e-04  Data: 0.021 (0.028)
Train: 51 [ 600/1251 ( 48%)]  Loss: 4.194 (4.11)  Time: 0.316s, 3244.72/s  (0.311s, 3290.74/s)  LR: 9.294e-04  Data: 0.024 (0.028)
Train: 51 [ 650/1251 ( 52%)]  Loss: 4.036 (4.11)  Time: 0.312s, 3285.48/s  (0.311s, 3292.56/s)  LR: 9.293e-04  Data: 0.022 (0.027)
Train: 51 [ 700/1251 ( 56%)]  Loss: 3.896 (4.09)  Time: 0.315s, 3252.25/s  (0.311s, 3294.12/s)  LR: 9.292e-04  Data: 0.025 (0.027)
Train: 51 [ 750/1251 ( 60%)]  Loss: 4.278 (4.10)  Time: 0.311s, 3297.76/s  (0.311s, 3295.63/s)  LR: 9.291e-04  Data: 0.024 (0.027)
Train: 51 [ 800/1251 ( 64%)]  Loss: 3.770 (4.08)  Time: 0.308s, 3320.25/s  (0.311s, 3296.40/s)  LR: 9.290e-04  Data: 0.024 (0.026)
Train: 51 [ 850/1251 ( 68%)]  Loss: 4.219 (4.09)  Time: 0.310s, 3301.37/s  (0.311s, 3297.20/s)  LR: 9.289e-04  Data: 0.021 (0.026)
Train: 51 [ 900/1251 ( 72%)]  Loss: 3.861 (4.08)  Time: 0.304s, 3370.82/s  (0.310s, 3298.36/s)  LR: 9.288e-04  Data: 0.026 (0.026)
Train: 51 [ 950/1251 ( 76%)]  Loss: 3.737 (4.06)  Time: 0.311s, 3289.47/s  (0.310s, 3298.89/s)  LR: 9.287e-04  Data: 0.028 (0.026)
Train: 51 [1000/1251 ( 80%)]  Loss: 3.902 (4.05)  Time: 0.309s, 3315.08/s  (0.310s, 3299.31/s)  LR: 9.286e-04  Data: 0.022 (0.026)
Train: 51 [1050/1251 ( 84%)]  Loss: 4.040 (4.05)  Time: 0.313s, 3270.47/s  (0.310s, 3299.64/s)  LR: 9.285e-04  Data: 0.024 (0.025)
Train: 51 [1100/1251 ( 88%)]  Loss: 3.634 (4.04)  Time: 0.311s, 3291.12/s  (0.310s, 3300.58/s)  LR: 9.284e-04  Data: 0.027 (0.025)
Train: 51 [1150/1251 ( 92%)]  Loss: 4.026 (4.03)  Time: 0.312s, 3285.79/s  (0.310s, 3301.33/s)  LR: 9.283e-04  Data: 0.017 (0.025)
Train: 51 [1200/1251 ( 96%)]  Loss: 4.263 (4.04)  Time: 0.311s, 3290.86/s  (0.310s, 3302.31/s)  LR: 9.282e-04  Data: 0.023 (0.025)
Train: 51 [1250/1251 (100%)]  Loss: 3.949 (4.04)  Time: 0.277s, 3695.45/s  (0.310s, 3304.87/s)  LR: 9.280e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.116 (2.116)  Loss:  0.8320 (0.8320)  Acc@1: 82.7148 (82.7148)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.046 (0.232)  Loss:  0.9155 (1.5055)  Acc@1: 81.7217 (66.8080)  Acc@5: 93.7500 (87.9120)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-49.pth.tar', 67.03600000244141)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-51.pth.tar', 66.80799994628906)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-48.pth.tar', 66.36800003173828)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-44.pth.tar', 66.18199994873046)

Train: 52 [   0/1251 (  0%)]  Loss: 3.661 (3.66)  Time: 2.180s,  469.82/s  (2.180s,  469.82/s)  LR: 9.280e-04  Data: 1.938 (1.938)
Train: 52 [  50/1251 (  4%)]  Loss: 3.680 (3.67)  Time: 0.300s, 3412.56/s  (0.329s, 3112.39/s)  LR: 9.279e-04  Data: 0.022 (0.060)
Train: 52 [ 100/1251 (  8%)]  Loss: 3.679 (3.67)  Time: 0.299s, 3421.49/s  (0.316s, 3241.97/s)  LR: 9.278e-04  Data: 0.023 (0.041)
Train: 52 [ 150/1251 ( 12%)]  Loss: 4.018 (3.76)  Time: 0.307s, 3340.70/s  (0.312s, 3281.52/s)  LR: 9.277e-04  Data: 0.021 (0.035)
Train: 52 [ 200/1251 ( 16%)]  Loss: 4.247 (3.86)  Time: 0.301s, 3402.56/s  (0.311s, 3297.72/s)  LR: 9.276e-04  Data: 0.023 (0.032)
Train: 52 [ 250/1251 ( 20%)]  Loss: 3.980 (3.88)  Time: 0.308s, 3326.30/s  (0.310s, 3307.70/s)  LR: 9.275e-04  Data: 0.021 (0.030)
Train: 52 [ 300/1251 ( 24%)]  Loss: 4.149 (3.92)  Time: 0.307s, 3330.33/s  (0.309s, 3312.15/s)  LR: 9.274e-04  Data: 0.021 (0.029)
Train: 52 [ 350/1251 ( 28%)]  Loss: 3.968 (3.92)  Time: 0.311s, 3296.73/s  (0.309s, 3315.19/s)  LR: 9.273e-04  Data: 0.022 (0.028)
Train: 52 [ 400/1251 ( 32%)]  Loss: 3.973 (3.93)  Time: 0.307s, 3335.51/s  (0.309s, 3316.85/s)  LR: 9.272e-04  Data: 0.023 (0.027)
Train: 52 [ 450/1251 ( 36%)]  Loss: 4.113 (3.95)  Time: 0.304s, 3363.41/s  (0.309s, 3319.18/s)  LR: 9.271e-04  Data: 0.018 (0.027)
Train: 52 [ 500/1251 ( 40%)]  Loss: 3.765 (3.93)  Time: 0.304s, 3364.87/s  (0.308s, 3320.11/s)  LR: 9.270e-04  Data: 0.025 (0.026)
Train: 52 [ 550/1251 ( 44%)]  Loss: 3.920 (3.93)  Time: 0.311s, 3288.66/s  (0.308s, 3321.49/s)  LR: 9.269e-04  Data: 0.022 (0.026)
Train: 52 [ 600/1251 ( 48%)]  Loss: 3.882 (3.93)  Time: 0.306s, 3346.24/s  (0.308s, 3322.73/s)  LR: 9.267e-04  Data: 0.022 (0.026)
Train: 52 [ 650/1251 ( 52%)]  Loss: 4.296 (3.95)  Time: 0.309s, 3310.05/s  (0.308s, 3323.77/s)  LR: 9.266e-04  Data: 0.022 (0.025)
Train: 52 [ 700/1251 ( 56%)]  Loss: 3.906 (3.95)  Time: 0.307s, 3337.97/s  (0.308s, 3325.25/s)  LR: 9.265e-04  Data: 0.024 (0.025)
Train: 52 [ 750/1251 ( 60%)]  Loss: 3.849 (3.94)  Time: 0.304s, 3366.98/s  (0.308s, 3327.09/s)  LR: 9.264e-04  Data: 0.021 (0.025)
Train: 52 [ 800/1251 ( 64%)]  Loss: 3.652 (3.93)  Time: 0.307s, 3340.01/s  (0.308s, 3327.88/s)  LR: 9.263e-04  Data: 0.023 (0.025)
Train: 52 [ 850/1251 ( 68%)]  Loss: 3.870 (3.92)  Time: 0.303s, 3384.43/s  (0.308s, 3328.14/s)  LR: 9.262e-04  Data: 0.025 (0.025)
Train: 52 [ 900/1251 ( 72%)]  Loss: 4.056 (3.93)  Time: 0.311s, 3287.90/s  (0.308s, 3328.51/s)  LR: 9.261e-04  Data: 0.026 (0.025)
Train: 52 [ 950/1251 ( 76%)]  Loss: 3.594 (3.91)  Time: 0.311s, 3294.57/s  (0.308s, 3328.88/s)  LR: 9.260e-04  Data: 0.028 (0.025)
Train: 52 [1000/1251 ( 80%)]  Loss: 3.969 (3.92)  Time: 0.305s, 3353.14/s  (0.308s, 3328.97/s)  LR: 9.259e-04  Data: 0.022 (0.024)
Train: 52 [1050/1251 ( 84%)]  Loss: 4.094 (3.92)  Time: 0.310s, 3301.57/s  (0.308s, 3329.60/s)  LR: 9.258e-04  Data: 0.022 (0.024)
Train: 52 [1100/1251 ( 88%)]  Loss: 4.213 (3.94)  Time: 0.306s, 3348.99/s  (0.308s, 3329.84/s)  LR: 9.257e-04  Data: 0.027 (0.024)
Train: 52 [1150/1251 ( 92%)]  Loss: 4.154 (3.95)  Time: 0.309s, 3311.63/s  (0.308s, 3329.90/s)  LR: 9.255e-04  Data: 0.024 (0.024)
Train: 52 [1200/1251 ( 96%)]  Loss: 4.465 (3.97)  Time: 0.304s, 3371.64/s  (0.307s, 3330.58/s)  LR: 9.254e-04  Data: 0.023 (0.024)
Train: 52 [1250/1251 (100%)]  Loss: 4.285 (3.98)  Time: 0.278s, 3686.94/s  (0.307s, 3332.57/s)  LR: 9.253e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.063 (2.063)  Loss:  0.8345 (0.8345)  Acc@1: 83.2031 (83.2031)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.057 (0.235)  Loss:  0.9014 (1.4817)  Acc@1: 80.6604 (67.3820)  Acc@5: 95.0472 (88.0840)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-52.pth.tar', 67.3820000024414)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-49.pth.tar', 67.03600000244141)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-51.pth.tar', 66.80799994628906)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-48.pth.tar', 66.36800003173828)

Train: 53 [   0/1251 (  0%)]  Loss: 4.118 (4.12)  Time: 2.688s,  380.97/s  (2.688s,  380.97/s)  LR: 9.253e-04  Data: 2.468 (2.468)
Train: 53 [  50/1251 (  4%)]  Loss: 3.672 (3.89)  Time: 0.294s, 3483.23/s  (0.331s, 3094.91/s)  LR: 9.252e-04  Data: 0.023 (0.070)
Train: 53 [ 100/1251 (  8%)]  Loss: 4.194 (3.99)  Time: 0.305s, 3362.25/s  (0.315s, 3250.75/s)  LR: 9.251e-04  Data: 0.021 (0.046)
Train: 53 [ 150/1251 ( 12%)]  Loss: 4.302 (4.07)  Time: 0.305s, 3359.34/s  (0.311s, 3297.77/s)  LR: 9.250e-04  Data: 0.027 (0.039)
Train: 53 [ 200/1251 ( 16%)]  Loss: 4.286 (4.11)  Time: 0.305s, 3354.04/s  (0.308s, 3320.65/s)  LR: 9.249e-04  Data: 0.024 (0.034)
Train: 53 [ 250/1251 ( 20%)]  Loss: 4.256 (4.14)  Time: 0.309s, 3309.17/s  (0.307s, 3331.72/s)  LR: 9.248e-04  Data: 0.024 (0.032)
Train: 53 [ 300/1251 ( 24%)]  Loss: 3.645 (4.07)  Time: 0.303s, 3382.09/s  (0.307s, 3338.49/s)  LR: 9.247e-04  Data: 0.024 (0.030)
Train: 53 [ 350/1251 ( 28%)]  Loss: 3.930 (4.05)  Time: 0.304s, 3372.65/s  (0.306s, 3344.46/s)  LR: 9.245e-04  Data: 0.023 (0.029)
Train: 53 [ 400/1251 ( 32%)]  Loss: 3.794 (4.02)  Time: 0.303s, 3379.81/s  (0.306s, 3348.81/s)  LR: 9.244e-04  Data: 0.026 (0.029)
Train: 53 [ 450/1251 ( 36%)]  Loss: 3.975 (4.02)  Time: 0.308s, 3324.85/s  (0.305s, 3352.21/s)  LR: 9.243e-04  Data: 0.024 (0.028)
Train: 53 [ 500/1251 ( 40%)]  Loss: 3.865 (4.00)  Time: 0.302s, 3386.67/s  (0.305s, 3354.12/s)  LR: 9.242e-04  Data: 0.023 (0.027)
Train: 53 [ 550/1251 ( 44%)]  Loss: 3.922 (4.00)  Time: 0.306s, 3345.52/s  (0.305s, 3355.71/s)  LR: 9.241e-04  Data: 0.024 (0.027)
Train: 53 [ 600/1251 ( 48%)]  Loss: 4.145 (4.01)  Time: 0.302s, 3393.01/s  (0.305s, 3357.34/s)  LR: 9.240e-04  Data: 0.023 (0.027)
Train: 53 [ 650/1251 ( 52%)]  Loss: 3.953 (4.00)  Time: 0.303s, 3384.97/s  (0.305s, 3359.32/s)  LR: 9.239e-04  Data: 0.021 (0.026)
Train: 53 [ 700/1251 ( 56%)]  Loss: 3.956 (4.00)  Time: 0.305s, 3360.09/s  (0.305s, 3359.78/s)  LR: 9.238e-04  Data: 0.023 (0.026)
Train: 53 [ 750/1251 ( 60%)]  Loss: 4.497 (4.03)  Time: 0.298s, 3439.18/s  (0.305s, 3360.27/s)  LR: 9.237e-04  Data: 0.024 (0.026)
Train: 53 [ 800/1251 ( 64%)]  Loss: 3.748 (4.02)  Time: 0.304s, 3365.01/s  (0.305s, 3360.82/s)  LR: 9.236e-04  Data: 0.023 (0.026)
Train: 53 [ 850/1251 ( 68%)]  Loss: 4.142 (4.02)  Time: 0.304s, 3364.67/s  (0.305s, 3361.10/s)  LR: 9.234e-04  Data: 0.025 (0.025)
Train: 53 [ 900/1251 ( 72%)]  Loss: 3.896 (4.02)  Time: 0.306s, 3344.75/s  (0.305s, 3362.42/s)  LR: 9.233e-04  Data: 0.025 (0.025)
Train: 53 [ 950/1251 ( 76%)]  Loss: 4.065 (4.02)  Time: 0.303s, 3374.64/s  (0.304s, 3363.99/s)  LR: 9.232e-04  Data: 0.024 (0.025)
Train: 53 [1000/1251 ( 80%)]  Loss: 4.249 (4.03)  Time: 0.302s, 3394.01/s  (0.304s, 3365.10/s)  LR: 9.231e-04  Data: 0.022 (0.025)
Train: 53 [1050/1251 ( 84%)]  Loss: 4.062 (4.03)  Time: 0.302s, 3392.61/s  (0.304s, 3365.96/s)  LR: 9.230e-04  Data: 0.025 (0.025)
Train: 53 [1100/1251 ( 88%)]  Loss: 4.145 (4.04)  Time: 0.301s, 3401.31/s  (0.304s, 3367.02/s)  LR: 9.229e-04  Data: 0.023 (0.025)
Train: 53 [1150/1251 ( 92%)]  Loss: 4.318 (4.05)  Time: 0.305s, 3353.86/s  (0.304s, 3367.62/s)  LR: 9.228e-04  Data: 0.024 (0.025)
Train: 53 [1200/1251 ( 96%)]  Loss: 4.035 (4.05)  Time: 0.301s, 3404.88/s  (0.304s, 3368.35/s)  LR: 9.227e-04  Data: 0.021 (0.025)
Train: 53 [1250/1251 (100%)]  Loss: 3.953 (4.04)  Time: 0.276s, 3711.29/s  (0.304s, 3371.17/s)  LR: 9.226e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.076 (2.076)  Loss:  0.8472 (0.8472)  Acc@1: 82.7148 (82.7148)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.052 (0.230)  Loss:  0.9224 (1.5001)  Acc@1: 80.4245 (66.8960)  Acc@5: 94.2217 (87.9380)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-52.pth.tar', 67.3820000024414)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-49.pth.tar', 67.03600000244141)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-53.pth.tar', 66.89600002929687)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-51.pth.tar', 66.80799994628906)

Train: 54 [   0/1251 (  0%)]  Loss: 3.329 (3.33)  Time: 2.309s,  443.41/s  (2.309s,  443.41/s)  LR: 9.226e-04  Data: 2.086 (2.086)
Train: 54 [  50/1251 (  4%)]  Loss: 4.115 (3.72)  Time: 0.296s, 3465.09/s  (0.323s, 3165.55/s)  LR: 9.224e-04  Data: 0.024 (0.064)
Train: 54 [ 100/1251 (  8%)]  Loss: 3.865 (3.77)  Time: 0.297s, 3450.77/s  (0.310s, 3300.97/s)  LR: 9.223e-04  Data: 0.018 (0.043)
Train: 54 [ 150/1251 ( 12%)]  Loss: 3.846 (3.79)  Time: 0.307s, 3334.72/s  (0.306s, 3341.06/s)  LR: 9.222e-04  Data: 0.026 (0.036)
Train: 54 [ 200/1251 ( 16%)]  Loss: 3.710 (3.77)  Time: 0.302s, 3392.21/s  (0.305s, 3356.41/s)  LR: 9.221e-04  Data: 0.023 (0.033)
Train: 54 [ 250/1251 ( 20%)]  Loss: 3.962 (3.80)  Time: 0.300s, 3407.95/s  (0.304s, 3365.22/s)  LR: 9.220e-04  Data: 0.023 (0.031)
Train: 54 [ 300/1251 ( 24%)]  Loss: 3.896 (3.82)  Time: 0.328s, 3120.32/s  (0.304s, 3367.64/s)  LR: 9.219e-04  Data: 0.025 (0.030)
Train: 54 [ 350/1251 ( 28%)]  Loss: 3.708 (3.80)  Time: 0.307s, 3334.07/s  (0.304s, 3370.83/s)  LR: 9.218e-04  Data: 0.030 (0.029)
Train: 54 [ 400/1251 ( 32%)]  Loss: 3.860 (3.81)  Time: 0.301s, 3400.76/s  (0.304s, 3373.62/s)  LR: 9.217e-04  Data: 0.023 (0.028)
Train: 54 [ 450/1251 ( 36%)]  Loss: 3.982 (3.83)  Time: 0.305s, 3356.48/s  (0.303s, 3375.36/s)  LR: 9.215e-04  Data: 0.025 (0.027)
Train: 54 [ 500/1251 ( 40%)]  Loss: 4.159 (3.86)  Time: 0.304s, 3372.98/s  (0.303s, 3375.83/s)  LR: 9.214e-04  Data: 0.020 (0.027)
Train: 54 [ 550/1251 ( 44%)]  Loss: 4.199 (3.89)  Time: 0.306s, 3348.65/s  (0.303s, 3378.02/s)  LR: 9.213e-04  Data: 0.022 (0.027)
Train: 54 [ 600/1251 ( 48%)]  Loss: 4.215 (3.91)  Time: 0.299s, 3419.94/s  (0.303s, 3378.74/s)  LR: 9.212e-04  Data: 0.023 (0.026)
Train: 54 [ 650/1251 ( 52%)]  Loss: 3.862 (3.91)  Time: 0.304s, 3369.36/s  (0.303s, 3379.27/s)  LR: 9.211e-04  Data: 0.024 (0.026)
Train: 54 [ 700/1251 ( 56%)]  Loss: 4.131 (3.92)  Time: 0.307s, 3334.06/s  (0.303s, 3378.97/s)  LR: 9.210e-04  Data: 0.022 (0.026)
Train: 54 [ 750/1251 ( 60%)]  Loss: 3.944 (3.92)  Time: 0.304s, 3371.04/s  (0.303s, 3379.42/s)  LR: 9.209e-04  Data: 0.019 (0.025)
Train: 54 [ 800/1251 ( 64%)]  Loss: 3.901 (3.92)  Time: 0.302s, 3385.48/s  (0.303s, 3379.36/s)  LR: 9.208e-04  Data: 0.021 (0.025)
Train: 54 [ 850/1251 ( 68%)]  Loss: 4.025 (3.93)  Time: 0.305s, 3354.11/s  (0.303s, 3378.90/s)  LR: 9.206e-04  Data: 0.022 (0.025)
Train: 54 [ 900/1251 ( 72%)]  Loss: 4.237 (3.94)  Time: 0.303s, 3375.10/s  (0.303s, 3378.80/s)  LR: 9.205e-04  Data: 0.020 (0.025)
Train: 54 [ 950/1251 ( 76%)]  Loss: 3.665 (3.93)  Time: 0.301s, 3405.80/s  (0.303s, 3379.19/s)  LR: 9.204e-04  Data: 0.021 (0.025)
Train: 54 [1000/1251 ( 80%)]  Loss: 4.216 (3.94)  Time: 0.302s, 3393.34/s  (0.303s, 3379.68/s)  LR: 9.203e-04  Data: 0.020 (0.025)
Train: 54 [1050/1251 ( 84%)]  Loss: 3.942 (3.94)  Time: 0.304s, 3365.54/s  (0.303s, 3379.59/s)  LR: 9.202e-04  Data: 0.023 (0.025)
Train: 54 [1100/1251 ( 88%)]  Loss: 4.031 (3.95)  Time: 0.310s, 3307.62/s  (0.303s, 3379.29/s)  LR: 9.201e-04  Data: 0.025 (0.024)
Train: 54 [1150/1251 ( 92%)]  Loss: 4.224 (3.96)  Time: 0.303s, 3382.67/s  (0.303s, 3379.43/s)  LR: 9.200e-04  Data: 0.024 (0.024)
Train: 54 [1200/1251 ( 96%)]  Loss: 3.888 (3.96)  Time: 0.303s, 3377.72/s  (0.303s, 3379.53/s)  LR: 9.199e-04  Data: 0.021 (0.024)
Train: 54 [1250/1251 (100%)]  Loss: 4.074 (3.96)  Time: 0.276s, 3714.01/s  (0.303s, 3381.52/s)  LR: 9.197e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.027 (2.027)  Loss:  0.7920 (0.7920)  Acc@1: 84.2773 (84.2773)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.053 (0.236)  Loss:  0.8638 (1.4386)  Acc@1: 80.8962 (67.2660)  Acc@5: 93.8679 (88.3660)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-52.pth.tar', 67.3820000024414)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-54.pth.tar', 67.26599997558594)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-49.pth.tar', 67.03600000244141)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-53.pth.tar', 66.89600002929687)

Train: 55 [   0/1251 (  0%)]  Loss: 4.218 (4.22)  Time: 2.768s,  369.90/s  (2.768s,  369.90/s)  LR: 9.197e-04  Data: 2.559 (2.559)
Train: 55 [  50/1251 (  4%)]  Loss: 3.672 (3.95)  Time: 0.296s, 3456.68/s  (0.327s, 3127.87/s)  LR: 9.196e-04  Data: 0.024 (0.073)
Train: 55 [ 100/1251 (  8%)]  Loss: 4.127 (4.01)  Time: 0.302s, 3389.38/s  (0.312s, 3286.39/s)  LR: 9.195e-04  Data: 0.022 (0.048)
Train: 55 [ 150/1251 ( 12%)]  Loss: 4.345 (4.09)  Time: 0.297s, 3452.23/s  (0.307s, 3332.14/s)  LR: 9.194e-04  Data: 0.023 (0.040)
Train: 55 [ 200/1251 ( 16%)]  Loss: 3.821 (4.04)  Time: 0.301s, 3406.30/s  (0.305s, 3353.20/s)  LR: 9.193e-04  Data: 0.023 (0.035)
Train: 55 [ 250/1251 ( 20%)]  Loss: 4.096 (4.05)  Time: 0.299s, 3425.83/s  (0.305s, 3362.09/s)  LR: 9.192e-04  Data: 0.023 (0.033)
Train: 55 [ 300/1251 ( 24%)]  Loss: 3.972 (4.04)  Time: 0.299s, 3426.92/s  (0.304s, 3367.45/s)  LR: 9.191e-04  Data: 0.024 (0.031)
Train: 55 [ 350/1251 ( 28%)]  Loss: 3.972 (4.03)  Time: 0.304s, 3370.21/s  (0.304s, 3370.25/s)  LR: 9.189e-04  Data: 0.022 (0.030)
Train: 55 [ 400/1251 ( 32%)]  Loss: 4.219 (4.05)  Time: 0.296s, 3453.71/s  (0.303s, 3374.09/s)  LR: 9.188e-04  Data: 0.019 (0.029)
Train: 55 [ 450/1251 ( 36%)]  Loss: 3.566 (4.00)  Time: 0.298s, 3438.92/s  (0.303s, 3377.10/s)  LR: 9.187e-04  Data: 0.021 (0.028)
Train: 55 [ 500/1251 ( 40%)]  Loss: 3.761 (3.98)  Time: 0.305s, 3362.35/s  (0.303s, 3378.03/s)  LR: 9.186e-04  Data: 0.025 (0.028)
Train: 55 [ 550/1251 ( 44%)]  Loss: 4.225 (4.00)  Time: 0.299s, 3422.51/s  (0.303s, 3379.63/s)  LR: 9.185e-04  Data: 0.021 (0.027)
Train: 55 [ 600/1251 ( 48%)]  Loss: 3.593 (3.97)  Time: 0.301s, 3403.19/s  (0.303s, 3380.55/s)  LR: 9.184e-04  Data: 0.022 (0.027)
Train: 55 [ 650/1251 ( 52%)]  Loss: 4.028 (3.97)  Time: 0.302s, 3396.11/s  (0.303s, 3381.33/s)  LR: 9.183e-04  Data: 0.019 (0.027)
Train: 55 [ 700/1251 ( 56%)]  Loss: 4.005 (3.97)  Time: 0.306s, 3349.58/s  (0.303s, 3381.84/s)  LR: 9.181e-04  Data: 0.022 (0.026)
Train: 55 [ 750/1251 ( 60%)]  Loss: 3.999 (3.98)  Time: 0.307s, 3339.14/s  (0.303s, 3382.23/s)  LR: 9.180e-04  Data: 0.022 (0.026)
Train: 55 [ 800/1251 ( 64%)]  Loss: 4.192 (3.99)  Time: 0.302s, 3395.59/s  (0.303s, 3382.49/s)  LR: 9.179e-04  Data: 0.024 (0.026)
Train: 55 [ 850/1251 ( 68%)]  Loss: 3.730 (3.97)  Time: 0.302s, 3389.20/s  (0.303s, 3382.89/s)  LR: 9.178e-04  Data: 0.021 (0.026)
Train: 55 [ 900/1251 ( 72%)]  Loss: 4.114 (3.98)  Time: 0.304s, 3367.39/s  (0.303s, 3382.88/s)  LR: 9.177e-04  Data: 0.029 (0.026)
Train: 55 [ 950/1251 ( 76%)]  Loss: 3.908 (3.98)  Time: 0.304s, 3367.34/s  (0.303s, 3382.91/s)  LR: 9.176e-04  Data: 0.022 (0.025)
Train: 55 [1000/1251 ( 80%)]  Loss: 4.162 (3.99)  Time: 0.305s, 3360.07/s  (0.303s, 3382.81/s)  LR: 9.175e-04  Data: 0.017 (0.025)
Train: 55 [1050/1251 ( 84%)]  Loss: 3.953 (3.99)  Time: 0.305s, 3353.11/s  (0.303s, 3382.54/s)  LR: 9.173e-04  Data: 0.023 (0.025)
Train: 55 [1100/1251 ( 88%)]  Loss: 3.868 (3.98)  Time: 0.307s, 3340.01/s  (0.303s, 3382.22/s)  LR: 9.172e-04  Data: 0.025 (0.025)
Train: 55 [1150/1251 ( 92%)]  Loss: 3.754 (3.97)  Time: 0.304s, 3368.42/s  (0.303s, 3381.89/s)  LR: 9.171e-04  Data: 0.023 (0.025)
Train: 55 [1200/1251 ( 96%)]  Loss: 4.059 (3.97)  Time: 0.302s, 3394.76/s  (0.303s, 3381.49/s)  LR: 9.170e-04  Data: 0.014 (0.025)
Train: 55 [1250/1251 (100%)]  Loss: 4.192 (3.98)  Time: 0.277s, 3702.58/s  (0.303s, 3383.68/s)  LR: 9.169e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.075 (2.075)  Loss:  0.7900 (0.7900)  Acc@1: 85.2539 (85.2539)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.050 (0.238)  Loss:  0.8506 (1.4585)  Acc@1: 82.1934 (67.3340)  Acc@5: 94.4575 (88.3180)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-52.pth.tar', 67.3820000024414)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-55.pth.tar', 67.33400002197266)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-54.pth.tar', 67.26599997558594)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-49.pth.tar', 67.03600000244141)

Train: 56 [   0/1251 (  0%)]  Loss: 3.755 (3.75)  Time: 1.991s,  514.36/s  (1.991s,  514.36/s)  LR: 9.169e-04  Data: 1.771 (1.771)
Train: 56 [  50/1251 (  4%)]  Loss: 4.247 (4.00)  Time: 0.297s, 3446.23/s  (0.318s, 3223.36/s)  LR: 9.168e-04  Data: 0.021 (0.059)
Train: 56 [ 100/1251 (  8%)]  Loss: 3.709 (3.90)  Time: 0.291s, 3515.87/s  (0.307s, 3338.33/s)  LR: 9.166e-04  Data: 0.022 (0.041)
Train: 56 [ 150/1251 ( 12%)]  Loss: 3.885 (3.90)  Time: 0.297s, 3442.07/s  (0.304s, 3366.71/s)  LR: 9.165e-04  Data: 0.023 (0.035)
Train: 56 [ 200/1251 ( 16%)]  Loss: 3.644 (3.85)  Time: 0.295s, 3470.07/s  (0.303s, 3381.86/s)  LR: 9.164e-04  Data: 0.023 (0.032)
Train: 56 [ 250/1251 ( 20%)]  Loss: 4.289 (3.92)  Time: 0.302s, 3391.93/s  (0.302s, 3388.54/s)  LR: 9.163e-04  Data: 0.022 (0.030)
Train: 56 [ 300/1251 ( 24%)]  Loss: 4.045 (3.94)  Time: 0.309s, 3317.15/s  (0.302s, 3392.11/s)  LR: 9.162e-04  Data: 0.021 (0.029)
Train: 56 [ 350/1251 ( 28%)]  Loss: 4.057 (3.95)  Time: 0.308s, 3323.95/s  (0.302s, 3395.77/s)  LR: 9.161e-04  Data: 0.023 (0.028)
Train: 56 [ 400/1251 ( 32%)]  Loss: 4.221 (3.98)  Time: 0.299s, 3421.90/s  (0.301s, 3399.94/s)  LR: 9.160e-04  Data: 0.027 (0.028)
Train: 56 [ 450/1251 ( 36%)]  Loss: 3.981 (3.98)  Time: 0.301s, 3403.34/s  (0.301s, 3402.04/s)  LR: 9.158e-04  Data: 0.024 (0.027)
Train: 56 [ 500/1251 ( 40%)]  Loss: 3.705 (3.96)  Time: 0.303s, 3384.86/s  (0.301s, 3403.48/s)  LR: 9.157e-04  Data: 0.026 (0.026)
Train: 56 [ 550/1251 ( 44%)]  Loss: 3.895 (3.95)  Time: 0.299s, 3419.12/s  (0.301s, 3403.59/s)  LR: 9.156e-04  Data: 0.022 (0.026)
Train: 56 [ 600/1251 ( 48%)]  Loss: 3.818 (3.94)  Time: 0.300s, 3409.36/s  (0.301s, 3404.39/s)  LR: 9.155e-04  Data: 0.026 (0.026)
Train: 56 [ 650/1251 ( 52%)]  Loss: 3.592 (3.92)  Time: 0.301s, 3403.19/s  (0.301s, 3404.62/s)  LR: 9.154e-04  Data: 0.027 (0.026)
Train: 56 [ 700/1251 ( 56%)]  Loss: 3.367 (3.88)  Time: 0.298s, 3433.79/s  (0.301s, 3405.21/s)  LR: 9.153e-04  Data: 0.021 (0.025)
Train: 56 [ 750/1251 ( 60%)]  Loss: 3.733 (3.87)  Time: 0.302s, 3390.98/s  (0.301s, 3404.67/s)  LR: 9.151e-04  Data: 0.026 (0.025)
Train: 56 [ 800/1251 ( 64%)]  Loss: 4.045 (3.88)  Time: 0.302s, 3388.53/s  (0.301s, 3404.59/s)  LR: 9.150e-04  Data: 0.019 (0.025)
Train: 56 [ 850/1251 ( 68%)]  Loss: 3.767 (3.88)  Time: 0.303s, 3384.53/s  (0.301s, 3404.64/s)  LR: 9.149e-04  Data: 0.024 (0.025)
Train: 56 [ 900/1251 ( 72%)]  Loss: 3.695 (3.87)  Time: 0.297s, 3448.86/s  (0.301s, 3404.62/s)  LR: 9.148e-04  Data: 0.021 (0.025)
Train: 56 [ 950/1251 ( 76%)]  Loss: 4.121 (3.88)  Time: 0.299s, 3421.34/s  (0.301s, 3404.05/s)  LR: 9.147e-04  Data: 0.022 (0.025)
Train: 56 [1000/1251 ( 80%)]  Loss: 3.439 (3.86)  Time: 0.301s, 3396.47/s  (0.301s, 3403.67/s)  LR: 9.146e-04  Data: 0.023 (0.025)
Train: 56 [1050/1251 ( 84%)]  Loss: 3.922 (3.86)  Time: 0.301s, 3396.42/s  (0.301s, 3402.90/s)  LR: 9.144e-04  Data: 0.021 (0.024)
Train: 56 [1100/1251 ( 88%)]  Loss: 3.828 (3.86)  Time: 0.301s, 3406.50/s  (0.301s, 3402.43/s)  LR: 9.143e-04  Data: 0.014 (0.024)
Train: 56 [1150/1251 ( 92%)]  Loss: 3.896 (3.86)  Time: 0.296s, 3461.07/s  (0.301s, 3402.39/s)  LR: 9.142e-04  Data: 0.019 (0.024)
Train: 56 [1200/1251 ( 96%)]  Loss: 4.048 (3.87)  Time: 0.299s, 3423.59/s  (0.301s, 3402.10/s)  LR: 9.141e-04  Data: 0.023 (0.024)
Train: 56 [1250/1251 (100%)]  Loss: 4.060 (3.88)  Time: 0.277s, 3697.97/s  (0.301s, 3404.21/s)  LR: 9.140e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.203 (2.203)  Loss:  0.7646 (0.7646)  Acc@1: 83.0078 (83.0078)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.041 (0.239)  Loss:  0.8960 (1.4488)  Acc@1: 80.1887 (67.4840)  Acc@5: 94.9292 (88.4760)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-56.pth.tar', 67.48400005615234)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-52.pth.tar', 67.3820000024414)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-55.pth.tar', 67.33400002197266)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-54.pth.tar', 67.26599997558594)

Train: 57 [   0/1251 (  0%)]  Loss: 3.971 (3.97)  Time: 2.233s,  458.58/s  (2.233s,  458.58/s)  LR: 9.140e-04  Data: 2.014 (2.014)
Train: 57 [  50/1251 (  4%)]  Loss: 3.863 (3.92)  Time: 0.296s, 3454.85/s  (0.321s, 3190.93/s)  LR: 9.139e-04  Data: 0.026 (0.064)
Train: 57 [ 100/1251 (  8%)]  Loss: 3.879 (3.90)  Time: 0.291s, 3523.61/s  (0.307s, 3332.58/s)  LR: 9.137e-04  Data: 0.022 (0.043)
Train: 57 [ 150/1251 ( 12%)]  Loss: 3.894 (3.90)  Time: 0.295s, 3467.17/s  (0.304s, 3370.55/s)  LR: 9.136e-04  Data: 0.023 (0.036)
Train: 57 [ 200/1251 ( 16%)]  Loss: 3.963 (3.91)  Time: 0.301s, 3399.36/s  (0.303s, 3383.34/s)  LR: 9.135e-04  Data: 0.022 (0.033)
Train: 57 [ 250/1251 ( 20%)]  Loss: 4.113 (3.95)  Time: 0.307s, 3332.31/s  (0.302s, 3392.64/s)  LR: 9.134e-04  Data: 0.023 (0.031)
Train: 57 [ 300/1251 ( 24%)]  Loss: 3.565 (3.89)  Time: 0.301s, 3404.79/s  (0.301s, 3396.94/s)  LR: 9.133e-04  Data: 0.022 (0.030)
Train: 57 [ 350/1251 ( 28%)]  Loss: 3.885 (3.89)  Time: 0.304s, 3367.82/s  (0.301s, 3400.93/s)  LR: 9.131e-04  Data: 0.021 (0.029)
Train: 57 [ 400/1251 ( 32%)]  Loss: 3.837 (3.89)  Time: 0.299s, 3421.03/s  (0.301s, 3403.43/s)  LR: 9.130e-04  Data: 0.024 (0.028)
Train: 57 [ 450/1251 ( 36%)]  Loss: 4.303 (3.93)  Time: 0.302s, 3393.26/s  (0.301s, 3404.60/s)  LR: 9.129e-04  Data: 0.025 (0.027)
Train: 57 [ 500/1251 ( 40%)]  Loss: 4.192 (3.95)  Time: 0.303s, 3378.43/s  (0.301s, 3405.81/s)  LR: 9.128e-04  Data: 0.023 (0.027)
Train: 57 [ 550/1251 ( 44%)]  Loss: 3.871 (3.94)  Time: 0.301s, 3405.66/s  (0.301s, 3406.41/s)  LR: 9.127e-04  Data: 0.016 (0.027)
Train: 57 [ 600/1251 ( 48%)]  Loss: 3.655 (3.92)  Time: 0.300s, 3411.73/s  (0.301s, 3406.85/s)  LR: 9.126e-04  Data: 0.025 (0.026)
Train: 57 [ 650/1251 ( 52%)]  Loss: 4.014 (3.93)  Time: 0.299s, 3428.06/s  (0.301s, 3407.46/s)  LR: 9.124e-04  Data: 0.026 (0.026)
Train: 57 [ 700/1251 ( 56%)]  Loss: 3.761 (3.92)  Time: 0.297s, 3449.49/s  (0.300s, 3407.94/s)  LR: 9.123e-04  Data: 0.022 (0.026)
Train: 57 [ 750/1251 ( 60%)]  Loss: 3.570 (3.90)  Time: 0.295s, 3473.97/s  (0.300s, 3407.84/s)  LR: 9.122e-04  Data: 0.022 (0.026)
Train: 57 [ 800/1251 ( 64%)]  Loss: 3.756 (3.89)  Time: 0.298s, 3432.49/s  (0.300s, 3408.03/s)  LR: 9.121e-04  Data: 0.028 (0.025)
Train: 57 [ 850/1251 ( 68%)]  Loss: 3.938 (3.89)  Time: 0.302s, 3389.52/s  (0.300s, 3408.20/s)  LR: 9.120e-04  Data: 0.023 (0.025)
Train: 57 [ 900/1251 ( 72%)]  Loss: 3.436 (3.87)  Time: 0.301s, 3397.63/s  (0.300s, 3407.69/s)  LR: 9.119e-04  Data: 0.022 (0.025)
Train: 57 [ 950/1251 ( 76%)]  Loss: 4.008 (3.87)  Time: 0.304s, 3368.86/s  (0.301s, 3406.93/s)  LR: 9.117e-04  Data: 0.021 (0.025)
Train: 57 [1000/1251 ( 80%)]  Loss: 3.818 (3.87)  Time: 0.299s, 3423.73/s  (0.301s, 3406.73/s)  LR: 9.116e-04  Data: 0.023 (0.025)
Train: 57 [1050/1251 ( 84%)]  Loss: 3.954 (3.87)  Time: 0.304s, 3367.90/s  (0.301s, 3406.33/s)  LR: 9.115e-04  Data: 0.025 (0.025)
Train: 57 [1100/1251 ( 88%)]  Loss: 4.058 (3.88)  Time: 0.304s, 3369.41/s  (0.301s, 3405.72/s)  LR: 9.114e-04  Data: 0.022 (0.025)
Train: 57 [1150/1251 ( 92%)]  Loss: 3.912 (3.88)  Time: 0.305s, 3357.18/s  (0.301s, 3404.89/s)  LR: 9.113e-04  Data: 0.022 (0.025)
Train: 57 [1200/1251 ( 96%)]  Loss: 4.058 (3.89)  Time: 0.301s, 3397.34/s  (0.301s, 3404.33/s)  LR: 9.111e-04  Data: 0.028 (0.025)
Train: 57 [1250/1251 (100%)]  Loss: 3.791 (3.89)  Time: 0.276s, 3708.55/s  (0.301s, 3406.27/s)  LR: 9.110e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.121 (2.121)  Loss:  0.7373 (0.7373)  Acc@1: 85.5469 (85.5469)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.040 (0.242)  Loss:  0.8262 (1.4543)  Acc@1: 81.9575 (67.7140)  Acc@5: 94.9292 (88.4140)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-57.pth.tar', 67.71399991943359)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-56.pth.tar', 67.48400005615234)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-52.pth.tar', 67.3820000024414)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-55.pth.tar', 67.33400002197266)

Train: 58 [   0/1251 (  0%)]  Loss: 3.886 (3.89)  Time: 2.168s,  472.39/s  (2.168s,  472.39/s)  LR: 9.110e-04  Data: 1.932 (1.932)
Train: 58 [  50/1251 (  4%)]  Loss: 4.139 (4.01)  Time: 0.292s, 3503.91/s  (0.317s, 3225.42/s)  LR: 9.109e-04  Data: 0.022 (0.061)
Train: 58 [ 100/1251 (  8%)]  Loss: 4.039 (4.02)  Time: 0.297s, 3442.49/s  (0.306s, 3341.87/s)  LR: 9.108e-04  Data: 0.021 (0.042)
Train: 58 [ 150/1251 ( 12%)]  Loss: 3.830 (3.97)  Time: 0.303s, 3378.99/s  (0.304s, 3372.53/s)  LR: 9.107e-04  Data: 0.022 (0.035)
Train: 58 [ 200/1251 ( 16%)]  Loss: 3.628 (3.90)  Time: 0.292s, 3505.12/s  (0.303s, 3383.38/s)  LR: 9.105e-04  Data: 0.019 (0.032)
Train: 58 [ 250/1251 ( 20%)]  Loss: 3.788 (3.88)  Time: 0.304s, 3363.89/s  (0.302s, 3388.24/s)  LR: 9.104e-04  Data: 0.023 (0.030)
Train: 58 [ 300/1251 ( 24%)]  Loss: 3.986 (3.90)  Time: 0.299s, 3430.44/s  (0.302s, 3393.67/s)  LR: 9.103e-04  Data: 0.026 (0.029)
Train: 58 [ 350/1251 ( 28%)]  Loss: 4.119 (3.93)  Time: 0.303s, 3375.13/s  (0.302s, 3394.28/s)  LR: 9.102e-04  Data: 0.026 (0.028)
Train: 58 [ 400/1251 ( 32%)]  Loss: 4.164 (3.95)  Time: 0.303s, 3382.24/s  (0.302s, 3395.41/s)  LR: 9.101e-04  Data: 0.020 (0.027)
Train: 58 [ 450/1251 ( 36%)]  Loss: 3.834 (3.94)  Time: 0.307s, 3330.97/s  (0.302s, 3395.04/s)  LR: 9.099e-04  Data: 0.026 (0.027)
Train: 58 [ 500/1251 ( 40%)]  Loss: 3.757 (3.92)  Time: 0.299s, 3420.15/s  (0.302s, 3395.84/s)  LR: 9.098e-04  Data: 0.020 (0.026)
Train: 58 [ 550/1251 ( 44%)]  Loss: 4.232 (3.95)  Time: 0.302s, 3389.36/s  (0.302s, 3395.65/s)  LR: 9.097e-04  Data: 0.021 (0.026)
Train: 58 [ 600/1251 ( 48%)]  Loss: 3.931 (3.95)  Time: 0.304s, 3367.93/s  (0.302s, 3395.53/s)  LR: 9.096e-04  Data: 0.024 (0.026)
Train: 58 [ 650/1251 ( 52%)]  Loss: 3.985 (3.95)  Time: 0.298s, 3434.35/s  (0.302s, 3395.03/s)  LR: 9.095e-04  Data: 0.022 (0.026)
Train: 58 [ 700/1251 ( 56%)]  Loss: 3.832 (3.94)  Time: 0.304s, 3369.79/s  (0.302s, 3394.56/s)  LR: 9.093e-04  Data: 0.020 (0.025)
Train: 58 [ 750/1251 ( 60%)]  Loss: 3.918 (3.94)  Time: 0.301s, 3402.47/s  (0.302s, 3394.51/s)  LR: 9.092e-04  Data: 0.024 (0.025)
Train: 58 [ 800/1251 ( 64%)]  Loss: 3.800 (3.93)  Time: 0.300s, 3408.59/s  (0.302s, 3394.90/s)  LR: 9.091e-04  Data: 0.027 (0.025)
Train: 58 [ 850/1251 ( 68%)]  Loss: 3.954 (3.93)  Time: 0.304s, 3373.05/s  (0.302s, 3393.86/s)  LR: 9.090e-04  Data: 0.028 (0.025)
Train: 58 [ 900/1251 ( 72%)]  Loss: 3.840 (3.93)  Time: 0.305s, 3357.37/s  (0.302s, 3393.15/s)  LR: 9.089e-04  Data: 0.021 (0.025)
Train: 58 [ 950/1251 ( 76%)]  Loss: 3.451 (3.91)  Time: 0.296s, 3457.70/s  (0.302s, 3392.88/s)  LR: 9.087e-04  Data: 0.018 (0.025)
Train: 58 [1000/1251 ( 80%)]  Loss: 3.708 (3.90)  Time: 0.297s, 3451.92/s  (0.302s, 3392.44/s)  LR: 9.086e-04  Data: 0.018 (0.025)
Train: 58 [1050/1251 ( 84%)]  Loss: 4.085 (3.90)  Time: 0.307s, 3335.55/s  (0.302s, 3391.83/s)  LR: 9.085e-04  Data: 0.024 (0.025)
Train: 58 [1100/1251 ( 88%)]  Loss: 3.770 (3.90)  Time: 0.302s, 3385.49/s  (0.302s, 3391.54/s)  LR: 9.084e-04  Data: 0.022 (0.024)
Train: 58 [1150/1251 ( 92%)]  Loss: 3.968 (3.90)  Time: 0.309s, 3316.89/s  (0.302s, 3391.12/s)  LR: 9.083e-04  Data: 0.024 (0.024)
Train: 58 [1200/1251 ( 96%)]  Loss: 3.695 (3.89)  Time: 0.303s, 3377.69/s  (0.302s, 3390.56/s)  LR: 9.081e-04  Data: 0.020 (0.024)
Train: 58 [1250/1251 (100%)]  Loss: 3.904 (3.89)  Time: 0.276s, 3703.47/s  (0.302s, 3392.28/s)  LR: 9.080e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.079 (2.079)  Loss:  0.7949 (0.7949)  Acc@1: 84.7656 (84.7656)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.041 (0.237)  Loss:  0.8721 (1.4530)  Acc@1: 79.8349 (67.6060)  Acc@5: 94.3396 (88.2900)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-57.pth.tar', 67.71399991943359)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-58.pth.tar', 67.60600003173828)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-56.pth.tar', 67.48400005615234)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-52.pth.tar', 67.3820000024414)

Train: 59 [   0/1251 (  0%)]  Loss: 4.007 (4.01)  Time: 2.355s,  434.73/s  (2.355s,  434.73/s)  LR: 9.080e-04  Data: 2.129 (2.129)
Train: 59 [  50/1251 (  4%)]  Loss: 4.147 (4.08)  Time: 0.298s, 3434.42/s  (0.323s, 3174.06/s)  LR: 9.079e-04  Data: 0.024 (0.065)
Train: 59 [ 100/1251 (  8%)]  Loss: 3.572 (3.91)  Time: 0.302s, 3385.73/s  (0.310s, 3304.28/s)  LR: 9.078e-04  Data: 0.025 (0.044)
Train: 59 [ 150/1251 ( 12%)]  Loss: 3.812 (3.88)  Time: 0.302s, 3390.31/s  (0.307s, 3339.46/s)  LR: 9.077e-04  Data: 0.022 (0.037)
Train: 59 [ 200/1251 ( 16%)]  Loss: 3.665 (3.84)  Time: 0.300s, 3408.06/s  (0.305s, 3355.99/s)  LR: 9.075e-04  Data: 0.026 (0.034)
Train: 59 [ 250/1251 ( 20%)]  Loss: 3.987 (3.87)  Time: 0.299s, 3419.78/s  (0.304s, 3363.44/s)  LR: 9.074e-04  Data: 0.024 (0.031)
Train: 59 [ 300/1251 ( 24%)]  Loss: 4.082 (3.90)  Time: 0.294s, 3481.55/s  (0.304s, 3368.68/s)  LR: 9.073e-04  Data: 0.022 (0.030)
Train: 59 [ 350/1251 ( 28%)]  Loss: 3.680 (3.87)  Time: 0.298s, 3433.70/s  (0.304s, 3371.18/s)  LR: 9.072e-04  Data: 0.019 (0.029)
Train: 59 [ 400/1251 ( 32%)]  Loss: 4.191 (3.90)  Time: 0.299s, 3422.69/s  (0.304s, 3373.90/s)  LR: 9.071e-04  Data: 0.022 (0.028)
Train: 59 [ 450/1251 ( 36%)]  Loss: 4.027 (3.92)  Time: 0.305s, 3362.50/s  (0.303s, 3375.66/s)  LR: 9.069e-04  Data: 0.021 (0.027)
Train: 59 [ 500/1251 ( 40%)]  Loss: 4.252 (3.95)  Time: 0.303s, 3380.14/s  (0.303s, 3376.40/s)  LR: 9.068e-04  Data: 0.029 (0.027)
Train: 59 [ 550/1251 ( 44%)]  Loss: 3.994 (3.95)  Time: 0.302s, 3389.59/s  (0.303s, 3376.66/s)  LR: 9.067e-04  Data: 0.023 (0.027)
Train: 59 [ 600/1251 ( 48%)]  Loss: 4.098 (3.96)  Time: 0.302s, 3390.82/s  (0.303s, 3377.02/s)  LR: 9.066e-04  Data: 0.019 (0.026)
Train: 59 [ 650/1251 ( 52%)]  Loss: 4.298 (3.99)  Time: 0.303s, 3379.25/s  (0.303s, 3377.00/s)  LR: 9.064e-04  Data: 0.023 (0.026)
Train: 59 [ 700/1251 ( 56%)]  Loss: 4.043 (3.99)  Time: 0.306s, 3344.68/s  (0.303s, 3377.15/s)  LR: 9.063e-04  Data: 0.021 (0.026)
Train: 59 [ 750/1251 ( 60%)]  Loss: 3.723 (3.97)  Time: 0.305s, 3360.22/s  (0.303s, 3376.48/s)  LR: 9.062e-04  Data: 0.021 (0.026)
Train: 59 [ 800/1251 ( 64%)]  Loss: 4.020 (3.98)  Time: 0.299s, 3422.50/s  (0.303s, 3376.86/s)  LR: 9.061e-04  Data: 0.020 (0.026)
Train: 59 [ 850/1251 ( 68%)]  Loss: 3.476 (3.95)  Time: 0.301s, 3397.89/s  (0.303s, 3376.56/s)  LR: 9.060e-04  Data: 0.022 (0.025)
Train: 59 [ 900/1251 ( 72%)]  Loss: 3.875 (3.94)  Time: 0.304s, 3373.03/s  (0.303s, 3376.05/s)  LR: 9.058e-04  Data: 0.023 (0.025)
Train: 59 [ 950/1251 ( 76%)]  Loss: 4.072 (3.95)  Time: 0.303s, 3379.55/s  (0.303s, 3375.83/s)  LR: 9.057e-04  Data: 0.026 (0.025)
Train: 59 [1000/1251 ( 80%)]  Loss: 3.656 (3.94)  Time: 0.301s, 3405.03/s  (0.303s, 3374.99/s)  LR: 9.056e-04  Data: 0.013 (0.025)
Train: 59 [1050/1251 ( 84%)]  Loss: 3.868 (3.93)  Time: 0.306s, 3344.22/s  (0.304s, 3373.82/s)  LR: 9.055e-04  Data: 0.022 (0.025)
Train: 59 [1100/1251 ( 88%)]  Loss: 4.156 (3.94)  Time: 0.305s, 3356.83/s  (0.304s, 3373.13/s)  LR: 9.054e-04  Data: 0.023 (0.025)
Train: 59 [1150/1251 ( 92%)]  Loss: 3.963 (3.94)  Time: 0.304s, 3372.80/s  (0.304s, 3372.53/s)  LR: 9.052e-04  Data: 0.025 (0.025)
Train: 59 [1200/1251 ( 96%)]  Loss: 4.087 (3.95)  Time: 0.301s, 3406.33/s  (0.304s, 3372.20/s)  LR: 9.051e-04  Data: 0.024 (0.025)
Train: 59 [1250/1251 (100%)]  Loss: 3.698 (3.94)  Time: 0.276s, 3707.77/s  (0.304s, 3373.39/s)  LR: 9.050e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.059 (2.059)  Loss:  0.7778 (0.7778)  Acc@1: 85.2539 (85.2539)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.044 (0.239)  Loss:  0.8818 (1.4452)  Acc@1: 80.6604 (67.7580)  Acc@5: 95.0472 (88.5420)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-59.pth.tar', 67.75800013183594)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-57.pth.tar', 67.71399991943359)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-58.pth.tar', 67.60600003173828)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-56.pth.tar', 67.48400005615234)

Train: 60 [   0/1251 (  0%)]  Loss: 3.696 (3.70)  Time: 2.032s,  504.02/s  (2.032s,  504.02/s)  LR: 9.050e-04  Data: 1.794 (1.794)
Train: 60 [  50/1251 (  4%)]  Loss: 4.133 (3.91)  Time: 0.298s, 3431.07/s  (0.328s, 3125.81/s)  LR: 9.049e-04  Data: 0.022 (0.060)
Train: 60 [ 100/1251 (  8%)]  Loss: 4.054 (3.96)  Time: 0.305s, 3362.08/s  (0.314s, 3263.06/s)  LR: 9.047e-04  Data: 0.022 (0.041)
Train: 60 [ 150/1251 ( 12%)]  Loss: 3.813 (3.92)  Time: 0.308s, 3320.83/s  (0.310s, 3302.52/s)  LR: 9.046e-04  Data: 0.022 (0.035)
Train: 60 [ 200/1251 ( 16%)]  Loss: 3.817 (3.90)  Time: 0.307s, 3335.99/s  (0.308s, 3323.20/s)  LR: 9.045e-04  Data: 0.023 (0.032)
Train: 60 [ 250/1251 ( 20%)]  Loss: 3.895 (3.90)  Time: 0.306s, 3349.34/s  (0.307s, 3333.95/s)  LR: 9.044e-04  Data: 0.025 (0.030)
Train: 60 [ 300/1251 ( 24%)]  Loss: 3.739 (3.88)  Time: 0.295s, 3473.18/s  (0.306s, 3341.61/s)  LR: 9.042e-04  Data: 0.019 (0.029)
Train: 60 [ 350/1251 ( 28%)]  Loss: 3.973 (3.89)  Time: 0.303s, 3382.56/s  (0.306s, 3345.77/s)  LR: 9.041e-04  Data: 0.023 (0.028)
Train: 60 [ 400/1251 ( 32%)]  Loss: 4.132 (3.92)  Time: 0.310s, 3307.94/s  (0.306s, 3346.91/s)  LR: 9.040e-04  Data: 0.026 (0.028)
Train: 60 [ 450/1251 ( 36%)]  Loss: 4.300 (3.96)  Time: 0.306s, 3345.27/s  (0.306s, 3347.69/s)  LR: 9.039e-04  Data: 0.022 (0.027)
Train: 60 [ 500/1251 ( 40%)]  Loss: 3.504 (3.91)  Time: 0.308s, 3327.69/s  (0.306s, 3348.06/s)  LR: 9.038e-04  Data: 0.028 (0.027)
Train: 60 [ 550/1251 ( 44%)]  Loss: 3.644 (3.89)  Time: 0.304s, 3370.93/s  (0.306s, 3349.05/s)  LR: 9.036e-04  Data: 0.025 (0.026)
Train: 60 [ 600/1251 ( 48%)]  Loss: 4.130 (3.91)  Time: 0.301s, 3403.94/s  (0.306s, 3349.40/s)  LR: 9.035e-04  Data: 0.019 (0.026)
Train: 60 [ 650/1251 ( 52%)]  Loss: 4.038 (3.92)  Time: 0.306s, 3350.63/s  (0.306s, 3349.83/s)  LR: 9.034e-04  Data: 0.022 (0.026)
Train: 60 [ 700/1251 ( 56%)]  Loss: 3.831 (3.91)  Time: 0.306s, 3346.92/s  (0.306s, 3349.99/s)  LR: 9.033e-04  Data: 0.022 (0.025)
Train: 60 [ 750/1251 ( 60%)]  Loss: 3.917 (3.91)  Time: 0.308s, 3324.85/s  (0.306s, 3349.74/s)  LR: 9.031e-04  Data: 0.025 (0.025)
Train: 60 [ 800/1251 ( 64%)]  Loss: 3.885 (3.91)  Time: 0.309s, 3312.20/s  (0.306s, 3349.54/s)  LR: 9.030e-04  Data: 0.022 (0.025)
Train: 60 [ 850/1251 ( 68%)]  Loss: 3.565 (3.89)  Time: 0.308s, 3326.23/s  (0.306s, 3350.60/s)  LR: 9.029e-04  Data: 0.022 (0.025)
Train: 60 [ 900/1251 ( 72%)]  Loss: 3.994 (3.90)  Time: 0.308s, 3323.89/s  (0.306s, 3350.77/s)  LR: 9.028e-04  Data: 0.025 (0.025)
Train: 60 [ 950/1251 ( 76%)]  Loss: 3.554 (3.88)  Time: 0.309s, 3309.80/s  (0.306s, 3350.51/s)  LR: 9.026e-04  Data: 0.024 (0.025)
Train: 60 [1000/1251 ( 80%)]  Loss: 3.891 (3.88)  Time: 0.305s, 3359.45/s  (0.306s, 3350.94/s)  LR: 9.025e-04  Data: 0.021 (0.025)
Train: 60 [1050/1251 ( 84%)]  Loss: 3.669 (3.87)  Time: 0.301s, 3403.96/s  (0.306s, 3350.83/s)  LR: 9.024e-04  Data: 0.018 (0.025)
Train: 60 [1100/1251 ( 88%)]  Loss: 4.200 (3.89)  Time: 0.308s, 3321.08/s  (0.306s, 3350.79/s)  LR: 9.023e-04  Data: 0.022 (0.024)
Train: 60 [1150/1251 ( 92%)]  Loss: 4.076 (3.89)  Time: 0.304s, 3370.02/s  (0.306s, 3350.84/s)  LR: 9.021e-04  Data: 0.024 (0.024)
Train: 60 [1200/1251 ( 96%)]  Loss: 4.214 (3.91)  Time: 0.303s, 3379.49/s  (0.306s, 3350.98/s)  LR: 9.020e-04  Data: 0.023 (0.024)
Train: 60 [1250/1251 (100%)]  Loss: 3.931 (3.91)  Time: 0.276s, 3714.56/s  (0.305s, 3353.11/s)  LR: 9.019e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.200 (2.200)  Loss:  0.7559 (0.7559)  Acc@1: 84.4727 (84.4727)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.060 (0.235)  Loss:  0.8374 (1.4183)  Acc@1: 83.2547 (68.3600)  Acc@5: 94.5755 (88.7000)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-60.pth.tar', 68.35999996582031)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-59.pth.tar', 67.75800013183594)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-57.pth.tar', 67.71399991943359)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-58.pth.tar', 67.60600003173828)

Train: 61 [   0/1251 (  0%)]  Loss: 4.029 (4.03)  Time: 2.430s,  421.44/s  (2.430s,  421.44/s)  LR: 9.019e-04  Data: 2.203 (2.203)
Train: 61 [  50/1251 (  4%)]  Loss: 3.904 (3.97)  Time: 0.301s, 3400.23/s  (0.337s, 3042.42/s)  LR: 9.018e-04  Data: 0.024 (0.075)
Train: 61 [ 100/1251 (  8%)]  Loss: 3.848 (3.93)  Time: 0.298s, 3435.53/s  (0.319s, 3212.77/s)  LR: 9.017e-04  Data: 0.020 (0.049)
Train: 61 [ 150/1251 ( 12%)]  Loss: 3.513 (3.82)  Time: 0.302s, 3392.03/s  (0.313s, 3269.44/s)  LR: 9.015e-04  Data: 0.022 (0.040)
Train: 61 [ 200/1251 ( 16%)]  Loss: 3.957 (3.85)  Time: 0.306s, 3351.88/s  (0.311s, 3293.64/s)  LR: 9.014e-04  Data: 0.022 (0.036)
Train: 61 [ 250/1251 ( 20%)]  Loss: 3.989 (3.87)  Time: 0.302s, 3386.92/s  (0.310s, 3305.52/s)  LR: 9.013e-04  Data: 0.021 (0.033)
Train: 61 [ 300/1251 ( 24%)]  Loss: 4.130 (3.91)  Time: 0.304s, 3371.96/s  (0.309s, 3314.55/s)  LR: 9.012e-04  Data: 0.023 (0.032)
Train: 61 [ 350/1251 ( 28%)]  Loss: 3.568 (3.87)  Time: 0.303s, 3381.61/s  (0.308s, 3321.08/s)  LR: 9.010e-04  Data: 0.025 (0.030)
Train: 61 [ 400/1251 ( 32%)]  Loss: 3.942 (3.88)  Time: 0.305s, 3359.91/s  (0.308s, 3324.77/s)  LR: 9.009e-04  Data: 0.022 (0.029)
Train: 61 [ 450/1251 ( 36%)]  Loss: 4.186 (3.91)  Time: 0.309s, 3319.08/s  (0.308s, 3328.09/s)  LR: 9.008e-04  Data: 0.023 (0.029)
Train: 61 [ 500/1251 ( 40%)]  Loss: 4.012 (3.92)  Time: 0.310s, 3301.40/s  (0.308s, 3329.33/s)  LR: 9.007e-04  Data: 0.024 (0.028)
Train: 61 [ 550/1251 ( 44%)]  Loss: 4.187 (3.94)  Time: 0.301s, 3403.94/s  (0.307s, 3331.30/s)  LR: 9.005e-04  Data: 0.020 (0.028)
Train: 61 [ 600/1251 ( 48%)]  Loss: 3.999 (3.94)  Time: 0.308s, 3322.91/s  (0.307s, 3331.97/s)  LR: 9.004e-04  Data: 0.026 (0.027)
Train: 61 [ 650/1251 ( 52%)]  Loss: 3.476 (3.91)  Time: 0.306s, 3341.14/s  (0.307s, 3333.02/s)  LR: 9.003e-04  Data: 0.024 (0.027)
Train: 61 [ 700/1251 ( 56%)]  Loss: 3.893 (3.91)  Time: 0.309s, 3308.71/s  (0.307s, 3333.99/s)  LR: 9.002e-04  Data: 0.023 (0.027)
Train: 61 [ 750/1251 ( 60%)]  Loss: 3.456 (3.88)  Time: 0.311s, 3296.10/s  (0.307s, 3334.46/s)  LR: 9.000e-04  Data: 0.024 (0.026)
Train: 61 [ 800/1251 ( 64%)]  Loss: 3.942 (3.88)  Time: 0.305s, 3357.11/s  (0.307s, 3334.74/s)  LR: 8.999e-04  Data: 0.020 (0.026)
Train: 61 [ 850/1251 ( 68%)]  Loss: 4.222 (3.90)  Time: 0.308s, 3328.42/s  (0.307s, 3334.59/s)  LR: 8.998e-04  Data: 0.024 (0.026)
Train: 61 [ 900/1251 ( 72%)]  Loss: 4.007 (3.91)  Time: 0.306s, 3351.35/s  (0.307s, 3334.95/s)  LR: 8.997e-04  Data: 0.022 (0.026)
Train: 61 [ 950/1251 ( 76%)]  Loss: 4.025 (3.91)  Time: 0.305s, 3354.65/s  (0.307s, 3334.73/s)  LR: 8.995e-04  Data: 0.020 (0.026)
Train: 61 [1000/1251 ( 80%)]  Loss: 3.940 (3.92)  Time: 0.300s, 3415.40/s  (0.307s, 3334.52/s)  LR: 8.994e-04  Data: 0.023 (0.025)
Train: 61 [1050/1251 ( 84%)]  Loss: 3.774 (3.91)  Time: 0.305s, 3360.48/s  (0.307s, 3335.18/s)  LR: 8.993e-04  Data: 0.021 (0.025)
Train: 61 [1100/1251 ( 88%)]  Loss: 3.834 (3.91)  Time: 0.308s, 3325.69/s  (0.307s, 3334.93/s)  LR: 8.992e-04  Data: 0.023 (0.025)
Train: 61 [1150/1251 ( 92%)]  Loss: 3.962 (3.91)  Time: 0.307s, 3336.85/s  (0.307s, 3334.71/s)  LR: 8.990e-04  Data: 0.024 (0.025)
Train: 61 [1200/1251 ( 96%)]  Loss: 3.760 (3.90)  Time: 0.305s, 3358.10/s  (0.307s, 3334.78/s)  LR: 8.989e-04  Data: 0.021 (0.025)
Train: 61 [1250/1251 (100%)]  Loss: 3.852 (3.90)  Time: 0.276s, 3708.15/s  (0.307s, 3336.58/s)  LR: 8.988e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.056 (2.056)  Loss:  0.7471 (0.7471)  Acc@1: 84.1797 (84.1797)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.067 (0.238)  Loss:  0.8418 (1.4120)  Acc@1: 82.1934 (67.9600)  Acc@5: 94.2217 (88.6900)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-60.pth.tar', 68.35999996582031)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-61.pth.tar', 67.96000002197266)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-59.pth.tar', 67.75800013183594)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-57.pth.tar', 67.71399991943359)

Train: 62 [   0/1251 (  0%)]  Loss: 3.530 (3.53)  Time: 2.410s,  424.98/s  (2.410s,  424.98/s)  LR: 8.988e-04  Data: 2.193 (2.193)
Train: 62 [  50/1251 (  4%)]  Loss: 3.918 (3.72)  Time: 0.295s, 3476.65/s  (0.332s, 3084.56/s)  LR: 8.986e-04  Data: 0.022 (0.065)
Train: 62 [ 100/1251 (  8%)]  Loss: 4.187 (3.88)  Time: 0.303s, 3374.46/s  (0.317s, 3227.12/s)  LR: 8.985e-04  Data: 0.022 (0.044)
Train: 62 [ 150/1251 ( 12%)]  Loss: 3.699 (3.83)  Time: 0.310s, 3308.24/s  (0.313s, 3269.76/s)  LR: 8.984e-04  Data: 0.026 (0.037)
Train: 62 [ 200/1251 ( 16%)]  Loss: 4.146 (3.90)  Time: 0.306s, 3345.68/s  (0.311s, 3288.93/s)  LR: 8.983e-04  Data: 0.022 (0.033)
Train: 62 [ 250/1251 ( 20%)]  Loss: 3.912 (3.90)  Time: 0.305s, 3354.31/s  (0.310s, 3300.23/s)  LR: 8.981e-04  Data: 0.023 (0.031)
Train: 62 [ 300/1251 ( 24%)]  Loss: 4.009 (3.91)  Time: 0.311s, 3295.69/s  (0.310s, 3307.61/s)  LR: 8.980e-04  Data: 0.023 (0.030)
Train: 62 [ 350/1251 ( 28%)]  Loss: 3.856 (3.91)  Time: 0.305s, 3357.98/s  (0.309s, 3311.41/s)  LR: 8.979e-04  Data: 0.025 (0.029)
Train: 62 [ 400/1251 ( 32%)]  Loss: 4.056 (3.92)  Time: 0.308s, 3322.01/s  (0.309s, 3313.77/s)  LR: 8.978e-04  Data: 0.023 (0.028)
Train: 62 [ 450/1251 ( 36%)]  Loss: 4.069 (3.94)  Time: 0.309s, 3316.65/s  (0.309s, 3316.11/s)  LR: 8.976e-04  Data: 0.025 (0.027)
Train: 62 [ 500/1251 ( 40%)]  Loss: 4.157 (3.96)  Time: 0.312s, 3284.56/s  (0.309s, 3317.10/s)  LR: 8.975e-04  Data: 0.025 (0.027)
Train: 62 [ 550/1251 ( 44%)]  Loss: 4.165 (3.98)  Time: 0.307s, 3333.06/s  (0.309s, 3318.17/s)  LR: 8.974e-04  Data: 0.024 (0.026)
Train: 62 [ 600/1251 ( 48%)]  Loss: 3.929 (3.97)  Time: 0.307s, 3332.93/s  (0.309s, 3317.58/s)  LR: 8.973e-04  Data: 0.024 (0.026)
Train: 62 [ 650/1251 ( 52%)]  Loss: 4.078 (3.98)  Time: 0.308s, 3319.49/s  (0.309s, 3318.04/s)  LR: 8.971e-04  Data: 0.022 (0.026)
Train: 62 [ 700/1251 ( 56%)]  Loss: 4.061 (3.98)  Time: 0.311s, 3290.28/s  (0.309s, 3317.41/s)  LR: 8.970e-04  Data: 0.025 (0.026)
Train: 62 [ 750/1251 ( 60%)]  Loss: 3.689 (3.97)  Time: 0.312s, 3286.41/s  (0.309s, 3316.81/s)  LR: 8.969e-04  Data: 0.021 (0.025)
Train: 62 [ 800/1251 ( 64%)]  Loss: 4.126 (3.98)  Time: 0.311s, 3295.16/s  (0.309s, 3316.59/s)  LR: 8.967e-04  Data: 0.024 (0.025)
Train: 62 [ 850/1251 ( 68%)]  Loss: 3.778 (3.96)  Time: 0.310s, 3303.64/s  (0.309s, 3315.88/s)  LR: 8.966e-04  Data: 0.023 (0.025)
Train: 62 [ 900/1251 ( 72%)]  Loss: 3.305 (3.93)  Time: 0.313s, 3271.81/s  (0.309s, 3315.51/s)  LR: 8.965e-04  Data: 0.022 (0.025)
Train: 62 [ 950/1251 ( 76%)]  Loss: 3.882 (3.93)  Time: 0.313s, 3271.48/s  (0.309s, 3314.75/s)  LR: 8.964e-04  Data: 0.020 (0.025)
Train: 62 [1000/1251 ( 80%)]  Loss: 3.548 (3.91)  Time: 0.307s, 3338.91/s  (0.309s, 3314.05/s)  LR: 8.962e-04  Data: 0.019 (0.025)
Train: 62 [1050/1251 ( 84%)]  Loss: 3.967 (3.91)  Time: 0.310s, 3299.52/s  (0.309s, 3313.40/s)  LR: 8.961e-04  Data: 0.022 (0.025)
Train: 62 [1100/1251 ( 88%)]  Loss: 4.100 (3.92)  Time: 0.318s, 3221.40/s  (0.309s, 3313.04/s)  LR: 8.960e-04  Data: 0.026 (0.025)
Train: 62 [1150/1251 ( 92%)]  Loss: 3.599 (3.91)  Time: 0.314s, 3263.62/s  (0.309s, 3312.31/s)  LR: 8.959e-04  Data: 0.019 (0.024)
Train: 62 [1200/1251 ( 96%)]  Loss: 3.906 (3.91)  Time: 0.314s, 3265.20/s  (0.309s, 3311.63/s)  LR: 8.957e-04  Data: 0.022 (0.024)
Train: 62 [1250/1251 (100%)]  Loss: 3.875 (3.91)  Time: 0.276s, 3712.23/s  (0.309s, 3313.32/s)  LR: 8.956e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.075 (2.075)  Loss:  0.7832 (0.7832)  Acc@1: 84.6680 (84.6680)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.067 (0.237)  Loss:  0.8452 (1.4147)  Acc@1: 81.3679 (68.1380)  Acc@5: 93.8679 (88.5900)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-60.pth.tar', 68.35999996582031)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-62.pth.tar', 68.137999921875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-61.pth.tar', 67.96000002197266)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-59.pth.tar', 67.75800013183594)

Train: 63 [   0/1251 (  0%)]  Loss: 3.798 (3.80)  Time: 2.092s,  489.39/s  (2.092s,  489.39/s)  LR: 8.956e-04  Data: 1.873 (1.873)
Train: 63 [  50/1251 (  4%)]  Loss: 4.004 (3.90)  Time: 0.300s, 3415.08/s  (0.330s, 3107.30/s)  LR: 8.955e-04  Data: 0.024 (0.060)
Train: 63 [ 100/1251 (  8%)]  Loss: 3.984 (3.93)  Time: 0.303s, 3379.19/s  (0.317s, 3228.55/s)  LR: 8.953e-04  Data: 0.021 (0.041)
Train: 63 [ 150/1251 ( 12%)]  Loss: 3.958 (3.94)  Time: 0.309s, 3310.41/s  (0.314s, 3262.29/s)  LR: 8.952e-04  Data: 0.022 (0.035)
Train: 63 [ 200/1251 ( 16%)]  Loss: 4.078 (3.96)  Time: 0.306s, 3348.33/s  (0.312s, 3278.67/s)  LR: 8.951e-04  Data: 0.021 (0.032)
Train: 63 [ 250/1251 ( 20%)]  Loss: 3.831 (3.94)  Time: 0.314s, 3266.00/s  (0.312s, 3284.74/s)  LR: 8.950e-04  Data: 0.027 (0.030)
Train: 63 [ 300/1251 ( 24%)]  Loss: 4.113 (3.97)  Time: 0.314s, 3259.27/s  (0.311s, 3289.77/s)  LR: 8.948e-04  Data: 0.025 (0.029)
Train: 63 [ 350/1251 ( 28%)]  Loss: 3.719 (3.94)  Time: 0.308s, 3322.34/s  (0.311s, 3292.96/s)  LR: 8.947e-04  Data: 0.020 (0.028)
Train: 63 [ 400/1251 ( 32%)]  Loss: 3.695 (3.91)  Time: 0.311s, 3292.59/s  (0.311s, 3293.38/s)  LR: 8.946e-04  Data: 0.022 (0.027)
Train: 63 [ 450/1251 ( 36%)]  Loss: 4.263 (3.94)  Time: 0.311s, 3287.54/s  (0.311s, 3294.05/s)  LR: 8.944e-04  Data: 0.024 (0.027)
Train: 63 [ 500/1251 ( 40%)]  Loss: 4.239 (3.97)  Time: 0.310s, 3308.17/s  (0.311s, 3294.28/s)  LR: 8.943e-04  Data: 0.022 (0.026)
Train: 63 [ 550/1251 ( 44%)]  Loss: 3.819 (3.96)  Time: 0.313s, 3273.86/s  (0.311s, 3293.87/s)  LR: 8.942e-04  Data: 0.023 (0.026)
Train: 63 [ 600/1251 ( 48%)]  Loss: 3.786 (3.95)  Time: 0.304s, 3365.37/s  (0.311s, 3293.80/s)  LR: 8.941e-04  Data: 0.023 (0.026)
Train: 63 [ 650/1251 ( 52%)]  Loss: 3.819 (3.94)  Time: 0.315s, 3250.66/s  (0.311s, 3293.30/s)  LR: 8.939e-04  Data: 0.020 (0.026)
Train: 63 [ 700/1251 ( 56%)]  Loss: 3.887 (3.93)  Time: 0.318s, 3219.50/s  (0.311s, 3292.20/s)  LR: 8.938e-04  Data: 0.025 (0.025)
Train: 63 [ 750/1251 ( 60%)]  Loss: 4.108 (3.94)  Time: 0.310s, 3301.53/s  (0.311s, 3291.59/s)  LR: 8.937e-04  Data: 0.017 (0.025)
Train: 63 [ 800/1251 ( 64%)]  Loss: 3.756 (3.93)  Time: 0.312s, 3280.21/s  (0.311s, 3291.01/s)  LR: 8.935e-04  Data: 0.018 (0.025)
Train: 63 [ 850/1251 ( 68%)]  Loss: 3.967 (3.93)  Time: 0.310s, 3300.36/s  (0.311s, 3290.43/s)  LR: 8.934e-04  Data: 0.024 (0.025)
Train: 63 [ 900/1251 ( 72%)]  Loss: 4.027 (3.94)  Time: 0.308s, 3327.49/s  (0.311s, 3289.74/s)  LR: 8.933e-04  Data: 0.022 (0.025)
Train: 63 [ 950/1251 ( 76%)]  Loss: 4.106 (3.95)  Time: 0.311s, 3290.28/s  (0.311s, 3289.35/s)  LR: 8.932e-04  Data: 0.022 (0.025)
Train: 63 [1000/1251 ( 80%)]  Loss: 3.597 (3.93)  Time: 0.303s, 3378.17/s  (0.311s, 3288.52/s)  LR: 8.930e-04  Data: 0.021 (0.025)
Train: 63 [1050/1251 ( 84%)]  Loss: 3.947 (3.93)  Time: 0.324s, 3163.73/s  (0.311s, 3287.57/s)  LR: 8.929e-04  Data: 0.020 (0.024)
Train: 63 [1100/1251 ( 88%)]  Loss: 3.779 (3.93)  Time: 0.314s, 3256.08/s  (0.311s, 3287.46/s)  LR: 8.928e-04  Data: 0.024 (0.024)
Train: 63 [1150/1251 ( 92%)]  Loss: 3.600 (3.91)  Time: 0.315s, 3246.65/s  (0.312s, 3287.26/s)  LR: 8.926e-04  Data: 0.023 (0.024)
Train: 63 [1200/1251 ( 96%)]  Loss: 3.939 (3.91)  Time: 0.308s, 3320.27/s  (0.312s, 3286.80/s)  LR: 8.925e-04  Data: 0.015 (0.024)
Train: 63 [1250/1251 (100%)]  Loss: 3.939 (3.91)  Time: 0.283s, 3619.61/s  (0.311s, 3288.30/s)  LR: 8.924e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.032 (2.032)  Loss:  0.7944 (0.7944)  Acc@1: 83.6914 (83.6914)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.065 (0.238)  Loss:  0.8623 (1.4296)  Acc@1: 81.4859 (68.4060)  Acc@5: 94.3396 (89.0020)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-63.pth.tar', 68.40600010253907)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-60.pth.tar', 68.35999996582031)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-62.pth.tar', 68.137999921875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-61.pth.tar', 67.96000002197266)

Train: 64 [   0/1251 (  0%)]  Loss: 3.758 (3.76)  Time: 2.348s,  436.17/s  (2.348s,  436.17/s)  LR: 8.924e-04  Data: 2.126 (2.126)
Train: 64 [  50/1251 (  4%)]  Loss: 3.758 (3.76)  Time: 0.303s, 3375.78/s  (0.337s, 3039.44/s)  LR: 8.923e-04  Data: 0.023 (0.064)
Train: 64 [ 100/1251 (  8%)]  Loss: 3.608 (3.71)  Time: 0.312s, 3280.46/s  (0.322s, 3178.13/s)  LR: 8.921e-04  Data: 0.020 (0.044)
Train: 64 [ 150/1251 ( 12%)]  Loss: 3.796 (3.73)  Time: 0.313s, 3270.06/s  (0.318s, 3218.96/s)  LR: 8.920e-04  Data: 0.021 (0.037)
Train: 64 [ 200/1251 ( 16%)]  Loss: 4.101 (3.80)  Time: 0.313s, 3266.62/s  (0.316s, 3238.28/s)  LR: 8.919e-04  Data: 0.023 (0.033)
Train: 64 [ 250/1251 ( 20%)]  Loss: 3.528 (3.76)  Time: 0.308s, 3324.25/s  (0.315s, 3249.48/s)  LR: 8.917e-04  Data: 0.023 (0.031)
Train: 64 [ 300/1251 ( 24%)]  Loss: 3.762 (3.76)  Time: 0.314s, 3259.58/s  (0.315s, 3255.83/s)  LR: 8.916e-04  Data: 0.020 (0.030)
Train: 64 [ 350/1251 ( 28%)]  Loss: 3.820 (3.77)  Time: 0.310s, 3306.71/s  (0.314s, 3260.23/s)  LR: 8.915e-04  Data: 0.023 (0.029)
Train: 64 [ 400/1251 ( 32%)]  Loss: 3.742 (3.76)  Time: 0.308s, 3328.93/s  (0.314s, 3264.07/s)  LR: 8.913e-04  Data: 0.022 (0.028)
Train: 64 [ 450/1251 ( 36%)]  Loss: 3.575 (3.74)  Time: 0.316s, 3241.69/s  (0.314s, 3265.92/s)  LR: 8.912e-04  Data: 0.023 (0.027)
Train: 64 [ 500/1251 ( 40%)]  Loss: 4.167 (3.78)  Time: 0.319s, 3205.44/s  (0.313s, 3267.28/s)  LR: 8.911e-04  Data: 0.023 (0.027)
Train: 64 [ 550/1251 ( 44%)]  Loss: 3.526 (3.76)  Time: 0.322s, 3184.65/s  (0.313s, 3267.50/s)  LR: 8.910e-04  Data: 0.025 (0.026)
Train: 64 [ 600/1251 ( 48%)]  Loss: 4.227 (3.80)  Time: 0.318s, 3216.26/s  (0.313s, 3267.08/s)  LR: 8.908e-04  Data: 0.021 (0.026)
Train: 64 [ 650/1251 ( 52%)]  Loss: 4.116 (3.82)  Time: 0.312s, 3282.34/s  (0.313s, 3267.59/s)  LR: 8.907e-04  Data: 0.019 (0.026)
Train: 64 [ 700/1251 ( 56%)]  Loss: 4.027 (3.83)  Time: 0.316s, 3238.01/s  (0.313s, 3268.31/s)  LR: 8.906e-04  Data: 0.025 (0.026)
Train: 64 [ 750/1251 ( 60%)]  Loss: 3.909 (3.84)  Time: 0.308s, 3321.27/s  (0.313s, 3268.51/s)  LR: 8.904e-04  Data: 0.024 (0.025)
Train: 64 [ 800/1251 ( 64%)]  Loss: 3.958 (3.85)  Time: 0.307s, 3335.93/s  (0.313s, 3269.16/s)  LR: 8.903e-04  Data: 0.023 (0.025)
Train: 64 [ 850/1251 ( 68%)]  Loss: 4.214 (3.87)  Time: 0.313s, 3273.62/s  (0.313s, 3269.34/s)  LR: 8.902e-04  Data: 0.020 (0.025)
Train: 64 [ 900/1251 ( 72%)]  Loss: 3.929 (3.87)  Time: 0.321s, 3188.51/s  (0.313s, 3269.95/s)  LR: 8.900e-04  Data: 0.024 (0.025)
Train: 64 [ 950/1251 ( 76%)]  Loss: 3.800 (3.87)  Time: 0.308s, 3320.42/s  (0.313s, 3269.98/s)  LR: 8.899e-04  Data: 0.023 (0.025)
Train: 64 [1000/1251 ( 80%)]  Loss: 3.957 (3.87)  Time: 0.308s, 3324.48/s  (0.313s, 3270.85/s)  LR: 8.898e-04  Data: 0.022 (0.025)
Train: 64 [1050/1251 ( 84%)]  Loss: 3.886 (3.87)  Time: 0.311s, 3296.06/s  (0.313s, 3272.01/s)  LR: 8.897e-04  Data: 0.024 (0.025)
Train: 64 [1100/1251 ( 88%)]  Loss: 3.960 (3.87)  Time: 0.312s, 3287.04/s  (0.313s, 3272.98/s)  LR: 8.895e-04  Data: 0.023 (0.025)
Train: 64 [1150/1251 ( 92%)]  Loss: 3.624 (3.86)  Time: 0.320s, 3195.62/s  (0.313s, 3273.49/s)  LR: 8.894e-04  Data: 0.022 (0.024)
Train: 64 [1200/1251 ( 96%)]  Loss: 3.675 (3.86)  Time: 0.314s, 3260.32/s  (0.313s, 3274.14/s)  LR: 8.893e-04  Data: 0.024 (0.024)
Train: 64 [1250/1251 (100%)]  Loss: 3.794 (3.85)  Time: 0.281s, 3642.32/s  (0.313s, 3276.58/s)  LR: 8.891e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.963 (1.963)  Loss:  0.7686 (0.7686)  Acc@1: 82.8125 (82.8125)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.043 (0.243)  Loss:  0.8506 (1.4095)  Acc@1: 82.3113 (68.4380)  Acc@5: 93.6321 (88.7320)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-64.pth.tar', 68.4380000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-63.pth.tar', 68.40600010253907)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-60.pth.tar', 68.35999996582031)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-62.pth.tar', 68.137999921875)

Train: 65 [   0/1251 (  0%)]  Loss: 3.475 (3.48)  Time: 2.186s,  468.33/s  (2.186s,  468.33/s)  LR: 8.891e-04  Data: 1.961 (1.961)
Train: 65 [  50/1251 (  4%)]  Loss: 4.111 (3.79)  Time: 0.301s, 3397.88/s  (0.333s, 3070.90/s)  LR: 8.890e-04  Data: 0.019 (0.061)
Train: 65 [ 100/1251 (  8%)]  Loss: 3.949 (3.85)  Time: 0.308s, 3319.55/s  (0.320s, 3202.87/s)  LR: 8.889e-04  Data: 0.024 (0.042)
Train: 65 [ 150/1251 ( 12%)]  Loss: 3.829 (3.84)  Time: 0.312s, 3277.97/s  (0.316s, 3240.59/s)  LR: 8.887e-04  Data: 0.025 (0.035)
Train: 65 [ 200/1251 ( 16%)]  Loss: 3.883 (3.85)  Time: 0.305s, 3362.81/s  (0.315s, 3254.06/s)  LR: 8.886e-04  Data: 0.023 (0.032)
Train: 65 [ 250/1251 ( 20%)]  Loss: 3.438 (3.78)  Time: 0.309s, 3314.29/s  (0.314s, 3261.38/s)  LR: 8.885e-04  Data: 0.023 (0.030)
Train: 65 [ 300/1251 ( 24%)]  Loss: 3.684 (3.77)  Time: 0.309s, 3311.08/s  (0.313s, 3266.49/s)  LR: 8.883e-04  Data: 0.023 (0.029)
Train: 65 [ 350/1251 ( 28%)]  Loss: 3.762 (3.77)  Time: 0.320s, 3204.67/s  (0.313s, 3270.43/s)  LR: 8.882e-04  Data: 0.026 (0.028)
Train: 65 [ 400/1251 ( 32%)]  Loss: 4.133 (3.81)  Time: 0.316s, 3243.28/s  (0.313s, 3270.68/s)  LR: 8.881e-04  Data: 0.026 (0.027)
Train: 65 [ 450/1251 ( 36%)]  Loss: 3.913 (3.82)  Time: 0.310s, 3299.77/s  (0.313s, 3272.00/s)  LR: 8.879e-04  Data: 0.024 (0.027)
Train: 65 [ 500/1251 ( 40%)]  Loss: 3.419 (3.78)  Time: 0.313s, 3271.19/s  (0.313s, 3272.59/s)  LR: 8.878e-04  Data: 0.022 (0.026)
Train: 65 [ 550/1251 ( 44%)]  Loss: 3.943 (3.79)  Time: 0.317s, 3225.27/s  (0.313s, 3273.57/s)  LR: 8.877e-04  Data: 0.025 (0.026)
Train: 65 [ 600/1251 ( 48%)]  Loss: 3.704 (3.79)  Time: 0.315s, 3245.94/s  (0.313s, 3273.96/s)  LR: 8.876e-04  Data: 0.023 (0.026)
Train: 65 [ 650/1251 ( 52%)]  Loss: 3.925 (3.80)  Time: 0.315s, 3253.74/s  (0.313s, 3274.57/s)  LR: 8.874e-04  Data: 0.027 (0.026)
Train: 65 [ 700/1251 ( 56%)]  Loss: 4.050 (3.81)  Time: 0.308s, 3324.49/s  (0.313s, 3274.91/s)  LR: 8.873e-04  Data: 0.020 (0.025)
Train: 65 [ 750/1251 ( 60%)]  Loss: 3.779 (3.81)  Time: 0.315s, 3251.81/s  (0.313s, 3274.49/s)  LR: 8.872e-04  Data: 0.024 (0.025)
Train: 65 [ 800/1251 ( 64%)]  Loss: 3.656 (3.80)  Time: 0.312s, 3278.00/s  (0.313s, 3274.29/s)  LR: 8.870e-04  Data: 0.021 (0.025)
Train: 65 [ 850/1251 ( 68%)]  Loss: 3.781 (3.80)  Time: 0.310s, 3304.72/s  (0.313s, 3274.35/s)  LR: 8.869e-04  Data: 0.021 (0.025)
Train: 65 [ 900/1251 ( 72%)]  Loss: 3.717 (3.80)  Time: 0.313s, 3266.77/s  (0.313s, 3274.07/s)  LR: 8.868e-04  Data: 0.023 (0.025)
Train: 65 [ 950/1251 ( 76%)]  Loss: 3.957 (3.81)  Time: 0.314s, 3263.48/s  (0.313s, 3273.69/s)  LR: 8.866e-04  Data: 0.027 (0.025)
Train: 65 [1000/1251 ( 80%)]  Loss: 3.732 (3.80)  Time: 0.321s, 3185.42/s  (0.313s, 3273.81/s)  LR: 8.865e-04  Data: 0.021 (0.025)
Train: 65 [1050/1251 ( 84%)]  Loss: 3.733 (3.80)  Time: 0.312s, 3279.54/s  (0.313s, 3274.03/s)  LR: 8.864e-04  Data: 0.023 (0.024)
Train: 65 [1100/1251 ( 88%)]  Loss: 4.006 (3.81)  Time: 0.311s, 3291.48/s  (0.313s, 3273.98/s)  LR: 8.862e-04  Data: 0.022 (0.024)
Train: 65 [1150/1251 ( 92%)]  Loss: 4.024 (3.82)  Time: 0.316s, 3240.20/s  (0.313s, 3274.02/s)  LR: 8.861e-04  Data: 0.017 (0.024)
Train: 65 [1200/1251 ( 96%)]  Loss: 3.695 (3.81)  Time: 0.311s, 3291.06/s  (0.313s, 3274.11/s)  LR: 8.860e-04  Data: 0.024 (0.024)
Train: 65 [1250/1251 (100%)]  Loss: 3.810 (3.81)  Time: 0.287s, 3566.73/s  (0.313s, 3275.90/s)  LR: 8.858e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.072 (2.072)  Loss:  0.7368 (0.7368)  Acc@1: 85.7422 (85.7422)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.059 (0.235)  Loss:  0.7593 (1.3759)  Acc@1: 82.5472 (68.6820)  Acc@5: 95.1651 (88.8580)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-65.pth.tar', 68.68200004638672)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-64.pth.tar', 68.4380000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-63.pth.tar', 68.40600010253907)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-60.pth.tar', 68.35999996582031)

Train: 66 [   0/1251 (  0%)]  Loss: 4.097 (4.10)  Time: 2.405s,  425.81/s  (2.405s,  425.81/s)  LR: 8.858e-04  Data: 2.189 (2.189)
Train: 66 [  50/1251 (  4%)]  Loss: 3.814 (3.96)  Time: 0.312s, 3286.63/s  (0.340s, 3016.16/s)  LR: 8.857e-04  Data: 0.025 (0.065)
Train: 66 [ 100/1251 (  8%)]  Loss: 3.634 (3.85)  Time: 0.310s, 3300.24/s  (0.324s, 3164.12/s)  LR: 8.856e-04  Data: 0.019 (0.044)
Train: 66 [ 150/1251 ( 12%)]  Loss: 3.904 (3.86)  Time: 0.310s, 3304.53/s  (0.319s, 3208.68/s)  LR: 8.854e-04  Data: 0.020 (0.037)
Train: 66 [ 200/1251 ( 16%)]  Loss: 3.947 (3.88)  Time: 0.314s, 3257.33/s  (0.317s, 3230.60/s)  LR: 8.853e-04  Data: 0.021 (0.033)
Train: 66 [ 250/1251 ( 20%)]  Loss: 3.813 (3.87)  Time: 0.309s, 3309.31/s  (0.316s, 3243.17/s)  LR: 8.852e-04  Data: 0.021 (0.031)
Train: 66 [ 300/1251 ( 24%)]  Loss: 3.651 (3.84)  Time: 0.311s, 3288.26/s  (0.315s, 3250.32/s)  LR: 8.850e-04  Data: 0.024 (0.030)
Train: 66 [ 350/1251 ( 28%)]  Loss: 4.149 (3.88)  Time: 0.312s, 3287.26/s  (0.315s, 3254.66/s)  LR: 8.849e-04  Data: 0.021 (0.029)
Train: 66 [ 400/1251 ( 32%)]  Loss: 3.936 (3.88)  Time: 0.315s, 3249.15/s  (0.314s, 3258.24/s)  LR: 8.848e-04  Data: 0.023 (0.028)
Train: 66 [ 450/1251 ( 36%)]  Loss: 4.043 (3.90)  Time: 0.311s, 3293.50/s  (0.314s, 3261.02/s)  LR: 8.846e-04  Data: 0.022 (0.027)
Train: 66 [ 500/1251 ( 40%)]  Loss: 3.722 (3.88)  Time: 0.313s, 3272.97/s  (0.314s, 3264.08/s)  LR: 8.845e-04  Data: 0.022 (0.027)
Train: 66 [ 550/1251 ( 44%)]  Loss: 3.967 (3.89)  Time: 0.314s, 3260.64/s  (0.314s, 3265.89/s)  LR: 8.844e-04  Data: 0.022 (0.027)
Train: 66 [ 600/1251 ( 48%)]  Loss: 4.171 (3.91)  Time: 0.312s, 3277.88/s  (0.313s, 3267.52/s)  LR: 8.842e-04  Data: 0.023 (0.026)
Train: 66 [ 650/1251 ( 52%)]  Loss: 3.760 (3.90)  Time: 0.315s, 3253.47/s  (0.313s, 3268.31/s)  LR: 8.841e-04  Data: 0.023 (0.026)
Train: 66 [ 700/1251 ( 56%)]  Loss: 4.119 (3.92)  Time: 0.310s, 3308.15/s  (0.313s, 3269.27/s)  LR: 8.840e-04  Data: 0.024 (0.026)
Train: 66 [ 750/1251 ( 60%)]  Loss: 4.017 (3.92)  Time: 0.311s, 3297.40/s  (0.313s, 3270.17/s)  LR: 8.838e-04  Data: 0.024 (0.026)
Train: 66 [ 800/1251 ( 64%)]  Loss: 3.783 (3.91)  Time: 0.309s, 3312.92/s  (0.313s, 3270.42/s)  LR: 8.837e-04  Data: 0.025 (0.025)
Train: 66 [ 850/1251 ( 68%)]  Loss: 4.258 (3.93)  Time: 0.310s, 3298.98/s  (0.313s, 3270.94/s)  LR: 8.836e-04  Data: 0.014 (0.025)
Train: 66 [ 900/1251 ( 72%)]  Loss: 3.719 (3.92)  Time: 0.312s, 3286.30/s  (0.313s, 3271.43/s)  LR: 8.834e-04  Data: 0.023 (0.025)
Train: 66 [ 950/1251 ( 76%)]  Loss: 3.634 (3.91)  Time: 0.316s, 3238.38/s  (0.313s, 3272.34/s)  LR: 8.833e-04  Data: 0.023 (0.025)
Train: 66 [1000/1251 ( 80%)]  Loss: 3.897 (3.91)  Time: 0.310s, 3306.42/s  (0.313s, 3272.55/s)  LR: 8.832e-04  Data: 0.021 (0.025)
Train: 66 [1050/1251 ( 84%)]  Loss: 3.901 (3.91)  Time: 0.318s, 3224.44/s  (0.313s, 3272.80/s)  LR: 8.830e-04  Data: 0.023 (0.025)
Train: 66 [1100/1251 ( 88%)]  Loss: 4.271 (3.92)  Time: 0.315s, 3252.06/s  (0.313s, 3273.23/s)  LR: 8.829e-04  Data: 0.022 (0.025)
Train: 66 [1150/1251 ( 92%)]  Loss: 3.819 (3.92)  Time: 0.310s, 3298.99/s  (0.313s, 3273.63/s)  LR: 8.828e-04  Data: 0.024 (0.025)
Train: 66 [1200/1251 ( 96%)]  Loss: 3.794 (3.91)  Time: 0.305s, 3355.41/s  (0.313s, 3273.83/s)  LR: 8.826e-04  Data: 0.023 (0.024)
Train: 66 [1250/1251 (100%)]  Loss: 3.820 (3.91)  Time: 0.279s, 3669.84/s  (0.313s, 3275.92/s)  LR: 8.825e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.351 (2.351)  Loss:  0.7046 (0.7046)  Acc@1: 84.5703 (84.5703)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.070 (0.236)  Loss:  0.7915 (1.3952)  Acc@1: 82.4293 (68.7920)  Acc@5: 95.2830 (88.8440)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-66.pth.tar', 68.79200012451172)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-65.pth.tar', 68.68200004638672)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-64.pth.tar', 68.4380000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-63.pth.tar', 68.40600010253907)

Train: 67 [   0/1251 (  0%)]  Loss: 3.631 (3.63)  Time: 2.191s,  467.37/s  (2.191s,  467.37/s)  LR: 8.825e-04  Data: 1.970 (1.970)
Train: 67 [  50/1251 (  4%)]  Loss: 3.870 (3.75)  Time: 0.304s, 3369.75/s  (0.332s, 3087.97/s)  LR: 8.824e-04  Data: 0.024 (0.062)
Train: 67 [ 100/1251 (  8%)]  Loss: 3.837 (3.78)  Time: 0.305s, 3353.21/s  (0.319s, 3212.03/s)  LR: 8.822e-04  Data: 0.025 (0.043)
Train: 67 [ 150/1251 ( 12%)]  Loss: 3.945 (3.82)  Time: 0.315s, 3252.08/s  (0.315s, 3249.50/s)  LR: 8.821e-04  Data: 0.025 (0.036)
Train: 67 [ 200/1251 ( 16%)]  Loss: 4.046 (3.87)  Time: 0.310s, 3307.99/s  (0.314s, 3264.54/s)  LR: 8.819e-04  Data: 0.024 (0.033)
Train: 67 [ 250/1251 ( 20%)]  Loss: 3.902 (3.87)  Time: 0.313s, 3269.65/s  (0.313s, 3272.24/s)  LR: 8.818e-04  Data: 0.013 (0.031)
Train: 67 [ 300/1251 ( 24%)]  Loss: 4.077 (3.90)  Time: 0.313s, 3270.70/s  (0.313s, 3275.91/s)  LR: 8.817e-04  Data: 0.020 (0.029)
Train: 67 [ 350/1251 ( 28%)]  Loss: 3.974 (3.91)  Time: 0.309s, 3310.21/s  (0.312s, 3278.98/s)  LR: 8.815e-04  Data: 0.021 (0.028)
Train: 67 [ 400/1251 ( 32%)]  Loss: 4.087 (3.93)  Time: 0.310s, 3302.20/s  (0.312s, 3281.84/s)  LR: 8.814e-04  Data: 0.017 (0.028)
Train: 67 [ 450/1251 ( 36%)]  Loss: 3.831 (3.92)  Time: 0.306s, 3346.91/s  (0.312s, 3283.74/s)  LR: 8.813e-04  Data: 0.023 (0.027)
Train: 67 [ 500/1251 ( 40%)]  Loss: 3.442 (3.88)  Time: 0.307s, 3337.48/s  (0.312s, 3286.58/s)  LR: 8.811e-04  Data: 0.023 (0.027)
Train: 67 [ 550/1251 ( 44%)]  Loss: 3.888 (3.88)  Time: 0.308s, 3323.36/s  (0.311s, 3288.92/s)  LR: 8.810e-04  Data: 0.022 (0.026)
Train: 67 [ 600/1251 ( 48%)]  Loss: 3.777 (3.87)  Time: 0.304s, 3364.30/s  (0.311s, 3290.80/s)  LR: 8.809e-04  Data: 0.023 (0.026)
Train: 67 [ 650/1251 ( 52%)]  Loss: 4.126 (3.89)  Time: 0.310s, 3298.89/s  (0.311s, 3292.27/s)  LR: 8.807e-04  Data: 0.022 (0.026)
Train: 67 [ 700/1251 ( 56%)]  Loss: 3.939 (3.89)  Time: 0.308s, 3319.80/s  (0.311s, 3292.97/s)  LR: 8.806e-04  Data: 0.026 (0.025)
Train: 67 [ 750/1251 ( 60%)]  Loss: 3.322 (3.86)  Time: 0.312s, 3279.48/s  (0.311s, 3293.64/s)  LR: 8.805e-04  Data: 0.023 (0.025)
Train: 67 [ 800/1251 ( 64%)]  Loss: 3.925 (3.86)  Time: 0.311s, 3296.15/s  (0.311s, 3294.17/s)  LR: 8.803e-04  Data: 0.026 (0.025)
Train: 67 [ 850/1251 ( 68%)]  Loss: 4.038 (3.87)  Time: 0.309s, 3314.19/s  (0.311s, 3294.53/s)  LR: 8.802e-04  Data: 0.023 (0.025)
Train: 67 [ 900/1251 ( 72%)]  Loss: 3.684 (3.86)  Time: 0.308s, 3322.02/s  (0.311s, 3294.61/s)  LR: 8.801e-04  Data: 0.025 (0.025)
Train: 67 [ 950/1251 ( 76%)]  Loss: 4.049 (3.87)  Time: 0.307s, 3332.17/s  (0.311s, 3295.24/s)  LR: 8.799e-04  Data: 0.019 (0.025)
Train: 67 [1000/1251 ( 80%)]  Loss: 3.827 (3.87)  Time: 0.312s, 3280.72/s  (0.311s, 3295.45/s)  LR: 8.798e-04  Data: 0.026 (0.024)
Train: 67 [1050/1251 ( 84%)]  Loss: 4.212 (3.88)  Time: 0.313s, 3276.72/s  (0.311s, 3296.31/s)  LR: 8.796e-04  Data: 0.023 (0.024)
Train: 67 [1100/1251 ( 88%)]  Loss: 4.090 (3.89)  Time: 0.309s, 3316.56/s  (0.311s, 3296.47/s)  LR: 8.795e-04  Data: 0.020 (0.024)
Train: 67 [1150/1251 ( 92%)]  Loss: 3.843 (3.89)  Time: 0.312s, 3278.55/s  (0.311s, 3296.97/s)  LR: 8.794e-04  Data: 0.024 (0.024)
Train: 67 [1200/1251 ( 96%)]  Loss: 3.679 (3.88)  Time: 0.307s, 3333.81/s  (0.311s, 3297.05/s)  LR: 8.792e-04  Data: 0.024 (0.024)
Train: 67 [1250/1251 (100%)]  Loss: 3.861 (3.88)  Time: 0.276s, 3704.62/s  (0.310s, 3299.86/s)  LR: 8.791e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.019 (2.019)  Loss:  0.6748 (0.6748)  Acc@1: 86.2305 (86.2305)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.046 (0.241)  Loss:  0.7969 (1.3715)  Acc@1: 82.6651 (68.9160)  Acc@5: 94.4576 (89.1720)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-67.pth.tar', 68.91600009765625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-66.pth.tar', 68.79200012451172)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-65.pth.tar', 68.68200004638672)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-64.pth.tar', 68.4380000732422)

Train: 68 [   0/1251 (  0%)]  Loss: 3.899 (3.90)  Time: 2.408s,  425.33/s  (2.408s,  425.33/s)  LR: 8.791e-04  Data: 2.177 (2.177)
Train: 68 [  50/1251 (  4%)]  Loss: 3.272 (3.59)  Time: 0.307s, 3340.42/s  (0.334s, 3069.31/s)  LR: 8.790e-04  Data: 0.021 (0.066)
Train: 68 [ 100/1251 (  8%)]  Loss: 3.408 (3.53)  Time: 0.304s, 3370.00/s  (0.318s, 3215.12/s)  LR: 8.788e-04  Data: 0.022 (0.045)
Train: 68 [ 150/1251 ( 12%)]  Loss: 3.921 (3.63)  Time: 0.304s, 3370.60/s  (0.314s, 3261.57/s)  LR: 8.787e-04  Data: 0.023 (0.037)
Train: 68 [ 200/1251 ( 16%)]  Loss: 3.902 (3.68)  Time: 0.305s, 3354.04/s  (0.312s, 3284.48/s)  LR: 8.786e-04  Data: 0.015 (0.034)
Train: 68 [ 250/1251 ( 20%)]  Loss: 3.826 (3.70)  Time: 0.305s, 3352.45/s  (0.311s, 3295.83/s)  LR: 8.784e-04  Data: 0.020 (0.031)
Train: 68 [ 300/1251 ( 24%)]  Loss: 3.872 (3.73)  Time: 0.306s, 3351.32/s  (0.310s, 3302.11/s)  LR: 8.783e-04  Data: 0.022 (0.030)
Train: 68 [ 350/1251 ( 28%)]  Loss: 4.169 (3.78)  Time: 0.307s, 3338.07/s  (0.310s, 3304.66/s)  LR: 8.781e-04  Data: 0.023 (0.029)
Train: 68 [ 400/1251 ( 32%)]  Loss: 3.645 (3.77)  Time: 0.302s, 3385.40/s  (0.310s, 3307.31/s)  LR: 8.780e-04  Data: 0.024 (0.028)
Train: 68 [ 450/1251 ( 36%)]  Loss: 4.023 (3.79)  Time: 0.305s, 3358.81/s  (0.309s, 3309.17/s)  LR: 8.779e-04  Data: 0.021 (0.027)
Train: 68 [ 500/1251 ( 40%)]  Loss: 3.380 (3.76)  Time: 0.310s, 3298.07/s  (0.309s, 3310.44/s)  LR: 8.777e-04  Data: 0.023 (0.027)
Train: 68 [ 550/1251 ( 44%)]  Loss: 3.976 (3.77)  Time: 0.305s, 3354.36/s  (0.309s, 3311.12/s)  LR: 8.776e-04  Data: 0.024 (0.027)
Train: 68 [ 600/1251 ( 48%)]  Loss: 3.916 (3.79)  Time: 0.309s, 3314.82/s  (0.309s, 3311.93/s)  LR: 8.775e-04  Data: 0.019 (0.026)
Train: 68 [ 650/1251 ( 52%)]  Loss: 4.049 (3.80)  Time: 0.307s, 3335.78/s  (0.309s, 3312.87/s)  LR: 8.773e-04  Data: 0.023 (0.026)
Train: 68 [ 700/1251 ( 56%)]  Loss: 3.912 (3.81)  Time: 0.316s, 3237.03/s  (0.309s, 3314.20/s)  LR: 8.772e-04  Data: 0.030 (0.026)
Train: 68 [ 750/1251 ( 60%)]  Loss: 3.766 (3.81)  Time: 0.305s, 3359.20/s  (0.309s, 3315.54/s)  LR: 8.771e-04  Data: 0.021 (0.025)
Train: 68 [ 800/1251 ( 64%)]  Loss: 3.760 (3.81)  Time: 0.305s, 3356.03/s  (0.309s, 3316.74/s)  LR: 8.769e-04  Data: 0.019 (0.025)
Train: 68 [ 850/1251 ( 68%)]  Loss: 3.769 (3.80)  Time: 0.303s, 3380.50/s  (0.309s, 3317.83/s)  LR: 8.768e-04  Data: 0.025 (0.025)
Train: 68 [ 900/1251 ( 72%)]  Loss: 4.038 (3.82)  Time: 0.305s, 3354.04/s  (0.309s, 3318.67/s)  LR: 8.766e-04  Data: 0.024 (0.025)
Train: 68 [ 950/1251 ( 76%)]  Loss: 4.170 (3.83)  Time: 0.310s, 3303.00/s  (0.308s, 3319.42/s)  LR: 8.765e-04  Data: 0.022 (0.025)
Train: 68 [1000/1251 ( 80%)]  Loss: 3.324 (3.81)  Time: 0.308s, 3319.88/s  (0.308s, 3320.14/s)  LR: 8.764e-04  Data: 0.026 (0.025)
Train: 68 [1050/1251 ( 84%)]  Loss: 3.907 (3.81)  Time: 0.309s, 3313.87/s  (0.308s, 3320.83/s)  LR: 8.762e-04  Data: 0.024 (0.025)
Train: 68 [1100/1251 ( 88%)]  Loss: 3.832 (3.81)  Time: 0.308s, 3324.39/s  (0.308s, 3321.88/s)  LR: 8.761e-04  Data: 0.022 (0.025)
Train: 68 [1150/1251 ( 92%)]  Loss: 3.576 (3.80)  Time: 0.311s, 3296.72/s  (0.308s, 3322.67/s)  LR: 8.760e-04  Data: 0.024 (0.025)
Train: 68 [1200/1251 ( 96%)]  Loss: 3.678 (3.80)  Time: 0.305s, 3357.95/s  (0.308s, 3323.54/s)  LR: 8.758e-04  Data: 0.026 (0.024)
Train: 68 [1250/1251 (100%)]  Loss: 4.005 (3.81)  Time: 0.277s, 3700.02/s  (0.308s, 3326.04/s)  LR: 8.757e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.096 (2.096)  Loss:  0.7168 (0.7168)  Acc@1: 86.2305 (86.2305)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.044 (0.234)  Loss:  0.8062 (1.4060)  Acc@1: 83.4906 (68.7960)  Acc@5: 95.1651 (88.9740)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-67.pth.tar', 68.91600009765625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-68.pth.tar', 68.79600006835938)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-66.pth.tar', 68.79200012451172)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-65.pth.tar', 68.68200004638672)

Train: 69 [   0/1251 (  0%)]  Loss: 3.585 (3.59)  Time: 2.062s,  496.72/s  (2.062s,  496.72/s)  LR: 8.757e-04  Data: 1.819 (1.819)
Train: 69 [  50/1251 (  4%)]  Loss: 4.181 (3.88)  Time: 0.300s, 3416.50/s  (0.329s, 3116.07/s)  LR: 8.755e-04  Data: 0.020 (0.061)
Train: 69 [ 100/1251 (  8%)]  Loss: 3.890 (3.89)  Time: 0.298s, 3436.07/s  (0.315s, 3252.60/s)  LR: 8.754e-04  Data: 0.022 (0.042)
Train: 69 [ 150/1251 ( 12%)]  Loss: 3.978 (3.91)  Time: 0.303s, 3380.08/s  (0.311s, 3294.66/s)  LR: 8.753e-04  Data: 0.021 (0.036)
Train: 69 [ 200/1251 ( 16%)]  Loss: 3.894 (3.91)  Time: 0.302s, 3385.53/s  (0.309s, 3313.82/s)  LR: 8.751e-04  Data: 0.021 (0.032)
Train: 69 [ 250/1251 ( 20%)]  Loss: 4.162 (3.95)  Time: 0.308s, 3319.80/s  (0.308s, 3324.54/s)  LR: 8.750e-04  Data: 0.023 (0.031)
Train: 69 [ 300/1251 ( 24%)]  Loss: 3.992 (3.95)  Time: 0.307s, 3338.14/s  (0.307s, 3330.15/s)  LR: 8.749e-04  Data: 0.023 (0.029)
Train: 69 [ 350/1251 ( 28%)]  Loss: 4.012 (3.96)  Time: 0.302s, 3386.83/s  (0.307s, 3334.45/s)  LR: 8.747e-04  Data: 0.021 (0.028)
Train: 69 [ 400/1251 ( 32%)]  Loss: 3.743 (3.94)  Time: 0.306s, 3348.17/s  (0.307s, 3336.70/s)  LR: 8.746e-04  Data: 0.024 (0.028)
Train: 69 [ 450/1251 ( 36%)]  Loss: 3.526 (3.90)  Time: 0.303s, 3384.77/s  (0.307s, 3338.84/s)  LR: 8.744e-04  Data: 0.022 (0.027)
Train: 69 [ 500/1251 ( 40%)]  Loss: 3.927 (3.90)  Time: 0.305s, 3355.31/s  (0.307s, 3339.88/s)  LR: 8.743e-04  Data: 0.026 (0.027)
Train: 69 [ 550/1251 ( 44%)]  Loss: 3.656 (3.88)  Time: 0.300s, 3412.16/s  (0.306s, 3342.12/s)  LR: 8.742e-04  Data: 0.022 (0.026)
Train: 69 [ 600/1251 ( 48%)]  Loss: 3.773 (3.87)  Time: 0.306s, 3343.82/s  (0.306s, 3343.41/s)  LR: 8.740e-04  Data: 0.024 (0.026)
Train: 69 [ 650/1251 ( 52%)]  Loss: 3.786 (3.86)  Time: 0.305s, 3362.67/s  (0.306s, 3344.75/s)  LR: 8.739e-04  Data: 0.024 (0.026)
Train: 69 [ 700/1251 ( 56%)]  Loss: 3.678 (3.85)  Time: 0.307s, 3338.34/s  (0.306s, 3345.54/s)  LR: 8.737e-04  Data: 0.022 (0.026)
Train: 69 [ 750/1251 ( 60%)]  Loss: 3.984 (3.86)  Time: 0.307s, 3338.78/s  (0.306s, 3346.49/s)  LR: 8.736e-04  Data: 0.025 (0.025)
Train: 69 [ 800/1251 ( 64%)]  Loss: 3.761 (3.85)  Time: 0.308s, 3320.27/s  (0.306s, 3347.41/s)  LR: 8.735e-04  Data: 0.022 (0.025)
Train: 69 [ 850/1251 ( 68%)]  Loss: 3.750 (3.85)  Time: 0.304s, 3369.34/s  (0.306s, 3348.27/s)  LR: 8.733e-04  Data: 0.021 (0.025)
Train: 69 [ 900/1251 ( 72%)]  Loss: 3.793 (3.85)  Time: 0.302s, 3395.60/s  (0.306s, 3349.08/s)  LR: 8.732e-04  Data: 0.021 (0.025)
Train: 69 [ 950/1251 ( 76%)]  Loss: 3.995 (3.85)  Time: 0.313s, 3267.94/s  (0.306s, 3349.30/s)  LR: 8.730e-04  Data: 0.024 (0.025)
Train: 69 [1000/1251 ( 80%)]  Loss: 3.986 (3.86)  Time: 0.301s, 3403.83/s  (0.306s, 3349.99/s)  LR: 8.729e-04  Data: 0.022 (0.025)
Train: 69 [1050/1251 ( 84%)]  Loss: 3.815 (3.86)  Time: 0.304s, 3367.44/s  (0.306s, 3351.08/s)  LR: 8.728e-04  Data: 0.021 (0.025)
Train: 69 [1100/1251 ( 88%)]  Loss: 3.826 (3.86)  Time: 0.307s, 3333.23/s  (0.306s, 3351.78/s)  LR: 8.726e-04  Data: 0.022 (0.024)
Train: 69 [1150/1251 ( 92%)]  Loss: 3.971 (3.86)  Time: 0.301s, 3407.56/s  (0.305s, 3352.60/s)  LR: 8.725e-04  Data: 0.022 (0.024)
Train: 69 [1200/1251 ( 96%)]  Loss: 3.844 (3.86)  Time: 0.305s, 3361.14/s  (0.305s, 3353.39/s)  LR: 8.724e-04  Data: 0.026 (0.024)
Train: 69 [1250/1251 (100%)]  Loss: 3.958 (3.86)  Time: 0.277s, 3697.90/s  (0.305s, 3356.12/s)  LR: 8.722e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.279 (2.279)  Loss:  0.7881 (0.7881)  Acc@1: 82.5195 (82.5195)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.040 (0.238)  Loss:  0.7554 (1.3832)  Acc@1: 83.9623 (68.9180)  Acc@5: 95.2830 (89.1660)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-69.pth.tar', 68.91800001464844)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-67.pth.tar', 68.91600009765625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-68.pth.tar', 68.79600006835938)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-66.pth.tar', 68.79200012451172)

Train: 70 [   0/1251 (  0%)]  Loss: 3.624 (3.62)  Time: 2.628s,  389.69/s  (2.628s,  389.69/s)  LR: 8.722e-04  Data: 2.416 (2.416)
Train: 70 [  50/1251 (  4%)]  Loss: 3.868 (3.75)  Time: 0.289s, 3539.21/s  (0.327s, 3130.44/s)  LR: 8.721e-04  Data: 0.021 (0.070)
Train: 70 [ 100/1251 (  8%)]  Loss: 4.005 (3.83)  Time: 0.297s, 3445.30/s  (0.312s, 3281.23/s)  LR: 8.719e-04  Data: 0.021 (0.046)
Train: 70 [ 150/1251 ( 12%)]  Loss: 3.946 (3.86)  Time: 0.302s, 3391.84/s  (0.308s, 3322.76/s)  LR: 8.718e-04  Data: 0.016 (0.038)
Train: 70 [ 200/1251 ( 16%)]  Loss: 3.768 (3.84)  Time: 0.304s, 3370.58/s  (0.307s, 3340.53/s)  LR: 8.717e-04  Data: 0.025 (0.034)
Train: 70 [ 250/1251 ( 20%)]  Loss: 3.972 (3.86)  Time: 0.307s, 3338.75/s  (0.305s, 3352.46/s)  LR: 8.715e-04  Data: 0.019 (0.032)
Train: 70 [ 300/1251 ( 24%)]  Loss: 4.016 (3.89)  Time: 0.302s, 3392.56/s  (0.305s, 3358.79/s)  LR: 8.714e-04  Data: 0.021 (0.031)
Train: 70 [ 350/1251 ( 28%)]  Loss: 3.391 (3.82)  Time: 0.300s, 3417.14/s  (0.304s, 3364.64/s)  LR: 8.712e-04  Data: 0.018 (0.029)
Train: 70 [ 400/1251 ( 32%)]  Loss: 3.754 (3.82)  Time: 0.300s, 3414.53/s  (0.304s, 3368.40/s)  LR: 8.711e-04  Data: 0.024 (0.029)
Train: 70 [ 450/1251 ( 36%)]  Loss: 3.718 (3.81)  Time: 0.300s, 3409.05/s  (0.304s, 3369.97/s)  LR: 8.710e-04  Data: 0.021 (0.028)
Train: 70 [ 500/1251 ( 40%)]  Loss: 3.815 (3.81)  Time: 0.303s, 3384.10/s  (0.304s, 3371.65/s)  LR: 8.708e-04  Data: 0.021 (0.027)
Train: 70 [ 550/1251 ( 44%)]  Loss: 4.270 (3.85)  Time: 0.303s, 3382.04/s  (0.304s, 3373.37/s)  LR: 8.707e-04  Data: 0.024 (0.027)
Train: 70 [ 600/1251 ( 48%)]  Loss: 3.774 (3.84)  Time: 0.301s, 3404.66/s  (0.304s, 3373.12/s)  LR: 8.705e-04  Data: 0.024 (0.027)
Train: 70 [ 650/1251 ( 52%)]  Loss: 4.002 (3.85)  Time: 0.305s, 3360.97/s  (0.304s, 3373.57/s)  LR: 8.704e-04  Data: 0.024 (0.026)
Train: 70 [ 700/1251 ( 56%)]  Loss: 4.010 (3.86)  Time: 0.305s, 3356.56/s  (0.304s, 3373.58/s)  LR: 8.703e-04  Data: 0.014 (0.026)
Train: 70 [ 750/1251 ( 60%)]  Loss: 3.912 (3.87)  Time: 0.305s, 3352.58/s  (0.303s, 3374.14/s)  LR: 8.701e-04  Data: 0.026 (0.026)
Train: 70 [ 800/1251 ( 64%)]  Loss: 4.190 (3.88)  Time: 0.297s, 3451.63/s  (0.303s, 3374.63/s)  LR: 8.700e-04  Data: 0.022 (0.026)
Train: 70 [ 850/1251 ( 68%)]  Loss: 3.976 (3.89)  Time: 0.302s, 3387.59/s  (0.303s, 3375.23/s)  LR: 8.698e-04  Data: 0.025 (0.025)
Train: 70 [ 900/1251 ( 72%)]  Loss: 4.247 (3.91)  Time: 0.305s, 3359.80/s  (0.303s, 3375.40/s)  LR: 8.697e-04  Data: 0.021 (0.025)
Train: 70 [ 950/1251 ( 76%)]  Loss: 3.539 (3.89)  Time: 0.302s, 3394.40/s  (0.303s, 3375.96/s)  LR: 8.696e-04  Data: 0.024 (0.025)
Train: 70 [1000/1251 ( 80%)]  Loss: 3.641 (3.88)  Time: 0.305s, 3359.98/s  (0.303s, 3376.03/s)  LR: 8.694e-04  Data: 0.022 (0.025)
Train: 70 [1050/1251 ( 84%)]  Loss: 3.649 (3.87)  Time: 0.305s, 3358.84/s  (0.303s, 3376.61/s)  LR: 8.693e-04  Data: 0.021 (0.025)
Train: 70 [1100/1251 ( 88%)]  Loss: 3.738 (3.86)  Time: 0.301s, 3400.81/s  (0.303s, 3376.70/s)  LR: 8.691e-04  Data: 0.021 (0.025)
Train: 70 [1150/1251 ( 92%)]  Loss: 3.792 (3.86)  Time: 0.305s, 3354.68/s  (0.303s, 3377.30/s)  LR: 8.690e-04  Data: 0.023 (0.025)
Train: 70 [1200/1251 ( 96%)]  Loss: 3.767 (3.86)  Time: 0.306s, 3351.85/s  (0.303s, 3377.51/s)  LR: 8.688e-04  Data: 0.024 (0.025)
Train: 70 [1250/1251 (100%)]  Loss: 3.766 (3.85)  Time: 0.276s, 3709.18/s  (0.303s, 3379.82/s)  LR: 8.687e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.229 (2.229)  Loss:  0.6768 (0.6768)  Acc@1: 86.4258 (86.4258)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.041 (0.238)  Loss:  0.8506 (1.3873)  Acc@1: 82.1934 (69.3600)  Acc@5: 94.5755 (89.3120)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-70.pth.tar', 69.36000015136719)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-69.pth.tar', 68.91800001464844)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-67.pth.tar', 68.91600009765625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-68.pth.tar', 68.79600006835938)

Train: 71 [   0/1251 (  0%)]  Loss: 3.592 (3.59)  Time: 1.942s,  527.32/s  (1.942s,  527.32/s)  LR: 8.687e-04  Data: 1.696 (1.696)
Train: 71 [  50/1251 (  4%)]  Loss: 4.110 (3.85)  Time: 0.295s, 3475.62/s  (0.326s, 3143.39/s)  LR: 8.686e-04  Data: 0.021 (0.071)
Train: 71 [ 100/1251 (  8%)]  Loss: 3.997 (3.90)  Time: 0.294s, 3483.99/s  (0.310s, 3307.58/s)  LR: 8.684e-04  Data: 0.020 (0.048)
Train: 71 [ 150/1251 ( 12%)]  Loss: 3.801 (3.87)  Time: 0.299s, 3420.12/s  (0.305s, 3353.03/s)  LR: 8.683e-04  Data: 0.024 (0.039)
Train: 71 [ 200/1251 ( 16%)]  Loss: 3.750 (3.85)  Time: 0.303s, 3376.77/s  (0.304s, 3369.82/s)  LR: 8.681e-04  Data: 0.031 (0.035)
Train: 71 [ 250/1251 ( 20%)]  Loss: 3.820 (3.84)  Time: 0.302s, 3391.05/s  (0.303s, 3381.95/s)  LR: 8.680e-04  Data: 0.026 (0.033)
Train: 71 [ 300/1251 ( 24%)]  Loss: 4.139 (3.89)  Time: 0.301s, 3397.95/s  (0.302s, 3387.60/s)  LR: 8.679e-04  Data: 0.026 (0.031)
Train: 71 [ 350/1251 ( 28%)]  Loss: 4.098 (3.91)  Time: 0.304s, 3368.89/s  (0.302s, 3390.09/s)  LR: 8.677e-04  Data: 0.023 (0.030)
Train: 71 [ 400/1251 ( 32%)]  Loss: 4.248 (3.95)  Time: 0.297s, 3453.52/s  (0.302s, 3392.12/s)  LR: 8.676e-04  Data: 0.021 (0.029)
Train: 71 [ 450/1251 ( 36%)]  Loss: 4.043 (3.96)  Time: 0.305s, 3360.84/s  (0.302s, 3392.79/s)  LR: 8.674e-04  Data: 0.020 (0.028)
Train: 71 [ 500/1251 ( 40%)]  Loss: 3.813 (3.95)  Time: 0.304s, 3369.27/s  (0.302s, 3393.29/s)  LR: 8.673e-04  Data: 0.023 (0.028)
Train: 71 [ 550/1251 ( 44%)]  Loss: 3.840 (3.94)  Time: 0.299s, 3419.53/s  (0.302s, 3394.30/s)  LR: 8.672e-04  Data: 0.021 (0.027)
Train: 71 [ 600/1251 ( 48%)]  Loss: 3.734 (3.92)  Time: 0.301s, 3404.38/s  (0.302s, 3394.95/s)  LR: 8.670e-04  Data: 0.026 (0.027)
Train: 71 [ 650/1251 ( 52%)]  Loss: 3.668 (3.90)  Time: 0.296s, 3462.69/s  (0.301s, 3396.90/s)  LR: 8.669e-04  Data: 0.022 (0.027)
Train: 71 [ 700/1251 ( 56%)]  Loss: 3.483 (3.88)  Time: 0.303s, 3384.33/s  (0.301s, 3397.65/s)  LR: 8.667e-04  Data: 0.022 (0.026)
Train: 71 [ 750/1251 ( 60%)]  Loss: 3.818 (3.87)  Time: 0.300s, 3416.83/s  (0.301s, 3398.10/s)  LR: 8.666e-04  Data: 0.027 (0.026)
Train: 71 [ 800/1251 ( 64%)]  Loss: 3.958 (3.88)  Time: 0.295s, 3466.03/s  (0.301s, 3398.68/s)  LR: 8.664e-04  Data: 0.023 (0.026)
Train: 71 [ 850/1251 ( 68%)]  Loss: 3.729 (3.87)  Time: 0.298s, 3440.10/s  (0.301s, 3399.02/s)  LR: 8.663e-04  Data: 0.025 (0.026)
Train: 71 [ 900/1251 ( 72%)]  Loss: 3.776 (3.86)  Time: 0.299s, 3425.33/s  (0.301s, 3399.69/s)  LR: 8.662e-04  Data: 0.023 (0.026)
Train: 71 [ 950/1251 ( 76%)]  Loss: 3.872 (3.86)  Time: 0.299s, 3428.37/s  (0.301s, 3400.42/s)  LR: 8.660e-04  Data: 0.021 (0.026)
Train: 71 [1000/1251 ( 80%)]  Loss: 4.017 (3.87)  Time: 0.304s, 3369.41/s  (0.301s, 3400.17/s)  LR: 8.659e-04  Data: 0.019 (0.025)
Train: 71 [1050/1251 ( 84%)]  Loss: 3.379 (3.85)  Time: 0.298s, 3432.51/s  (0.301s, 3400.21/s)  LR: 8.657e-04  Data: 0.024 (0.025)
Train: 71 [1100/1251 ( 88%)]  Loss: 3.930 (3.85)  Time: 0.305s, 3358.91/s  (0.301s, 3400.54/s)  LR: 8.656e-04  Data: 0.019 (0.025)
Train: 71 [1150/1251 ( 92%)]  Loss: 4.030 (3.86)  Time: 0.305s, 3357.09/s  (0.301s, 3400.78/s)  LR: 8.654e-04  Data: 0.025 (0.025)
Train: 71 [1200/1251 ( 96%)]  Loss: 3.849 (3.86)  Time: 0.300s, 3416.84/s  (0.301s, 3400.99/s)  LR: 8.653e-04  Data: 0.023 (0.025)
Train: 71 [1250/1251 (100%)]  Loss: 3.943 (3.86)  Time: 0.278s, 3688.67/s  (0.301s, 3402.72/s)  LR: 8.652e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.056 (2.056)  Loss:  0.7212 (0.7212)  Acc@1: 85.1562 (85.1562)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.042 (0.236)  Loss:  0.8340 (1.3712)  Acc@1: 81.0142 (69.1940)  Acc@5: 94.6934 (89.2040)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-70.pth.tar', 69.36000015136719)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-71.pth.tar', 69.19400002685546)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-69.pth.tar', 68.91800001464844)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-67.pth.tar', 68.91600009765625)

Train: 72 [   0/1251 (  0%)]  Loss: 3.448 (3.45)  Time: 2.397s,  427.24/s  (2.397s,  427.24/s)  LR: 8.652e-04  Data: 2.174 (2.174)
Train: 72 [  50/1251 (  4%)]  Loss: 3.797 (3.62)  Time: 0.287s, 3571.64/s  (0.327s, 3135.52/s)  LR: 8.650e-04  Data: 0.023 (0.066)
Train: 72 [ 100/1251 (  8%)]  Loss: 3.844 (3.70)  Time: 0.296s, 3458.18/s  (0.311s, 3294.81/s)  LR: 8.649e-04  Data: 0.023 (0.045)
Train: 72 [ 150/1251 ( 12%)]  Loss: 4.022 (3.78)  Time: 0.298s, 3432.29/s  (0.306s, 3343.45/s)  LR: 8.647e-04  Data: 0.025 (0.037)
Train: 72 [ 200/1251 ( 16%)]  Loss: 3.762 (3.77)  Time: 0.298s, 3440.06/s  (0.304s, 3367.26/s)  LR: 8.646e-04  Data: 0.023 (0.034)
Train: 72 [ 250/1251 ( 20%)]  Loss: 3.637 (3.75)  Time: 0.292s, 3503.63/s  (0.303s, 3382.42/s)  LR: 8.644e-04  Data: 0.018 (0.032)
Train: 72 [ 300/1251 ( 24%)]  Loss: 3.751 (3.75)  Time: 0.303s, 3384.33/s  (0.302s, 3392.65/s)  LR: 8.643e-04  Data: 0.023 (0.030)
Train: 72 [ 350/1251 ( 28%)]  Loss: 3.611 (3.73)  Time: 0.291s, 3514.51/s  (0.301s, 3399.15/s)  LR: 8.642e-04  Data: 0.022 (0.029)
Train: 72 [ 400/1251 ( 32%)]  Loss: 3.771 (3.74)  Time: 0.301s, 3400.47/s  (0.301s, 3403.91/s)  LR: 8.640e-04  Data: 0.022 (0.028)
Train: 72 [ 450/1251 ( 36%)]  Loss: 4.224 (3.79)  Time: 0.290s, 3535.02/s  (0.300s, 3408.91/s)  LR: 8.639e-04  Data: 0.023 (0.028)
Train: 72 [ 500/1251 ( 40%)]  Loss: 3.925 (3.80)  Time: 0.298s, 3441.43/s  (0.300s, 3411.61/s)  LR: 8.637e-04  Data: 0.025 (0.027)
Train: 72 [ 550/1251 ( 44%)]  Loss: 4.238 (3.84)  Time: 0.302s, 3386.24/s  (0.300s, 3414.52/s)  LR: 8.636e-04  Data: 0.023 (0.027)
Train: 72 [ 600/1251 ( 48%)]  Loss: 3.639 (3.82)  Time: 0.299s, 3422.36/s  (0.300s, 3416.85/s)  LR: 8.634e-04  Data: 0.025 (0.027)
Train: 72 [ 650/1251 ( 52%)]  Loss: 3.975 (3.83)  Time: 0.303s, 3378.04/s  (0.300s, 3418.36/s)  LR: 8.633e-04  Data: 0.025 (0.026)
Train: 72 [ 700/1251 ( 56%)]  Loss: 3.779 (3.83)  Time: 0.299s, 3430.27/s  (0.299s, 3420.29/s)  LR: 8.632e-04  Data: 0.022 (0.026)
Train: 72 [ 750/1251 ( 60%)]  Loss: 3.579 (3.81)  Time: 0.298s, 3440.85/s  (0.299s, 3421.29/s)  LR: 8.630e-04  Data: 0.022 (0.026)
Train: 72 [ 800/1251 ( 64%)]  Loss: 4.075 (3.83)  Time: 0.301s, 3398.78/s  (0.299s, 3421.84/s)  LR: 8.629e-04  Data: 0.024 (0.026)
Train: 72 [ 850/1251 ( 68%)]  Loss: 3.744 (3.82)  Time: 0.293s, 3495.42/s  (0.299s, 3422.70/s)  LR: 8.627e-04  Data: 0.025 (0.026)
Train: 72 [ 900/1251 ( 72%)]  Loss: 4.022 (3.83)  Time: 0.302s, 3393.56/s  (0.299s, 3423.33/s)  LR: 8.626e-04  Data: 0.024 (0.025)
Train: 72 [ 950/1251 ( 76%)]  Loss: 4.169 (3.85)  Time: 0.296s, 3456.15/s  (0.299s, 3423.78/s)  LR: 8.624e-04  Data: 0.024 (0.025)
Train: 72 [1000/1251 ( 80%)]  Loss: 3.688 (3.84)  Time: 0.298s, 3440.51/s  (0.299s, 3424.51/s)  LR: 8.623e-04  Data: 0.020 (0.025)
Train: 72 [1050/1251 ( 84%)]  Loss: 3.808 (3.84)  Time: 0.297s, 3449.33/s  (0.299s, 3424.68/s)  LR: 8.622e-04  Data: 0.023 (0.025)
Train: 72 [1100/1251 ( 88%)]  Loss: 3.624 (3.83)  Time: 0.300s, 3414.38/s  (0.299s, 3424.69/s)  LR: 8.620e-04  Data: 0.025 (0.025)
Train: 72 [1150/1251 ( 92%)]  Loss: 4.073 (3.84)  Time: 0.304s, 3363.85/s  (0.299s, 3424.43/s)  LR: 8.619e-04  Data: 0.022 (0.025)
Train: 72 [1200/1251 ( 96%)]  Loss: 3.589 (3.83)  Time: 0.297s, 3452.21/s  (0.299s, 3424.63/s)  LR: 8.617e-04  Data: 0.019 (0.025)
Train: 72 [1250/1251 (100%)]  Loss: 3.851 (3.83)  Time: 0.276s, 3709.44/s  (0.299s, 3426.43/s)  LR: 8.616e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.078 (2.078)  Loss:  0.7021 (0.7021)  Acc@1: 85.1562 (85.1562)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.051 (0.237)  Loss:  0.8569 (1.3840)  Acc@1: 81.2500 (69.1200)  Acc@5: 94.8113 (89.4020)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-70.pth.tar', 69.36000015136719)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-71.pth.tar', 69.19400002685546)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-72.pth.tar', 69.12)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-69.pth.tar', 68.91800001464844)

Train: 73 [   0/1251 (  0%)]  Loss: 3.893 (3.89)  Time: 2.169s,  472.15/s  (2.169s,  472.15/s)  LR: 8.616e-04  Data: 1.908 (1.908)
Train: 73 [  50/1251 (  4%)]  Loss: 3.863 (3.88)  Time: 0.287s, 3567.77/s  (0.319s, 3207.63/s)  LR: 8.614e-04  Data: 0.022 (0.061)
Train: 73 [ 100/1251 (  8%)]  Loss: 4.006 (3.92)  Time: 0.298s, 3435.25/s  (0.306s, 3349.63/s)  LR: 8.613e-04  Data: 0.026 (0.042)
Train: 73 [ 150/1251 ( 12%)]  Loss: 3.622 (3.85)  Time: 0.294s, 3486.53/s  (0.302s, 3387.81/s)  LR: 8.611e-04  Data: 0.023 (0.036)
Train: 73 [ 200/1251 ( 16%)]  Loss: 3.888 (3.85)  Time: 0.300s, 3411.47/s  (0.301s, 3405.99/s)  LR: 8.610e-04  Data: 0.021 (0.033)
Train: 73 [ 250/1251 ( 20%)]  Loss: 3.802 (3.85)  Time: 0.294s, 3482.31/s  (0.300s, 3414.95/s)  LR: 8.609e-04  Data: 0.024 (0.031)
Train: 73 [ 300/1251 ( 24%)]  Loss: 4.060 (3.88)  Time: 0.297s, 3452.40/s  (0.299s, 3419.58/s)  LR: 8.607e-04  Data: 0.024 (0.029)
Train: 73 [ 350/1251 ( 28%)]  Loss: 3.536 (3.83)  Time: 0.302s, 3392.38/s  (0.299s, 3422.44/s)  LR: 8.606e-04  Data: 0.022 (0.029)
Train: 73 [ 400/1251 ( 32%)]  Loss: 3.965 (3.85)  Time: 0.299s, 3419.34/s  (0.299s, 3423.48/s)  LR: 8.604e-04  Data: 0.025 (0.028)
Train: 73 [ 450/1251 ( 36%)]  Loss: 4.086 (3.87)  Time: 0.294s, 3480.31/s  (0.299s, 3426.47/s)  LR: 8.603e-04  Data: 0.020 (0.027)
Train: 73 [ 500/1251 ( 40%)]  Loss: 3.957 (3.88)  Time: 0.302s, 3390.33/s  (0.299s, 3427.71/s)  LR: 8.601e-04  Data: 0.021 (0.027)
Train: 73 [ 550/1251 ( 44%)]  Loss: 3.908 (3.88)  Time: 0.295s, 3475.57/s  (0.299s, 3429.43/s)  LR: 8.600e-04  Data: 0.024 (0.027)
Train: 73 [ 600/1251 ( 48%)]  Loss: 3.672 (3.87)  Time: 0.295s, 3469.45/s  (0.298s, 3430.53/s)  LR: 8.598e-04  Data: 0.024 (0.026)
Train: 73 [ 650/1251 ( 52%)]  Loss: 4.348 (3.90)  Time: 0.299s, 3422.52/s  (0.298s, 3431.34/s)  LR: 8.597e-04  Data: 0.023 (0.026)
Train: 73 [ 700/1251 ( 56%)]  Loss: 3.858 (3.90)  Time: 0.304s, 3366.01/s  (0.298s, 3431.54/s)  LR: 8.595e-04  Data: 0.026 (0.026)
Train: 73 [ 750/1251 ( 60%)]  Loss: 3.600 (3.88)  Time: 0.301s, 3399.32/s  (0.298s, 3431.23/s)  LR: 8.594e-04  Data: 0.023 (0.026)
Train: 73 [ 800/1251 ( 64%)]  Loss: 3.808 (3.87)  Time: 0.292s, 3506.67/s  (0.298s, 3431.11/s)  LR: 8.593e-04  Data: 0.018 (0.025)
Train: 73 [ 850/1251 ( 68%)]  Loss: 3.946 (3.88)  Time: 0.297s, 3445.04/s  (0.298s, 3431.14/s)  LR: 8.591e-04  Data: 0.026 (0.025)
Train: 73 [ 900/1251 ( 72%)]  Loss: 3.673 (3.87)  Time: 0.303s, 3380.14/s  (0.298s, 3430.92/s)  LR: 8.590e-04  Data: 0.024 (0.025)
Train: 73 [ 950/1251 ( 76%)]  Loss: 3.810 (3.87)  Time: 0.299s, 3429.91/s  (0.299s, 3430.13/s)  LR: 8.588e-04  Data: 0.021 (0.025)
Train: 73 [1000/1251 ( 80%)]  Loss: 3.818 (3.86)  Time: 0.295s, 3468.22/s  (0.299s, 3429.22/s)  LR: 8.587e-04  Data: 0.022 (0.025)
Train: 73 [1050/1251 ( 84%)]  Loss: 3.784 (3.86)  Time: 0.299s, 3424.36/s  (0.299s, 3428.02/s)  LR: 8.585e-04  Data: 0.023 (0.025)
Train: 73 [1100/1251 ( 88%)]  Loss: 3.954 (3.86)  Time: 0.297s, 3442.05/s  (0.299s, 3426.97/s)  LR: 8.584e-04  Data: 0.019 (0.025)
Train: 73 [1150/1251 ( 92%)]  Loss: 3.676 (3.86)  Time: 0.302s, 3390.44/s  (0.299s, 3426.22/s)  LR: 8.582e-04  Data: 0.023 (0.025)
Train: 73 [1200/1251 ( 96%)]  Loss: 3.906 (3.86)  Time: 0.299s, 3429.14/s  (0.299s, 3425.56/s)  LR: 8.581e-04  Data: 0.021 (0.025)
Train: 73 [1250/1251 (100%)]  Loss: 4.148 (3.87)  Time: 0.275s, 3718.22/s  (0.299s, 3426.45/s)  LR: 8.580e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.121 (2.121)  Loss:  0.7124 (0.7124)  Acc@1: 85.9375 (85.9375)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.043 (0.238)  Loss:  0.8301 (1.3678)  Acc@1: 83.0189 (69.1060)  Acc@5: 94.4576 (89.3360)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-70.pth.tar', 69.36000015136719)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-71.pth.tar', 69.19400002685546)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-72.pth.tar', 69.12)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-73.pth.tar', 69.10599999267578)

Train: 74 [   0/1251 (  0%)]  Loss: 3.966 (3.97)  Time: 2.268s,  451.50/s  (2.268s,  451.50/s)  LR: 8.579e-04  Data: 2.044 (2.044)
Train: 74 [  50/1251 (  4%)]  Loss: 3.944 (3.96)  Time: 0.292s, 3504.32/s  (0.319s, 3205.15/s)  LR: 8.578e-04  Data: 0.022 (0.064)
Train: 74 [ 100/1251 (  8%)]  Loss: 3.727 (3.88)  Time: 0.296s, 3458.24/s  (0.307s, 3333.07/s)  LR: 8.577e-04  Data: 0.023 (0.044)
Train: 74 [ 150/1251 ( 12%)]  Loss: 4.083 (3.93)  Time: 0.293s, 3496.09/s  (0.304s, 3369.26/s)  LR: 8.575e-04  Data: 0.027 (0.037)
Train: 74 [ 200/1251 ( 16%)]  Loss: 3.869 (3.92)  Time: 0.302s, 3395.74/s  (0.302s, 3385.88/s)  LR: 8.574e-04  Data: 0.026 (0.034)
Train: 74 [ 250/1251 ( 20%)]  Loss: 3.693 (3.88)  Time: 0.300s, 3418.77/s  (0.301s, 3396.40/s)  LR: 8.572e-04  Data: 0.023 (0.031)
Train: 74 [ 300/1251 ( 24%)]  Loss: 3.571 (3.84)  Time: 0.301s, 3403.52/s  (0.301s, 3400.55/s)  LR: 8.571e-04  Data: 0.024 (0.030)
Train: 74 [ 350/1251 ( 28%)]  Loss: 3.865 (3.84)  Time: 0.298s, 3440.11/s  (0.301s, 3404.49/s)  LR: 8.569e-04  Data: 0.022 (0.029)
Train: 74 [ 400/1251 ( 32%)]  Loss: 3.816 (3.84)  Time: 0.293s, 3492.29/s  (0.301s, 3406.41/s)  LR: 8.568e-04  Data: 0.022 (0.028)
Train: 74 [ 450/1251 ( 36%)]  Loss: 4.120 (3.87)  Time: 0.303s, 3383.04/s  (0.301s, 3406.31/s)  LR: 8.566e-04  Data: 0.024 (0.028)
Train: 74 [ 500/1251 ( 40%)]  Loss: 3.618 (3.84)  Time: 0.299s, 3419.80/s  (0.301s, 3406.42/s)  LR: 8.565e-04  Data: 0.022 (0.027)
Train: 74 [ 550/1251 ( 44%)]  Loss: 3.693 (3.83)  Time: 0.300s, 3417.86/s  (0.301s, 3406.25/s)  LR: 8.563e-04  Data: 0.021 (0.027)
Train: 74 [ 600/1251 ( 48%)]  Loss: 3.872 (3.83)  Time: 0.303s, 3383.06/s  (0.300s, 3407.95/s)  LR: 8.562e-04  Data: 0.019 (0.027)
Train: 74 [ 650/1251 ( 52%)]  Loss: 3.970 (3.84)  Time: 0.300s, 3418.11/s  (0.300s, 3408.85/s)  LR: 8.560e-04  Data: 0.023 (0.026)
Train: 74 [ 700/1251 ( 56%)]  Loss: 4.070 (3.86)  Time: 0.304s, 3368.97/s  (0.300s, 3408.16/s)  LR: 8.559e-04  Data: 0.023 (0.026)
Train: 74 [ 750/1251 ( 60%)]  Loss: 3.733 (3.85)  Time: 0.299s, 3430.22/s  (0.301s, 3407.54/s)  LR: 8.558e-04  Data: 0.021 (0.026)
Train: 74 [ 800/1251 ( 64%)]  Loss: 4.121 (3.87)  Time: 0.302s, 3391.94/s  (0.301s, 3406.98/s)  LR: 8.556e-04  Data: 0.023 (0.026)
Train: 74 [ 850/1251 ( 68%)]  Loss: 3.966 (3.87)  Time: 0.303s, 3375.15/s  (0.301s, 3406.50/s)  LR: 8.555e-04  Data: 0.022 (0.026)
Train: 74 [ 900/1251 ( 72%)]  Loss: 4.031 (3.88)  Time: 0.306s, 3343.63/s  (0.301s, 3406.26/s)  LR: 8.553e-04  Data: 0.023 (0.025)
Train: 74 [ 950/1251 ( 76%)]  Loss: 3.708 (3.87)  Time: 0.307s, 3331.65/s  (0.301s, 3405.82/s)  LR: 8.552e-04  Data: 0.025 (0.025)
Train: 74 [1000/1251 ( 80%)]  Loss: 4.231 (3.89)  Time: 0.300s, 3414.49/s  (0.301s, 3405.38/s)  LR: 8.550e-04  Data: 0.024 (0.025)
Train: 74 [1050/1251 ( 84%)]  Loss: 3.835 (3.89)  Time: 0.297s, 3447.71/s  (0.301s, 3405.19/s)  LR: 8.549e-04  Data: 0.026 (0.025)
Train: 74 [1100/1251 ( 88%)]  Loss: 4.081 (3.89)  Time: 0.304s, 3373.59/s  (0.301s, 3404.64/s)  LR: 8.547e-04  Data: 0.022 (0.025)
Train: 74 [1150/1251 ( 92%)]  Loss: 4.107 (3.90)  Time: 0.302s, 3389.50/s  (0.301s, 3404.40/s)  LR: 8.546e-04  Data: 0.027 (0.025)
Train: 74 [1200/1251 ( 96%)]  Loss: 3.798 (3.90)  Time: 0.306s, 3351.18/s  (0.301s, 3403.70/s)  LR: 8.544e-04  Data: 0.025 (0.025)
Train: 74 [1250/1251 (100%)]  Loss: 3.981 (3.90)  Time: 0.276s, 3710.39/s  (0.301s, 3405.47/s)  LR: 8.543e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.106 (2.106)  Loss:  0.7183 (0.7183)  Acc@1: 84.9609 (84.9609)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.047 (0.233)  Loss:  0.8096 (1.3447)  Acc@1: 81.9575 (69.5820)  Acc@5: 94.4576 (89.4680)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-74.pth.tar', 69.5819999194336)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-70.pth.tar', 69.36000015136719)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-71.pth.tar', 69.19400002685546)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-72.pth.tar', 69.12)

Train: 75 [   0/1251 (  0%)]  Loss: 4.217 (4.22)  Time: 2.000s,  512.06/s  (2.000s,  512.06/s)  LR: 8.543e-04  Data: 1.775 (1.775)
Train: 75 [  50/1251 (  4%)]  Loss: 3.749 (3.98)  Time: 0.287s, 3564.70/s  (0.323s, 3167.79/s)  LR: 8.541e-04  Data: 0.025 (0.066)
Train: 75 [ 100/1251 (  8%)]  Loss: 4.103 (4.02)  Time: 0.296s, 3454.65/s  (0.309s, 3311.40/s)  LR: 8.540e-04  Data: 0.022 (0.045)
Train: 75 [ 150/1251 ( 12%)]  Loss: 4.014 (4.02)  Time: 0.295s, 3468.02/s  (0.305s, 3354.08/s)  LR: 8.538e-04  Data: 0.021 (0.037)
Train: 75 [ 200/1251 ( 16%)]  Loss: 4.240 (4.06)  Time: 0.300s, 3412.19/s  (0.304s, 3371.60/s)  LR: 8.537e-04  Data: 0.023 (0.034)
Train: 75 [ 250/1251 ( 20%)]  Loss: 3.947 (4.04)  Time: 0.294s, 3488.55/s  (0.303s, 3379.30/s)  LR: 8.535e-04  Data: 0.021 (0.032)
Train: 75 [ 300/1251 ( 24%)]  Loss: 3.810 (4.01)  Time: 0.304s, 3372.06/s  (0.302s, 3386.65/s)  LR: 8.534e-04  Data: 0.022 (0.030)
Train: 75 [ 350/1251 ( 28%)]  Loss: 3.628 (3.96)  Time: 0.297s, 3445.86/s  (0.302s, 3392.82/s)  LR: 8.533e-04  Data: 0.022 (0.029)
Train: 75 [ 400/1251 ( 32%)]  Loss: 3.960 (3.96)  Time: 0.299s, 3425.41/s  (0.302s, 3394.92/s)  LR: 8.531e-04  Data: 0.023 (0.029)
Train: 75 [ 450/1251 ( 36%)]  Loss: 4.232 (3.99)  Time: 0.298s, 3434.49/s  (0.301s, 3397.59/s)  LR: 8.530e-04  Data: 0.021 (0.028)
Train: 75 [ 500/1251 ( 40%)]  Loss: 3.698 (3.96)  Time: 0.303s, 3376.57/s  (0.301s, 3400.51/s)  LR: 8.528e-04  Data: 0.025 (0.027)
Train: 75 [ 550/1251 ( 44%)]  Loss: 3.973 (3.96)  Time: 0.301s, 3400.37/s  (0.301s, 3402.33/s)  LR: 8.527e-04  Data: 0.024 (0.027)
Train: 75 [ 600/1251 ( 48%)]  Loss: 3.832 (3.95)  Time: 0.300s, 3418.11/s  (0.301s, 3403.15/s)  LR: 8.525e-04  Data: 0.023 (0.027)
Train: 75 [ 650/1251 ( 52%)]  Loss: 3.740 (3.94)  Time: 0.299s, 3428.10/s  (0.301s, 3403.61/s)  LR: 8.524e-04  Data: 0.025 (0.026)
Train: 75 [ 700/1251 ( 56%)]  Loss: 3.947 (3.94)  Time: 0.296s, 3459.63/s  (0.301s, 3403.40/s)  LR: 8.522e-04  Data: 0.020 (0.026)
Train: 75 [ 750/1251 ( 60%)]  Loss: 3.745 (3.93)  Time: 0.300s, 3412.54/s  (0.301s, 3403.37/s)  LR: 8.521e-04  Data: 0.025 (0.026)
Train: 75 [ 800/1251 ( 64%)]  Loss: 4.177 (3.94)  Time: 0.300s, 3407.87/s  (0.301s, 3403.52/s)  LR: 8.519e-04  Data: 0.024 (0.026)
Train: 75 [ 850/1251 ( 68%)]  Loss: 3.975 (3.94)  Time: 0.294s, 3478.65/s  (0.301s, 3403.47/s)  LR: 8.518e-04  Data: 0.020 (0.026)
Train: 75 [ 900/1251 ( 72%)]  Loss: 3.628 (3.93)  Time: 0.305s, 3362.10/s  (0.301s, 3402.78/s)  LR: 8.516e-04  Data: 0.022 (0.026)
Train: 75 [ 950/1251 ( 76%)]  Loss: 3.941 (3.93)  Time: 0.297s, 3446.47/s  (0.301s, 3402.54/s)  LR: 8.515e-04  Data: 0.024 (0.026)
Train: 75 [1000/1251 ( 80%)]  Loss: 4.019 (3.93)  Time: 0.308s, 3328.09/s  (0.301s, 3402.22/s)  LR: 8.513e-04  Data: 0.022 (0.025)
Train: 75 [1050/1251 ( 84%)]  Loss: 3.970 (3.93)  Time: 0.304s, 3370.50/s  (0.301s, 3401.77/s)  LR: 8.512e-04  Data: 0.021 (0.025)
Train: 75 [1100/1251 ( 88%)]  Loss: 4.078 (3.94)  Time: 0.304s, 3366.56/s  (0.301s, 3401.77/s)  LR: 8.510e-04  Data: 0.026 (0.025)
Train: 75 [1150/1251 ( 92%)]  Loss: 3.807 (3.93)  Time: 0.308s, 3321.13/s  (0.301s, 3401.47/s)  LR: 8.509e-04  Data: 0.024 (0.025)
Train: 75 [1200/1251 ( 96%)]  Loss: 4.043 (3.94)  Time: 0.300s, 3413.50/s  (0.301s, 3400.63/s)  LR: 8.507e-04  Data: 0.016 (0.025)
Train: 75 [1250/1251 (100%)]  Loss: 4.047 (3.94)  Time: 0.275s, 3718.39/s  (0.301s, 3401.89/s)  LR: 8.506e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.092 (2.092)  Loss:  0.7710 (0.7710)  Acc@1: 84.4727 (84.4727)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.060 (0.236)  Loss:  0.7837 (1.3655)  Acc@1: 82.6651 (69.4700)  Acc@5: 95.1651 (89.5280)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-74.pth.tar', 69.5819999194336)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-75.pth.tar', 69.46999996826172)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-70.pth.tar', 69.36000015136719)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-71.pth.tar', 69.19400002685546)

Train: 76 [   0/1251 (  0%)]  Loss: 3.818 (3.82)  Time: 2.242s,  456.65/s  (2.242s,  456.65/s)  LR: 8.506e-04  Data: 2.021 (2.021)
Train: 76 [  50/1251 (  4%)]  Loss: 4.384 (4.10)  Time: 0.298s, 3434.71/s  (0.327s, 3134.47/s)  LR: 8.504e-04  Data: 0.022 (0.063)
Train: 76 [ 100/1251 (  8%)]  Loss: 4.108 (4.10)  Time: 0.298s, 3433.49/s  (0.312s, 3284.40/s)  LR: 8.503e-04  Data: 0.022 (0.043)
Train: 76 [ 150/1251 ( 12%)]  Loss: 3.211 (3.88)  Time: 0.299s, 3421.03/s  (0.308s, 3329.62/s)  LR: 8.501e-04  Data: 0.025 (0.036)
Train: 76 [ 200/1251 ( 16%)]  Loss: 3.915 (3.89)  Time: 0.302s, 3387.30/s  (0.305s, 3352.04/s)  LR: 8.500e-04  Data: 0.021 (0.033)
Train: 76 [ 250/1251 ( 20%)]  Loss: 3.526 (3.83)  Time: 0.296s, 3460.55/s  (0.305s, 3362.23/s)  LR: 8.498e-04  Data: 0.021 (0.031)
Train: 76 [ 300/1251 ( 24%)]  Loss: 3.408 (3.77)  Time: 0.301s, 3405.35/s  (0.304s, 3369.06/s)  LR: 8.497e-04  Data: 0.022 (0.030)
Train: 76 [ 350/1251 ( 28%)]  Loss: 3.782 (3.77)  Time: 0.304s, 3363.52/s  (0.304s, 3371.67/s)  LR: 8.495e-04  Data: 0.025 (0.029)
Train: 76 [ 400/1251 ( 32%)]  Loss: 3.874 (3.78)  Time: 0.303s, 3382.92/s  (0.303s, 3374.42/s)  LR: 8.494e-04  Data: 0.021 (0.028)
Train: 76 [ 450/1251 ( 36%)]  Loss: 3.980 (3.80)  Time: 0.300s, 3409.15/s  (0.303s, 3377.57/s)  LR: 8.492e-04  Data: 0.026 (0.028)
Train: 76 [ 500/1251 ( 40%)]  Loss: 3.781 (3.80)  Time: 0.304s, 3370.09/s  (0.303s, 3379.24/s)  LR: 8.491e-04  Data: 0.023 (0.027)
Train: 76 [ 550/1251 ( 44%)]  Loss: 4.084 (3.82)  Time: 0.304s, 3368.25/s  (0.303s, 3381.14/s)  LR: 8.489e-04  Data: 0.022 (0.027)
Train: 76 [ 600/1251 ( 48%)]  Loss: 3.489 (3.80)  Time: 0.302s, 3387.60/s  (0.303s, 3381.75/s)  LR: 8.488e-04  Data: 0.026 (0.027)
Train: 76 [ 650/1251 ( 52%)]  Loss: 4.263 (3.83)  Time: 0.301s, 3401.78/s  (0.303s, 3382.33/s)  LR: 8.486e-04  Data: 0.021 (0.026)
Train: 76 [ 700/1251 ( 56%)]  Loss: 3.816 (3.83)  Time: 0.304s, 3367.27/s  (0.303s, 3382.59/s)  LR: 8.485e-04  Data: 0.025 (0.026)
Train: 76 [ 750/1251 ( 60%)]  Loss: 3.722 (3.82)  Time: 0.308s, 3328.57/s  (0.303s, 3382.37/s)  LR: 8.483e-04  Data: 0.024 (0.026)
Train: 76 [ 800/1251 ( 64%)]  Loss: 3.565 (3.81)  Time: 0.299s, 3427.63/s  (0.303s, 3382.40/s)  LR: 8.482e-04  Data: 0.023 (0.026)
Train: 76 [ 850/1251 ( 68%)]  Loss: 3.839 (3.81)  Time: 0.302s, 3393.24/s  (0.303s, 3382.46/s)  LR: 8.480e-04  Data: 0.027 (0.025)
Train: 76 [ 900/1251 ( 72%)]  Loss: 4.160 (3.83)  Time: 0.307s, 3332.44/s  (0.303s, 3382.31/s)  LR: 8.479e-04  Data: 0.026 (0.025)
Train: 76 [ 950/1251 ( 76%)]  Loss: 3.734 (3.82)  Time: 0.301s, 3402.67/s  (0.303s, 3382.31/s)  LR: 8.477e-04  Data: 0.022 (0.025)
Train: 76 [1000/1251 ( 80%)]  Loss: 3.750 (3.82)  Time: 0.306s, 3347.86/s  (0.303s, 3381.97/s)  LR: 8.476e-04  Data: 0.024 (0.025)
Train: 76 [1050/1251 ( 84%)]  Loss: 4.118 (3.83)  Time: 0.303s, 3384.11/s  (0.303s, 3382.06/s)  LR: 8.474e-04  Data: 0.022 (0.025)
Train: 76 [1100/1251 ( 88%)]  Loss: 4.125 (3.85)  Time: 0.303s, 3382.91/s  (0.303s, 3382.04/s)  LR: 8.473e-04  Data: 0.022 (0.025)
Train: 76 [1150/1251 ( 92%)]  Loss: 3.857 (3.85)  Time: 0.308s, 3324.91/s  (0.303s, 3382.51/s)  LR: 8.471e-04  Data: 0.024 (0.025)
Train: 76 [1200/1251 ( 96%)]  Loss: 3.999 (3.85)  Time: 0.300s, 3412.27/s  (0.303s, 3382.11/s)  LR: 8.470e-04  Data: 0.023 (0.025)
Train: 76 [1250/1251 (100%)]  Loss: 3.947 (3.86)  Time: 0.276s, 3711.80/s  (0.303s, 3383.98/s)  LR: 8.468e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.155 (2.155)  Loss:  0.7593 (0.7593)  Acc@1: 86.1328 (86.1328)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.045 (0.235)  Loss:  0.8896 (1.3698)  Acc@1: 80.3066 (69.6560)  Acc@5: 94.1038 (89.6120)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-76.pth.tar', 69.65600010742187)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-74.pth.tar', 69.5819999194336)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-75.pth.tar', 69.46999996826172)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-70.pth.tar', 69.36000015136719)

Train: 77 [   0/1251 (  0%)]  Loss: 4.060 (4.06)  Time: 2.158s,  474.41/s  (2.158s,  474.41/s)  LR: 8.468e-04  Data: 1.921 (1.921)
Train: 77 [  50/1251 (  4%)]  Loss: 3.831 (3.95)  Time: 0.294s, 3480.70/s  (0.321s, 3186.48/s)  LR: 8.467e-04  Data: 0.025 (0.061)
Train: 77 [ 100/1251 (  8%)]  Loss: 3.389 (3.76)  Time: 0.298s, 3440.31/s  (0.309s, 3314.25/s)  LR: 8.465e-04  Data: 0.022 (0.042)
Train: 77 [ 150/1251 ( 12%)]  Loss: 3.958 (3.81)  Time: 0.305s, 3361.17/s  (0.306s, 3348.71/s)  LR: 8.464e-04  Data: 0.027 (0.036)
Train: 77 [ 200/1251 ( 16%)]  Loss: 3.710 (3.79)  Time: 0.304s, 3368.64/s  (0.305s, 3362.01/s)  LR: 8.462e-04  Data: 0.025 (0.033)
Train: 77 [ 250/1251 ( 20%)]  Loss: 4.098 (3.84)  Time: 0.302s, 3392.96/s  (0.304s, 3368.52/s)  LR: 8.461e-04  Data: 0.023 (0.031)
Train: 77 [ 300/1251 ( 24%)]  Loss: 3.902 (3.85)  Time: 0.299s, 3419.91/s  (0.304s, 3371.14/s)  LR: 8.459e-04  Data: 0.020 (0.029)
Train: 77 [ 350/1251 ( 28%)]  Loss: 3.952 (3.86)  Time: 0.302s, 3385.30/s  (0.304s, 3372.75/s)  LR: 8.458e-04  Data: 0.024 (0.028)
Train: 77 [ 400/1251 ( 32%)]  Loss: 4.020 (3.88)  Time: 0.302s, 3386.24/s  (0.304s, 3373.39/s)  LR: 8.456e-04  Data: 0.025 (0.028)
Train: 77 [ 450/1251 ( 36%)]  Loss: 3.665 (3.86)  Time: 0.302s, 3385.72/s  (0.304s, 3373.95/s)  LR: 8.455e-04  Data: 0.028 (0.027)
Train: 77 [ 500/1251 ( 40%)]  Loss: 3.372 (3.81)  Time: 0.304s, 3373.71/s  (0.303s, 3374.58/s)  LR: 8.453e-04  Data: 0.024 (0.027)
Train: 77 [ 550/1251 ( 44%)]  Loss: 3.793 (3.81)  Time: 0.305s, 3359.19/s  (0.303s, 3374.30/s)  LR: 8.452e-04  Data: 0.025 (0.026)
Train: 77 [ 600/1251 ( 48%)]  Loss: 3.608 (3.80)  Time: 0.297s, 3452.73/s  (0.303s, 3374.67/s)  LR: 8.450e-04  Data: 0.022 (0.026)
Train: 77 [ 650/1251 ( 52%)]  Loss: 3.638 (3.79)  Time: 0.304s, 3373.60/s  (0.303s, 3374.75/s)  LR: 8.449e-04  Data: 0.022 (0.026)
Train: 77 [ 700/1251 ( 56%)]  Loss: 3.960 (3.80)  Time: 0.303s, 3377.55/s  (0.303s, 3374.13/s)  LR: 8.447e-04  Data: 0.021 (0.026)
Train: 77 [ 750/1251 ( 60%)]  Loss: 4.014 (3.81)  Time: 0.304s, 3367.47/s  (0.303s, 3374.03/s)  LR: 8.446e-04  Data: 0.025 (0.026)
Train: 77 [ 800/1251 ( 64%)]  Loss: 3.977 (3.82)  Time: 0.301s, 3403.27/s  (0.304s, 3373.84/s)  LR: 8.444e-04  Data: 0.021 (0.025)
Train: 77 [ 850/1251 ( 68%)]  Loss: 3.947 (3.83)  Time: 0.301s, 3401.69/s  (0.304s, 3373.06/s)  LR: 8.443e-04  Data: 0.026 (0.025)
Train: 77 [ 900/1251 ( 72%)]  Loss: 3.637 (3.82)  Time: 0.306s, 3345.21/s  (0.304s, 3373.57/s)  LR: 8.441e-04  Data: 0.023 (0.025)
Train: 77 [ 950/1251 ( 76%)]  Loss: 3.876 (3.82)  Time: 0.305s, 3362.60/s  (0.304s, 3373.18/s)  LR: 8.440e-04  Data: 0.020 (0.025)
Train: 77 [1000/1251 ( 80%)]  Loss: 3.810 (3.82)  Time: 0.300s, 3407.80/s  (0.304s, 3373.12/s)  LR: 8.438e-04  Data: 0.026 (0.025)
Train: 77 [1050/1251 ( 84%)]  Loss: 3.759 (3.82)  Time: 0.300s, 3410.01/s  (0.304s, 3373.43/s)  LR: 8.437e-04  Data: 0.023 (0.025)
Train: 77 [1100/1251 ( 88%)]  Loss: 4.115 (3.83)  Time: 0.304s, 3369.68/s  (0.304s, 3373.31/s)  LR: 8.435e-04  Data: 0.025 (0.025)
Train: 77 [1150/1251 ( 92%)]  Loss: 3.879 (3.83)  Time: 0.302s, 3392.23/s  (0.304s, 3373.21/s)  LR: 8.434e-04  Data: 0.025 (0.025)
Train: 77 [1200/1251 ( 96%)]  Loss: 3.833 (3.83)  Time: 0.310s, 3307.97/s  (0.304s, 3373.05/s)  LR: 8.432e-04  Data: 0.025 (0.025)
Train: 77 [1250/1251 (100%)]  Loss: 3.775 (3.83)  Time: 0.276s, 3711.94/s  (0.303s, 3374.33/s)  LR: 8.431e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.036 (2.036)  Loss:  0.7588 (0.7588)  Acc@1: 84.1797 (84.1797)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.044 (0.233)  Loss:  0.8008 (1.3641)  Acc@1: 83.3726 (69.7100)  Acc@5: 94.9293 (89.6100)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-77.pth.tar', 69.70999988769532)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-76.pth.tar', 69.65600010742187)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-74.pth.tar', 69.5819999194336)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-75.pth.tar', 69.46999996826172)

Train: 78 [   0/1251 (  0%)]  Loss: 3.621 (3.62)  Time: 2.197s,  466.18/s  (2.197s,  466.18/s)  LR: 8.431e-04  Data: 1.965 (1.965)
Train: 78 [  50/1251 (  4%)]  Loss: 4.190 (3.91)  Time: 0.297s, 3443.56/s  (0.322s, 3175.21/s)  LR: 8.429e-04  Data: 0.025 (0.062)
Train: 78 [ 100/1251 (  8%)]  Loss: 3.994 (3.94)  Time: 0.301s, 3396.58/s  (0.310s, 3301.79/s)  LR: 8.428e-04  Data: 0.023 (0.042)
Train: 78 [ 150/1251 ( 12%)]  Loss: 3.957 (3.94)  Time: 0.301s, 3399.82/s  (0.307s, 3338.97/s)  LR: 8.426e-04  Data: 0.028 (0.036)
Train: 78 [ 200/1251 ( 16%)]  Loss: 3.970 (3.95)  Time: 0.304s, 3365.11/s  (0.305s, 3353.97/s)  LR: 8.425e-04  Data: 0.022 (0.033)
Train: 78 [ 250/1251 ( 20%)]  Loss: 4.011 (3.96)  Time: 0.301s, 3396.44/s  (0.305s, 3361.13/s)  LR: 8.423e-04  Data: 0.023 (0.031)
Train: 78 [ 300/1251 ( 24%)]  Loss: 3.898 (3.95)  Time: 0.305s, 3359.42/s  (0.304s, 3364.81/s)  LR: 8.421e-04  Data: 0.028 (0.030)
Train: 78 [ 350/1251 ( 28%)]  Loss: 3.888 (3.94)  Time: 0.302s, 3391.71/s  (0.304s, 3366.91/s)  LR: 8.420e-04  Data: 0.028 (0.029)
Train: 78 [ 400/1251 ( 32%)]  Loss: 3.463 (3.89)  Time: 0.303s, 3380.18/s  (0.304s, 3368.86/s)  LR: 8.418e-04  Data: 0.021 (0.028)
Train: 78 [ 450/1251 ( 36%)]  Loss: 3.674 (3.87)  Time: 0.303s, 3379.67/s  (0.304s, 3371.12/s)  LR: 8.417e-04  Data: 0.019 (0.028)
Train: 78 [ 500/1251 ( 40%)]  Loss: 3.900 (3.87)  Time: 0.305s, 3359.37/s  (0.304s, 3371.83/s)  LR: 8.415e-04  Data: 0.022 (0.027)
Train: 78 [ 550/1251 ( 44%)]  Loss: 3.872 (3.87)  Time: 0.304s, 3370.32/s  (0.304s, 3371.64/s)  LR: 8.414e-04  Data: 0.025 (0.027)
Train: 78 [ 600/1251 ( 48%)]  Loss: 3.617 (3.85)  Time: 0.307s, 3336.57/s  (0.304s, 3370.18/s)  LR: 8.412e-04  Data: 0.023 (0.026)
Train: 78 [ 650/1251 ( 52%)]  Loss: 3.646 (3.84)  Time: 0.304s, 3373.90/s  (0.304s, 3369.75/s)  LR: 8.411e-04  Data: 0.021 (0.026)
Train: 78 [ 700/1251 ( 56%)]  Loss: 3.800 (3.83)  Time: 0.308s, 3322.69/s  (0.304s, 3369.07/s)  LR: 8.409e-04  Data: 0.024 (0.026)
Train: 78 [ 750/1251 ( 60%)]  Loss: 4.164 (3.85)  Time: 0.303s, 3376.15/s  (0.304s, 3368.93/s)  LR: 8.408e-04  Data: 0.030 (0.026)
Train: 78 [ 800/1251 ( 64%)]  Loss: 3.853 (3.85)  Time: 0.302s, 3387.86/s  (0.304s, 3368.92/s)  LR: 8.406e-04  Data: 0.024 (0.026)
Train: 78 [ 850/1251 ( 68%)]  Loss: 3.697 (3.85)  Time: 0.305s, 3353.17/s  (0.304s, 3368.92/s)  LR: 8.405e-04  Data: 0.022 (0.025)
Train: 78 [ 900/1251 ( 72%)]  Loss: 3.738 (3.84)  Time: 0.304s, 3367.26/s  (0.304s, 3367.80/s)  LR: 8.403e-04  Data: 0.023 (0.025)
Train: 78 [ 950/1251 ( 76%)]  Loss: 4.010 (3.85)  Time: 0.304s, 3370.37/s  (0.304s, 3367.94/s)  LR: 8.402e-04  Data: 0.023 (0.025)
Train: 78 [1000/1251 ( 80%)]  Loss: 4.062 (3.86)  Time: 0.304s, 3367.44/s  (0.304s, 3367.17/s)  LR: 8.400e-04  Data: 0.023 (0.025)
Train: 78 [1050/1251 ( 84%)]  Loss: 4.018 (3.87)  Time: 0.303s, 3375.20/s  (0.304s, 3366.19/s)  LR: 8.399e-04  Data: 0.022 (0.025)
Train: 78 [1100/1251 ( 88%)]  Loss: 3.447 (3.85)  Time: 0.304s, 3363.91/s  (0.304s, 3365.95/s)  LR: 8.397e-04  Data: 0.015 (0.025)
Train: 78 [1150/1251 ( 92%)]  Loss: 3.759 (3.84)  Time: 0.304s, 3371.07/s  (0.304s, 3366.16/s)  LR: 8.396e-04  Data: 0.024 (0.025)
Train: 78 [1200/1251 ( 96%)]  Loss: 3.537 (3.83)  Time: 0.304s, 3367.96/s  (0.304s, 3365.99/s)  LR: 8.394e-04  Data: 0.020 (0.025)
Train: 78 [1250/1251 (100%)]  Loss: 3.845 (3.83)  Time: 0.276s, 3711.24/s  (0.304s, 3367.48/s)  LR: 8.392e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.263 (2.263)  Loss:  0.6968 (0.6968)  Acc@1: 86.5234 (86.5234)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.052 (0.236)  Loss:  0.7915 (1.3484)  Acc@1: 82.7830 (69.8340)  Acc@5: 95.4009 (89.6060)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-78.pth.tar', 69.83399989013672)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-77.pth.tar', 69.70999988769532)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-76.pth.tar', 69.65600010742187)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-74.pth.tar', 69.5819999194336)

Train: 79 [   0/1251 (  0%)]  Loss: 4.000 (4.00)  Time: 2.045s,  500.62/s  (2.045s,  500.62/s)  LR: 8.392e-04  Data: 1.809 (1.809)
Train: 79 [  50/1251 (  4%)]  Loss: 3.772 (3.89)  Time: 0.293s, 3489.60/s  (0.325s, 3148.74/s)  LR: 8.391e-04  Data: 0.021 (0.061)
Train: 79 [ 100/1251 (  8%)]  Loss: 4.184 (3.99)  Time: 0.300s, 3412.18/s  (0.312s, 3285.44/s)  LR: 8.389e-04  Data: 0.023 (0.042)
Train: 79 [ 150/1251 ( 12%)]  Loss: 3.650 (3.90)  Time: 0.306s, 3345.87/s  (0.308s, 3324.93/s)  LR: 8.388e-04  Data: 0.023 (0.036)
Train: 79 [ 200/1251 ( 16%)]  Loss: 3.668 (3.86)  Time: 0.303s, 3384.21/s  (0.306s, 3341.12/s)  LR: 8.386e-04  Data: 0.022 (0.033)
Train: 79 [ 250/1251 ( 20%)]  Loss: 3.534 (3.80)  Time: 0.302s, 3396.09/s  (0.306s, 3348.47/s)  LR: 8.385e-04  Data: 0.022 (0.031)
Train: 79 [ 300/1251 ( 24%)]  Loss: 4.250 (3.87)  Time: 0.303s, 3378.60/s  (0.305s, 3352.39/s)  LR: 8.383e-04  Data: 0.024 (0.029)
Train: 79 [ 350/1251 ( 28%)]  Loss: 3.807 (3.86)  Time: 0.305s, 3361.30/s  (0.305s, 3353.60/s)  LR: 8.382e-04  Data: 0.024 (0.029)
Train: 79 [ 400/1251 ( 32%)]  Loss: 4.102 (3.89)  Time: 0.306s, 3347.04/s  (0.305s, 3354.43/s)  LR: 8.380e-04  Data: 0.020 (0.028)
Train: 79 [ 450/1251 ( 36%)]  Loss: 3.745 (3.87)  Time: 0.307s, 3340.42/s  (0.305s, 3355.29/s)  LR: 8.379e-04  Data: 0.023 (0.027)
Train: 79 [ 500/1251 ( 40%)]  Loss: 4.054 (3.89)  Time: 0.306s, 3342.28/s  (0.305s, 3355.49/s)  LR: 8.377e-04  Data: 0.021 (0.027)
Train: 79 [ 550/1251 ( 44%)]  Loss: 3.681 (3.87)  Time: 0.309s, 3311.84/s  (0.305s, 3355.36/s)  LR: 8.376e-04  Data: 0.022 (0.026)
Train: 79 [ 600/1251 ( 48%)]  Loss: 3.826 (3.87)  Time: 0.314s, 3261.17/s  (0.305s, 3355.29/s)  LR: 8.374e-04  Data: 0.022 (0.026)
Train: 79 [ 650/1251 ( 52%)]  Loss: 3.646 (3.85)  Time: 0.302s, 3392.86/s  (0.305s, 3355.02/s)  LR: 8.372e-04  Data: 0.024 (0.026)
Train: 79 [ 700/1251 ( 56%)]  Loss: 3.578 (3.83)  Time: 0.307s, 3334.67/s  (0.305s, 3353.38/s)  LR: 8.371e-04  Data: 0.021 (0.026)
Train: 79 [ 750/1251 ( 60%)]  Loss: 3.896 (3.84)  Time: 0.307s, 3334.07/s  (0.305s, 3352.77/s)  LR: 8.369e-04  Data: 0.023 (0.026)
Train: 79 [ 800/1251 ( 64%)]  Loss: 3.783 (3.83)  Time: 0.306s, 3344.52/s  (0.305s, 3352.60/s)  LR: 8.368e-04  Data: 0.023 (0.025)
Train: 79 [ 850/1251 ( 68%)]  Loss: 3.908 (3.84)  Time: 0.309s, 3317.71/s  (0.306s, 3351.83/s)  LR: 8.366e-04  Data: 0.023 (0.025)
Train: 79 [ 900/1251 ( 72%)]  Loss: 3.850 (3.84)  Time: 0.305s, 3355.00/s  (0.306s, 3350.86/s)  LR: 8.365e-04  Data: 0.023 (0.025)
Train: 79 [ 950/1251 ( 76%)]  Loss: 3.827 (3.84)  Time: 0.314s, 3258.25/s  (0.306s, 3349.70/s)  LR: 8.363e-04  Data: 0.024 (0.025)
Train: 79 [1000/1251 ( 80%)]  Loss: 3.451 (3.82)  Time: 0.301s, 3398.99/s  (0.306s, 3348.86/s)  LR: 8.362e-04  Data: 0.020 (0.025)
Train: 79 [1050/1251 ( 84%)]  Loss: 3.958 (3.83)  Time: 0.303s, 3374.06/s  (0.306s, 3348.67/s)  LR: 8.360e-04  Data: 0.022 (0.025)
Train: 79 [1100/1251 ( 88%)]  Loss: 3.945 (3.83)  Time: 0.311s, 3295.81/s  (0.306s, 3348.05/s)  LR: 8.359e-04  Data: 0.021 (0.025)
Train: 79 [1150/1251 ( 92%)]  Loss: 4.018 (3.84)  Time: 0.316s, 3238.21/s  (0.306s, 3347.13/s)  LR: 8.357e-04  Data: 0.022 (0.025)
Train: 79 [1200/1251 ( 96%)]  Loss: 4.071 (3.85)  Time: 0.305s, 3360.14/s  (0.306s, 3346.49/s)  LR: 8.355e-04  Data: 0.023 (0.025)
Train: 79 [1250/1251 (100%)]  Loss: 3.805 (3.85)  Time: 0.276s, 3706.33/s  (0.306s, 3347.70/s)  LR: 8.354e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.130 (2.130)  Loss:  0.6597 (0.6597)  Acc@1: 86.7188 (86.7188)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.053 (0.235)  Loss:  0.7856 (1.3526)  Acc@1: 82.6651 (69.7780)  Acc@5: 95.6368 (89.4040)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-78.pth.tar', 69.83399989013672)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-79.pth.tar', 69.77799996826172)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-77.pth.tar', 69.70999988769532)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-76.pth.tar', 69.65600010742187)

Train: 80 [   0/1251 (  0%)]  Loss: 3.704 (3.70)  Time: 2.304s,  444.53/s  (2.304s,  444.53/s)  LR: 8.354e-04  Data: 2.078 (2.078)
Train: 80 [  50/1251 (  4%)]  Loss: 3.870 (3.79)  Time: 0.301s, 3400.94/s  (0.329s, 3115.18/s)  LR: 8.352e-04  Data: 0.022 (0.064)
Train: 80 [ 100/1251 (  8%)]  Loss: 3.803 (3.79)  Time: 0.305s, 3353.59/s  (0.315s, 3249.32/s)  LR: 8.351e-04  Data: 0.022 (0.044)
Train: 80 [ 150/1251 ( 12%)]  Loss: 4.027 (3.85)  Time: 0.316s, 3245.48/s  (0.312s, 3287.13/s)  LR: 8.349e-04  Data: 0.023 (0.037)
Train: 80 [ 200/1251 ( 16%)]  Loss: 3.593 (3.80)  Time: 0.301s, 3400.14/s  (0.310s, 3304.14/s)  LR: 8.348e-04  Data: 0.022 (0.033)
Train: 80 [ 250/1251 ( 20%)]  Loss: 3.926 (3.82)  Time: 0.311s, 3289.97/s  (0.309s, 3314.01/s)  LR: 8.346e-04  Data: 0.017 (0.031)
Train: 80 [ 300/1251 ( 24%)]  Loss: 3.924 (3.84)  Time: 0.310s, 3302.63/s  (0.309s, 3317.47/s)  LR: 8.345e-04  Data: 0.026 (0.030)
Train: 80 [ 350/1251 ( 28%)]  Loss: 3.881 (3.84)  Time: 0.307s, 3337.86/s  (0.308s, 3319.85/s)  LR: 8.343e-04  Data: 0.023 (0.029)
Train: 80 [ 400/1251 ( 32%)]  Loss: 3.681 (3.82)  Time: 0.303s, 3376.48/s  (0.308s, 3321.99/s)  LR: 8.341e-04  Data: 0.022 (0.028)
Train: 80 [ 450/1251 ( 36%)]  Loss: 4.152 (3.86)  Time: 0.308s, 3319.41/s  (0.308s, 3322.72/s)  LR: 8.340e-04  Data: 0.022 (0.028)
Train: 80 [ 500/1251 ( 40%)]  Loss: 3.646 (3.84)  Time: 0.313s, 3269.49/s  (0.308s, 3323.68/s)  LR: 8.338e-04  Data: 0.023 (0.027)
Train: 80 [ 550/1251 ( 44%)]  Loss: 3.683 (3.82)  Time: 0.301s, 3402.99/s  (0.308s, 3324.21/s)  LR: 8.337e-04  Data: 0.021 (0.027)
Train: 80 [ 600/1251 ( 48%)]  Loss: 3.331 (3.79)  Time: 0.309s, 3309.36/s  (0.308s, 3324.20/s)  LR: 8.335e-04  Data: 0.024 (0.026)
Train: 80 [ 650/1251 ( 52%)]  Loss: 3.549 (3.77)  Time: 0.308s, 3321.66/s  (0.308s, 3323.23/s)  LR: 8.334e-04  Data: 0.022 (0.026)
Train: 80 [ 700/1251 ( 56%)]  Loss: 3.512 (3.75)  Time: 0.310s, 3298.30/s  (0.308s, 3322.90/s)  LR: 8.332e-04  Data: 0.023 (0.026)
Train: 80 [ 750/1251 ( 60%)]  Loss: 3.866 (3.76)  Time: 0.312s, 3280.03/s  (0.308s, 3322.43/s)  LR: 8.331e-04  Data: 0.023 (0.026)
Train: 80 [ 800/1251 ( 64%)]  Loss: 4.152 (3.78)  Time: 0.309s, 3313.45/s  (0.308s, 3321.90/s)  LR: 8.329e-04  Data: 0.020 (0.026)
Train: 80 [ 850/1251 ( 68%)]  Loss: 3.505 (3.77)  Time: 0.313s, 3270.50/s  (0.308s, 3321.22/s)  LR: 8.328e-04  Data: 0.026 (0.025)
Train: 80 [ 900/1251 ( 72%)]  Loss: 3.874 (3.77)  Time: 0.312s, 3283.17/s  (0.308s, 3321.01/s)  LR: 8.326e-04  Data: 0.025 (0.025)
Train: 80 [ 950/1251 ( 76%)]  Loss: 3.733 (3.77)  Time: 0.309s, 3312.26/s  (0.308s, 3320.44/s)  LR: 8.324e-04  Data: 0.023 (0.025)
Train: 80 [1000/1251 ( 80%)]  Loss: 3.808 (3.77)  Time: 0.306s, 3344.56/s  (0.308s, 3319.99/s)  LR: 8.323e-04  Data: 0.022 (0.025)
Train: 80 [1050/1251 ( 84%)]  Loss: 3.902 (3.78)  Time: 0.308s, 3324.05/s  (0.308s, 3319.58/s)  LR: 8.321e-04  Data: 0.023 (0.025)
Train: 80 [1100/1251 ( 88%)]  Loss: 3.848 (3.78)  Time: 0.308s, 3325.12/s  (0.309s, 3318.91/s)  LR: 8.320e-04  Data: 0.022 (0.025)
Train: 80 [1150/1251 ( 92%)]  Loss: 3.867 (3.78)  Time: 0.305s, 3352.56/s  (0.309s, 3318.63/s)  LR: 8.318e-04  Data: 0.025 (0.025)
Train: 80 [1200/1251 ( 96%)]  Loss: 3.902 (3.79)  Time: 0.309s, 3319.03/s  (0.309s, 3318.32/s)  LR: 8.317e-04  Data: 0.023 (0.025)
Train: 80 [1250/1251 (100%)]  Loss: 3.746 (3.79)  Time: 0.279s, 3666.78/s  (0.308s, 3319.62/s)  LR: 8.315e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.276 (2.276)  Loss:  0.7310 (0.7310)  Acc@1: 85.3516 (85.3516)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.067 (0.239)  Loss:  0.8154 (1.3425)  Acc@1: 82.3113 (69.8160)  Acc@5: 94.9292 (89.5900)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-78.pth.tar', 69.83399989013672)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-80.pth.tar', 69.81599994384766)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-79.pth.tar', 69.77799996826172)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-77.pth.tar', 69.70999988769532)

Train: 81 [   0/1251 (  0%)]  Loss: 3.857 (3.86)  Time: 2.401s,  426.45/s  (2.401s,  426.45/s)  LR: 8.315e-04  Data: 2.179 (2.179)
Train: 81 [  50/1251 (  4%)]  Loss: 3.699 (3.78)  Time: 0.298s, 3433.10/s  (0.344s, 2977.84/s)  LR: 8.313e-04  Data: 0.024 (0.078)
Train: 81 [ 100/1251 (  8%)]  Loss: 3.995 (3.85)  Time: 0.305s, 3359.87/s  (0.324s, 3160.70/s)  LR: 8.312e-04  Data: 0.024 (0.051)
Train: 81 [ 150/1251 ( 12%)]  Loss: 4.043 (3.90)  Time: 0.311s, 3297.31/s  (0.318s, 3216.73/s)  LR: 8.310e-04  Data: 0.020 (0.041)
Train: 81 [ 200/1251 ( 16%)]  Loss: 3.624 (3.84)  Time: 0.307s, 3332.90/s  (0.316s, 3242.44/s)  LR: 8.309e-04  Data: 0.018 (0.037)
Train: 81 [ 250/1251 ( 20%)]  Loss: 3.645 (3.81)  Time: 0.310s, 3303.74/s  (0.314s, 3258.01/s)  LR: 8.307e-04  Data: 0.022 (0.034)
Train: 81 [ 300/1251 ( 24%)]  Loss: 3.782 (3.81)  Time: 0.309s, 3315.25/s  (0.313s, 3267.03/s)  LR: 8.306e-04  Data: 0.025 (0.032)
Train: 81 [ 350/1251 ( 28%)]  Loss: 3.588 (3.78)  Time: 0.311s, 3289.59/s  (0.313s, 3273.06/s)  LR: 8.304e-04  Data: 0.022 (0.031)
Train: 81 [ 400/1251 ( 32%)]  Loss: 3.456 (3.74)  Time: 0.310s, 3306.61/s  (0.312s, 3277.25/s)  LR: 8.302e-04  Data: 0.021 (0.030)
Train: 81 [ 450/1251 ( 36%)]  Loss: 3.964 (3.77)  Time: 0.311s, 3294.38/s  (0.312s, 3281.16/s)  LR: 8.301e-04  Data: 0.021 (0.029)
Train: 81 [ 500/1251 ( 40%)]  Loss: 4.018 (3.79)  Time: 0.307s, 3331.45/s  (0.312s, 3283.62/s)  LR: 8.299e-04  Data: 0.029 (0.028)
Train: 81 [ 550/1251 ( 44%)]  Loss: 3.778 (3.79)  Time: 0.313s, 3272.55/s  (0.312s, 3285.04/s)  LR: 8.298e-04  Data: 0.025 (0.028)
Train: 81 [ 600/1251 ( 48%)]  Loss: 3.599 (3.77)  Time: 0.313s, 3270.96/s  (0.312s, 3287.01/s)  LR: 8.296e-04  Data: 0.022 (0.028)
Train: 81 [ 650/1251 ( 52%)]  Loss: 4.145 (3.80)  Time: 0.311s, 3292.23/s  (0.311s, 3288.73/s)  LR: 8.295e-04  Data: 0.025 (0.027)
Train: 81 [ 700/1251 ( 56%)]  Loss: 3.679 (3.79)  Time: 0.307s, 3338.18/s  (0.311s, 3289.56/s)  LR: 8.293e-04  Data: 0.021 (0.027)
Train: 81 [ 750/1251 ( 60%)]  Loss: 3.955 (3.80)  Time: 0.317s, 3229.21/s  (0.311s, 3290.02/s)  LR: 8.292e-04  Data: 0.026 (0.027)
Train: 81 [ 800/1251 ( 64%)]  Loss: 3.322 (3.77)  Time: 0.311s, 3291.42/s  (0.311s, 3290.08/s)  LR: 8.290e-04  Data: 0.021 (0.026)
Train: 81 [ 850/1251 ( 68%)]  Loss: 3.657 (3.77)  Time: 0.308s, 3327.79/s  (0.311s, 3290.91/s)  LR: 8.288e-04  Data: 0.021 (0.026)
Train: 81 [ 900/1251 ( 72%)]  Loss: 3.745 (3.77)  Time: 0.313s, 3276.32/s  (0.311s, 3291.17/s)  LR: 8.287e-04  Data: 0.021 (0.026)
Train: 81 [ 950/1251 ( 76%)]  Loss: 3.894 (3.77)  Time: 0.308s, 3322.89/s  (0.311s, 3291.93/s)  LR: 8.285e-04  Data: 0.021 (0.026)
Train: 81 [1000/1251 ( 80%)]  Loss: 3.904 (3.78)  Time: 0.308s, 3328.48/s  (0.311s, 3292.46/s)  LR: 8.284e-04  Data: 0.022 (0.026)
Train: 81 [1050/1251 ( 84%)]  Loss: 3.319 (3.76)  Time: 0.307s, 3338.81/s  (0.311s, 3292.51/s)  LR: 8.282e-04  Data: 0.021 (0.026)
Train: 81 [1100/1251 ( 88%)]  Loss: 3.731 (3.76)  Time: 0.315s, 3253.01/s  (0.311s, 3293.13/s)  LR: 8.280e-04  Data: 0.022 (0.025)
Train: 81 [1150/1251 ( 92%)]  Loss: 3.702 (3.75)  Time: 0.314s, 3262.78/s  (0.311s, 3293.14/s)  LR: 8.279e-04  Data: 0.022 (0.025)
Train: 81 [1200/1251 ( 96%)]  Loss: 3.631 (3.75)  Time: 0.313s, 3269.52/s  (0.311s, 3293.03/s)  LR: 8.277e-04  Data: 0.022 (0.025)
Train: 81 [1250/1251 (100%)]  Loss: 3.921 (3.76)  Time: 0.283s, 3621.04/s  (0.311s, 3294.95/s)  LR: 8.276e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.040 (2.040)  Loss:  0.7095 (0.7095)  Acc@1: 85.3516 (85.3516)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.060 (0.236)  Loss:  0.7920 (1.3433)  Acc@1: 82.7830 (69.8480)  Acc@5: 95.0472 (89.6020)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-81.pth.tar', 69.84800001953126)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-78.pth.tar', 69.83399989013672)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-80.pth.tar', 69.81599994384766)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-79.pth.tar', 69.77799996826172)

Train: 82 [   0/1251 (  0%)]  Loss: 3.570 (3.57)  Time: 2.001s,  511.82/s  (2.001s,  511.82/s)  LR: 8.276e-04  Data: 1.757 (1.757)
Train: 82 [  50/1251 (  4%)]  Loss: 3.875 (3.72)  Time: 0.304s, 3371.06/s  (0.334s, 3067.37/s)  LR: 8.274e-04  Data: 0.023 (0.062)
Train: 82 [ 100/1251 (  8%)]  Loss: 3.956 (3.80)  Time: 0.304s, 3369.73/s  (0.320s, 3202.06/s)  LR: 8.273e-04  Data: 0.020 (0.043)
Train: 82 [ 150/1251 ( 12%)]  Loss: 3.881 (3.82)  Time: 0.305s, 3361.93/s  (0.316s, 3243.88/s)  LR: 8.271e-04  Data: 0.019 (0.036)
Train: 82 [ 200/1251 ( 16%)]  Loss: 4.038 (3.86)  Time: 0.311s, 3296.47/s  (0.314s, 3262.31/s)  LR: 8.269e-04  Data: 0.028 (0.033)
Train: 82 [ 250/1251 ( 20%)]  Loss: 3.705 (3.84)  Time: 0.310s, 3306.49/s  (0.313s, 3273.52/s)  LR: 8.268e-04  Data: 0.024 (0.031)
Train: 82 [ 300/1251 ( 24%)]  Loss: 3.607 (3.80)  Time: 0.316s, 3241.55/s  (0.312s, 3280.20/s)  LR: 8.266e-04  Data: 0.025 (0.030)
Train: 82 [ 350/1251 ( 28%)]  Loss: 4.220 (3.86)  Time: 0.312s, 3278.30/s  (0.312s, 3284.74/s)  LR: 8.265e-04  Data: 0.023 (0.029)
Train: 82 [ 400/1251 ( 32%)]  Loss: 4.053 (3.88)  Time: 0.306s, 3342.23/s  (0.312s, 3286.50/s)  LR: 8.263e-04  Data: 0.024 (0.028)
Train: 82 [ 450/1251 ( 36%)]  Loss: 3.978 (3.89)  Time: 0.312s, 3279.86/s  (0.311s, 3287.51/s)  LR: 8.262e-04  Data: 0.020 (0.027)
Train: 82 [ 500/1251 ( 40%)]  Loss: 3.944 (3.89)  Time: 0.315s, 3254.70/s  (0.311s, 3288.39/s)  LR: 8.260e-04  Data: 0.024 (0.027)
Train: 82 [ 550/1251 ( 44%)]  Loss: 3.497 (3.86)  Time: 0.317s, 3230.40/s  (0.311s, 3287.83/s)  LR: 8.258e-04  Data: 0.023 (0.027)
Train: 82 [ 600/1251 ( 48%)]  Loss: 3.589 (3.84)  Time: 0.310s, 3298.45/s  (0.311s, 3288.32/s)  LR: 8.257e-04  Data: 0.022 (0.026)
Train: 82 [ 650/1251 ( 52%)]  Loss: 3.853 (3.84)  Time: 0.321s, 3192.96/s  (0.311s, 3288.96/s)  LR: 8.255e-04  Data: 0.028 (0.026)
Train: 82 [ 700/1251 ( 56%)]  Loss: 3.993 (3.85)  Time: 0.315s, 3250.87/s  (0.311s, 3290.07/s)  LR: 8.254e-04  Data: 0.023 (0.026)
Train: 82 [ 750/1251 ( 60%)]  Loss: 3.621 (3.84)  Time: 0.313s, 3273.94/s  (0.311s, 3290.19/s)  LR: 8.252e-04  Data: 0.021 (0.026)
Train: 82 [ 800/1251 ( 64%)]  Loss: 3.591 (3.82)  Time: 0.308s, 3329.91/s  (0.311s, 3291.15/s)  LR: 8.250e-04  Data: 0.024 (0.025)
Train: 82 [ 850/1251 ( 68%)]  Loss: 3.752 (3.82)  Time: 0.302s, 3387.59/s  (0.311s, 3291.52/s)  LR: 8.249e-04  Data: 0.019 (0.025)
Train: 82 [ 900/1251 ( 72%)]  Loss: 4.187 (3.84)  Time: 0.315s, 3246.69/s  (0.311s, 3291.92/s)  LR: 8.247e-04  Data: 0.022 (0.025)
Train: 82 [ 950/1251 ( 76%)]  Loss: 3.927 (3.84)  Time: 0.308s, 3323.24/s  (0.311s, 3292.24/s)  LR: 8.246e-04  Data: 0.024 (0.025)
Train: 82 [1000/1251 ( 80%)]  Loss: 3.820 (3.84)  Time: 0.307s, 3331.76/s  (0.311s, 3292.72/s)  LR: 8.244e-04  Data: 0.023 (0.025)
Train: 82 [1050/1251 ( 84%)]  Loss: 3.223 (3.81)  Time: 0.310s, 3300.40/s  (0.311s, 3292.93/s)  LR: 8.243e-04  Data: 0.021 (0.025)
Train: 82 [1100/1251 ( 88%)]  Loss: 3.471 (3.80)  Time: 0.310s, 3305.65/s  (0.311s, 3293.52/s)  LR: 8.241e-04  Data: 0.022 (0.025)
Train: 82 [1150/1251 ( 92%)]  Loss: 3.867 (3.80)  Time: 0.318s, 3216.82/s  (0.311s, 3293.17/s)  LR: 8.239e-04  Data: 0.022 (0.025)
Train: 82 [1200/1251 ( 96%)]  Loss: 3.899 (3.80)  Time: 0.309s, 3310.26/s  (0.311s, 3292.61/s)  LR: 8.238e-04  Data: 0.018 (0.025)
Train: 82 [1250/1251 (100%)]  Loss: 3.966 (3.81)  Time: 0.291s, 3518.42/s  (0.311s, 3294.46/s)  LR: 8.236e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.071 (2.071)  Loss:  0.7197 (0.7197)  Acc@1: 85.1562 (85.1562)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.041 (0.233)  Loss:  0.8330 (1.3313)  Acc@1: 81.6038 (69.9800)  Acc@5: 93.8679 (89.7180)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-82.pth.tar', 69.97999989501953)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-81.pth.tar', 69.84800001953126)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-78.pth.tar', 69.83399989013672)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-80.pth.tar', 69.81599994384766)

Train: 83 [   0/1251 (  0%)]  Loss: 3.278 (3.28)  Time: 2.448s,  418.37/s  (2.448s,  418.37/s)  LR: 8.236e-04  Data: 2.222 (2.222)
Train: 83 [  50/1251 (  4%)]  Loss: 3.738 (3.51)  Time: 0.297s, 3452.01/s  (0.332s, 3085.48/s)  LR: 8.235e-04  Data: 0.022 (0.066)
Train: 83 [ 100/1251 (  8%)]  Loss: 3.191 (3.40)  Time: 0.309s, 3316.57/s  (0.318s, 3219.96/s)  LR: 8.233e-04  Data: 0.024 (0.045)
Train: 83 [ 150/1251 ( 12%)]  Loss: 3.980 (3.55)  Time: 0.308s, 3323.40/s  (0.314s, 3257.38/s)  LR: 8.231e-04  Data: 0.024 (0.037)
Train: 83 [ 200/1251 ( 16%)]  Loss: 3.847 (3.61)  Time: 0.310s, 3302.73/s  (0.313s, 3273.62/s)  LR: 8.230e-04  Data: 0.021 (0.034)
Train: 83 [ 250/1251 ( 20%)]  Loss: 4.060 (3.68)  Time: 0.313s, 3272.66/s  (0.312s, 3281.65/s)  LR: 8.228e-04  Data: 0.020 (0.032)
Train: 83 [ 300/1251 ( 24%)]  Loss: 3.642 (3.68)  Time: 0.308s, 3320.49/s  (0.312s, 3286.32/s)  LR: 8.227e-04  Data: 0.023 (0.030)
Train: 83 [ 350/1251 ( 28%)]  Loss: 4.087 (3.73)  Time: 0.309s, 3319.21/s  (0.311s, 3290.39/s)  LR: 8.225e-04  Data: 0.021 (0.029)
Train: 83 [ 400/1251 ( 32%)]  Loss: 3.676 (3.72)  Time: 0.309s, 3311.12/s  (0.311s, 3292.16/s)  LR: 8.223e-04  Data: 0.024 (0.028)
Train: 83 [ 450/1251 ( 36%)]  Loss: 4.224 (3.77)  Time: 0.311s, 3290.90/s  (0.311s, 3293.69/s)  LR: 8.222e-04  Data: 0.020 (0.028)
Train: 83 [ 500/1251 ( 40%)]  Loss: 3.931 (3.79)  Time: 0.310s, 3303.08/s  (0.311s, 3294.47/s)  LR: 8.220e-04  Data: 0.025 (0.027)
Train: 83 [ 550/1251 ( 44%)]  Loss: 4.098 (3.81)  Time: 0.312s, 3285.67/s  (0.311s, 3295.65/s)  LR: 8.219e-04  Data: 0.024 (0.027)
Train: 83 [ 600/1251 ( 48%)]  Loss: 3.663 (3.80)  Time: 0.307s, 3340.20/s  (0.311s, 3296.20/s)  LR: 8.217e-04  Data: 0.023 (0.026)
Train: 83 [ 650/1251 ( 52%)]  Loss: 3.862 (3.81)  Time: 0.311s, 3296.99/s  (0.311s, 3296.56/s)  LR: 8.215e-04  Data: 0.023 (0.026)
Train: 83 [ 700/1251 ( 56%)]  Loss: 3.715 (3.80)  Time: 0.312s, 3284.33/s  (0.311s, 3297.19/s)  LR: 8.214e-04  Data: 0.025 (0.026)
Train: 83 [ 750/1251 ( 60%)]  Loss: 3.975 (3.81)  Time: 0.309s, 3313.37/s  (0.311s, 3297.75/s)  LR: 8.212e-04  Data: 0.023 (0.026)
Train: 83 [ 800/1251 ( 64%)]  Loss: 3.808 (3.81)  Time: 0.309s, 3308.95/s  (0.310s, 3298.17/s)  LR: 8.211e-04  Data: 0.023 (0.026)
Train: 83 [ 850/1251 ( 68%)]  Loss: 4.051 (3.82)  Time: 0.309s, 3317.63/s  (0.310s, 3298.57/s)  LR: 8.209e-04  Data: 0.026 (0.025)
Train: 83 [ 900/1251 ( 72%)]  Loss: 3.979 (3.83)  Time: 0.312s, 3285.40/s  (0.310s, 3298.88/s)  LR: 8.207e-04  Data: 0.026 (0.025)
Train: 83 [ 950/1251 ( 76%)]  Loss: 3.724 (3.83)  Time: 0.307s, 3337.69/s  (0.310s, 3299.12/s)  LR: 8.206e-04  Data: 0.020 (0.025)
Train: 83 [1000/1251 ( 80%)]  Loss: 3.884 (3.83)  Time: 0.312s, 3284.27/s  (0.310s, 3299.00/s)  LR: 8.204e-04  Data: 0.026 (0.025)
Train: 83 [1050/1251 ( 84%)]  Loss: 4.049 (3.84)  Time: 0.305s, 3358.27/s  (0.310s, 3298.91/s)  LR: 8.203e-04  Data: 0.021 (0.025)
Train: 83 [1100/1251 ( 88%)]  Loss: 3.822 (3.84)  Time: 0.310s, 3300.24/s  (0.310s, 3299.45/s)  LR: 8.201e-04  Data: 0.021 (0.025)
Train: 83 [1150/1251 ( 92%)]  Loss: 3.893 (3.84)  Time: 0.311s, 3289.72/s  (0.310s, 3299.31/s)  LR: 8.199e-04  Data: 0.024 (0.025)
Train: 83 [1200/1251 ( 96%)]  Loss: 3.963 (3.85)  Time: 0.306s, 3346.93/s  (0.310s, 3299.35/s)  LR: 8.198e-04  Data: 0.022 (0.025)
Train: 83 [1250/1251 (100%)]  Loss: 3.885 (3.85)  Time: 0.283s, 3618.82/s  (0.310s, 3301.14/s)  LR: 8.196e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.060 (2.060)  Loss:  0.7339 (0.7339)  Acc@1: 83.9844 (83.9844)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.044 (0.245)  Loss:  0.7563 (1.3439)  Acc@1: 83.9623 (70.1540)  Acc@5: 96.4623 (89.7060)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-83.pth.tar', 70.15400001464843)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-82.pth.tar', 69.97999989501953)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-81.pth.tar', 69.84800001953126)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-78.pth.tar', 69.83399989013672)

Train: 84 [   0/1251 (  0%)]  Loss: 3.696 (3.70)  Time: 2.277s,  449.69/s  (2.277s,  449.69/s)  LR: 8.196e-04  Data: 2.056 (2.056)
Train: 84 [  50/1251 (  4%)]  Loss: 3.797 (3.75)  Time: 0.298s, 3437.86/s  (0.334s, 3068.96/s)  LR: 8.195e-04  Data: 0.021 (0.063)
Train: 84 [ 100/1251 (  8%)]  Loss: 3.753 (3.75)  Time: 0.311s, 3294.77/s  (0.319s, 3206.80/s)  LR: 8.193e-04  Data: 0.022 (0.043)
Train: 84 [ 150/1251 ( 12%)]  Loss: 3.453 (3.67)  Time: 0.309s, 3318.98/s  (0.315s, 3251.25/s)  LR: 8.191e-04  Data: 0.022 (0.036)
Train: 84 [ 200/1251 ( 16%)]  Loss: 3.878 (3.72)  Time: 0.303s, 3376.71/s  (0.313s, 3270.78/s)  LR: 8.190e-04  Data: 0.026 (0.033)
Train: 84 [ 250/1251 ( 20%)]  Loss: 3.852 (3.74)  Time: 0.309s, 3319.11/s  (0.312s, 3282.36/s)  LR: 8.188e-04  Data: 0.021 (0.031)
Train: 84 [ 300/1251 ( 24%)]  Loss: 3.903 (3.76)  Time: 0.307s, 3337.00/s  (0.311s, 3287.99/s)  LR: 8.187e-04  Data: 0.025 (0.030)
Train: 84 [ 350/1251 ( 28%)]  Loss: 3.787 (3.76)  Time: 0.307s, 3332.80/s  (0.311s, 3291.38/s)  LR: 8.185e-04  Data: 0.022 (0.029)
Train: 84 [ 400/1251 ( 32%)]  Loss: 4.109 (3.80)  Time: 0.310s, 3303.27/s  (0.311s, 3295.48/s)  LR: 8.183e-04  Data: 0.021 (0.028)
Train: 84 [ 450/1251 ( 36%)]  Loss: 3.566 (3.78)  Time: 0.309s, 3310.05/s  (0.311s, 3297.86/s)  LR: 8.182e-04  Data: 0.021 (0.027)
Train: 84 [ 500/1251 ( 40%)]  Loss: 3.857 (3.79)  Time: 0.314s, 3256.12/s  (0.310s, 3299.91/s)  LR: 8.180e-04  Data: 0.027 (0.027)
Train: 84 [ 550/1251 ( 44%)]  Loss: 3.600 (3.77)  Time: 0.312s, 3283.59/s  (0.310s, 3301.06/s)  LR: 8.178e-04  Data: 0.026 (0.026)
Train: 84 [ 600/1251 ( 48%)]  Loss: 3.889 (3.78)  Time: 0.307s, 3331.07/s  (0.310s, 3301.88/s)  LR: 8.177e-04  Data: 0.022 (0.026)
Train: 84 [ 650/1251 ( 52%)]  Loss: 3.646 (3.77)  Time: 0.303s, 3383.92/s  (0.310s, 3302.33/s)  LR: 8.175e-04  Data: 0.022 (0.026)
Train: 84 [ 700/1251 ( 56%)]  Loss: 3.831 (3.77)  Time: 0.309s, 3315.31/s  (0.310s, 3302.89/s)  LR: 8.174e-04  Data: 0.022 (0.026)
Train: 84 [ 750/1251 ( 60%)]  Loss: 4.126 (3.80)  Time: 0.311s, 3297.74/s  (0.310s, 3303.09/s)  LR: 8.172e-04  Data: 0.023 (0.026)
Train: 84 [ 800/1251 ( 64%)]  Loss: 3.942 (3.81)  Time: 0.305s, 3355.35/s  (0.310s, 3303.69/s)  LR: 8.170e-04  Data: 0.025 (0.025)
Train: 84 [ 850/1251 ( 68%)]  Loss: 3.557 (3.79)  Time: 0.311s, 3297.72/s  (0.310s, 3304.23/s)  LR: 8.169e-04  Data: 0.025 (0.025)
Train: 84 [ 900/1251 ( 72%)]  Loss: 4.004 (3.80)  Time: 0.308s, 3326.47/s  (0.310s, 3304.78/s)  LR: 8.167e-04  Data: 0.020 (0.025)
Train: 84 [ 950/1251 ( 76%)]  Loss: 3.618 (3.79)  Time: 0.310s, 3301.14/s  (0.310s, 3305.26/s)  LR: 8.166e-04  Data: 0.023 (0.025)
Train: 84 [1000/1251 ( 80%)]  Loss: 3.865 (3.80)  Time: 0.306s, 3346.89/s  (0.310s, 3305.53/s)  LR: 8.164e-04  Data: 0.024 (0.025)
Train: 84 [1050/1251 ( 84%)]  Loss: 3.716 (3.79)  Time: 0.311s, 3296.99/s  (0.310s, 3305.81/s)  LR: 8.162e-04  Data: 0.022 (0.025)
Train: 84 [1100/1251 ( 88%)]  Loss: 3.678 (3.79)  Time: 0.312s, 3278.76/s  (0.310s, 3305.66/s)  LR: 8.161e-04  Data: 0.022 (0.025)
Train: 84 [1150/1251 ( 92%)]  Loss: 3.796 (3.79)  Time: 0.315s, 3254.63/s  (0.310s, 3305.98/s)  LR: 8.159e-04  Data: 0.024 (0.025)
Train: 84 [1200/1251 ( 96%)]  Loss: 3.990 (3.80)  Time: 0.309s, 3318.65/s  (0.310s, 3306.31/s)  LR: 8.157e-04  Data: 0.021 (0.025)
Train: 84 [1250/1251 (100%)]  Loss: 3.898 (3.80)  Time: 0.279s, 3668.32/s  (0.310s, 3308.54/s)  LR: 8.156e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.158 (2.158)  Loss:  0.7227 (0.7227)  Acc@1: 84.7656 (84.7656)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.065 (0.237)  Loss:  0.7930 (1.3244)  Acc@1: 81.6038 (70.1240)  Acc@5: 95.5189 (89.8640)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-83.pth.tar', 70.15400001464843)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-84.pth.tar', 70.12400002441406)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-82.pth.tar', 69.97999989501953)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-81.pth.tar', 69.84800001953126)

Train: 85 [   0/1251 (  0%)]  Loss: 4.081 (4.08)  Time: 2.450s,  417.94/s  (2.450s,  417.94/s)  LR: 8.156e-04  Data: 2.223 (2.223)
Train: 85 [  50/1251 (  4%)]  Loss: 3.685 (3.88)  Time: 0.301s, 3405.47/s  (0.333s, 3072.56/s)  LR: 8.154e-04  Data: 0.023 (0.066)
Train: 85 [ 100/1251 (  8%)]  Loss: 4.024 (3.93)  Time: 0.305s, 3352.43/s  (0.318s, 3218.74/s)  LR: 8.153e-04  Data: 0.021 (0.045)
Train: 85 [ 150/1251 ( 12%)]  Loss: 3.953 (3.94)  Time: 0.309s, 3310.53/s  (0.314s, 3258.16/s)  LR: 8.151e-04  Data: 0.025 (0.038)
Train: 85 [ 200/1251 ( 16%)]  Loss: 3.748 (3.90)  Time: 0.305s, 3359.89/s  (0.312s, 3278.49/s)  LR: 8.149e-04  Data: 0.024 (0.034)
Train: 85 [ 250/1251 ( 20%)]  Loss: 3.771 (3.88)  Time: 0.306s, 3350.06/s  (0.311s, 3288.29/s)  LR: 8.148e-04  Data: 0.024 (0.032)
Train: 85 [ 300/1251 ( 24%)]  Loss: 3.885 (3.88)  Time: 0.308s, 3321.98/s  (0.311s, 3296.99/s)  LR: 8.146e-04  Data: 0.024 (0.030)
Train: 85 [ 350/1251 ( 28%)]  Loss: 3.796 (3.87)  Time: 0.310s, 3307.09/s  (0.310s, 3301.36/s)  LR: 8.144e-04  Data: 0.023 (0.029)
Train: 85 [ 400/1251 ( 32%)]  Loss: 3.683 (3.85)  Time: 0.308s, 3321.25/s  (0.310s, 3305.14/s)  LR: 8.143e-04  Data: 0.026 (0.029)
Train: 85 [ 450/1251 ( 36%)]  Loss: 3.223 (3.78)  Time: 0.307s, 3332.57/s  (0.310s, 3308.41/s)  LR: 8.141e-04  Data: 0.020 (0.028)
Train: 85 [ 500/1251 ( 40%)]  Loss: 3.766 (3.78)  Time: 0.306s, 3351.74/s  (0.309s, 3310.34/s)  LR: 8.140e-04  Data: 0.022 (0.027)
Train: 85 [ 550/1251 ( 44%)]  Loss: 3.832 (3.79)  Time: 0.307s, 3333.22/s  (0.309s, 3310.62/s)  LR: 8.138e-04  Data: 0.021 (0.027)
Train: 85 [ 600/1251 ( 48%)]  Loss: 3.857 (3.79)  Time: 0.307s, 3333.42/s  (0.309s, 3311.27/s)  LR: 8.136e-04  Data: 0.023 (0.027)
Train: 85 [ 650/1251 ( 52%)]  Loss: 3.836 (3.80)  Time: 0.307s, 3335.65/s  (0.309s, 3312.40/s)  LR: 8.135e-04  Data: 0.020 (0.026)
Train: 85 [ 700/1251 ( 56%)]  Loss: 3.646 (3.79)  Time: 0.311s, 3291.96/s  (0.309s, 3312.79/s)  LR: 8.133e-04  Data: 0.023 (0.026)
Train: 85 [ 750/1251 ( 60%)]  Loss: 3.838 (3.79)  Time: 0.307s, 3338.28/s  (0.309s, 3313.22/s)  LR: 8.132e-04  Data: 0.023 (0.026)
Train: 85 [ 800/1251 ( 64%)]  Loss: 3.565 (3.78)  Time: 0.310s, 3304.85/s  (0.309s, 3314.07/s)  LR: 8.130e-04  Data: 0.022 (0.026)
Train: 85 [ 850/1251 ( 68%)]  Loss: 3.648 (3.77)  Time: 0.310s, 3301.53/s  (0.309s, 3314.18/s)  LR: 8.128e-04  Data: 0.020 (0.026)
Train: 85 [ 900/1251 ( 72%)]  Loss: 3.876 (3.77)  Time: 0.312s, 3285.61/s  (0.309s, 3314.52/s)  LR: 8.127e-04  Data: 0.024 (0.025)
Train: 85 [ 950/1251 ( 76%)]  Loss: 4.212 (3.80)  Time: 0.312s, 3286.85/s  (0.309s, 3314.21/s)  LR: 8.125e-04  Data: 0.026 (0.025)
Train: 85 [1000/1251 ( 80%)]  Loss: 3.803 (3.80)  Time: 0.316s, 3243.03/s  (0.309s, 3314.28/s)  LR: 8.123e-04  Data: 0.029 (0.025)
Train: 85 [1050/1251 ( 84%)]  Loss: 3.878 (3.80)  Time: 0.310s, 3304.23/s  (0.309s, 3314.21/s)  LR: 8.122e-04  Data: 0.022 (0.025)
Train: 85 [1100/1251 ( 88%)]  Loss: 3.913 (3.81)  Time: 0.307s, 3338.54/s  (0.309s, 3314.40/s)  LR: 8.120e-04  Data: 0.023 (0.025)
Train: 85 [1150/1251 ( 92%)]  Loss: 4.084 (3.82)  Time: 0.310s, 3304.57/s  (0.309s, 3314.51/s)  LR: 8.118e-04  Data: 0.026 (0.025)
Train: 85 [1200/1251 ( 96%)]  Loss: 3.798 (3.82)  Time: 0.316s, 3239.37/s  (0.309s, 3314.50/s)  LR: 8.117e-04  Data: 0.022 (0.025)
Train: 85 [1250/1251 (100%)]  Loss: 3.782 (3.81)  Time: 0.276s, 3716.84/s  (0.309s, 3316.63/s)  LR: 8.115e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.059 (2.059)  Loss:  0.6895 (0.6895)  Acc@1: 86.3281 (86.3281)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.063 (0.238)  Loss:  0.7856 (1.3338)  Acc@1: 83.1368 (70.2380)  Acc@5: 95.8726 (89.9360)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-85.pth.tar', 70.2380000439453)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-83.pth.tar', 70.15400001464843)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-84.pth.tar', 70.12400002441406)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-82.pth.tar', 69.97999989501953)

Train: 86 [   0/1251 (  0%)]  Loss: 3.696 (3.70)  Time: 2.083s,  491.55/s  (2.083s,  491.55/s)  LR: 8.115e-04  Data: 1.864 (1.864)
Train: 86 [  50/1251 (  4%)]  Loss: 3.982 (3.84)  Time: 0.303s, 3378.88/s  (0.329s, 3108.85/s)  LR: 8.114e-04  Data: 0.023 (0.060)
Train: 86 [ 100/1251 (  8%)]  Loss: 3.818 (3.83)  Time: 0.305s, 3356.69/s  (0.316s, 3243.45/s)  LR: 8.112e-04  Data: 0.022 (0.042)
Train: 86 [ 150/1251 ( 12%)]  Loss: 3.532 (3.76)  Time: 0.298s, 3433.09/s  (0.312s, 3284.28/s)  LR: 8.110e-04  Data: 0.021 (0.035)
Train: 86 [ 200/1251 ( 16%)]  Loss: 3.745 (3.75)  Time: 0.306s, 3343.72/s  (0.310s, 3302.81/s)  LR: 8.109e-04  Data: 0.022 (0.032)
Train: 86 [ 250/1251 ( 20%)]  Loss: 3.666 (3.74)  Time: 0.304s, 3363.29/s  (0.309s, 3312.73/s)  LR: 8.107e-04  Data: 0.021 (0.030)
Train: 86 [ 300/1251 ( 24%)]  Loss: 3.827 (3.75)  Time: 0.313s, 3276.59/s  (0.308s, 3319.97/s)  LR: 8.105e-04  Data: 0.022 (0.029)
Train: 86 [ 350/1251 ( 28%)]  Loss: 3.840 (3.76)  Time: 0.310s, 3307.36/s  (0.308s, 3324.46/s)  LR: 8.104e-04  Data: 0.023 (0.028)
Train: 86 [ 400/1251 ( 32%)]  Loss: 3.928 (3.78)  Time: 0.307s, 3340.52/s  (0.308s, 3328.35/s)  LR: 8.102e-04  Data: 0.022 (0.028)
Train: 86 [ 450/1251 ( 36%)]  Loss: 3.732 (3.78)  Time: 0.309s, 3309.86/s  (0.307s, 3330.78/s)  LR: 8.100e-04  Data: 0.025 (0.027)
Train: 86 [ 500/1251 ( 40%)]  Loss: 3.856 (3.78)  Time: 0.304s, 3364.52/s  (0.307s, 3331.81/s)  LR: 8.099e-04  Data: 0.021 (0.027)
Train: 86 [ 550/1251 ( 44%)]  Loss: 3.762 (3.78)  Time: 0.305s, 3360.43/s  (0.307s, 3332.80/s)  LR: 8.097e-04  Data: 0.028 (0.026)
Train: 86 [ 600/1251 ( 48%)]  Loss: 3.714 (3.78)  Time: 0.306s, 3350.74/s  (0.307s, 3333.02/s)  LR: 8.096e-04  Data: 0.020 (0.026)
Train: 86 [ 650/1251 ( 52%)]  Loss: 3.793 (3.78)  Time: 0.306s, 3345.18/s  (0.307s, 3334.84/s)  LR: 8.094e-04  Data: 0.024 (0.026)
Train: 86 [ 700/1251 ( 56%)]  Loss: 3.789 (3.78)  Time: 0.306s, 3348.60/s  (0.307s, 3335.77/s)  LR: 8.092e-04  Data: 0.022 (0.026)
Train: 86 [ 750/1251 ( 60%)]  Loss: 3.727 (3.78)  Time: 0.308s, 3323.58/s  (0.307s, 3336.45/s)  LR: 8.091e-04  Data: 0.025 (0.026)
Train: 86 [ 800/1251 ( 64%)]  Loss: 4.148 (3.80)  Time: 0.309s, 3313.57/s  (0.307s, 3337.18/s)  LR: 8.089e-04  Data: 0.024 (0.025)
Train: 86 [ 850/1251 ( 68%)]  Loss: 3.974 (3.81)  Time: 0.306s, 3350.26/s  (0.307s, 3337.75/s)  LR: 8.087e-04  Data: 0.023 (0.025)
Train: 86 [ 900/1251 ( 72%)]  Loss: 3.716 (3.80)  Time: 0.303s, 3377.07/s  (0.307s, 3338.11/s)  LR: 8.086e-04  Data: 0.019 (0.025)
Train: 86 [ 950/1251 ( 76%)]  Loss: 3.415 (3.78)  Time: 0.303s, 3376.09/s  (0.307s, 3338.89/s)  LR: 8.084e-04  Data: 0.022 (0.025)
Train: 86 [1000/1251 ( 80%)]  Loss: 3.796 (3.78)  Time: 0.307s, 3331.64/s  (0.307s, 3339.78/s)  LR: 8.082e-04  Data: 0.017 (0.025)
Train: 86 [1050/1251 ( 84%)]  Loss: 4.040 (3.80)  Time: 0.307s, 3334.74/s  (0.307s, 3340.09/s)  LR: 8.081e-04  Data: 0.024 (0.025)
Train: 86 [1100/1251 ( 88%)]  Loss: 3.600 (3.79)  Time: 0.312s, 3285.14/s  (0.307s, 3340.70/s)  LR: 8.079e-04  Data: 0.022 (0.025)
Train: 86 [1150/1251 ( 92%)]  Loss: 3.951 (3.79)  Time: 0.307s, 3333.88/s  (0.306s, 3341.13/s)  LR: 8.078e-04  Data: 0.018 (0.025)
Train: 86 [1200/1251 ( 96%)]  Loss: 4.082 (3.81)  Time: 0.304s, 3369.60/s  (0.306s, 3341.95/s)  LR: 8.076e-04  Data: 0.023 (0.025)
Train: 86 [1250/1251 (100%)]  Loss: 3.355 (3.79)  Time: 0.276s, 3707.70/s  (0.306s, 3344.19/s)  LR: 8.074e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.062 (2.062)  Loss:  0.6616 (0.6616)  Acc@1: 87.2070 (87.2070)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.057 (0.237)  Loss:  0.7964 (1.3008)  Acc@1: 82.5472 (70.7880)  Acc@5: 95.1651 (90.0220)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-86.pth.tar', 70.78800017578125)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-85.pth.tar', 70.2380000439453)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-83.pth.tar', 70.15400001464843)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-84.pth.tar', 70.12400002441406)

Train: 87 [   0/1251 (  0%)]  Loss: 4.005 (4.00)  Time: 2.217s,  461.85/s  (2.217s,  461.85/s)  LR: 8.074e-04  Data: 1.990 (1.990)
Train: 87 [  50/1251 (  4%)]  Loss: 3.789 (3.90)  Time: 0.292s, 3508.04/s  (0.320s, 3197.97/s)  LR: 8.073e-04  Data: 0.021 (0.063)
Train: 87 [ 100/1251 (  8%)]  Loss: 3.962 (3.92)  Time: 0.300s, 3414.72/s  (0.309s, 3315.79/s)  LR: 8.071e-04  Data: 0.023 (0.043)
Train: 87 [ 150/1251 ( 12%)]  Loss: 3.579 (3.83)  Time: 0.298s, 3432.66/s  (0.306s, 3348.01/s)  LR: 8.069e-04  Data: 0.023 (0.037)
Train: 87 [ 200/1251 ( 16%)]  Loss: 3.992 (3.87)  Time: 0.298s, 3430.92/s  (0.305s, 3362.20/s)  LR: 8.068e-04  Data: 0.025 (0.033)
Train: 87 [ 250/1251 ( 20%)]  Loss: 3.790 (3.85)  Time: 0.301s, 3400.70/s  (0.304s, 3367.80/s)  LR: 8.066e-04  Data: 0.023 (0.031)
Train: 87 [ 300/1251 ( 24%)]  Loss: 3.849 (3.85)  Time: 0.305s, 3352.89/s  (0.304s, 3371.33/s)  LR: 8.064e-04  Data: 0.022 (0.030)
Train: 87 [ 350/1251 ( 28%)]  Loss: 3.588 (3.82)  Time: 0.307s, 3339.77/s  (0.304s, 3373.89/s)  LR: 8.063e-04  Data: 0.020 (0.029)
Train: 87 [ 400/1251 ( 32%)]  Loss: 3.874 (3.83)  Time: 0.304s, 3367.07/s  (0.303s, 3375.11/s)  LR: 8.061e-04  Data: 0.025 (0.028)
Train: 87 [ 450/1251 ( 36%)]  Loss: 3.680 (3.81)  Time: 0.297s, 3452.21/s  (0.303s, 3376.93/s)  LR: 8.059e-04  Data: 0.019 (0.028)
Train: 87 [ 500/1251 ( 40%)]  Loss: 3.348 (3.77)  Time: 0.300s, 3407.86/s  (0.303s, 3377.89/s)  LR: 8.058e-04  Data: 0.022 (0.027)
Train: 87 [ 550/1251 ( 44%)]  Loss: 3.569 (3.75)  Time: 0.304s, 3367.17/s  (0.303s, 3377.78/s)  LR: 8.056e-04  Data: 0.022 (0.027)
Train: 87 [ 600/1251 ( 48%)]  Loss: 3.408 (3.73)  Time: 0.301s, 3404.42/s  (0.303s, 3377.95/s)  LR: 8.054e-04  Data: 0.021 (0.026)
Train: 87 [ 650/1251 ( 52%)]  Loss: 3.874 (3.74)  Time: 0.299s, 3421.96/s  (0.303s, 3378.22/s)  LR: 8.053e-04  Data: 0.022 (0.026)
Train: 87 [ 700/1251 ( 56%)]  Loss: 3.558 (3.72)  Time: 0.303s, 3384.68/s  (0.303s, 3378.65/s)  LR: 8.051e-04  Data: 0.023 (0.026)
Train: 87 [ 750/1251 ( 60%)]  Loss: 3.899 (3.74)  Time: 0.303s, 3378.86/s  (0.303s, 3378.90/s)  LR: 8.049e-04  Data: 0.024 (0.026)
Train: 87 [ 800/1251 ( 64%)]  Loss: 4.090 (3.76)  Time: 0.298s, 3430.94/s  (0.303s, 3379.26/s)  LR: 8.048e-04  Data: 0.022 (0.026)
Train: 87 [ 850/1251 ( 68%)]  Loss: 3.780 (3.76)  Time: 0.304s, 3366.03/s  (0.303s, 3379.39/s)  LR: 8.046e-04  Data: 0.021 (0.025)
Train: 87 [ 900/1251 ( 72%)]  Loss: 3.799 (3.76)  Time: 0.307s, 3335.78/s  (0.303s, 3380.01/s)  LR: 8.044e-04  Data: 0.028 (0.025)
Train: 87 [ 950/1251 ( 76%)]  Loss: 3.667 (3.75)  Time: 0.299s, 3423.18/s  (0.303s, 3380.60/s)  LR: 8.043e-04  Data: 0.023 (0.025)
Train: 87 [1000/1251 ( 80%)]  Loss: 3.978 (3.77)  Time: 0.303s, 3380.16/s  (0.303s, 3380.29/s)  LR: 8.041e-04  Data: 0.022 (0.025)
Train: 87 [1050/1251 ( 84%)]  Loss: 3.745 (3.76)  Time: 0.305s, 3354.97/s  (0.303s, 3380.39/s)  LR: 8.040e-04  Data: 0.024 (0.025)
Train: 87 [1100/1251 ( 88%)]  Loss: 3.561 (3.76)  Time: 0.303s, 3380.58/s  (0.303s, 3380.48/s)  LR: 8.038e-04  Data: 0.021 (0.025)
Train: 87 [1150/1251 ( 92%)]  Loss: 3.960 (3.76)  Time: 0.306s, 3349.21/s  (0.303s, 3381.03/s)  LR: 8.036e-04  Data: 0.019 (0.025)
Train: 87 [1200/1251 ( 96%)]  Loss: 3.643 (3.76)  Time: 0.305s, 3359.37/s  (0.303s, 3381.19/s)  LR: 8.035e-04  Data: 0.024 (0.025)
Train: 87 [1250/1251 (100%)]  Loss: 3.654 (3.76)  Time: 0.275s, 3717.30/s  (0.303s, 3382.49/s)  LR: 8.033e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.080 (2.080)  Loss:  0.6987 (0.6987)  Acc@1: 86.3281 (86.3281)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.048 (0.232)  Loss:  0.8228 (1.3317)  Acc@1: 82.4292 (70.3360)  Acc@5: 94.8113 (90.0560)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-86.pth.tar', 70.78800017578125)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-87.pth.tar', 70.33599999511719)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-85.pth.tar', 70.2380000439453)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-83.pth.tar', 70.15400001464843)

Train: 88 [   0/1251 (  0%)]  Loss: 3.650 (3.65)  Time: 2.170s,  471.92/s  (2.170s,  471.92/s)  LR: 8.033e-04  Data: 1.946 (1.946)
Train: 88 [  50/1251 (  4%)]  Loss: 3.856 (3.75)  Time: 0.302s, 3393.50/s  (0.326s, 3144.13/s)  LR: 8.031e-04  Data: 0.020 (0.062)
Train: 88 [ 100/1251 (  8%)]  Loss: 3.646 (3.72)  Time: 0.295s, 3475.02/s  (0.312s, 3282.45/s)  LR: 8.030e-04  Data: 0.023 (0.043)
Train: 88 [ 150/1251 ( 12%)]  Loss: 3.868 (3.75)  Time: 0.301s, 3401.18/s  (0.308s, 3327.12/s)  LR: 8.028e-04  Data: 0.021 (0.036)
Train: 88 [ 200/1251 ( 16%)]  Loss: 3.892 (3.78)  Time: 0.299s, 3421.13/s  (0.306s, 3349.48/s)  LR: 8.026e-04  Data: 0.023 (0.033)
Train: 88 [ 250/1251 ( 20%)]  Loss: 3.582 (3.75)  Time: 0.295s, 3466.01/s  (0.305s, 3361.01/s)  LR: 8.025e-04  Data: 0.020 (0.031)
Train: 88 [ 300/1251 ( 24%)]  Loss: 3.532 (3.72)  Time: 0.303s, 3375.45/s  (0.304s, 3366.39/s)  LR: 8.023e-04  Data: 0.022 (0.030)
Train: 88 [ 350/1251 ( 28%)]  Loss: 4.062 (3.76)  Time: 0.300s, 3408.02/s  (0.304s, 3370.53/s)  LR: 8.021e-04  Data: 0.024 (0.029)
Train: 88 [ 400/1251 ( 32%)]  Loss: 4.033 (3.79)  Time: 0.299s, 3426.81/s  (0.304s, 3373.11/s)  LR: 8.020e-04  Data: 0.020 (0.028)
Train: 88 [ 450/1251 ( 36%)]  Loss: 3.793 (3.79)  Time: 0.302s, 3392.04/s  (0.303s, 3375.50/s)  LR: 8.018e-04  Data: 0.023 (0.027)
Train: 88 [ 500/1251 ( 40%)]  Loss: 3.854 (3.80)  Time: 0.299s, 3421.74/s  (0.303s, 3377.12/s)  LR: 8.016e-04  Data: 0.021 (0.027)
Train: 88 [ 550/1251 ( 44%)]  Loss: 3.539 (3.78)  Time: 0.303s, 3378.36/s  (0.303s, 3379.15/s)  LR: 8.015e-04  Data: 0.024 (0.027)
Train: 88 [ 600/1251 ( 48%)]  Loss: 3.316 (3.74)  Time: 0.301s, 3400.06/s  (0.303s, 3380.85/s)  LR: 8.013e-04  Data: 0.022 (0.026)
Train: 88 [ 650/1251 ( 52%)]  Loss: 3.633 (3.73)  Time: 0.298s, 3431.53/s  (0.303s, 3382.40/s)  LR: 8.011e-04  Data: 0.022 (0.026)
Train: 88 [ 700/1251 ( 56%)]  Loss: 4.028 (3.75)  Time: 0.307s, 3335.41/s  (0.303s, 3383.93/s)  LR: 8.010e-04  Data: 0.023 (0.026)
Train: 88 [ 750/1251 ( 60%)]  Loss: 3.779 (3.75)  Time: 0.303s, 3375.40/s  (0.303s, 3385.02/s)  LR: 8.008e-04  Data: 0.024 (0.026)
Train: 88 [ 800/1251 ( 64%)]  Loss: 3.926 (3.76)  Time: 0.300s, 3409.40/s  (0.302s, 3386.01/s)  LR: 8.006e-04  Data: 0.025 (0.026)
Train: 88 [ 850/1251 ( 68%)]  Loss: 4.091 (3.78)  Time: 0.299s, 3424.56/s  (0.302s, 3386.96/s)  LR: 8.005e-04  Data: 0.021 (0.025)
Train: 88 [ 900/1251 ( 72%)]  Loss: 3.765 (3.78)  Time: 0.307s, 3335.99/s  (0.302s, 3388.15/s)  LR: 8.003e-04  Data: 0.024 (0.025)
Train: 88 [ 950/1251 ( 76%)]  Loss: 3.797 (3.78)  Time: 0.299s, 3421.64/s  (0.302s, 3388.86/s)  LR: 8.001e-04  Data: 0.023 (0.025)
Train: 88 [1000/1251 ( 80%)]  Loss: 3.753 (3.78)  Time: 0.300s, 3414.92/s  (0.302s, 3389.32/s)  LR: 8.000e-04  Data: 0.026 (0.025)
Train: 88 [1050/1251 ( 84%)]  Loss: 3.749 (3.78)  Time: 0.298s, 3439.96/s  (0.302s, 3389.54/s)  LR: 7.998e-04  Data: 0.023 (0.025)
Train: 88 [1100/1251 ( 88%)]  Loss: 3.697 (3.78)  Time: 0.302s, 3386.17/s  (0.302s, 3390.06/s)  LR: 7.996e-04  Data: 0.023 (0.025)
Train: 88 [1150/1251 ( 92%)]  Loss: 3.626 (3.77)  Time: 0.300s, 3413.69/s  (0.302s, 3390.40/s)  LR: 7.995e-04  Data: 0.022 (0.025)
Train: 88 [1200/1251 ( 96%)]  Loss: 3.436 (3.76)  Time: 0.301s, 3403.17/s  (0.302s, 3390.87/s)  LR: 7.993e-04  Data: 0.026 (0.025)
Train: 88 [1250/1251 (100%)]  Loss: 4.168 (3.77)  Time: 0.276s, 3708.98/s  (0.302s, 3393.08/s)  LR: 7.991e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.054 (2.054)  Loss:  0.6670 (0.6670)  Acc@1: 86.5234 (86.5234)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.055 (0.235)  Loss:  0.7925 (1.3034)  Acc@1: 82.4292 (70.6460)  Acc@5: 95.0472 (89.9540)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-86.pth.tar', 70.78800017578125)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-88.pth.tar', 70.6459999951172)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-87.pth.tar', 70.33599999511719)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-85.pth.tar', 70.2380000439453)

Train: 89 [   0/1251 (  0%)]  Loss: 4.007 (4.01)  Time: 2.423s,  422.59/s  (2.423s,  422.59/s)  LR: 7.991e-04  Data: 2.212 (2.212)
Train: 89 [  50/1251 (  4%)]  Loss: 3.514 (3.76)  Time: 0.291s, 3524.72/s  (0.322s, 3182.95/s)  LR: 7.990e-04  Data: 0.025 (0.066)
Train: 89 [ 100/1251 (  8%)]  Loss: 3.822 (3.78)  Time: 0.299s, 3420.91/s  (0.308s, 3321.29/s)  LR: 7.988e-04  Data: 0.029 (0.045)
Train: 89 [ 150/1251 ( 12%)]  Loss: 4.072 (3.85)  Time: 0.298s, 3441.97/s  (0.305s, 3360.75/s)  LR: 7.986e-04  Data: 0.023 (0.038)
Train: 89 [ 200/1251 ( 16%)]  Loss: 3.666 (3.82)  Time: 0.299s, 3429.15/s  (0.303s, 3377.88/s)  LR: 7.984e-04  Data: 0.022 (0.034)
Train: 89 [ 250/1251 ( 20%)]  Loss: 3.863 (3.82)  Time: 0.296s, 3457.06/s  (0.302s, 3387.35/s)  LR: 7.983e-04  Data: 0.023 (0.032)
Train: 89 [ 300/1251 ( 24%)]  Loss: 3.909 (3.84)  Time: 0.300s, 3409.10/s  (0.302s, 3390.41/s)  LR: 7.981e-04  Data: 0.024 (0.030)
Train: 89 [ 350/1251 ( 28%)]  Loss: 3.870 (3.84)  Time: 0.303s, 3376.29/s  (0.302s, 3393.18/s)  LR: 7.979e-04  Data: 0.023 (0.029)
Train: 89 [ 400/1251 ( 32%)]  Loss: 3.592 (3.81)  Time: 0.303s, 3374.13/s  (0.302s, 3395.41/s)  LR: 7.978e-04  Data: 0.024 (0.029)
Train: 89 [ 450/1251 ( 36%)]  Loss: 4.207 (3.85)  Time: 0.300s, 3411.98/s  (0.301s, 3397.03/s)  LR: 7.976e-04  Data: 0.024 (0.028)
Train: 89 [ 500/1251 ( 40%)]  Loss: 3.761 (3.84)  Time: 0.300s, 3418.70/s  (0.301s, 3398.37/s)  LR: 7.974e-04  Data: 0.024 (0.028)
Train: 89 [ 550/1251 ( 44%)]  Loss: 4.031 (3.86)  Time: 0.301s, 3405.50/s  (0.301s, 3399.81/s)  LR: 7.973e-04  Data: 0.022 (0.027)
Train: 89 [ 600/1251 ( 48%)]  Loss: 3.688 (3.85)  Time: 0.299s, 3427.32/s  (0.301s, 3400.71/s)  LR: 7.971e-04  Data: 0.025 (0.027)
Train: 89 [ 650/1251 ( 52%)]  Loss: 3.884 (3.85)  Time: 0.303s, 3383.11/s  (0.301s, 3401.41/s)  LR: 7.969e-04  Data: 0.021 (0.027)
Train: 89 [ 700/1251 ( 56%)]  Loss: 3.538 (3.83)  Time: 0.303s, 3378.99/s  (0.301s, 3402.12/s)  LR: 7.968e-04  Data: 0.025 (0.026)
Train: 89 [ 750/1251 ( 60%)]  Loss: 3.757 (3.82)  Time: 0.302s, 3391.97/s  (0.301s, 3402.97/s)  LR: 7.966e-04  Data: 0.022 (0.026)
Train: 89 [ 800/1251 ( 64%)]  Loss: 3.955 (3.83)  Time: 0.296s, 3457.45/s  (0.301s, 3403.33/s)  LR: 7.964e-04  Data: 0.025 (0.026)
Train: 89 [ 850/1251 ( 68%)]  Loss: 3.950 (3.84)  Time: 0.302s, 3391.91/s  (0.301s, 3403.37/s)  LR: 7.963e-04  Data: 0.024 (0.026)
Train: 89 [ 900/1251 ( 72%)]  Loss: 4.144 (3.85)  Time: 0.305s, 3357.52/s  (0.301s, 3403.48/s)  LR: 7.961e-04  Data: 0.022 (0.026)
Train: 89 [ 950/1251 ( 76%)]  Loss: 3.649 (3.84)  Time: 0.306s, 3345.30/s  (0.301s, 3403.33/s)  LR: 7.959e-04  Data: 0.023 (0.025)
Train: 89 [1000/1251 ( 80%)]  Loss: 3.454 (3.83)  Time: 0.304s, 3369.52/s  (0.301s, 3403.39/s)  LR: 7.958e-04  Data: 0.024 (0.025)
Train: 89 [1050/1251 ( 84%)]  Loss: 3.917 (3.83)  Time: 0.295s, 3472.33/s  (0.301s, 3403.81/s)  LR: 7.956e-04  Data: 0.021 (0.025)
Train: 89 [1100/1251 ( 88%)]  Loss: 3.611 (3.82)  Time: 0.308s, 3326.28/s  (0.301s, 3403.79/s)  LR: 7.954e-04  Data: 0.026 (0.025)
Train: 89 [1150/1251 ( 92%)]  Loss: 3.656 (3.81)  Time: 0.296s, 3453.96/s  (0.301s, 3403.98/s)  LR: 7.953e-04  Data: 0.024 (0.025)
Train: 89 [1200/1251 ( 96%)]  Loss: 3.849 (3.81)  Time: 0.298s, 3440.38/s  (0.301s, 3404.14/s)  LR: 7.951e-04  Data: 0.016 (0.025)
Train: 89 [1250/1251 (100%)]  Loss: 3.588 (3.81)  Time: 0.273s, 3756.95/s  (0.301s, 3406.35/s)  LR: 7.949e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.060 (2.060)  Loss:  0.6899 (0.6899)  Acc@1: 85.2539 (85.2539)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.041 (0.237)  Loss:  0.7568 (1.3009)  Acc@1: 83.1368 (70.7400)  Acc@5: 94.8113 (90.0780)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-86.pth.tar', 70.78800017578125)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-89.pth.tar', 70.73999991455078)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-88.pth.tar', 70.6459999951172)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-87.pth.tar', 70.33599999511719)

Train: 90 [   0/1251 (  0%)]  Loss: 3.931 (3.93)  Time: 2.202s,  465.11/s  (2.202s,  465.11/s)  LR: 7.949e-04  Data: 1.978 (1.978)
Train: 90 [  50/1251 (  4%)]  Loss: 3.553 (3.74)  Time: 0.294s, 3482.77/s  (0.319s, 3205.41/s)  LR: 7.948e-04  Data: 0.024 (0.061)
Train: 90 [ 100/1251 (  8%)]  Loss: 3.820 (3.77)  Time: 0.299s, 3429.29/s  (0.307s, 3336.04/s)  LR: 7.946e-04  Data: 0.023 (0.042)
Train: 90 [ 150/1251 ( 12%)]  Loss: 3.711 (3.75)  Time: 0.306s, 3347.47/s  (0.304s, 3371.33/s)  LR: 7.944e-04  Data: 0.025 (0.036)
Train: 90 [ 200/1251 ( 16%)]  Loss: 3.417 (3.69)  Time: 0.296s, 3457.85/s  (0.302s, 3386.88/s)  LR: 7.942e-04  Data: 0.023 (0.033)
Train: 90 [ 250/1251 ( 20%)]  Loss: 3.623 (3.68)  Time: 0.288s, 3551.59/s  (0.301s, 3398.32/s)  LR: 7.941e-04  Data: 0.021 (0.031)
Train: 90 [ 300/1251 ( 24%)]  Loss: 3.605 (3.67)  Time: 0.299s, 3424.76/s  (0.301s, 3404.80/s)  LR: 7.939e-04  Data: 0.027 (0.030)
Train: 90 [ 350/1251 ( 28%)]  Loss: 4.026 (3.71)  Time: 0.290s, 3532.77/s  (0.301s, 3406.93/s)  LR: 7.937e-04  Data: 0.021 (0.029)
Train: 90 [ 400/1251 ( 32%)]  Loss: 3.776 (3.72)  Time: 0.298s, 3439.49/s  (0.300s, 3409.11/s)  LR: 7.936e-04  Data: 0.030 (0.028)
Train: 90 [ 450/1251 ( 36%)]  Loss: 3.861 (3.73)  Time: 0.295s, 3476.84/s  (0.300s, 3410.40/s)  LR: 7.934e-04  Data: 0.022 (0.027)
Train: 90 [ 500/1251 ( 40%)]  Loss: 3.813 (3.74)  Time: 0.304s, 3370.18/s  (0.300s, 3411.98/s)  LR: 7.932e-04  Data: 0.024 (0.027)
Train: 90 [ 550/1251 ( 44%)]  Loss: 3.892 (3.75)  Time: 0.299s, 3423.44/s  (0.300s, 3412.35/s)  LR: 7.931e-04  Data: 0.026 (0.027)
Train: 90 [ 600/1251 ( 48%)]  Loss: 3.829 (3.76)  Time: 0.294s, 3479.49/s  (0.300s, 3412.96/s)  LR: 7.929e-04  Data: 0.022 (0.026)
Train: 90 [ 650/1251 ( 52%)]  Loss: 3.761 (3.76)  Time: 0.304s, 3369.33/s  (0.300s, 3413.72/s)  LR: 7.927e-04  Data: 0.021 (0.026)
Train: 90 [ 700/1251 ( 56%)]  Loss: 3.496 (3.74)  Time: 0.299s, 3427.49/s  (0.300s, 3413.95/s)  LR: 7.926e-04  Data: 0.022 (0.026)
Train: 90 [ 750/1251 ( 60%)]  Loss: 3.656 (3.74)  Time: 0.304s, 3369.89/s  (0.300s, 3415.07/s)  LR: 7.924e-04  Data: 0.020 (0.026)
Train: 90 [ 800/1251 ( 64%)]  Loss: 4.001 (3.75)  Time: 0.300s, 3417.12/s  (0.300s, 3415.97/s)  LR: 7.922e-04  Data: 0.023 (0.026)
Train: 90 [ 850/1251 ( 68%)]  Loss: 3.781 (3.75)  Time: 0.300s, 3411.25/s  (0.300s, 3416.26/s)  LR: 7.920e-04  Data: 0.025 (0.025)
Train: 90 [ 900/1251 ( 72%)]  Loss: 3.735 (3.75)  Time: 0.301s, 3404.22/s  (0.300s, 3416.52/s)  LR: 7.919e-04  Data: 0.024 (0.025)
Train: 90 [ 950/1251 ( 76%)]  Loss: 3.980 (3.76)  Time: 0.302s, 3385.37/s  (0.300s, 3417.44/s)  LR: 7.917e-04  Data: 0.021 (0.025)
Train: 90 [1000/1251 ( 80%)]  Loss: 3.943 (3.77)  Time: 0.297s, 3452.09/s  (0.300s, 3417.61/s)  LR: 7.915e-04  Data: 0.016 (0.025)
Train: 90 [1050/1251 ( 84%)]  Loss: 4.272 (3.79)  Time: 0.292s, 3507.77/s  (0.300s, 3417.87/s)  LR: 7.914e-04  Data: 0.022 (0.025)
Train: 90 [1100/1251 ( 88%)]  Loss: 3.756 (3.79)  Time: 0.301s, 3401.15/s  (0.300s, 3418.07/s)  LR: 7.912e-04  Data: 0.026 (0.025)
Train: 90 [1150/1251 ( 92%)]  Loss: 3.857 (3.80)  Time: 0.303s, 3378.25/s  (0.300s, 3417.96/s)  LR: 7.910e-04  Data: 0.027 (0.025)
Train: 90 [1200/1251 ( 96%)]  Loss: 3.683 (3.79)  Time: 0.299s, 3423.77/s  (0.300s, 3418.06/s)  LR: 7.909e-04  Data: 0.019 (0.025)
Train: 90 [1250/1251 (100%)]  Loss: 3.565 (3.78)  Time: 0.273s, 3745.30/s  (0.299s, 3420.44/s)  LR: 7.907e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.366 (2.366)  Loss:  0.7324 (0.7324)  Acc@1: 85.8398 (85.8398)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.043 (0.237)  Loss:  0.8223 (1.3140)  Acc@1: 81.9576 (70.7140)  Acc@5: 95.0472 (90.2120)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-86.pth.tar', 70.78800017578125)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-89.pth.tar', 70.73999991455078)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-90.pth.tar', 70.71400004882813)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-88.pth.tar', 70.6459999951172)

Train: 91 [   0/1251 (  0%)]  Loss: 3.474 (3.47)  Time: 2.442s,  419.27/s  (2.442s,  419.27/s)  LR: 7.907e-04  Data: 2.218 (2.218)
Train: 91 [  50/1251 (  4%)]  Loss: 4.002 (3.74)  Time: 0.280s, 3652.32/s  (0.333s, 3079.09/s)  LR: 7.905e-04  Data: 0.020 (0.077)
Train: 91 [ 100/1251 (  8%)]  Loss: 4.029 (3.84)  Time: 0.296s, 3460.08/s  (0.312s, 3279.43/s)  LR: 7.903e-04  Data: 0.022 (0.050)
Train: 91 [ 150/1251 ( 12%)]  Loss: 3.821 (3.83)  Time: 0.300s, 3416.80/s  (0.306s, 3343.40/s)  LR: 7.902e-04  Data: 0.022 (0.041)
Train: 91 [ 200/1251 ( 16%)]  Loss: 3.851 (3.84)  Time: 0.301s, 3396.64/s  (0.304s, 3367.71/s)  LR: 7.900e-04  Data: 0.024 (0.037)
Train: 91 [ 250/1251 ( 20%)]  Loss: 4.035 (3.87)  Time: 0.293s, 3489.40/s  (0.303s, 3380.90/s)  LR: 7.898e-04  Data: 0.022 (0.034)
Train: 91 [ 300/1251 ( 24%)]  Loss: 3.747 (3.85)  Time: 0.294s, 3480.46/s  (0.302s, 3391.63/s)  LR: 7.897e-04  Data: 0.023 (0.032)
Train: 91 [ 350/1251 ( 28%)]  Loss: 3.337 (3.79)  Time: 0.301s, 3405.65/s  (0.302s, 3395.82/s)  LR: 7.895e-04  Data: 0.027 (0.031)
Train: 91 [ 400/1251 ( 32%)]  Loss: 3.806 (3.79)  Time: 0.299s, 3428.31/s  (0.301s, 3398.69/s)  LR: 7.893e-04  Data: 0.021 (0.030)
Train: 91 [ 450/1251 ( 36%)]  Loss: 3.561 (3.77)  Time: 0.304s, 3371.02/s  (0.301s, 3401.71/s)  LR: 7.892e-04  Data: 0.022 (0.029)
Train: 91 [ 500/1251 ( 40%)]  Loss: 3.560 (3.75)  Time: 0.298s, 3432.46/s  (0.301s, 3403.31/s)  LR: 7.890e-04  Data: 0.022 (0.029)
Train: 91 [ 550/1251 ( 44%)]  Loss: 3.727 (3.75)  Time: 0.303s, 3375.54/s  (0.301s, 3405.74/s)  LR: 7.888e-04  Data: 0.022 (0.028)
Train: 91 [ 600/1251 ( 48%)]  Loss: 3.419 (3.72)  Time: 0.300s, 3409.00/s  (0.300s, 3407.83/s)  LR: 7.886e-04  Data: 0.023 (0.028)
Train: 91 [ 650/1251 ( 52%)]  Loss: 3.670 (3.72)  Time: 0.296s, 3454.25/s  (0.300s, 3408.38/s)  LR: 7.885e-04  Data: 0.021 (0.027)
Train: 91 [ 700/1251 ( 56%)]  Loss: 3.665 (3.71)  Time: 0.302s, 3391.05/s  (0.300s, 3408.61/s)  LR: 7.883e-04  Data: 0.024 (0.027)
Train: 91 [ 750/1251 ( 60%)]  Loss: 3.577 (3.71)  Time: 0.303s, 3381.67/s  (0.300s, 3409.01/s)  LR: 7.881e-04  Data: 0.021 (0.027)
Train: 91 [ 800/1251 ( 64%)]  Loss: 3.686 (3.70)  Time: 0.299s, 3429.23/s  (0.300s, 3409.19/s)  LR: 7.880e-04  Data: 0.014 (0.026)
Train: 91 [ 850/1251 ( 68%)]  Loss: 4.109 (3.73)  Time: 0.301s, 3396.39/s  (0.300s, 3409.37/s)  LR: 7.878e-04  Data: 0.023 (0.026)
Train: 91 [ 900/1251 ( 72%)]  Loss: 3.792 (3.73)  Time: 0.301s, 3398.74/s  (0.300s, 3408.82/s)  LR: 7.876e-04  Data: 0.026 (0.026)
Train: 91 [ 950/1251 ( 76%)]  Loss: 4.043 (3.75)  Time: 0.301s, 3398.58/s  (0.300s, 3409.30/s)  LR: 7.875e-04  Data: 0.022 (0.026)
Train: 91 [1000/1251 ( 80%)]  Loss: 3.902 (3.75)  Time: 0.302s, 3393.68/s  (0.300s, 3409.42/s)  LR: 7.873e-04  Data: 0.024 (0.026)
Train: 91 [1050/1251 ( 84%)]  Loss: 3.769 (3.75)  Time: 0.304s, 3365.38/s  (0.300s, 3409.74/s)  LR: 7.871e-04  Data: 0.024 (0.026)
Train: 91 [1100/1251 ( 88%)]  Loss: 3.668 (3.75)  Time: 0.303s, 3375.10/s  (0.300s, 3409.42/s)  LR: 7.869e-04  Data: 0.026 (0.026)
Train: 91 [1150/1251 ( 92%)]  Loss: 3.931 (3.76)  Time: 0.301s, 3396.76/s  (0.300s, 3408.67/s)  LR: 7.868e-04  Data: 0.021 (0.025)
Train: 91 [1200/1251 ( 96%)]  Loss: 3.853 (3.76)  Time: 0.300s, 3408.50/s  (0.300s, 3408.35/s)  LR: 7.866e-04  Data: 0.024 (0.025)
Train: 91 [1250/1251 (100%)]  Loss: 3.738 (3.76)  Time: 0.276s, 3712.01/s  (0.300s, 3409.99/s)  LR: 7.864e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.065 (2.065)  Loss:  0.6709 (0.6709)  Acc@1: 87.2070 (87.2070)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.057 (0.241)  Loss:  0.7935 (1.3052)  Acc@1: 83.9623 (70.9860)  Acc@5: 95.7547 (90.3820)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-91.pth.tar', 70.98600001464844)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-86.pth.tar', 70.78800017578125)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-89.pth.tar', 70.73999991455078)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-90.pth.tar', 70.71400004882813)

Train: 92 [   0/1251 (  0%)]  Loss: 3.837 (3.84)  Time: 2.038s,  502.37/s  (2.038s,  502.37/s)  LR: 7.864e-04  Data: 1.809 (1.809)
Train: 92 [  50/1251 (  4%)]  Loss: 3.478 (3.66)  Time: 0.291s, 3515.05/s  (0.320s, 3203.90/s)  LR: 7.863e-04  Data: 0.019 (0.060)
Train: 92 [ 100/1251 (  8%)]  Loss: 3.941 (3.75)  Time: 0.296s, 3463.48/s  (0.307s, 3338.11/s)  LR: 7.861e-04  Data: 0.027 (0.042)
Train: 92 [ 150/1251 ( 12%)]  Loss: 3.653 (3.73)  Time: 0.294s, 3488.07/s  (0.304s, 3372.72/s)  LR: 7.859e-04  Data: 0.024 (0.035)
Train: 92 [ 200/1251 ( 16%)]  Loss: 3.397 (3.66)  Time: 0.299s, 3419.11/s  (0.302s, 3387.88/s)  LR: 7.857e-04  Data: 0.025 (0.032)
Train: 92 [ 250/1251 ( 20%)]  Loss: 3.552 (3.64)  Time: 0.296s, 3457.69/s  (0.302s, 3395.66/s)  LR: 7.856e-04  Data: 0.022 (0.030)
Train: 92 [ 300/1251 ( 24%)]  Loss: 3.553 (3.63)  Time: 0.300s, 3407.92/s  (0.301s, 3400.52/s)  LR: 7.854e-04  Data: 0.026 (0.029)
Train: 92 [ 350/1251 ( 28%)]  Loss: 3.970 (3.67)  Time: 0.301s, 3407.64/s  (0.301s, 3402.61/s)  LR: 7.852e-04  Data: 0.020 (0.029)
Train: 92 [ 400/1251 ( 32%)]  Loss: 4.208 (3.73)  Time: 0.300s, 3410.32/s  (0.301s, 3404.14/s)  LR: 7.851e-04  Data: 0.020 (0.028)
Train: 92 [ 450/1251 ( 36%)]  Loss: 3.633 (3.72)  Time: 0.300s, 3413.33/s  (0.301s, 3406.31/s)  LR: 7.849e-04  Data: 0.023 (0.027)
Train: 92 [ 500/1251 ( 40%)]  Loss: 3.896 (3.74)  Time: 0.301s, 3399.75/s  (0.301s, 3407.32/s)  LR: 7.847e-04  Data: 0.027 (0.027)
Train: 92 [ 550/1251 ( 44%)]  Loss: 3.705 (3.74)  Time: 0.300s, 3418.78/s  (0.301s, 3407.44/s)  LR: 7.845e-04  Data: 0.024 (0.027)
Train: 92 [ 600/1251 ( 48%)]  Loss: 3.675 (3.73)  Time: 0.303s, 3375.51/s  (0.300s, 3407.71/s)  LR: 7.844e-04  Data: 0.025 (0.026)
Train: 92 [ 650/1251 ( 52%)]  Loss: 3.735 (3.73)  Time: 0.302s, 3391.60/s  (0.300s, 3408.26/s)  LR: 7.842e-04  Data: 0.025 (0.026)
Train: 92 [ 700/1251 ( 56%)]  Loss: 3.622 (3.72)  Time: 0.296s, 3458.02/s  (0.300s, 3408.72/s)  LR: 7.840e-04  Data: 0.020 (0.026)
Train: 92 [ 750/1251 ( 60%)]  Loss: 3.863 (3.73)  Time: 0.307s, 3332.83/s  (0.300s, 3408.30/s)  LR: 7.839e-04  Data: 0.023 (0.026)
Train: 92 [ 800/1251 ( 64%)]  Loss: 3.688 (3.73)  Time: 0.301s, 3401.53/s  (0.300s, 3408.01/s)  LR: 7.837e-04  Data: 0.022 (0.026)
Train: 92 [ 850/1251 ( 68%)]  Loss: 4.098 (3.75)  Time: 0.306s, 3344.95/s  (0.300s, 3408.04/s)  LR: 7.835e-04  Data: 0.024 (0.025)
Train: 92 [ 900/1251 ( 72%)]  Loss: 4.159 (3.77)  Time: 0.306s, 3344.46/s  (0.301s, 3407.46/s)  LR: 7.833e-04  Data: 0.025 (0.025)
Train: 92 [ 950/1251 ( 76%)]  Loss: 3.748 (3.77)  Time: 0.300s, 3413.43/s  (0.301s, 3406.92/s)  LR: 7.832e-04  Data: 0.023 (0.025)
Train: 92 [1000/1251 ( 80%)]  Loss: 3.770 (3.77)  Time: 0.295s, 3472.09/s  (0.301s, 3406.39/s)  LR: 7.830e-04  Data: 0.022 (0.025)
Train: 92 [1050/1251 ( 84%)]  Loss: 3.732 (3.77)  Time: 0.302s, 3392.78/s  (0.301s, 3405.85/s)  LR: 7.828e-04  Data: 0.018 (0.025)
Train: 92 [1100/1251 ( 88%)]  Loss: 3.740 (3.77)  Time: 0.306s, 3343.49/s  (0.301s, 3405.52/s)  LR: 7.827e-04  Data: 0.023 (0.025)
Train: 92 [1150/1251 ( 92%)]  Loss: 4.037 (3.78)  Time: 0.302s, 3387.19/s  (0.301s, 3405.44/s)  LR: 7.825e-04  Data: 0.023 (0.025)
Train: 92 [1200/1251 ( 96%)]  Loss: 3.806 (3.78)  Time: 0.306s, 3341.88/s  (0.301s, 3404.95/s)  LR: 7.823e-04  Data: 0.021 (0.025)
Train: 92 [1250/1251 (100%)]  Loss: 3.668 (3.78)  Time: 0.276s, 3712.22/s  (0.301s, 3406.23/s)  LR: 7.821e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.127 (2.127)  Loss:  0.7109 (0.7109)  Acc@1: 86.5234 (86.5234)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.041 (0.234)  Loss:  0.7983 (1.3134)  Acc@1: 83.2547 (70.7320)  Acc@5: 95.4009 (90.1340)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-91.pth.tar', 70.98600001464844)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-86.pth.tar', 70.78800017578125)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-89.pth.tar', 70.73999991455078)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-92.pth.tar', 70.73199996582031)

Train: 93 [   0/1251 (  0%)]  Loss: 3.765 (3.76)  Time: 2.031s,  504.22/s  (2.031s,  504.22/s)  LR: 7.821e-04  Data: 1.816 (1.816)
Train: 93 [  50/1251 (  4%)]  Loss: 3.568 (3.67)  Time: 0.292s, 3512.46/s  (0.322s, 3183.79/s)  LR: 7.820e-04  Data: 0.021 (0.059)
Train: 93 [ 100/1251 (  8%)]  Loss: 3.810 (3.71)  Time: 0.297s, 3449.72/s  (0.309s, 3313.21/s)  LR: 7.818e-04  Data: 0.022 (0.041)
Train: 93 [ 150/1251 ( 12%)]  Loss: 3.784 (3.73)  Time: 0.301s, 3401.90/s  (0.305s, 3353.67/s)  LR: 7.816e-04  Data: 0.023 (0.035)
Train: 93 [ 200/1251 ( 16%)]  Loss: 3.634 (3.71)  Time: 0.301s, 3403.46/s  (0.304s, 3372.27/s)  LR: 7.814e-04  Data: 0.015 (0.032)
Train: 93 [ 250/1251 ( 20%)]  Loss: 3.573 (3.69)  Time: 0.299s, 3423.38/s  (0.303s, 3381.21/s)  LR: 7.813e-04  Data: 0.023 (0.030)
Train: 93 [ 300/1251 ( 24%)]  Loss: 3.865 (3.71)  Time: 0.313s, 3273.07/s  (0.303s, 3384.82/s)  LR: 7.811e-04  Data: 0.021 (0.029)
Train: 93 [ 350/1251 ( 28%)]  Loss: 3.328 (3.67)  Time: 0.303s, 3376.35/s  (0.302s, 3389.51/s)  LR: 7.809e-04  Data: 0.022 (0.028)
Train: 93 [ 400/1251 ( 32%)]  Loss: 3.586 (3.66)  Time: 0.299s, 3419.29/s  (0.302s, 3391.99/s)  LR: 7.808e-04  Data: 0.019 (0.028)
Train: 93 [ 450/1251 ( 36%)]  Loss: 4.070 (3.70)  Time: 0.300s, 3415.30/s  (0.302s, 3393.16/s)  LR: 7.806e-04  Data: 0.024 (0.027)
Train: 93 [ 500/1251 ( 40%)]  Loss: 3.748 (3.70)  Time: 0.304s, 3373.86/s  (0.302s, 3394.85/s)  LR: 7.804e-04  Data: 0.023 (0.027)
Train: 93 [ 550/1251 ( 44%)]  Loss: 3.950 (3.72)  Time: 0.303s, 3383.66/s  (0.302s, 3396.01/s)  LR: 7.802e-04  Data: 0.025 (0.026)
Train: 93 [ 600/1251 ( 48%)]  Loss: 3.700 (3.72)  Time: 0.299s, 3422.94/s  (0.301s, 3397.15/s)  LR: 7.801e-04  Data: 0.022 (0.026)
Train: 93 [ 650/1251 ( 52%)]  Loss: 3.944 (3.74)  Time: 0.302s, 3394.32/s  (0.301s, 3398.25/s)  LR: 7.799e-04  Data: 0.022 (0.026)
Train: 93 [ 700/1251 ( 56%)]  Loss: 4.048 (3.76)  Time: 0.302s, 3386.76/s  (0.301s, 3398.36/s)  LR: 7.797e-04  Data: 0.025 (0.026)
Train: 93 [ 750/1251 ( 60%)]  Loss: 3.801 (3.76)  Time: 0.307s, 3338.58/s  (0.301s, 3398.68/s)  LR: 7.795e-04  Data: 0.015 (0.025)
Train: 93 [ 800/1251 ( 64%)]  Loss: 4.005 (3.78)  Time: 0.307s, 3335.79/s  (0.301s, 3398.82/s)  LR: 7.794e-04  Data: 0.024 (0.025)
Train: 93 [ 850/1251 ( 68%)]  Loss: 3.781 (3.78)  Time: 0.296s, 3459.93/s  (0.301s, 3399.46/s)  LR: 7.792e-04  Data: 0.022 (0.025)
Train: 93 [ 900/1251 ( 72%)]  Loss: 3.634 (3.77)  Time: 0.304s, 3363.07/s  (0.301s, 3399.19/s)  LR: 7.790e-04  Data: 0.022 (0.025)
Train: 93 [ 950/1251 ( 76%)]  Loss: 3.655 (3.76)  Time: 0.303s, 3379.80/s  (0.301s, 3399.30/s)  LR: 7.789e-04  Data: 0.022 (0.025)
Train: 93 [1000/1251 ( 80%)]  Loss: 3.763 (3.76)  Time: 0.302s, 3393.67/s  (0.301s, 3398.99/s)  LR: 7.787e-04  Data: 0.017 (0.025)
Train: 93 [1050/1251 ( 84%)]  Loss: 3.967 (3.77)  Time: 0.301s, 3407.61/s  (0.301s, 3398.59/s)  LR: 7.785e-04  Data: 0.020 (0.025)
Train: 93 [1100/1251 ( 88%)]  Loss: 3.803 (3.77)  Time: 0.300s, 3408.16/s  (0.301s, 3398.69/s)  LR: 7.783e-04  Data: 0.023 (0.025)
Train: 93 [1150/1251 ( 92%)]  Loss: 3.683 (3.77)  Time: 0.304s, 3372.71/s  (0.301s, 3398.51/s)  LR: 7.782e-04  Data: 0.021 (0.025)
Train: 93 [1200/1251 ( 96%)]  Loss: 3.509 (3.76)  Time: 0.305s, 3356.14/s  (0.301s, 3398.19/s)  LR: 7.780e-04  Data: 0.020 (0.024)
Train: 93 [1250/1251 (100%)]  Loss: 3.803 (3.76)  Time: 0.276s, 3714.89/s  (0.301s, 3399.74/s)  LR: 7.778e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.053 (2.053)  Loss:  0.6475 (0.6475)  Acc@1: 87.5000 (87.5000)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.057 (0.240)  Loss:  0.7207 (1.3007)  Acc@1: 83.3727 (70.8480)  Acc@5: 96.6981 (90.2080)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-91.pth.tar', 70.98600001464844)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-93.pth.tar', 70.84800014648438)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-86.pth.tar', 70.78800017578125)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-89.pth.tar', 70.73999991455078)

Train: 94 [   0/1251 (  0%)]  Loss: 3.525 (3.53)  Time: 2.298s,  445.59/s  (2.298s,  445.59/s)  LR: 7.778e-04  Data: 2.059 (2.059)
Train: 94 [  50/1251 (  4%)]  Loss: 3.872 (3.70)  Time: 0.292s, 3510.02/s  (0.318s, 3215.87/s)  LR: 7.776e-04  Data: 0.024 (0.064)
Train: 94 [ 100/1251 (  8%)]  Loss: 3.730 (3.71)  Time: 0.299s, 3420.01/s  (0.306s, 3341.00/s)  LR: 7.775e-04  Data: 0.024 (0.044)
Train: 94 [ 150/1251 ( 12%)]  Loss: 3.685 (3.70)  Time: 0.299s, 3420.80/s  (0.304s, 3369.79/s)  LR: 7.773e-04  Data: 0.023 (0.037)
Train: 94 [ 200/1251 ( 16%)]  Loss: 3.421 (3.65)  Time: 0.294s, 3479.55/s  (0.303s, 3382.54/s)  LR: 7.771e-04  Data: 0.020 (0.033)
Train: 94 [ 250/1251 ( 20%)]  Loss: 3.682 (3.65)  Time: 0.306s, 3342.38/s  (0.302s, 3388.39/s)  LR: 7.769e-04  Data: 0.022 (0.031)
Train: 94 [ 300/1251 ( 24%)]  Loss: 3.194 (3.59)  Time: 0.300s, 3416.07/s  (0.302s, 3391.67/s)  LR: 7.768e-04  Data: 0.023 (0.030)
Train: 94 [ 350/1251 ( 28%)]  Loss: 3.857 (3.62)  Time: 0.300s, 3417.15/s  (0.302s, 3393.89/s)  LR: 7.766e-04  Data: 0.021 (0.029)
Train: 94 [ 400/1251 ( 32%)]  Loss: 3.521 (3.61)  Time: 0.302s, 3387.22/s  (0.302s, 3395.84/s)  LR: 7.764e-04  Data: 0.026 (0.028)
Train: 94 [ 450/1251 ( 36%)]  Loss: 3.889 (3.64)  Time: 0.303s, 3377.33/s  (0.302s, 3396.10/s)  LR: 7.762e-04  Data: 0.025 (0.028)
Train: 94 [ 500/1251 ( 40%)]  Loss: 3.675 (3.64)  Time: 0.304s, 3372.60/s  (0.302s, 3395.31/s)  LR: 7.761e-04  Data: 0.022 (0.027)
Train: 94 [ 550/1251 ( 44%)]  Loss: 3.651 (3.64)  Time: 0.309s, 3318.92/s  (0.302s, 3394.99/s)  LR: 7.759e-04  Data: 0.024 (0.027)
Train: 94 [ 600/1251 ( 48%)]  Loss: 3.663 (3.64)  Time: 0.305s, 3358.00/s  (0.302s, 3394.48/s)  LR: 7.757e-04  Data: 0.018 (0.026)
Train: 94 [ 650/1251 ( 52%)]  Loss: 3.565 (3.64)  Time: 0.302s, 3388.48/s  (0.302s, 3394.46/s)  LR: 7.756e-04  Data: 0.030 (0.026)
Train: 94 [ 700/1251 ( 56%)]  Loss: 3.775 (3.65)  Time: 0.301s, 3400.10/s  (0.302s, 3393.74/s)  LR: 7.754e-04  Data: 0.025 (0.026)
Train: 94 [ 750/1251 ( 60%)]  Loss: 3.371 (3.63)  Time: 0.297s, 3450.07/s  (0.302s, 3393.60/s)  LR: 7.752e-04  Data: 0.023 (0.026)
Train: 94 [ 800/1251 ( 64%)]  Loss: 3.849 (3.64)  Time: 0.307s, 3335.28/s  (0.302s, 3392.35/s)  LR: 7.750e-04  Data: 0.025 (0.026)
Train: 94 [ 850/1251 ( 68%)]  Loss: 3.586 (3.64)  Time: 0.300s, 3416.71/s  (0.302s, 3391.90/s)  LR: 7.749e-04  Data: 0.026 (0.025)
Train: 94 [ 900/1251 ( 72%)]  Loss: 3.832 (3.65)  Time: 0.307s, 3335.25/s  (0.302s, 3391.50/s)  LR: 7.747e-04  Data: 0.022 (0.025)
Train: 94 [ 950/1251 ( 76%)]  Loss: 3.981 (3.67)  Time: 0.306s, 3347.60/s  (0.302s, 3390.47/s)  LR: 7.745e-04  Data: 0.025 (0.025)
Train: 94 [1000/1251 ( 80%)]  Loss: 3.333 (3.65)  Time: 0.309s, 3318.77/s  (0.302s, 3389.30/s)  LR: 7.743e-04  Data: 0.024 (0.025)
Train: 94 [1050/1251 ( 84%)]  Loss: 3.419 (3.64)  Time: 0.304s, 3368.02/s  (0.302s, 3388.20/s)  LR: 7.742e-04  Data: 0.021 (0.025)
Train: 94 [1100/1251 ( 88%)]  Loss: 3.497 (3.63)  Time: 0.305s, 3358.97/s  (0.302s, 3388.25/s)  LR: 7.740e-04  Data: 0.022 (0.025)
Train: 94 [1150/1251 ( 92%)]  Loss: 3.582 (3.63)  Time: 0.310s, 3302.26/s  (0.302s, 3388.13/s)  LR: 7.738e-04  Data: 0.021 (0.025)
Train: 94 [1200/1251 ( 96%)]  Loss: 3.572 (3.63)  Time: 0.305s, 3362.88/s  (0.302s, 3387.75/s)  LR: 7.736e-04  Data: 0.026 (0.025)
Train: 94 [1250/1251 (100%)]  Loss: 3.927 (3.64)  Time: 0.275s, 3719.02/s  (0.302s, 3388.94/s)  LR: 7.735e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.063 (2.063)  Loss:  0.7217 (0.7217)  Acc@1: 87.0117 (87.0117)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.056 (0.238)  Loss:  0.7378 (1.3174)  Acc@1: 84.0802 (70.8720)  Acc@5: 95.6368 (90.2500)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-91.pth.tar', 70.98600001464844)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-94.pth.tar', 70.87200006591797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-93.pth.tar', 70.84800014648438)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-86.pth.tar', 70.78800017578125)

Train: 95 [   0/1251 (  0%)]  Loss: 3.799 (3.80)  Time: 2.280s,  449.10/s  (2.280s,  449.10/s)  LR: 7.735e-04  Data: 2.066 (2.066)
Train: 95 [  50/1251 (  4%)]  Loss: 3.744 (3.77)  Time: 0.298s, 3435.65/s  (0.325s, 3149.76/s)  LR: 7.733e-04  Data: 0.023 (0.065)
Train: 95 [ 100/1251 (  8%)]  Loss: 3.651 (3.73)  Time: 0.303s, 3383.04/s  (0.312s, 3283.85/s)  LR: 7.731e-04  Data: 0.025 (0.044)
Train: 95 [ 150/1251 ( 12%)]  Loss: 3.209 (3.60)  Time: 0.297s, 3449.29/s  (0.308s, 3324.17/s)  LR: 7.729e-04  Data: 0.021 (0.037)
Train: 95 [ 200/1251 ( 16%)]  Loss: 3.518 (3.58)  Time: 0.301s, 3400.16/s  (0.306s, 3341.50/s)  LR: 7.728e-04  Data: 0.023 (0.034)
Train: 95 [ 250/1251 ( 20%)]  Loss: 3.836 (3.63)  Time: 0.297s, 3443.45/s  (0.306s, 3349.31/s)  LR: 7.726e-04  Data: 0.022 (0.031)
Train: 95 [ 300/1251 ( 24%)]  Loss: 3.755 (3.64)  Time: 0.304s, 3365.34/s  (0.305s, 3353.21/s)  LR: 7.724e-04  Data: 0.021 (0.030)
Train: 95 [ 350/1251 ( 28%)]  Loss: 3.733 (3.66)  Time: 0.306s, 3342.70/s  (0.305s, 3356.41/s)  LR: 7.722e-04  Data: 0.026 (0.029)
Train: 95 [ 400/1251 ( 32%)]  Loss: 3.824 (3.67)  Time: 0.305s, 3355.93/s  (0.305s, 3358.32/s)  LR: 7.721e-04  Data: 0.019 (0.028)
Train: 95 [ 450/1251 ( 36%)]  Loss: 3.670 (3.67)  Time: 0.300s, 3411.70/s  (0.305s, 3359.55/s)  LR: 7.719e-04  Data: 0.023 (0.028)
Train: 95 [ 500/1251 ( 40%)]  Loss: 3.870 (3.69)  Time: 0.316s, 3240.71/s  (0.305s, 3360.40/s)  LR: 7.717e-04  Data: 0.026 (0.027)
Train: 95 [ 550/1251 ( 44%)]  Loss: 3.806 (3.70)  Time: 0.306s, 3343.27/s  (0.305s, 3360.66/s)  LR: 7.715e-04  Data: 0.023 (0.027)
Train: 95 [ 600/1251 ( 48%)]  Loss: 4.025 (3.73)  Time: 0.303s, 3381.81/s  (0.305s, 3360.32/s)  LR: 7.714e-04  Data: 0.023 (0.027)
Train: 95 [ 650/1251 ( 52%)]  Loss: 3.363 (3.70)  Time: 0.301s, 3400.38/s  (0.305s, 3360.59/s)  LR: 7.712e-04  Data: 0.020 (0.026)
Train: 95 [ 700/1251 ( 56%)]  Loss: 3.487 (3.69)  Time: 0.304s, 3367.81/s  (0.305s, 3359.72/s)  LR: 7.710e-04  Data: 0.021 (0.026)
Train: 95 [ 750/1251 ( 60%)]  Loss: 3.449 (3.67)  Time: 0.300s, 3418.00/s  (0.305s, 3358.64/s)  LR: 7.708e-04  Data: 0.021 (0.026)
Train: 95 [ 800/1251 ( 64%)]  Loss: 3.698 (3.67)  Time: 0.310s, 3302.92/s  (0.305s, 3357.52/s)  LR: 7.707e-04  Data: 0.025 (0.026)
Train: 95 [ 850/1251 ( 68%)]  Loss: 4.206 (3.70)  Time: 0.306s, 3343.33/s  (0.305s, 3356.88/s)  LR: 7.705e-04  Data: 0.025 (0.025)
Train: 95 [ 900/1251 ( 72%)]  Loss: 3.301 (3.68)  Time: 0.303s, 3377.61/s  (0.305s, 3356.46/s)  LR: 7.703e-04  Data: 0.022 (0.025)
Train: 95 [ 950/1251 ( 76%)]  Loss: 3.616 (3.68)  Time: 0.307s, 3336.04/s  (0.305s, 3356.45/s)  LR: 7.701e-04  Data: 0.020 (0.025)
Train: 95 [1000/1251 ( 80%)]  Loss: 3.849 (3.69)  Time: 0.308s, 3319.93/s  (0.305s, 3356.54/s)  LR: 7.700e-04  Data: 0.025 (0.025)
Train: 95 [1050/1251 ( 84%)]  Loss: 4.211 (3.71)  Time: 0.305s, 3355.05/s  (0.305s, 3355.81/s)  LR: 7.698e-04  Data: 0.026 (0.025)
Train: 95 [1100/1251 ( 88%)]  Loss: 3.365 (3.69)  Time: 0.306s, 3342.08/s  (0.305s, 3355.49/s)  LR: 7.696e-04  Data: 0.024 (0.025)
Train: 95 [1150/1251 ( 92%)]  Loss: 3.643 (3.69)  Time: 0.304s, 3372.06/s  (0.305s, 3355.64/s)  LR: 7.694e-04  Data: 0.023 (0.025)
Train: 95 [1200/1251 ( 96%)]  Loss: 3.230 (3.67)  Time: 0.307s, 3340.57/s  (0.305s, 3355.60/s)  LR: 7.692e-04  Data: 0.022 (0.025)
Train: 95 [1250/1251 (100%)]  Loss: 3.791 (3.68)  Time: 0.275s, 3720.24/s  (0.305s, 3357.32/s)  LR: 7.691e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.315 (2.315)  Loss:  0.6885 (0.6885)  Acc@1: 85.0586 (85.0586)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.063 (0.242)  Loss:  0.7412 (1.2726)  Acc@1: 84.6698 (70.9560)  Acc@5: 95.7547 (90.4100)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-91.pth.tar', 70.98600001464844)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-95.pth.tar', 70.95600006347657)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-94.pth.tar', 70.87200006591797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-93.pth.tar', 70.84800014648438)

Train: 96 [   0/1251 (  0%)]  Loss: 3.810 (3.81)  Time: 2.358s,  434.28/s  (2.358s,  434.28/s)  LR: 7.691e-04  Data: 2.133 (2.133)
Train: 96 [  50/1251 (  4%)]  Loss: 3.564 (3.69)  Time: 0.293s, 3493.95/s  (0.328s, 3124.23/s)  LR: 7.689e-04  Data: 0.021 (0.064)
Train: 96 [ 100/1251 (  8%)]  Loss: 3.415 (3.60)  Time: 0.304s, 3365.51/s  (0.313s, 3266.41/s)  LR: 7.687e-04  Data: 0.023 (0.044)
Train: 96 [ 150/1251 ( 12%)]  Loss: 3.707 (3.62)  Time: 0.304s, 3372.77/s  (0.310s, 3307.47/s)  LR: 7.685e-04  Data: 0.026 (0.037)
Train: 96 [ 200/1251 ( 16%)]  Loss: 3.393 (3.58)  Time: 0.303s, 3384.02/s  (0.308s, 3323.86/s)  LR: 7.684e-04  Data: 0.022 (0.033)
Train: 96 [ 250/1251 ( 20%)]  Loss: 3.827 (3.62)  Time: 0.293s, 3492.28/s  (0.307s, 3334.39/s)  LR: 7.682e-04  Data: 0.021 (0.031)
Train: 96 [ 300/1251 ( 24%)]  Loss: 3.915 (3.66)  Time: 0.307s, 3338.60/s  (0.307s, 3338.43/s)  LR: 7.680e-04  Data: 0.024 (0.030)
Train: 96 [ 350/1251 ( 28%)]  Loss: 3.572 (3.65)  Time: 0.307s, 3337.29/s  (0.306s, 3342.41/s)  LR: 7.678e-04  Data: 0.024 (0.029)
Train: 96 [ 400/1251 ( 32%)]  Loss: 3.941 (3.68)  Time: 0.309s, 3313.43/s  (0.306s, 3343.90/s)  LR: 7.677e-04  Data: 0.026 (0.028)
Train: 96 [ 450/1251 ( 36%)]  Loss: 3.323 (3.65)  Time: 0.303s, 3384.91/s  (0.306s, 3345.62/s)  LR: 7.675e-04  Data: 0.021 (0.028)
Train: 96 [ 500/1251 ( 40%)]  Loss: 3.493 (3.63)  Time: 0.305s, 3352.20/s  (0.306s, 3346.60/s)  LR: 7.673e-04  Data: 0.022 (0.027)
Train: 96 [ 550/1251 ( 44%)]  Loss: 3.803 (3.65)  Time: 0.308s, 3322.52/s  (0.306s, 3347.68/s)  LR: 7.671e-04  Data: 0.021 (0.027)
Train: 96 [ 600/1251 ( 48%)]  Loss: 3.847 (3.66)  Time: 0.313s, 3275.67/s  (0.306s, 3347.90/s)  LR: 7.670e-04  Data: 0.018 (0.026)
Train: 96 [ 650/1251 ( 52%)]  Loss: 3.932 (3.68)  Time: 0.303s, 3377.04/s  (0.306s, 3347.95/s)  LR: 7.668e-04  Data: 0.022 (0.026)
Train: 96 [ 700/1251 ( 56%)]  Loss: 3.545 (3.67)  Time: 0.310s, 3307.81/s  (0.306s, 3348.21/s)  LR: 7.666e-04  Data: 0.024 (0.026)
Train: 96 [ 750/1251 ( 60%)]  Loss: 3.762 (3.68)  Time: 0.315s, 3251.33/s  (0.306s, 3348.72/s)  LR: 7.664e-04  Data: 0.022 (0.026)
Train: 96 [ 800/1251 ( 64%)]  Loss: 3.610 (3.67)  Time: 0.307s, 3337.99/s  (0.306s, 3349.06/s)  LR: 7.663e-04  Data: 0.024 (0.026)
Train: 96 [ 850/1251 ( 68%)]  Loss: 3.864 (3.68)  Time: 0.305s, 3359.34/s  (0.306s, 3349.47/s)  LR: 7.661e-04  Data: 0.022 (0.025)
Train: 96 [ 900/1251 ( 72%)]  Loss: 3.608 (3.68)  Time: 0.309s, 3310.54/s  (0.306s, 3349.30/s)  LR: 7.659e-04  Data: 0.024 (0.025)
Train: 96 [ 950/1251 ( 76%)]  Loss: 4.003 (3.70)  Time: 0.307s, 3330.33/s  (0.306s, 3349.45/s)  LR: 7.657e-04  Data: 0.024 (0.025)
Train: 96 [1000/1251 ( 80%)]  Loss: 3.819 (3.70)  Time: 0.301s, 3405.24/s  (0.306s, 3349.26/s)  LR: 7.655e-04  Data: 0.023 (0.025)
Train: 96 [1050/1251 ( 84%)]  Loss: 3.626 (3.70)  Time: 0.304s, 3365.27/s  (0.306s, 3349.04/s)  LR: 7.654e-04  Data: 0.020 (0.025)
Train: 96 [1100/1251 ( 88%)]  Loss: 3.520 (3.69)  Time: 0.315s, 3248.05/s  (0.306s, 3348.47/s)  LR: 7.652e-04  Data: 0.023 (0.025)
Train: 96 [1150/1251 ( 92%)]  Loss: 3.586 (3.69)  Time: 0.305s, 3357.11/s  (0.306s, 3348.30/s)  LR: 7.650e-04  Data: 0.023 (0.025)
Train: 96 [1200/1251 ( 96%)]  Loss: 3.503 (3.68)  Time: 0.305s, 3360.36/s  (0.306s, 3348.34/s)  LR: 7.648e-04  Data: 0.022 (0.025)
Train: 96 [1250/1251 (100%)]  Loss: 3.538 (3.67)  Time: 0.277s, 3703.36/s  (0.306s, 3350.30/s)  LR: 7.647e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.237 (2.237)  Loss:  0.6865 (0.6865)  Acc@1: 86.2305 (86.2305)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.041 (0.238)  Loss:  0.7456 (1.2723)  Acc@1: 85.3774 (71.2640)  Acc@5: 95.7547 (90.4580)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-96.pth.tar', 71.26400011230469)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-91.pth.tar', 70.98600001464844)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-95.pth.tar', 70.95600006347657)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-94.pth.tar', 70.87200006591797)

Train: 97 [   0/1251 (  0%)]  Loss: 3.813 (3.81)  Time: 2.126s,  481.70/s  (2.126s,  481.70/s)  LR: 7.647e-04  Data: 1.889 (1.889)
Train: 97 [  50/1251 (  4%)]  Loss: 3.748 (3.78)  Time: 0.300s, 3414.58/s  (0.325s, 3148.94/s)  LR: 7.645e-04  Data: 0.022 (0.061)
Train: 97 [ 100/1251 (  8%)]  Loss: 3.748 (3.77)  Time: 0.301s, 3397.89/s  (0.312s, 3278.06/s)  LR: 7.643e-04  Data: 0.021 (0.042)
Train: 97 [ 150/1251 ( 12%)]  Loss: 3.684 (3.75)  Time: 0.305s, 3359.71/s  (0.309s, 3314.05/s)  LR: 7.641e-04  Data: 0.023 (0.036)
Train: 97 [ 200/1251 ( 16%)]  Loss: 3.806 (3.76)  Time: 0.306s, 3347.82/s  (0.308s, 3325.79/s)  LR: 7.639e-04  Data: 0.023 (0.032)
Train: 97 [ 250/1251 ( 20%)]  Loss: 3.639 (3.74)  Time: 0.309s, 3312.71/s  (0.307s, 3331.53/s)  LR: 7.638e-04  Data: 0.022 (0.031)
Train: 97 [ 300/1251 ( 24%)]  Loss: 3.548 (3.71)  Time: 0.308s, 3326.76/s  (0.307s, 3333.21/s)  LR: 7.636e-04  Data: 0.023 (0.029)
Train: 97 [ 350/1251 ( 28%)]  Loss: 3.784 (3.72)  Time: 0.311s, 3288.36/s  (0.307s, 3335.48/s)  LR: 7.634e-04  Data: 0.025 (0.028)
Train: 97 [ 400/1251 ( 32%)]  Loss: 3.832 (3.73)  Time: 0.312s, 3277.01/s  (0.307s, 3337.11/s)  LR: 7.632e-04  Data: 0.022 (0.028)
Train: 97 [ 450/1251 ( 36%)]  Loss: 3.797 (3.74)  Time: 0.305s, 3359.41/s  (0.307s, 3338.20/s)  LR: 7.631e-04  Data: 0.022 (0.027)
Train: 97 [ 500/1251 ( 40%)]  Loss: 3.723 (3.74)  Time: 0.311s, 3288.66/s  (0.307s, 3338.71/s)  LR: 7.629e-04  Data: 0.020 (0.027)
Train: 97 [ 550/1251 ( 44%)]  Loss: 3.664 (3.73)  Time: 0.310s, 3299.84/s  (0.307s, 3338.65/s)  LR: 7.627e-04  Data: 0.023 (0.026)
Train: 97 [ 600/1251 ( 48%)]  Loss: 3.815 (3.74)  Time: 0.305s, 3360.44/s  (0.307s, 3338.27/s)  LR: 7.625e-04  Data: 0.023 (0.026)
Train: 97 [ 650/1251 ( 52%)]  Loss: 3.583 (3.73)  Time: 0.314s, 3263.22/s  (0.307s, 3337.80/s)  LR: 7.624e-04  Data: 0.027 (0.026)
Train: 97 [ 700/1251 ( 56%)]  Loss: 3.935 (3.74)  Time: 0.309s, 3314.51/s  (0.307s, 3337.86/s)  LR: 7.622e-04  Data: 0.025 (0.026)
Train: 97 [ 750/1251 ( 60%)]  Loss: 3.539 (3.73)  Time: 0.306s, 3345.68/s  (0.307s, 3337.07/s)  LR: 7.620e-04  Data: 0.019 (0.026)
Train: 97 [ 800/1251 ( 64%)]  Loss: 3.632 (3.72)  Time: 0.308s, 3322.04/s  (0.307s, 3335.97/s)  LR: 7.618e-04  Data: 0.021 (0.025)
Train: 97 [ 850/1251 ( 68%)]  Loss: 3.786 (3.73)  Time: 0.308s, 3322.65/s  (0.307s, 3334.77/s)  LR: 7.616e-04  Data: 0.023 (0.025)
Train: 97 [ 900/1251 ( 72%)]  Loss: 3.757 (3.73)  Time: 0.309s, 3311.77/s  (0.307s, 3334.46/s)  LR: 7.615e-04  Data: 0.023 (0.025)
Train: 97 [ 950/1251 ( 76%)]  Loss: 3.485 (3.72)  Time: 0.304s, 3366.72/s  (0.307s, 3334.51/s)  LR: 7.613e-04  Data: 0.021 (0.025)
Train: 97 [1000/1251 ( 80%)]  Loss: 3.921 (3.73)  Time: 0.309s, 3308.79/s  (0.307s, 3334.69/s)  LR: 7.611e-04  Data: 0.016 (0.025)
Train: 97 [1050/1251 ( 84%)]  Loss: 3.501 (3.72)  Time: 0.296s, 3454.67/s  (0.307s, 3333.72/s)  LR: 7.609e-04  Data: 0.021 (0.025)
Train: 97 [1100/1251 ( 88%)]  Loss: 3.795 (3.72)  Time: 0.308s, 3324.80/s  (0.307s, 3332.96/s)  LR: 7.608e-04  Data: 0.023 (0.025)
Train: 97 [1150/1251 ( 92%)]  Loss: 3.487 (3.71)  Time: 0.310s, 3307.87/s  (0.307s, 3333.18/s)  LR: 7.606e-04  Data: 0.025 (0.025)
Train: 97 [1200/1251 ( 96%)]  Loss: 3.726 (3.71)  Time: 0.312s, 3287.26/s  (0.307s, 3332.55/s)  LR: 7.604e-04  Data: 0.023 (0.025)
Train: 97 [1250/1251 (100%)]  Loss: 3.283 (3.69)  Time: 0.276s, 3707.45/s  (0.307s, 3333.92/s)  LR: 7.602e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.076 (2.076)  Loss:  0.6494 (0.6494)  Acc@1: 86.3281 (86.3281)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.041 (0.242)  Loss:  0.7173 (1.2756)  Acc@1: 84.5519 (71.1260)  Acc@5: 95.7547 (90.5740)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-96.pth.tar', 71.26400011230469)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-97.pth.tar', 71.12600001220703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-91.pth.tar', 70.98600001464844)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-95.pth.tar', 70.95600006347657)

Train: 98 [   0/1251 (  0%)]  Loss: 3.557 (3.56)  Time: 2.386s,  429.22/s  (2.386s,  429.22/s)  LR: 7.602e-04  Data: 2.163 (2.163)
Train: 98 [  50/1251 (  4%)]  Loss: 3.795 (3.68)  Time: 0.295s, 3467.99/s  (0.332s, 3087.64/s)  LR: 7.600e-04  Data: 0.021 (0.066)
Train: 98 [ 100/1251 (  8%)]  Loss: 3.697 (3.68)  Time: 0.307s, 3338.23/s  (0.317s, 3230.26/s)  LR: 7.599e-04  Data: 0.023 (0.045)
Train: 98 [ 150/1251 ( 12%)]  Loss: 3.655 (3.68)  Time: 0.305s, 3358.70/s  (0.313s, 3275.32/s)  LR: 7.597e-04  Data: 0.023 (0.038)
Train: 98 [ 200/1251 ( 16%)]  Loss: 3.629 (3.67)  Time: 0.306s, 3351.01/s  (0.311s, 3293.83/s)  LR: 7.595e-04  Data: 0.023 (0.034)
Train: 98 [ 250/1251 ( 20%)]  Loss: 3.770 (3.68)  Time: 0.307s, 3338.66/s  (0.310s, 3303.04/s)  LR: 7.593e-04  Data: 0.022 (0.032)
Train: 98 [ 300/1251 ( 24%)]  Loss: 3.534 (3.66)  Time: 0.302s, 3393.97/s  (0.310s, 3307.84/s)  LR: 7.591e-04  Data: 0.023 (0.030)
Train: 98 [ 350/1251 ( 28%)]  Loss: 3.851 (3.69)  Time: 0.307s, 3333.07/s  (0.309s, 3309.78/s)  LR: 7.590e-04  Data: 0.023 (0.029)
Train: 98 [ 400/1251 ( 32%)]  Loss: 3.819 (3.70)  Time: 0.310s, 3304.72/s  (0.309s, 3310.93/s)  LR: 7.588e-04  Data: 0.024 (0.028)
Train: 98 [ 450/1251 ( 36%)]  Loss: 3.597 (3.69)  Time: 0.306s, 3341.54/s  (0.309s, 3312.41/s)  LR: 7.586e-04  Data: 0.026 (0.028)
Train: 98 [ 500/1251 ( 40%)]  Loss: 3.775 (3.70)  Time: 0.312s, 3285.66/s  (0.309s, 3314.02/s)  LR: 7.584e-04  Data: 0.022 (0.027)
Train: 98 [ 550/1251 ( 44%)]  Loss: 3.688 (3.70)  Time: 0.303s, 3376.40/s  (0.309s, 3314.30/s)  LR: 7.583e-04  Data: 0.021 (0.027)
Train: 98 [ 600/1251 ( 48%)]  Loss: 3.637 (3.69)  Time: 0.306s, 3349.56/s  (0.309s, 3315.16/s)  LR: 7.581e-04  Data: 0.025 (0.026)
Train: 98 [ 650/1251 ( 52%)]  Loss: 3.643 (3.69)  Time: 0.307s, 3332.36/s  (0.309s, 3315.68/s)  LR: 7.579e-04  Data: 0.022 (0.026)
Train: 98 [ 700/1251 ( 56%)]  Loss: 3.890 (3.70)  Time: 0.316s, 3236.12/s  (0.309s, 3314.72/s)  LR: 7.577e-04  Data: 0.026 (0.026)
Train: 98 [ 750/1251 ( 60%)]  Loss: 3.808 (3.71)  Time: 0.313s, 3270.22/s  (0.309s, 3314.80/s)  LR: 7.575e-04  Data: 0.024 (0.026)
Train: 98 [ 800/1251 ( 64%)]  Loss: 3.628 (3.70)  Time: 0.311s, 3297.24/s  (0.309s, 3314.89/s)  LR: 7.574e-04  Data: 0.025 (0.026)
Train: 98 [ 850/1251 ( 68%)]  Loss: 3.710 (3.70)  Time: 0.305s, 3354.10/s  (0.309s, 3315.11/s)  LR: 7.572e-04  Data: 0.023 (0.025)
Train: 98 [ 900/1251 ( 72%)]  Loss: 3.625 (3.70)  Time: 0.313s, 3267.85/s  (0.309s, 3314.90/s)  LR: 7.570e-04  Data: 0.027 (0.025)
Train: 98 [ 950/1251 ( 76%)]  Loss: 3.774 (3.70)  Time: 0.311s, 3294.95/s  (0.309s, 3315.35/s)  LR: 7.568e-04  Data: 0.024 (0.025)
Train: 98 [1000/1251 ( 80%)]  Loss: 3.583 (3.70)  Time: 0.309s, 3309.50/s  (0.309s, 3315.24/s)  LR: 7.566e-04  Data: 0.023 (0.025)
Train: 98 [1050/1251 ( 84%)]  Loss: 3.664 (3.70)  Time: 0.307s, 3337.45/s  (0.309s, 3314.81/s)  LR: 7.565e-04  Data: 0.021 (0.025)
Train: 98 [1100/1251 ( 88%)]  Loss: 3.568 (3.69)  Time: 0.311s, 3290.60/s  (0.309s, 3314.42/s)  LR: 7.563e-04  Data: 0.021 (0.025)
Train: 98 [1150/1251 ( 92%)]  Loss: 3.717 (3.69)  Time: 0.309s, 3313.13/s  (0.309s, 3314.53/s)  LR: 7.561e-04  Data: 0.022 (0.025)
Train: 98 [1200/1251 ( 96%)]  Loss: 3.900 (3.70)  Time: 0.309s, 3312.28/s  (0.309s, 3314.63/s)  LR: 7.559e-04  Data: 0.022 (0.025)
Train: 98 [1250/1251 (100%)]  Loss: 3.683 (3.70)  Time: 0.276s, 3713.62/s  (0.309s, 3316.72/s)  LR: 7.557e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.083 (2.083)  Loss:  0.6558 (0.6558)  Acc@1: 86.2305 (86.2305)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.061 (0.239)  Loss:  0.7651 (1.2712)  Acc@1: 82.9009 (71.2060)  Acc@5: 95.7547 (90.3740)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-96.pth.tar', 71.26400011230469)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-98.pth.tar', 71.20599994140625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-97.pth.tar', 71.12600001220703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-91.pth.tar', 70.98600001464844)

Train: 99 [   0/1251 (  0%)]  Loss: 3.972 (3.97)  Time: 2.435s,  420.46/s  (2.435s,  420.46/s)  LR: 7.557e-04  Data: 2.219 (2.219)
Train: 99 [  50/1251 (  4%)]  Loss: 3.501 (3.74)  Time: 0.302s, 3391.12/s  (0.330s, 3100.24/s)  LR: 7.556e-04  Data: 0.023 (0.066)
Train: 99 [ 100/1251 (  8%)]  Loss: 3.797 (3.76)  Time: 0.307s, 3337.10/s  (0.317s, 3234.65/s)  LR: 7.554e-04  Data: 0.021 (0.044)
Train: 99 [ 150/1251 ( 12%)]  Loss: 3.498 (3.69)  Time: 0.302s, 3388.32/s  (0.313s, 3276.03/s)  LR: 7.552e-04  Data: 0.024 (0.038)
Train: 99 [ 200/1251 ( 16%)]  Loss: 2.989 (3.55)  Time: 0.303s, 3375.04/s  (0.311s, 3294.26/s)  LR: 7.550e-04  Data: 0.025 (0.034)
Train: 99 [ 250/1251 ( 20%)]  Loss: 3.782 (3.59)  Time: 0.314s, 3261.52/s  (0.310s, 3303.55/s)  LR: 7.548e-04  Data: 0.023 (0.032)
Train: 99 [ 300/1251 ( 24%)]  Loss: 4.026 (3.65)  Time: 0.313s, 3271.32/s  (0.310s, 3307.90/s)  LR: 7.547e-04  Data: 0.022 (0.030)
Train: 99 [ 350/1251 ( 28%)]  Loss: 3.424 (3.62)  Time: 0.308s, 3325.25/s  (0.309s, 3309.07/s)  LR: 7.545e-04  Data: 0.021 (0.029)
Train: 99 [ 400/1251 ( 32%)]  Loss: 3.880 (3.65)  Time: 0.302s, 3390.53/s  (0.309s, 3310.60/s)  LR: 7.543e-04  Data: 0.019 (0.028)
Train: 99 [ 450/1251 ( 36%)]  Loss: 3.960 (3.68)  Time: 0.310s, 3298.69/s  (0.309s, 3310.92/s)  LR: 7.541e-04  Data: 0.026 (0.028)
Train: 99 [ 500/1251 ( 40%)]  Loss: 3.470 (3.66)  Time: 0.310s, 3302.66/s  (0.309s, 3311.62/s)  LR: 7.540e-04  Data: 0.022 (0.027)
Train: 99 [ 550/1251 ( 44%)]  Loss: 3.508 (3.65)  Time: 0.312s, 3281.77/s  (0.309s, 3311.18/s)  LR: 7.538e-04  Data: 0.020 (0.027)
Train: 99 [ 600/1251 ( 48%)]  Loss: 3.422 (3.63)  Time: 0.312s, 3278.76/s  (0.309s, 3310.90/s)  LR: 7.536e-04  Data: 0.013 (0.027)
Train: 99 [ 650/1251 ( 52%)]  Loss: 3.542 (3.63)  Time: 0.303s, 3375.91/s  (0.309s, 3310.95/s)  LR: 7.534e-04  Data: 0.021 (0.026)
Train: 99 [ 700/1251 ( 56%)]  Loss: 3.648 (3.63)  Time: 0.311s, 3297.14/s  (0.309s, 3311.14/s)  LR: 7.532e-04  Data: 0.022 (0.026)
Train: 99 [ 750/1251 ( 60%)]  Loss: 3.349 (3.61)  Time: 0.312s, 3278.43/s  (0.309s, 3310.36/s)  LR: 7.531e-04  Data: 0.023 (0.026)
Train: 99 [ 800/1251 ( 64%)]  Loss: 3.899 (3.63)  Time: 0.307s, 3330.66/s  (0.309s, 3310.67/s)  LR: 7.529e-04  Data: 0.019 (0.026)
Train: 99 [ 850/1251 ( 68%)]  Loss: 3.711 (3.63)  Time: 0.303s, 3383.74/s  (0.309s, 3310.39/s)  LR: 7.527e-04  Data: 0.019 (0.025)
Train: 99 [ 900/1251 ( 72%)]  Loss: 3.939 (3.65)  Time: 0.306s, 3341.09/s  (0.309s, 3310.10/s)  LR: 7.525e-04  Data: 0.021 (0.025)
Train: 99 [ 950/1251 ( 76%)]  Loss: 3.617 (3.65)  Time: 0.307s, 3333.82/s  (0.309s, 3310.24/s)  LR: 7.523e-04  Data: 0.022 (0.025)
Train: 99 [1000/1251 ( 80%)]  Loss: 3.809 (3.65)  Time: 0.309s, 3308.83/s  (0.309s, 3310.42/s)  LR: 7.522e-04  Data: 0.023 (0.025)
Train: 99 [1050/1251 ( 84%)]  Loss: 3.442 (3.64)  Time: 0.313s, 3272.51/s  (0.309s, 3310.28/s)  LR: 7.520e-04  Data: 0.023 (0.025)
Train: 99 [1100/1251 ( 88%)]  Loss: 3.598 (3.64)  Time: 0.306s, 3348.48/s  (0.309s, 3310.40/s)  LR: 7.518e-04  Data: 0.023 (0.025)
Train: 99 [1150/1251 ( 92%)]  Loss: 3.762 (3.65)  Time: 0.309s, 3315.38/s  (0.309s, 3310.35/s)  LR: 7.516e-04  Data: 0.023 (0.025)
Train: 99 [1200/1251 ( 96%)]  Loss: 3.746 (3.65)  Time: 0.307s, 3339.97/s  (0.309s, 3310.22/s)  LR: 7.514e-04  Data: 0.020 (0.025)
Train: 99 [1250/1251 (100%)]  Loss: 4.027 (3.67)  Time: 0.278s, 3677.38/s  (0.309s, 3312.11/s)  LR: 7.512e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.087 (2.087)  Loss:  0.6914 (0.6914)  Acc@1: 86.0352 (86.0352)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.040 (0.235)  Loss:  0.7002 (1.2858)  Acc@1: 84.7877 (71.1280)  Acc@5: 96.2264 (90.4840)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-96.pth.tar', 71.26400011230469)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-98.pth.tar', 71.20599994140625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-99.pth.tar', 71.12799998535156)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-97.pth.tar', 71.12600001220703)

Train: 100 [   0/1251 (  0%)]  Loss: 3.846 (3.85)  Time: 2.268s,  451.50/s  (2.268s,  451.50/s)  LR: 7.512e-04  Data: 2.045 (2.045)
Train: 100 [  50/1251 (  4%)]  Loss: 3.406 (3.63)  Time: 0.303s, 3383.83/s  (0.331s, 3097.38/s)  LR: 7.511e-04  Data: 0.024 (0.064)
Train: 100 [ 100/1251 (  8%)]  Loss: 3.609 (3.62)  Time: 0.303s, 3382.89/s  (0.317s, 3227.68/s)  LR: 7.509e-04  Data: 0.023 (0.043)
Train: 100 [ 150/1251 ( 12%)]  Loss: 3.620 (3.62)  Time: 0.305s, 3353.03/s  (0.314s, 3264.06/s)  LR: 7.507e-04  Data: 0.017 (0.036)
Train: 100 [ 200/1251 ( 16%)]  Loss: 4.040 (3.70)  Time: 0.308s, 3322.49/s  (0.312s, 3281.74/s)  LR: 7.505e-04  Data: 0.025 (0.033)
Train: 100 [ 250/1251 ( 20%)]  Loss: 3.896 (3.74)  Time: 0.306s, 3351.23/s  (0.311s, 3289.56/s)  LR: 7.503e-04  Data: 0.021 (0.031)
Train: 100 [ 300/1251 ( 24%)]  Loss: 3.339 (3.68)  Time: 0.310s, 3305.39/s  (0.311s, 3294.51/s)  LR: 7.502e-04  Data: 0.023 (0.030)
Train: 100 [ 350/1251 ( 28%)]  Loss: 3.769 (3.69)  Time: 0.314s, 3257.34/s  (0.311s, 3297.30/s)  LR: 7.500e-04  Data: 0.025 (0.029)
Train: 100 [ 400/1251 ( 32%)]  Loss: 3.268 (3.64)  Time: 0.307s, 3336.49/s  (0.310s, 3298.39/s)  LR: 7.498e-04  Data: 0.020 (0.028)
Train: 100 [ 450/1251 ( 36%)]  Loss: 3.753 (3.65)  Time: 0.312s, 3281.74/s  (0.310s, 3299.87/s)  LR: 7.496e-04  Data: 0.026 (0.027)
Train: 100 [ 500/1251 ( 40%)]  Loss: 3.481 (3.64)  Time: 0.310s, 3298.22/s  (0.310s, 3300.92/s)  LR: 7.494e-04  Data: 0.024 (0.027)
Train: 100 [ 550/1251 ( 44%)]  Loss: 3.571 (3.63)  Time: 0.307s, 3337.99/s  (0.310s, 3301.42/s)  LR: 7.493e-04  Data: 0.023 (0.027)
Train: 100 [ 600/1251 ( 48%)]  Loss: 3.846 (3.65)  Time: 0.307s, 3335.23/s  (0.310s, 3301.94/s)  LR: 7.491e-04  Data: 0.026 (0.026)
Train: 100 [ 650/1251 ( 52%)]  Loss: 3.676 (3.65)  Time: 0.311s, 3297.45/s  (0.310s, 3302.41/s)  LR: 7.489e-04  Data: 0.023 (0.026)
Train: 100 [ 700/1251 ( 56%)]  Loss: 3.685 (3.65)  Time: 0.312s, 3280.13/s  (0.310s, 3301.83/s)  LR: 7.487e-04  Data: 0.025 (0.026)
Train: 100 [ 750/1251 ( 60%)]  Loss: 3.542 (3.65)  Time: 0.315s, 3248.15/s  (0.310s, 3301.64/s)  LR: 7.485e-04  Data: 0.026 (0.026)
Train: 100 [ 800/1251 ( 64%)]  Loss: 3.431 (3.63)  Time: 0.310s, 3307.33/s  (0.310s, 3300.99/s)  LR: 7.484e-04  Data: 0.024 (0.025)
Train: 100 [ 850/1251 ( 68%)]  Loss: 3.816 (3.64)  Time: 0.306s, 3342.02/s  (0.310s, 3301.41/s)  LR: 7.482e-04  Data: 0.020 (0.025)
Train: 100 [ 900/1251 ( 72%)]  Loss: 3.660 (3.64)  Time: 0.314s, 3264.51/s  (0.310s, 3300.91/s)  LR: 7.480e-04  Data: 0.023 (0.025)
Train: 100 [ 950/1251 ( 76%)]  Loss: 3.882 (3.66)  Time: 0.310s, 3298.13/s  (0.310s, 3300.79/s)  LR: 7.478e-04  Data: 0.025 (0.025)
Train: 100 [1000/1251 ( 80%)]  Loss: 3.677 (3.66)  Time: 0.312s, 3278.85/s  (0.310s, 3300.79/s)  LR: 7.476e-04  Data: 0.022 (0.025)
Train: 100 [1050/1251 ( 84%)]  Loss: 3.754 (3.66)  Time: 0.314s, 3263.00/s  (0.310s, 3300.70/s)  LR: 7.474e-04  Data: 0.026 (0.025)
Train: 100 [1100/1251 ( 88%)]  Loss: 3.680 (3.66)  Time: 0.320s, 3203.66/s  (0.310s, 3300.33/s)  LR: 7.473e-04  Data: 0.024 (0.025)
Train: 100 [1150/1251 ( 92%)]  Loss: 3.555 (3.66)  Time: 0.314s, 3266.31/s  (0.310s, 3300.16/s)  LR: 7.471e-04  Data: 0.027 (0.025)
Train: 100 [1200/1251 ( 96%)]  Loss: 3.938 (3.67)  Time: 0.316s, 3239.82/s  (0.310s, 3299.83/s)  LR: 7.469e-04  Data: 0.026 (0.025)
Train: 100 [1250/1251 (100%)]  Loss: 3.885 (3.68)  Time: 0.284s, 3607.99/s  (0.310s, 3301.64/s)  LR: 7.467e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.168 (2.168)  Loss:  0.6567 (0.6567)  Acc@1: 87.5977 (87.5977)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.044 (0.238)  Loss:  0.7554 (1.2624)  Acc@1: 83.8443 (71.0500)  Acc@5: 94.3396 (90.4440)
Train: 101 [   0/1251 (  0%)]  Loss: 3.562 (3.56)  Time: 2.434s,  420.77/s  (2.434s,  420.77/s)  LR: 7.467e-04  Data: 2.214 (2.214)
Train: 101 [  50/1251 (  4%)]  Loss: 3.699 (3.63)  Time: 0.302s, 3389.61/s  (0.346s, 2958.21/s)  LR: 7.465e-04  Data: 0.023 (0.079)
Train: 101 [ 100/1251 (  8%)]  Loss: 3.878 (3.71)  Time: 0.303s, 3384.12/s  (0.326s, 3144.82/s)  LR: 7.464e-04  Data: 0.027 (0.051)
Train: 101 [ 150/1251 ( 12%)]  Loss: 4.138 (3.82)  Time: 0.316s, 3242.15/s  (0.320s, 3204.61/s)  LR: 7.462e-04  Data: 0.026 (0.042)
Train: 101 [ 200/1251 ( 16%)]  Loss: 3.635 (3.78)  Time: 0.309s, 3312.49/s  (0.317s, 3233.06/s)  LR: 7.460e-04  Data: 0.022 (0.037)
Train: 101 [ 250/1251 ( 20%)]  Loss: 3.582 (3.75)  Time: 0.308s, 3324.53/s  (0.315s, 3249.70/s)  LR: 7.458e-04  Data: 0.023 (0.034)
Train: 101 [ 300/1251 ( 24%)]  Loss: 3.953 (3.78)  Time: 0.309s, 3318.43/s  (0.314s, 3260.19/s)  LR: 7.456e-04  Data: 0.023 (0.032)
Train: 101 [ 350/1251 ( 28%)]  Loss: 3.699 (3.77)  Time: 0.308s, 3327.76/s  (0.313s, 3267.75/s)  LR: 7.455e-04  Data: 0.025 (0.031)
Train: 101 [ 400/1251 ( 32%)]  Loss: 3.960 (3.79)  Time: 0.306s, 3340.99/s  (0.313s, 3272.67/s)  LR: 7.453e-04  Data: 0.021 (0.030)
Train: 101 [ 450/1251 ( 36%)]  Loss: 3.835 (3.79)  Time: 0.308s, 3325.63/s  (0.313s, 3275.42/s)  LR: 7.451e-04  Data: 0.023 (0.029)
Train: 101 [ 500/1251 ( 40%)]  Loss: 3.352 (3.75)  Time: 0.313s, 3272.80/s  (0.312s, 3278.37/s)  LR: 7.449e-04  Data: 0.024 (0.029)
Train: 101 [ 550/1251 ( 44%)]  Loss: 3.314 (3.72)  Time: 0.314s, 3261.23/s  (0.312s, 3280.17/s)  LR: 7.447e-04  Data: 0.024 (0.028)
Train: 101 [ 600/1251 ( 48%)]  Loss: 3.745 (3.72)  Time: 0.304s, 3363.86/s  (0.312s, 3281.89/s)  LR: 7.445e-04  Data: 0.020 (0.028)
Train: 101 [ 650/1251 ( 52%)]  Loss: 3.852 (3.73)  Time: 0.305s, 3362.85/s  (0.312s, 3284.13/s)  LR: 7.444e-04  Data: 0.021 (0.027)
Train: 101 [ 700/1251 ( 56%)]  Loss: 3.790 (3.73)  Time: 0.314s, 3263.95/s  (0.312s, 3286.12/s)  LR: 7.442e-04  Data: 0.023 (0.027)
Train: 101 [ 750/1251 ( 60%)]  Loss: 3.762 (3.73)  Time: 0.315s, 3252.49/s  (0.311s, 3287.70/s)  LR: 7.440e-04  Data: 0.025 (0.027)
Train: 101 [ 800/1251 ( 64%)]  Loss: 3.491 (3.72)  Time: 0.307s, 3338.66/s  (0.311s, 3288.99/s)  LR: 7.438e-04  Data: 0.022 (0.026)
Train: 101 [ 850/1251 ( 68%)]  Loss: 3.834 (3.73)  Time: 0.309s, 3318.62/s  (0.311s, 3289.87/s)  LR: 7.436e-04  Data: 0.022 (0.026)
Train: 101 [ 900/1251 ( 72%)]  Loss: 3.812 (3.73)  Time: 0.313s, 3276.28/s  (0.311s, 3290.95/s)  LR: 7.434e-04  Data: 0.022 (0.026)
Train: 101 [ 950/1251 ( 76%)]  Loss: 3.715 (3.73)  Time: 0.311s, 3297.63/s  (0.311s, 3291.48/s)  LR: 7.433e-04  Data: 0.023 (0.026)
Train: 101 [1000/1251 ( 80%)]  Loss: 3.835 (3.74)  Time: 0.308s, 3321.99/s  (0.311s, 3292.52/s)  LR: 7.431e-04  Data: 0.025 (0.026)
Train: 101 [1050/1251 ( 84%)]  Loss: 3.471 (3.72)  Time: 0.312s, 3286.95/s  (0.311s, 3293.14/s)  LR: 7.429e-04  Data: 0.025 (0.026)
Train: 101 [1100/1251 ( 88%)]  Loss: 3.795 (3.73)  Time: 0.313s, 3274.32/s  (0.311s, 3293.63/s)  LR: 7.427e-04  Data: 0.027 (0.026)
Train: 101 [1150/1251 ( 92%)]  Loss: 3.605 (3.72)  Time: 0.304s, 3367.23/s  (0.311s, 3294.09/s)  LR: 7.425e-04  Data: 0.022 (0.025)
Train: 101 [1200/1251 ( 96%)]  Loss: 3.502 (3.71)  Time: 0.310s, 3300.86/s  (0.311s, 3294.71/s)  LR: 7.424e-04  Data: 0.024 (0.025)
Train: 101 [1250/1251 (100%)]  Loss: 3.705 (3.71)  Time: 0.277s, 3694.33/s  (0.311s, 3297.45/s)  LR: 7.422e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.054 (2.054)  Loss:  0.6499 (0.6499)  Acc@1: 86.5234 (86.5234)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.065 (0.236)  Loss:  0.7637 (1.2640)  Acc@1: 84.1981 (71.3580)  Acc@5: 95.7547 (90.7900)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-101.pth.tar', 71.3580001171875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-96.pth.tar', 71.26400011230469)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-98.pth.tar', 71.20599994140625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-99.pth.tar', 71.12799998535156)

Train: 102 [   0/1251 (  0%)]  Loss: 3.768 (3.77)  Time: 2.521s,  406.13/s  (2.521s,  406.13/s)  LR: 7.422e-04  Data: 2.308 (2.308)
Train: 102 [  50/1251 (  4%)]  Loss: 3.339 (3.55)  Time: 0.298s, 3438.09/s  (0.333s, 3079.48/s)  LR: 7.420e-04  Data: 0.026 (0.068)
Train: 102 [ 100/1251 (  8%)]  Loss: 3.847 (3.65)  Time: 0.309s, 3318.39/s  (0.318s, 3223.31/s)  LR: 7.418e-04  Data: 0.025 (0.045)
Train: 102 [ 150/1251 ( 12%)]  Loss: 3.751 (3.68)  Time: 0.307s, 3340.69/s  (0.313s, 3267.48/s)  LR: 7.416e-04  Data: 0.024 (0.038)
Train: 102 [ 200/1251 ( 16%)]  Loss: 3.433 (3.63)  Time: 0.309s, 3311.94/s  (0.312s, 3283.08/s)  LR: 7.414e-04  Data: 0.021 (0.034)
Train: 102 [ 250/1251 ( 20%)]  Loss: 3.913 (3.68)  Time: 0.307s, 3336.29/s  (0.311s, 3293.30/s)  LR: 7.413e-04  Data: 0.024 (0.032)
Train: 102 [ 300/1251 ( 24%)]  Loss: 3.617 (3.67)  Time: 0.310s, 3300.47/s  (0.310s, 3300.21/s)  LR: 7.411e-04  Data: 0.025 (0.030)
Train: 102 [ 350/1251 ( 28%)]  Loss: 3.692 (3.67)  Time: 0.311s, 3292.34/s  (0.310s, 3301.90/s)  LR: 7.409e-04  Data: 0.023 (0.029)
Train: 102 [ 400/1251 ( 32%)]  Loss: 3.489 (3.65)  Time: 0.309s, 3311.89/s  (0.310s, 3304.10/s)  LR: 7.407e-04  Data: 0.022 (0.028)
Train: 102 [ 450/1251 ( 36%)]  Loss: 4.003 (3.69)  Time: 0.307s, 3337.94/s  (0.310s, 3305.98/s)  LR: 7.405e-04  Data: 0.022 (0.028)
Train: 102 [ 500/1251 ( 40%)]  Loss: 3.617 (3.68)  Time: 0.307s, 3331.82/s  (0.310s, 3307.70/s)  LR: 7.403e-04  Data: 0.021 (0.027)
Train: 102 [ 550/1251 ( 44%)]  Loss: 3.760 (3.69)  Time: 0.311s, 3295.23/s  (0.310s, 3308.47/s)  LR: 7.402e-04  Data: 0.022 (0.027)
Train: 102 [ 600/1251 ( 48%)]  Loss: 4.143 (3.72)  Time: 0.310s, 3305.75/s  (0.309s, 3309.00/s)  LR: 7.400e-04  Data: 0.024 (0.027)
Train: 102 [ 650/1251 ( 52%)]  Loss: 3.593 (3.71)  Time: 0.308s, 3320.03/s  (0.309s, 3309.01/s)  LR: 7.398e-04  Data: 0.023 (0.026)
Train: 102 [ 700/1251 ( 56%)]  Loss: 3.840 (3.72)  Time: 0.309s, 3310.90/s  (0.309s, 3309.41/s)  LR: 7.396e-04  Data: 0.022 (0.026)
Train: 102 [ 750/1251 ( 60%)]  Loss: 3.684 (3.72)  Time: 0.310s, 3303.97/s  (0.309s, 3309.93/s)  LR: 7.394e-04  Data: 0.024 (0.026)
Train: 102 [ 800/1251 ( 64%)]  Loss: 3.587 (3.71)  Time: 0.310s, 3298.30/s  (0.309s, 3310.00/s)  LR: 7.392e-04  Data: 0.024 (0.026)
Train: 102 [ 850/1251 ( 68%)]  Loss: 3.942 (3.72)  Time: 0.309s, 3317.72/s  (0.309s, 3310.32/s)  LR: 7.391e-04  Data: 0.024 (0.026)
Train: 102 [ 900/1251 ( 72%)]  Loss: 3.623 (3.72)  Time: 0.314s, 3258.47/s  (0.309s, 3310.67/s)  LR: 7.389e-04  Data: 0.023 (0.025)
Train: 102 [ 950/1251 ( 76%)]  Loss: 3.702 (3.72)  Time: 0.314s, 3262.30/s  (0.309s, 3310.71/s)  LR: 7.387e-04  Data: 0.021 (0.025)
Train: 102 [1000/1251 ( 80%)]  Loss: 3.458 (3.70)  Time: 0.306s, 3346.11/s  (0.309s, 3311.12/s)  LR: 7.385e-04  Data: 0.019 (0.025)
Train: 102 [1050/1251 ( 84%)]  Loss: 3.580 (3.70)  Time: 0.306s, 3344.86/s  (0.309s, 3311.54/s)  LR: 7.383e-04  Data: 0.024 (0.025)
Train: 102 [1100/1251 ( 88%)]  Loss: 3.492 (3.69)  Time: 0.310s, 3300.19/s  (0.309s, 3311.80/s)  LR: 7.381e-04  Data: 0.021 (0.025)
Train: 102 [1150/1251 ( 92%)]  Loss: 3.345 (3.68)  Time: 0.309s, 3316.40/s  (0.309s, 3312.14/s)  LR: 7.380e-04  Data: 0.025 (0.025)
Train: 102 [1200/1251 ( 96%)]  Loss: 3.977 (3.69)  Time: 0.307s, 3336.72/s  (0.309s, 3312.40/s)  LR: 7.378e-04  Data: 0.025 (0.025)
Train: 102 [1250/1251 (100%)]  Loss: 3.592 (3.68)  Time: 0.276s, 3709.15/s  (0.309s, 3314.82/s)  LR: 7.376e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.073 (2.073)  Loss:  0.6724 (0.6724)  Acc@1: 87.0117 (87.0117)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.058 (0.237)  Loss:  0.7788 (1.2876)  Acc@1: 83.3726 (71.4140)  Acc@5: 95.6368 (90.3780)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-102.pth.tar', 71.41400001708985)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-101.pth.tar', 71.3580001171875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-96.pth.tar', 71.26400011230469)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-98.pth.tar', 71.20599994140625)

Train: 103 [   0/1251 (  0%)]  Loss: 3.663 (3.66)  Time: 2.305s,  444.32/s  (2.305s,  444.32/s)  LR: 7.376e-04  Data: 2.083 (2.083)
Train: 103 [  50/1251 (  4%)]  Loss: 3.908 (3.79)  Time: 0.298s, 3441.31/s  (0.330s, 3099.80/s)  LR: 7.374e-04  Data: 0.023 (0.066)
Train: 103 [ 100/1251 (  8%)]  Loss: 3.952 (3.84)  Time: 0.308s, 3327.64/s  (0.316s, 3240.39/s)  LR: 7.372e-04  Data: 0.025 (0.044)
Train: 103 [ 150/1251 ( 12%)]  Loss: 3.911 (3.86)  Time: 0.303s, 3382.49/s  (0.313s, 3276.35/s)  LR: 7.370e-04  Data: 0.023 (0.037)
Train: 103 [ 200/1251 ( 16%)]  Loss: 3.628 (3.81)  Time: 0.301s, 3406.08/s  (0.311s, 3294.90/s)  LR: 7.369e-04  Data: 0.024 (0.034)
Train: 103 [ 250/1251 ( 20%)]  Loss: 3.966 (3.84)  Time: 0.311s, 3292.92/s  (0.310s, 3301.99/s)  LR: 7.367e-04  Data: 0.023 (0.032)
Train: 103 [ 300/1251 ( 24%)]  Loss: 3.484 (3.79)  Time: 0.302s, 3390.25/s  (0.309s, 3309.34/s)  LR: 7.365e-04  Data: 0.022 (0.030)
Train: 103 [ 350/1251 ( 28%)]  Loss: 3.854 (3.80)  Time: 0.301s, 3403.78/s  (0.309s, 3315.06/s)  LR: 7.363e-04  Data: 0.023 (0.029)
Train: 103 [ 400/1251 ( 32%)]  Loss: 3.524 (3.77)  Time: 0.309s, 3309.50/s  (0.309s, 3318.35/s)  LR: 7.361e-04  Data: 0.024 (0.028)
Train: 103 [ 450/1251 ( 36%)]  Loss: 4.058 (3.79)  Time: 0.303s, 3374.99/s  (0.308s, 3320.42/s)  LR: 7.359e-04  Data: 0.022 (0.028)
Train: 103 [ 500/1251 ( 40%)]  Loss: 3.463 (3.76)  Time: 0.310s, 3306.46/s  (0.308s, 3321.50/s)  LR: 7.358e-04  Data: 0.021 (0.027)
Train: 103 [ 550/1251 ( 44%)]  Loss: 3.763 (3.76)  Time: 0.309s, 3318.89/s  (0.308s, 3322.64/s)  LR: 7.356e-04  Data: 0.022 (0.027)
Train: 103 [ 600/1251 ( 48%)]  Loss: 3.942 (3.78)  Time: 0.305s, 3353.33/s  (0.308s, 3323.15/s)  LR: 7.354e-04  Data: 0.023 (0.027)
Train: 103 [ 650/1251 ( 52%)]  Loss: 3.523 (3.76)  Time: 0.311s, 3287.95/s  (0.308s, 3324.15/s)  LR: 7.352e-04  Data: 0.023 (0.026)
Train: 103 [ 700/1251 ( 56%)]  Loss: 3.680 (3.75)  Time: 0.311s, 3295.27/s  (0.308s, 3324.48/s)  LR: 7.350e-04  Data: 0.023 (0.026)
Train: 103 [ 750/1251 ( 60%)]  Loss: 3.448 (3.74)  Time: 0.309s, 3312.03/s  (0.308s, 3325.41/s)  LR: 7.348e-04  Data: 0.022 (0.026)
Train: 103 [ 800/1251 ( 64%)]  Loss: 3.764 (3.74)  Time: 0.308s, 3327.58/s  (0.308s, 3326.53/s)  LR: 7.346e-04  Data: 0.021 (0.026)
Train: 103 [ 850/1251 ( 68%)]  Loss: 3.878 (3.74)  Time: 0.306s, 3346.94/s  (0.308s, 3326.95/s)  LR: 7.345e-04  Data: 0.027 (0.025)
Train: 103 [ 900/1251 ( 72%)]  Loss: 4.173 (3.77)  Time: 0.312s, 3277.22/s  (0.308s, 3327.21/s)  LR: 7.343e-04  Data: 0.027 (0.025)
Train: 103 [ 950/1251 ( 76%)]  Loss: 3.664 (3.76)  Time: 0.306s, 3341.44/s  (0.308s, 3328.02/s)  LR: 7.341e-04  Data: 0.027 (0.025)
Train: 103 [1000/1251 ( 80%)]  Loss: 3.731 (3.76)  Time: 0.306s, 3351.06/s  (0.308s, 3328.67/s)  LR: 7.339e-04  Data: 0.025 (0.025)
Train: 103 [1050/1251 ( 84%)]  Loss: 3.659 (3.76)  Time: 0.304s, 3373.76/s  (0.308s, 3329.02/s)  LR: 7.337e-04  Data: 0.021 (0.025)
Train: 103 [1100/1251 ( 88%)]  Loss: 3.917 (3.76)  Time: 0.307s, 3339.29/s  (0.308s, 3329.66/s)  LR: 7.335e-04  Data: 0.023 (0.025)
Train: 103 [1150/1251 ( 92%)]  Loss: 3.318 (3.74)  Time: 0.308s, 3319.34/s  (0.307s, 3330.20/s)  LR: 7.334e-04  Data: 0.023 (0.025)
Train: 103 [1200/1251 ( 96%)]  Loss: 3.528 (3.74)  Time: 0.305s, 3354.78/s  (0.307s, 3331.26/s)  LR: 7.332e-04  Data: 0.021 (0.025)
Train: 103 [1250/1251 (100%)]  Loss: 3.794 (3.74)  Time: 0.276s, 3706.43/s  (0.307s, 3333.56/s)  LR: 7.330e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.221 (2.221)  Loss:  0.6982 (0.6982)  Acc@1: 86.4258 (86.4258)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.058 (0.239)  Loss:  0.7812 (1.2817)  Acc@1: 84.3160 (71.2640)  Acc@5: 95.1651 (90.4680)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-102.pth.tar', 71.41400001708985)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-101.pth.tar', 71.3580001171875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-96.pth.tar', 71.26400011230469)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-103.pth.tar', 71.2640000390625)

Train: 104 [   0/1251 (  0%)]  Loss: 3.677 (3.68)  Time: 2.317s,  441.86/s  (2.317s,  441.86/s)  LR: 7.330e-04  Data: 2.095 (2.095)
Train: 104 [  50/1251 (  4%)]  Loss: 3.573 (3.62)  Time: 0.299s, 3427.93/s  (0.330s, 3100.65/s)  LR: 7.328e-04  Data: 0.021 (0.065)
Train: 104 [ 100/1251 (  8%)]  Loss: 3.599 (3.62)  Time: 0.302s, 3389.05/s  (0.315s, 3246.88/s)  LR: 7.326e-04  Data: 0.026 (0.045)
Train: 104 [ 150/1251 ( 12%)]  Loss: 3.680 (3.63)  Time: 0.301s, 3398.96/s  (0.311s, 3292.23/s)  LR: 7.324e-04  Data: 0.023 (0.037)
Train: 104 [ 200/1251 ( 16%)]  Loss: 3.635 (3.63)  Time: 0.306s, 3351.37/s  (0.309s, 3313.35/s)  LR: 7.322e-04  Data: 0.019 (0.034)
Train: 104 [ 250/1251 ( 20%)]  Loss: 3.832 (3.67)  Time: 0.300s, 3413.85/s  (0.308s, 3325.50/s)  LR: 7.321e-04  Data: 0.024 (0.032)
Train: 104 [ 300/1251 ( 24%)]  Loss: 3.802 (3.69)  Time: 0.307s, 3331.73/s  (0.307s, 3331.76/s)  LR: 7.319e-04  Data: 0.019 (0.030)
Train: 104 [ 350/1251 ( 28%)]  Loss: 3.117 (3.61)  Time: 0.308s, 3320.75/s  (0.307s, 3336.26/s)  LR: 7.317e-04  Data: 0.026 (0.029)
Train: 104 [ 400/1251 ( 32%)]  Loss: 4.052 (3.66)  Time: 0.307s, 3330.95/s  (0.307s, 3339.64/s)  LR: 7.315e-04  Data: 0.024 (0.028)
Train: 104 [ 450/1251 ( 36%)]  Loss: 3.740 (3.67)  Time: 0.302s, 3391.93/s  (0.306s, 3341.66/s)  LR: 7.313e-04  Data: 0.024 (0.028)
Train: 104 [ 500/1251 ( 40%)]  Loss: 3.785 (3.68)  Time: 0.308s, 3325.45/s  (0.306s, 3342.89/s)  LR: 7.311e-04  Data: 0.023 (0.027)
Train: 104 [ 550/1251 ( 44%)]  Loss: 3.523 (3.67)  Time: 0.306s, 3342.38/s  (0.306s, 3345.38/s)  LR: 7.310e-04  Data: 0.026 (0.027)
Train: 104 [ 600/1251 ( 48%)]  Loss: 3.869 (3.68)  Time: 0.307s, 3334.41/s  (0.306s, 3346.38/s)  LR: 7.308e-04  Data: 0.020 (0.027)
Train: 104 [ 650/1251 ( 52%)]  Loss: 3.909 (3.70)  Time: 0.307s, 3332.68/s  (0.306s, 3347.79/s)  LR: 7.306e-04  Data: 0.024 (0.026)
Train: 104 [ 700/1251 ( 56%)]  Loss: 3.708 (3.70)  Time: 0.305s, 3360.52/s  (0.306s, 3348.51/s)  LR: 7.304e-04  Data: 0.023 (0.026)
Train: 104 [ 750/1251 ( 60%)]  Loss: 3.721 (3.70)  Time: 0.304s, 3367.21/s  (0.306s, 3349.39/s)  LR: 7.302e-04  Data: 0.018 (0.026)
Train: 104 [ 800/1251 ( 64%)]  Loss: 3.657 (3.70)  Time: 0.307s, 3336.91/s  (0.306s, 3349.54/s)  LR: 7.300e-04  Data: 0.026 (0.026)
Train: 104 [ 850/1251 ( 68%)]  Loss: 3.692 (3.70)  Time: 0.305s, 3358.70/s  (0.306s, 3350.26/s)  LR: 7.298e-04  Data: 0.022 (0.026)
Train: 104 [ 900/1251 ( 72%)]  Loss: 3.678 (3.70)  Time: 0.305s, 3354.64/s  (0.306s, 3350.38/s)  LR: 7.297e-04  Data: 0.025 (0.025)
Train: 104 [ 950/1251 ( 76%)]  Loss: 3.733 (3.70)  Time: 0.305s, 3357.92/s  (0.306s, 3350.71/s)  LR: 7.295e-04  Data: 0.015 (0.025)
Train: 104 [1000/1251 ( 80%)]  Loss: 3.365 (3.68)  Time: 0.306s, 3344.99/s  (0.306s, 3350.99/s)  LR: 7.293e-04  Data: 0.022 (0.025)
Train: 104 [1050/1251 ( 84%)]  Loss: 3.429 (3.67)  Time: 0.303s, 3375.90/s  (0.306s, 3351.41/s)  LR: 7.291e-04  Data: 0.024 (0.025)
Train: 104 [1100/1251 ( 88%)]  Loss: 3.390 (3.66)  Time: 0.309s, 3315.06/s  (0.306s, 3351.49/s)  LR: 7.289e-04  Data: 0.026 (0.025)
Train: 104 [1150/1251 ( 92%)]  Loss: 3.609 (3.66)  Time: 0.307s, 3333.10/s  (0.306s, 3351.71/s)  LR: 7.287e-04  Data: 0.026 (0.025)
Train: 104 [1200/1251 ( 96%)]  Loss: 3.466 (3.65)  Time: 0.307s, 3339.60/s  (0.305s, 3352.03/s)  LR: 7.285e-04  Data: 0.022 (0.025)
Train: 104 [1250/1251 (100%)]  Loss: 4.025 (3.66)  Time: 0.279s, 3675.95/s  (0.305s, 3354.09/s)  LR: 7.284e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.079 (2.079)  Loss:  0.6372 (0.6372)  Acc@1: 87.8906 (87.8906)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.053 (0.236)  Loss:  0.7466 (1.2627)  Acc@1: 83.4906 (71.5460)  Acc@5: 95.9906 (90.6300)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-104.pth.tar', 71.54600006835938)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-102.pth.tar', 71.41400001708985)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-101.pth.tar', 71.3580001171875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-96.pth.tar', 71.26400011230469)

Train: 105 [   0/1251 (  0%)]  Loss: 3.662 (3.66)  Time: 2.161s,  473.87/s  (2.161s,  473.87/s)  LR: 7.284e-04  Data: 1.937 (1.937)
Train: 105 [  50/1251 (  4%)]  Loss: 3.720 (3.69)  Time: 0.287s, 3568.22/s  (0.321s, 3191.72/s)  LR: 7.282e-04  Data: 0.023 (0.061)
Train: 105 [ 100/1251 (  8%)]  Loss: 3.634 (3.67)  Time: 0.302s, 3391.12/s  (0.309s, 3310.72/s)  LR: 7.280e-04  Data: 0.025 (0.042)
Train: 105 [ 150/1251 ( 12%)]  Loss: 3.435 (3.61)  Time: 0.301s, 3402.49/s  (0.306s, 3342.51/s)  LR: 7.278e-04  Data: 0.021 (0.035)
Train: 105 [ 200/1251 ( 16%)]  Loss: 3.911 (3.67)  Time: 0.300s, 3410.66/s  (0.305s, 3355.50/s)  LR: 7.276e-04  Data: 0.024 (0.033)
Train: 105 [ 250/1251 ( 20%)]  Loss: 3.877 (3.71)  Time: 0.305s, 3361.66/s  (0.304s, 3362.97/s)  LR: 7.274e-04  Data: 0.027 (0.031)
Train: 105 [ 300/1251 ( 24%)]  Loss: 3.589 (3.69)  Time: 0.299s, 3423.63/s  (0.304s, 3367.67/s)  LR: 7.272e-04  Data: 0.018 (0.029)
Train: 105 [ 350/1251 ( 28%)]  Loss: 3.675 (3.69)  Time: 0.303s, 3376.01/s  (0.304s, 3372.25/s)  LR: 7.271e-04  Data: 0.024 (0.029)
Train: 105 [ 400/1251 ( 32%)]  Loss: 3.354 (3.65)  Time: 0.300s, 3416.41/s  (0.303s, 3375.12/s)  LR: 7.269e-04  Data: 0.022 (0.028)
Train: 105 [ 450/1251 ( 36%)]  Loss: 3.864 (3.67)  Time: 0.302s, 3394.51/s  (0.303s, 3377.52/s)  LR: 7.267e-04  Data: 0.023 (0.027)
Train: 105 [ 500/1251 ( 40%)]  Loss: 3.614 (3.67)  Time: 0.298s, 3436.82/s  (0.303s, 3379.59/s)  LR: 7.265e-04  Data: 0.023 (0.027)
Train: 105 [ 550/1251 ( 44%)]  Loss: 3.703 (3.67)  Time: 0.301s, 3405.55/s  (0.303s, 3380.86/s)  LR: 7.263e-04  Data: 0.021 (0.027)
Train: 105 [ 600/1251 ( 48%)]  Loss: 3.843 (3.68)  Time: 0.298s, 3433.62/s  (0.303s, 3382.89/s)  LR: 7.261e-04  Data: 0.024 (0.026)
Train: 105 [ 650/1251 ( 52%)]  Loss: 3.691 (3.68)  Time: 0.296s, 3457.33/s  (0.302s, 3385.24/s)  LR: 7.259e-04  Data: 0.022 (0.026)
Train: 105 [ 700/1251 ( 56%)]  Loss: 3.665 (3.68)  Time: 0.309s, 3317.79/s  (0.302s, 3386.63/s)  LR: 7.258e-04  Data: 0.027 (0.026)
Train: 105 [ 750/1251 ( 60%)]  Loss: 3.753 (3.69)  Time: 0.302s, 3386.02/s  (0.302s, 3388.10/s)  LR: 7.256e-04  Data: 0.023 (0.026)
Train: 105 [ 800/1251 ( 64%)]  Loss: 3.570 (3.68)  Time: 0.295s, 3474.75/s  (0.302s, 3391.19/s)  LR: 7.254e-04  Data: 0.022 (0.025)
Train: 105 [ 850/1251 ( 68%)]  Loss: 3.362 (3.66)  Time: 0.298s, 3432.81/s  (0.302s, 3394.40/s)  LR: 7.252e-04  Data: 0.025 (0.025)
Train: 105 [ 900/1251 ( 72%)]  Loss: 3.232 (3.64)  Time: 0.303s, 3375.70/s  (0.302s, 3395.24/s)  LR: 7.250e-04  Data: 0.026 (0.025)
Train: 105 [ 950/1251 ( 76%)]  Loss: 3.705 (3.64)  Time: 0.303s, 3385.05/s  (0.302s, 3394.99/s)  LR: 7.248e-04  Data: 0.022 (0.025)
Train: 105 [1000/1251 ( 80%)]  Loss: 3.908 (3.66)  Time: 0.296s, 3453.63/s  (0.302s, 3395.25/s)  LR: 7.246e-04  Data: 0.022 (0.025)
Train: 105 [1050/1251 ( 84%)]  Loss: 3.361 (3.64)  Time: 0.295s, 3466.99/s  (0.302s, 3395.59/s)  LR: 7.245e-04  Data: 0.022 (0.025)
Train: 105 [1100/1251 ( 88%)]  Loss: 3.503 (3.64)  Time: 0.307s, 3336.36/s  (0.302s, 3396.16/s)  LR: 7.243e-04  Data: 0.025 (0.025)
Train: 105 [1150/1251 ( 92%)]  Loss: 4.083 (3.65)  Time: 0.305s, 3352.15/s  (0.301s, 3396.39/s)  LR: 7.241e-04  Data: 0.026 (0.025)
Train: 105 [1200/1251 ( 96%)]  Loss: 3.681 (3.66)  Time: 0.295s, 3469.38/s  (0.301s, 3397.02/s)  LR: 7.239e-04  Data: 0.017 (0.025)
Train: 105 [1250/1251 (100%)]  Loss: 3.409 (3.65)  Time: 0.276s, 3704.69/s  (0.301s, 3399.13/s)  LR: 7.237e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.494 (2.494)  Loss:  0.7168 (0.7168)  Acc@1: 85.2539 (85.2539)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.047 (0.249)  Loss:  0.7197 (1.2642)  Acc@1: 84.5519 (71.6420)  Acc@5: 95.5189 (90.6780)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-105.pth.tar', 71.64200014160156)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-104.pth.tar', 71.54600006835938)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-102.pth.tar', 71.41400001708985)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-101.pth.tar', 71.3580001171875)

Train: 106 [   0/1251 (  0%)]  Loss: 3.581 (3.58)  Time: 2.490s,  411.18/s  (2.490s,  411.18/s)  LR: 7.237e-04  Data: 2.272 (2.272)
Train: 106 [  50/1251 (  4%)]  Loss: 3.530 (3.56)  Time: 0.294s, 3485.47/s  (0.323s, 3170.99/s)  LR: 7.235e-04  Data: 0.023 (0.067)
Train: 106 [ 100/1251 (  8%)]  Loss: 3.667 (3.59)  Time: 0.299s, 3424.42/s  (0.309s, 3313.34/s)  LR: 7.233e-04  Data: 0.024 (0.045)
Train: 106 [ 150/1251 ( 12%)]  Loss: 3.727 (3.63)  Time: 0.301s, 3397.26/s  (0.305s, 3354.42/s)  LR: 7.231e-04  Data: 0.024 (0.038)
Train: 106 [ 200/1251 ( 16%)]  Loss: 3.714 (3.64)  Time: 0.295s, 3467.83/s  (0.304s, 3373.75/s)  LR: 7.230e-04  Data: 0.023 (0.034)
Train: 106 [ 250/1251 ( 20%)]  Loss: 3.582 (3.63)  Time: 0.305s, 3355.80/s  (0.303s, 3379.00/s)  LR: 7.228e-04  Data: 0.024 (0.032)
Train: 106 [ 300/1251 ( 24%)]  Loss: 3.706 (3.64)  Time: 0.308s, 3329.74/s  (0.303s, 3378.41/s)  LR: 7.226e-04  Data: 0.028 (0.031)
Train: 106 [ 350/1251 ( 28%)]  Loss: 3.885 (3.67)  Time: 0.303s, 3375.53/s  (0.303s, 3382.29/s)  LR: 7.224e-04  Data: 0.023 (0.029)
Train: 106 [ 400/1251 ( 32%)]  Loss: 3.589 (3.66)  Time: 0.303s, 3380.42/s  (0.302s, 3385.83/s)  LR: 7.222e-04  Data: 0.022 (0.029)
Train: 106 [ 450/1251 ( 36%)]  Loss: 3.298 (3.63)  Time: 0.302s, 3396.18/s  (0.302s, 3388.82/s)  LR: 7.220e-04  Data: 0.022 (0.028)
Train: 106 [ 500/1251 ( 40%)]  Loss: 3.415 (3.61)  Time: 0.299s, 3422.09/s  (0.302s, 3392.35/s)  LR: 7.218e-04  Data: 0.022 (0.028)
Train: 106 [ 550/1251 ( 44%)]  Loss: 3.450 (3.60)  Time: 0.306s, 3351.39/s  (0.302s, 3394.90/s)  LR: 7.216e-04  Data: 0.021 (0.027)
Train: 106 [ 600/1251 ( 48%)]  Loss: 3.804 (3.61)  Time: 0.302s, 3395.58/s  (0.302s, 3395.97/s)  LR: 7.215e-04  Data: 0.022 (0.027)
Train: 106 [ 650/1251 ( 52%)]  Loss: 3.528 (3.61)  Time: 0.294s, 3483.19/s  (0.301s, 3396.60/s)  LR: 7.213e-04  Data: 0.022 (0.027)
Train: 106 [ 700/1251 ( 56%)]  Loss: 3.719 (3.61)  Time: 0.303s, 3382.30/s  (0.301s, 3398.75/s)  LR: 7.211e-04  Data: 0.026 (0.026)
Train: 106 [ 750/1251 ( 60%)]  Loss: 3.655 (3.62)  Time: 0.299s, 3428.92/s  (0.301s, 3399.81/s)  LR: 7.209e-04  Data: 0.022 (0.026)
Train: 106 [ 800/1251 ( 64%)]  Loss: 3.538 (3.61)  Time: 0.307s, 3335.05/s  (0.301s, 3400.91/s)  LR: 7.207e-04  Data: 0.022 (0.026)
Train: 106 [ 850/1251 ( 68%)]  Loss: 3.582 (3.61)  Time: 0.304s, 3372.38/s  (0.301s, 3401.91/s)  LR: 7.205e-04  Data: 0.024 (0.026)
Train: 106 [ 900/1251 ( 72%)]  Loss: 3.832 (3.62)  Time: 0.297s, 3442.27/s  (0.301s, 3402.67/s)  LR: 7.203e-04  Data: 0.022 (0.026)
Train: 106 [ 950/1251 ( 76%)]  Loss: 4.075 (3.64)  Time: 0.300s, 3418.23/s  (0.301s, 3403.06/s)  LR: 7.202e-04  Data: 0.021 (0.026)
Train: 106 [1000/1251 ( 80%)]  Loss: 3.548 (3.64)  Time: 0.299s, 3424.19/s  (0.301s, 3404.33/s)  LR: 7.200e-04  Data: 0.023 (0.025)
Train: 106 [1050/1251 ( 84%)]  Loss: 3.651 (3.64)  Time: 0.299s, 3422.39/s  (0.301s, 3404.71/s)  LR: 7.198e-04  Data: 0.023 (0.025)
Train: 106 [1100/1251 ( 88%)]  Loss: 3.403 (3.63)  Time: 0.303s, 3375.86/s  (0.301s, 3405.19/s)  LR: 7.196e-04  Data: 0.025 (0.025)
Train: 106 [1150/1251 ( 92%)]  Loss: 3.806 (3.64)  Time: 0.299s, 3419.79/s  (0.301s, 3405.92/s)  LR: 7.194e-04  Data: 0.026 (0.025)
Train: 106 [1200/1251 ( 96%)]  Loss: 3.642 (3.64)  Time: 0.296s, 3464.79/s  (0.301s, 3407.08/s)  LR: 7.192e-04  Data: 0.023 (0.025)
Train: 106 [1250/1251 (100%)]  Loss: 3.652 (3.64)  Time: 0.276s, 3715.58/s  (0.300s, 3409.56/s)  LR: 7.190e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.076 (2.076)  Loss:  0.6914 (0.6914)  Acc@1: 86.6211 (86.6211)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.040 (0.238)  Loss:  0.7666 (1.2915)  Acc@1: 83.0189 (71.8360)  Acc@5: 96.5802 (90.9380)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-106.pth.tar', 71.83599999267578)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-105.pth.tar', 71.64200014160156)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-104.pth.tar', 71.54600006835938)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-102.pth.tar', 71.41400001708985)

Train: 107 [   0/1251 (  0%)]  Loss: 3.991 (3.99)  Time: 2.316s,  442.17/s  (2.316s,  442.17/s)  LR: 7.190e-04  Data: 2.092 (2.092)
Train: 107 [  50/1251 (  4%)]  Loss: 3.906 (3.95)  Time: 0.282s, 3636.41/s  (0.322s, 3183.63/s)  LR: 7.188e-04  Data: 0.022 (0.066)
Train: 107 [ 100/1251 (  8%)]  Loss: 3.466 (3.79)  Time: 0.292s, 3507.24/s  (0.307s, 3339.37/s)  LR: 7.186e-04  Data: 0.024 (0.044)
Train: 107 [ 150/1251 ( 12%)]  Loss: 3.523 (3.72)  Time: 0.293s, 3498.54/s  (0.303s, 3384.88/s)  LR: 7.185e-04  Data: 0.026 (0.038)
Train: 107 [ 200/1251 ( 16%)]  Loss: 3.729 (3.72)  Time: 0.299s, 3430.21/s  (0.301s, 3406.95/s)  LR: 7.183e-04  Data: 0.028 (0.034)
Train: 107 [ 250/1251 ( 20%)]  Loss: 3.856 (3.75)  Time: 0.294s, 3477.93/s  (0.300s, 3418.17/s)  LR: 7.181e-04  Data: 0.027 (0.032)
Train: 107 [ 300/1251 ( 24%)]  Loss: 3.982 (3.78)  Time: 0.303s, 3378.35/s  (0.299s, 3424.43/s)  LR: 7.179e-04  Data: 0.024 (0.031)
Train: 107 [ 350/1251 ( 28%)]  Loss: 4.000 (3.81)  Time: 0.291s, 3518.03/s  (0.299s, 3429.03/s)  LR: 7.177e-04  Data: 0.020 (0.030)
Train: 107 [ 400/1251 ( 32%)]  Loss: 3.810 (3.81)  Time: 0.299s, 3420.40/s  (0.298s, 3431.23/s)  LR: 7.175e-04  Data: 0.025 (0.029)
Train: 107 [ 450/1251 ( 36%)]  Loss: 3.341 (3.76)  Time: 0.291s, 3524.88/s  (0.298s, 3432.20/s)  LR: 7.173e-04  Data: 0.020 (0.028)
Train: 107 [ 500/1251 ( 40%)]  Loss: 3.658 (3.75)  Time: 0.298s, 3439.56/s  (0.298s, 3432.95/s)  LR: 7.171e-04  Data: 0.022 (0.028)
Train: 107 [ 550/1251 ( 44%)]  Loss: 3.718 (3.75)  Time: 0.304s, 3366.62/s  (0.298s, 3433.59/s)  LR: 7.170e-04  Data: 0.024 (0.027)
Train: 107 [ 600/1251 ( 48%)]  Loss: 3.671 (3.74)  Time: 0.291s, 3523.75/s  (0.298s, 3434.54/s)  LR: 7.168e-04  Data: 0.026 (0.027)
Train: 107 [ 650/1251 ( 52%)]  Loss: 3.458 (3.72)  Time: 0.293s, 3492.36/s  (0.298s, 3435.46/s)  LR: 7.166e-04  Data: 0.024 (0.027)
Train: 107 [ 700/1251 ( 56%)]  Loss: 3.835 (3.73)  Time: 0.299s, 3428.54/s  (0.298s, 3436.72/s)  LR: 7.164e-04  Data: 0.025 (0.026)
Train: 107 [ 750/1251 ( 60%)]  Loss: 3.900 (3.74)  Time: 0.297s, 3442.02/s  (0.298s, 3437.71/s)  LR: 7.162e-04  Data: 0.016 (0.026)
Train: 107 [ 800/1251 ( 64%)]  Loss: 3.970 (3.75)  Time: 0.297s, 3449.83/s  (0.298s, 3438.07/s)  LR: 7.160e-04  Data: 0.020 (0.026)
Train: 107 [ 850/1251 ( 68%)]  Loss: 3.656 (3.75)  Time: 0.298s, 3435.49/s  (0.298s, 3438.33/s)  LR: 7.158e-04  Data: 0.021 (0.026)
Train: 107 [ 900/1251 ( 72%)]  Loss: 3.362 (3.73)  Time: 0.301s, 3398.52/s  (0.298s, 3438.24/s)  LR: 7.156e-04  Data: 0.023 (0.026)
Train: 107 [ 950/1251 ( 76%)]  Loss: 3.693 (3.73)  Time: 0.302s, 3395.20/s  (0.298s, 3437.90/s)  LR: 7.155e-04  Data: 0.024 (0.026)
Train: 107 [1000/1251 ( 80%)]  Loss: 3.286 (3.71)  Time: 0.298s, 3439.64/s  (0.298s, 3437.58/s)  LR: 7.153e-04  Data: 0.021 (0.025)
Train: 107 [1050/1251 ( 84%)]  Loss: 3.407 (3.69)  Time: 0.297s, 3442.23/s  (0.298s, 3437.27/s)  LR: 7.151e-04  Data: 0.023 (0.025)
Train: 107 [1100/1251 ( 88%)]  Loss: 3.922 (3.70)  Time: 0.303s, 3378.31/s  (0.298s, 3437.20/s)  LR: 7.149e-04  Data: 0.025 (0.025)
Train: 107 [1150/1251 ( 92%)]  Loss: 3.860 (3.71)  Time: 0.298s, 3434.88/s  (0.298s, 3436.69/s)  LR: 7.147e-04  Data: 0.022 (0.025)
Train: 107 [1200/1251 ( 96%)]  Loss: 2.931 (3.68)  Time: 0.303s, 3382.54/s  (0.298s, 3436.10/s)  LR: 7.145e-04  Data: 0.021 (0.025)
Train: 107 [1250/1251 (100%)]  Loss: 3.828 (3.68)  Time: 0.276s, 3711.52/s  (0.298s, 3437.49/s)  LR: 7.143e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.090 (2.090)  Loss:  0.6558 (0.6558)  Acc@1: 86.2305 (86.2305)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.041 (0.235)  Loss:  0.7285 (1.2452)  Acc@1: 85.1415 (71.7140)  Acc@5: 95.5189 (90.8620)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-106.pth.tar', 71.83599999267578)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-107.pth.tar', 71.71400000976563)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-105.pth.tar', 71.64200014160156)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-104.pth.tar', 71.54600006835938)

Train: 108 [   0/1251 (  0%)]  Loss: 3.636 (3.64)  Time: 2.202s,  465.06/s  (2.202s,  465.06/s)  LR: 7.143e-04  Data: 1.964 (1.964)
Train: 108 [  50/1251 (  4%)]  Loss: 3.512 (3.57)  Time: 0.293s, 3490.80/s  (0.319s, 3212.56/s)  LR: 7.141e-04  Data: 0.029 (0.062)
Train: 108 [ 100/1251 (  8%)]  Loss: 3.513 (3.55)  Time: 0.292s, 3510.55/s  (0.306s, 3349.12/s)  LR: 7.139e-04  Data: 0.025 (0.043)
Train: 108 [ 150/1251 ( 12%)]  Loss: 3.641 (3.58)  Time: 0.304s, 3363.35/s  (0.302s, 3391.32/s)  LR: 7.138e-04  Data: 0.023 (0.036)
Train: 108 [ 200/1251 ( 16%)]  Loss: 3.907 (3.64)  Time: 0.300s, 3418.43/s  (0.300s, 3409.71/s)  LR: 7.136e-04  Data: 0.021 (0.033)
Train: 108 [ 250/1251 ( 20%)]  Loss: 3.857 (3.68)  Time: 0.299s, 3430.05/s  (0.299s, 3420.85/s)  LR: 7.134e-04  Data: 0.026 (0.031)
Train: 108 [ 300/1251 ( 24%)]  Loss: 3.762 (3.69)  Time: 0.298s, 3436.53/s  (0.299s, 3426.92/s)  LR: 7.132e-04  Data: 0.022 (0.029)
Train: 108 [ 350/1251 ( 28%)]  Loss: 3.349 (3.65)  Time: 0.293s, 3495.80/s  (0.299s, 3430.24/s)  LR: 7.130e-04  Data: 0.024 (0.029)
Train: 108 [ 400/1251 ( 32%)]  Loss: 3.196 (3.60)  Time: 0.298s, 3441.91/s  (0.298s, 3431.71/s)  LR: 7.128e-04  Data: 0.022 (0.028)
Train: 108 [ 450/1251 ( 36%)]  Loss: 4.216 (3.66)  Time: 0.299s, 3426.65/s  (0.298s, 3433.47/s)  LR: 7.126e-04  Data: 0.024 (0.027)
Train: 108 [ 500/1251 ( 40%)]  Loss: 3.470 (3.64)  Time: 0.299s, 3419.15/s  (0.298s, 3434.53/s)  LR: 7.124e-04  Data: 0.026 (0.027)
Train: 108 [ 550/1251 ( 44%)]  Loss: 3.775 (3.65)  Time: 0.294s, 3478.03/s  (0.298s, 3435.40/s)  LR: 7.122e-04  Data: 0.026 (0.027)
Train: 108 [ 600/1251 ( 48%)]  Loss: 3.638 (3.65)  Time: 0.295s, 3476.48/s  (0.298s, 3436.81/s)  LR: 7.121e-04  Data: 0.023 (0.026)
Train: 108 [ 650/1251 ( 52%)]  Loss: 3.495 (3.64)  Time: 0.293s, 3497.37/s  (0.298s, 3437.09/s)  LR: 7.119e-04  Data: 0.023 (0.026)
Train: 108 [ 700/1251 ( 56%)]  Loss: 3.641 (3.64)  Time: 0.306s, 3342.75/s  (0.298s, 3437.74/s)  LR: 7.117e-04  Data: 0.025 (0.026)
Train: 108 [ 750/1251 ( 60%)]  Loss: 3.705 (3.64)  Time: 0.296s, 3460.19/s  (0.298s, 3437.94/s)  LR: 7.115e-04  Data: 0.022 (0.026)
Train: 108 [ 800/1251 ( 64%)]  Loss: 3.700 (3.65)  Time: 0.298s, 3441.56/s  (0.298s, 3438.30/s)  LR: 7.113e-04  Data: 0.019 (0.026)
Train: 108 [ 850/1251 ( 68%)]  Loss: 3.545 (3.64)  Time: 0.299s, 3424.16/s  (0.298s, 3438.46/s)  LR: 7.111e-04  Data: 0.025 (0.025)
Train: 108 [ 900/1251 ( 72%)]  Loss: 3.822 (3.65)  Time: 0.296s, 3455.58/s  (0.298s, 3438.12/s)  LR: 7.109e-04  Data: 0.022 (0.025)
Train: 108 [ 950/1251 ( 76%)]  Loss: 3.815 (3.66)  Time: 0.299s, 3421.80/s  (0.298s, 3437.77/s)  LR: 7.107e-04  Data: 0.022 (0.025)
Train: 108 [1000/1251 ( 80%)]  Loss: 3.847 (3.67)  Time: 0.299s, 3430.33/s  (0.298s, 3437.59/s)  LR: 7.105e-04  Data: 0.023 (0.025)
Train: 108 [1050/1251 ( 84%)]  Loss: 3.486 (3.66)  Time: 0.294s, 3485.23/s  (0.298s, 3438.08/s)  LR: 7.104e-04  Data: 0.023 (0.025)
Train: 108 [1100/1251 ( 88%)]  Loss: 3.655 (3.66)  Time: 0.300s, 3418.76/s  (0.298s, 3437.42/s)  LR: 7.102e-04  Data: 0.021 (0.025)
Train: 108 [1150/1251 ( 92%)]  Loss: 3.627 (3.66)  Time: 0.304s, 3372.71/s  (0.298s, 3437.01/s)  LR: 7.100e-04  Data: 0.023 (0.025)
Train: 108 [1200/1251 ( 96%)]  Loss: 4.075 (3.68)  Time: 0.297s, 3451.92/s  (0.298s, 3436.41/s)  LR: 7.098e-04  Data: 0.022 (0.025)
Train: 108 [1250/1251 (100%)]  Loss: 3.680 (3.68)  Time: 0.275s, 3720.07/s  (0.298s, 3437.94/s)  LR: 7.096e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.083 (2.083)  Loss:  0.6758 (0.6758)  Acc@1: 87.5977 (87.5977)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.041 (0.240)  Loss:  0.7822 (1.2663)  Acc@1: 84.4340 (71.9220)  Acc@5: 95.7547 (90.9320)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-108.pth.tar', 71.9219999609375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-106.pth.tar', 71.83599999267578)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-107.pth.tar', 71.71400000976563)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-105.pth.tar', 71.64200014160156)

Train: 109 [   0/1251 (  0%)]  Loss: 3.534 (3.53)  Time: 2.173s,  471.25/s  (2.173s,  471.25/s)  LR: 7.096e-04  Data: 1.939 (1.939)
Train: 109 [  50/1251 (  4%)]  Loss: 3.488 (3.51)  Time: 0.287s, 3567.08/s  (0.319s, 3209.58/s)  LR: 7.094e-04  Data: 0.023 (0.062)
Train: 109 [ 100/1251 (  8%)]  Loss: 3.651 (3.56)  Time: 0.304s, 3373.15/s  (0.306s, 3342.88/s)  LR: 7.092e-04  Data: 0.026 (0.043)
Train: 109 [ 150/1251 ( 12%)]  Loss: 3.662 (3.58)  Time: 0.302s, 3395.52/s  (0.303s, 3377.30/s)  LR: 7.090e-04  Data: 0.024 (0.036)
Train: 109 [ 200/1251 ( 16%)]  Loss: 3.615 (3.59)  Time: 0.298s, 3432.25/s  (0.302s, 3393.16/s)  LR: 7.088e-04  Data: 0.022 (0.033)
Train: 109 [ 250/1251 ( 20%)]  Loss: 3.589 (3.59)  Time: 0.295s, 3465.53/s  (0.301s, 3401.23/s)  LR: 7.086e-04  Data: 0.021 (0.031)
Train: 109 [ 300/1251 ( 24%)]  Loss: 3.751 (3.61)  Time: 0.300s, 3408.69/s  (0.301s, 3405.01/s)  LR: 7.085e-04  Data: 0.023 (0.030)
Train: 109 [ 350/1251 ( 28%)]  Loss: 3.468 (3.59)  Time: 0.301s, 3401.37/s  (0.301s, 3407.60/s)  LR: 7.083e-04  Data: 0.023 (0.029)
Train: 109 [ 400/1251 ( 32%)]  Loss: 3.694 (3.61)  Time: 0.306s, 3350.80/s  (0.300s, 3408.79/s)  LR: 7.081e-04  Data: 0.023 (0.028)
Train: 109 [ 450/1251 ( 36%)]  Loss: 3.402 (3.59)  Time: 0.297s, 3453.37/s  (0.300s, 3410.35/s)  LR: 7.079e-04  Data: 0.023 (0.028)
Train: 109 [ 500/1251 ( 40%)]  Loss: 3.572 (3.58)  Time: 0.306s, 3348.54/s  (0.300s, 3412.07/s)  LR: 7.077e-04  Data: 0.022 (0.027)
Train: 109 [ 550/1251 ( 44%)]  Loss: 3.362 (3.57)  Time: 0.298s, 3441.63/s  (0.300s, 3412.48/s)  LR: 7.075e-04  Data: 0.022 (0.027)
Train: 109 [ 600/1251 ( 48%)]  Loss: 3.328 (3.55)  Time: 0.299s, 3419.18/s  (0.300s, 3413.94/s)  LR: 7.073e-04  Data: 0.024 (0.026)
Train: 109 [ 650/1251 ( 52%)]  Loss: 3.802 (3.57)  Time: 0.303s, 3376.89/s  (0.300s, 3414.82/s)  LR: 7.071e-04  Data: 0.024 (0.026)
Train: 109 [ 700/1251 ( 56%)]  Loss: 3.666 (3.57)  Time: 0.301s, 3403.45/s  (0.300s, 3415.01/s)  LR: 7.069e-04  Data: 0.023 (0.026)
Train: 109 [ 750/1251 ( 60%)]  Loss: 3.542 (3.57)  Time: 0.294s, 3488.74/s  (0.300s, 3415.24/s)  LR: 7.068e-04  Data: 0.021 (0.026)
Train: 109 [ 800/1251 ( 64%)]  Loss: 3.147 (3.55)  Time: 0.298s, 3441.59/s  (0.300s, 3415.52/s)  LR: 7.066e-04  Data: 0.022 (0.026)
Train: 109 [ 850/1251 ( 68%)]  Loss: 3.851 (3.56)  Time: 0.295s, 3466.99/s  (0.300s, 3415.35/s)  LR: 7.064e-04  Data: 0.023 (0.025)
Train: 109 [ 900/1251 ( 72%)]  Loss: 3.798 (3.57)  Time: 0.306s, 3348.76/s  (0.300s, 3415.13/s)  LR: 7.062e-04  Data: 0.026 (0.025)
Train: 109 [ 950/1251 ( 76%)]  Loss: 3.461 (3.57)  Time: 0.299s, 3421.00/s  (0.300s, 3414.85/s)  LR: 7.060e-04  Data: 0.022 (0.025)
Train: 109 [1000/1251 ( 80%)]  Loss: 3.530 (3.57)  Time: 0.311s, 3291.52/s  (0.300s, 3414.15/s)  LR: 7.058e-04  Data: 0.023 (0.025)
Train: 109 [1050/1251 ( 84%)]  Loss: 3.631 (3.57)  Time: 0.301s, 3397.34/s  (0.300s, 3413.52/s)  LR: 7.056e-04  Data: 0.022 (0.025)
Train: 109 [1100/1251 ( 88%)]  Loss: 3.765 (3.58)  Time: 0.305s, 3359.27/s  (0.300s, 3413.07/s)  LR: 7.054e-04  Data: 0.024 (0.025)
Train: 109 [1150/1251 ( 92%)]  Loss: 3.624 (3.58)  Time: 0.307s, 3336.63/s  (0.300s, 3412.88/s)  LR: 7.052e-04  Data: 0.023 (0.025)
Train: 109 [1200/1251 ( 96%)]  Loss: 3.644 (3.58)  Time: 0.307s, 3337.86/s  (0.300s, 3412.55/s)  LR: 7.050e-04  Data: 0.024 (0.025)
Train: 109 [1250/1251 (100%)]  Loss: 3.484 (3.58)  Time: 0.276s, 3710.41/s  (0.300s, 3414.73/s)  LR: 7.049e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.088 (2.088)  Loss:  0.5991 (0.5991)  Acc@1: 87.5000 (87.5000)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.041 (0.241)  Loss:  0.7515 (1.2401)  Acc@1: 83.3727 (71.9260)  Acc@5: 95.6368 (90.8380)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-109.pth.tar', 71.92600014648437)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-108.pth.tar', 71.9219999609375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-106.pth.tar', 71.83599999267578)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-107.pth.tar', 71.71400000976563)

Train: 110 [   0/1251 (  0%)]  Loss: 3.848 (3.85)  Time: 2.482s,  412.59/s  (2.482s,  412.59/s)  LR: 7.048e-04  Data: 2.265 (2.265)
Train: 110 [  50/1251 (  4%)]  Loss: 3.730 (3.79)  Time: 0.287s, 3567.91/s  (0.324s, 3164.79/s)  LR: 7.047e-04  Data: 0.019 (0.068)
Train: 110 [ 100/1251 (  8%)]  Loss: 3.626 (3.73)  Time: 0.295s, 3474.66/s  (0.309s, 3317.44/s)  LR: 7.045e-04  Data: 0.023 (0.046)
Train: 110 [ 150/1251 ( 12%)]  Loss: 3.749 (3.74)  Time: 0.298s, 3438.43/s  (0.304s, 3365.15/s)  LR: 7.043e-04  Data: 0.022 (0.038)
Train: 110 [ 200/1251 ( 16%)]  Loss: 3.829 (3.76)  Time: 0.300s, 3416.58/s  (0.303s, 3385.01/s)  LR: 7.041e-04  Data: 0.018 (0.034)
Train: 110 [ 250/1251 ( 20%)]  Loss: 3.456 (3.71)  Time: 0.297s, 3447.22/s  (0.302s, 3395.63/s)  LR: 7.039e-04  Data: 0.024 (0.032)
Train: 110 [ 300/1251 ( 24%)]  Loss: 3.749 (3.71)  Time: 0.297s, 3453.01/s  (0.301s, 3402.32/s)  LR: 7.037e-04  Data: 0.021 (0.031)
Train: 110 [ 350/1251 ( 28%)]  Loss: 3.457 (3.68)  Time: 0.297s, 3445.89/s  (0.301s, 3406.81/s)  LR: 7.035e-04  Data: 0.021 (0.030)
Train: 110 [ 400/1251 ( 32%)]  Loss: 3.359 (3.64)  Time: 0.296s, 3463.86/s  (0.300s, 3409.47/s)  LR: 7.033e-04  Data: 0.022 (0.029)
Train: 110 [ 450/1251 ( 36%)]  Loss: 3.809 (3.66)  Time: 0.299s, 3425.66/s  (0.300s, 3410.68/s)  LR: 7.031e-04  Data: 0.023 (0.028)
Train: 110 [ 500/1251 ( 40%)]  Loss: 3.831 (3.68)  Time: 0.301s, 3403.28/s  (0.300s, 3412.42/s)  LR: 7.029e-04  Data: 0.022 (0.028)
Train: 110 [ 550/1251 ( 44%)]  Loss: 3.336 (3.65)  Time: 0.293s, 3498.21/s  (0.300s, 3413.52/s)  LR: 7.028e-04  Data: 0.020 (0.027)
Train: 110 [ 600/1251 ( 48%)]  Loss: 3.907 (3.67)  Time: 0.301s, 3398.23/s  (0.300s, 3414.01/s)  LR: 7.026e-04  Data: 0.025 (0.027)
Train: 110 [ 650/1251 ( 52%)]  Loss: 3.557 (3.66)  Time: 0.295s, 3471.81/s  (0.300s, 3414.03/s)  LR: 7.024e-04  Data: 0.024 (0.027)
Train: 110 [ 700/1251 ( 56%)]  Loss: 3.782 (3.67)  Time: 0.302s, 3394.89/s  (0.300s, 3413.71/s)  LR: 7.022e-04  Data: 0.028 (0.026)
Train: 110 [ 750/1251 ( 60%)]  Loss: 3.388 (3.65)  Time: 0.305s, 3355.93/s  (0.300s, 3414.19/s)  LR: 7.020e-04  Data: 0.025 (0.026)
Train: 110 [ 800/1251 ( 64%)]  Loss: 3.866 (3.66)  Time: 0.295s, 3468.75/s  (0.300s, 3414.17/s)  LR: 7.018e-04  Data: 0.022 (0.026)
Train: 110 [ 850/1251 ( 68%)]  Loss: 3.945 (3.68)  Time: 0.301s, 3404.31/s  (0.300s, 3413.81/s)  LR: 7.016e-04  Data: 0.026 (0.026)
Train: 110 [ 900/1251 ( 72%)]  Loss: 3.601 (3.68)  Time: 0.307s, 3339.12/s  (0.300s, 3413.24/s)  LR: 7.014e-04  Data: 0.027 (0.026)
Train: 110 [ 950/1251 ( 76%)]  Loss: 3.702 (3.68)  Time: 0.298s, 3435.02/s  (0.300s, 3413.24/s)  LR: 7.012e-04  Data: 0.025 (0.025)
Train: 110 [1000/1251 ( 80%)]  Loss: 3.419 (3.66)  Time: 0.303s, 3375.29/s  (0.300s, 3412.14/s)  LR: 7.010e-04  Data: 0.019 (0.025)
Train: 110 [1050/1251 ( 84%)]  Loss: 3.549 (3.66)  Time: 0.298s, 3434.41/s  (0.300s, 3411.52/s)  LR: 7.008e-04  Data: 0.024 (0.025)
Train: 110 [1100/1251 ( 88%)]  Loss: 3.675 (3.66)  Time: 0.300s, 3411.18/s  (0.300s, 3410.75/s)  LR: 7.007e-04  Data: 0.024 (0.025)
Train: 110 [1150/1251 ( 92%)]  Loss: 3.757 (3.66)  Time: 0.305s, 3355.80/s  (0.300s, 3409.44/s)  LR: 7.005e-04  Data: 0.023 (0.025)
Train: 110 [1200/1251 ( 96%)]  Loss: 3.792 (3.67)  Time: 0.303s, 3379.84/s  (0.300s, 3408.86/s)  LR: 7.003e-04  Data: 0.026 (0.025)
Train: 110 [1250/1251 (100%)]  Loss: 3.463 (3.66)  Time: 0.276s, 3708.72/s  (0.300s, 3409.79/s)  LR: 7.001e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.194 (2.194)  Loss:  0.6699 (0.6699)  Acc@1: 86.4258 (86.4258)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.049 (0.238)  Loss:  0.7661 (1.2350)  Acc@1: 84.6698 (72.0620)  Acc@5: 95.4009 (91.0980)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-110.pth.tar', 72.06199993408202)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-109.pth.tar', 71.92600014648437)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-108.pth.tar', 71.9219999609375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-106.pth.tar', 71.83599999267578)

Train: 111 [   0/1251 (  0%)]  Loss: 3.972 (3.97)  Time: 2.658s,  385.29/s  (2.658s,  385.29/s)  LR: 7.001e-04  Data: 2.445 (2.445)
Train: 111 [  50/1251 (  4%)]  Loss: 3.914 (3.94)  Time: 0.293s, 3496.89/s  (0.333s, 3070.89/s)  LR: 6.999e-04  Data: 0.024 (0.080)
Train: 111 [ 100/1251 (  8%)]  Loss: 3.426 (3.77)  Time: 0.293s, 3489.53/s  (0.314s, 3261.64/s)  LR: 6.997e-04  Data: 0.022 (0.052)
Train: 111 [ 150/1251 ( 12%)]  Loss: 3.456 (3.69)  Time: 0.298s, 3439.87/s  (0.309s, 3316.72/s)  LR: 6.995e-04  Data: 0.023 (0.042)
Train: 111 [ 200/1251 ( 16%)]  Loss: 3.737 (3.70)  Time: 0.299s, 3428.54/s  (0.306s, 3342.22/s)  LR: 6.993e-04  Data: 0.021 (0.037)
Train: 111 [ 250/1251 ( 20%)]  Loss: 3.816 (3.72)  Time: 0.294s, 3479.47/s  (0.305s, 3357.53/s)  LR: 6.991e-04  Data: 0.021 (0.034)
Train: 111 [ 300/1251 ( 24%)]  Loss: 3.759 (3.73)  Time: 0.301s, 3404.70/s  (0.304s, 3366.53/s)  LR: 6.989e-04  Data: 0.022 (0.032)
Train: 111 [ 350/1251 ( 28%)]  Loss: 3.433 (3.69)  Time: 0.299s, 3425.39/s  (0.304s, 3373.16/s)  LR: 6.987e-04  Data: 0.025 (0.031)
Train: 111 [ 400/1251 ( 32%)]  Loss: 3.659 (3.69)  Time: 0.300s, 3418.99/s  (0.303s, 3377.03/s)  LR: 6.985e-04  Data: 0.023 (0.030)
Train: 111 [ 450/1251 ( 36%)]  Loss: 3.500 (3.67)  Time: 0.295s, 3468.25/s  (0.303s, 3379.07/s)  LR: 6.984e-04  Data: 0.020 (0.029)
Train: 111 [ 500/1251 ( 40%)]  Loss: 3.696 (3.67)  Time: 0.311s, 3297.58/s  (0.303s, 3381.46/s)  LR: 6.982e-04  Data: 0.027 (0.029)
Train: 111 [ 550/1251 ( 44%)]  Loss: 3.346 (3.64)  Time: 0.303s, 3381.62/s  (0.303s, 3382.16/s)  LR: 6.980e-04  Data: 0.024 (0.028)
Train: 111 [ 600/1251 ( 48%)]  Loss: 3.807 (3.66)  Time: 0.297s, 3442.98/s  (0.303s, 3382.67/s)  LR: 6.978e-04  Data: 0.020 (0.028)
Train: 111 [ 650/1251 ( 52%)]  Loss: 3.977 (3.68)  Time: 0.302s, 3387.69/s  (0.303s, 3382.58/s)  LR: 6.976e-04  Data: 0.021 (0.028)
Train: 111 [ 700/1251 ( 56%)]  Loss: 3.639 (3.68)  Time: 0.299s, 3419.65/s  (0.303s, 3382.59/s)  LR: 6.974e-04  Data: 0.024 (0.027)
Train: 111 [ 750/1251 ( 60%)]  Loss: 3.856 (3.69)  Time: 0.308s, 3329.13/s  (0.303s, 3382.54/s)  LR: 6.972e-04  Data: 0.021 (0.027)
Train: 111 [ 800/1251 ( 64%)]  Loss: 3.373 (3.67)  Time: 0.300s, 3414.09/s  (0.303s, 3382.02/s)  LR: 6.970e-04  Data: 0.027 (0.027)
Train: 111 [ 850/1251 ( 68%)]  Loss: 3.866 (3.68)  Time: 0.308s, 3329.02/s  (0.303s, 3382.15/s)  LR: 6.968e-04  Data: 0.023 (0.026)
Train: 111 [ 900/1251 ( 72%)]  Loss: 3.485 (3.67)  Time: 0.307s, 3338.97/s  (0.303s, 3381.32/s)  LR: 6.966e-04  Data: 0.024 (0.026)
Train: 111 [ 950/1251 ( 76%)]  Loss: 3.651 (3.67)  Time: 0.305s, 3359.11/s  (0.303s, 3380.60/s)  LR: 6.964e-04  Data: 0.023 (0.026)
Train: 111 [1000/1251 ( 80%)]  Loss: 3.601 (3.67)  Time: 0.302s, 3391.78/s  (0.303s, 3380.14/s)  LR: 6.962e-04  Data: 0.022 (0.026)
Train: 111 [1050/1251 ( 84%)]  Loss: 3.473 (3.66)  Time: 0.305s, 3356.88/s  (0.303s, 3379.64/s)  LR: 6.961e-04  Data: 0.023 (0.026)
Train: 111 [1100/1251 ( 88%)]  Loss: 3.710 (3.66)  Time: 0.309s, 3316.34/s  (0.303s, 3378.66/s)  LR: 6.959e-04  Data: 0.023 (0.026)
Train: 111 [1150/1251 ( 92%)]  Loss: 3.441 (3.65)  Time: 0.308s, 3320.55/s  (0.303s, 3377.60/s)  LR: 6.957e-04  Data: 0.024 (0.026)
Train: 111 [1200/1251 ( 96%)]  Loss: 3.870 (3.66)  Time: 0.304s, 3368.90/s  (0.303s, 3376.57/s)  LR: 6.955e-04  Data: 0.031 (0.025)
Train: 111 [1250/1251 (100%)]  Loss: 3.571 (3.66)  Time: 0.275s, 3720.91/s  (0.303s, 3377.62/s)  LR: 6.953e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.070 (2.070)  Loss:  0.6670 (0.6670)  Acc@1: 86.8164 (86.8164)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.041 (0.239)  Loss:  0.7041 (1.2317)  Acc@1: 84.7877 (72.3160)  Acc@5: 96.3443 (91.1840)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-111.pth.tar', 72.31599998535157)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-110.pth.tar', 72.06199993408202)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-109.pth.tar', 71.92600014648437)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-108.pth.tar', 71.9219999609375)

Train: 112 [   0/1251 (  0%)]  Loss: 3.548 (3.55)  Time: 2.185s,  468.57/s  (2.185s,  468.57/s)  LR: 6.953e-04  Data: 1.957 (1.957)
Train: 112 [  50/1251 (  4%)]  Loss: 3.706 (3.63)  Time: 0.293s, 3489.42/s  (0.325s, 3148.68/s)  LR: 6.951e-04  Data: 0.024 (0.062)
Train: 112 [ 100/1251 (  8%)]  Loss: 3.494 (3.58)  Time: 0.297s, 3445.25/s  (0.311s, 3291.46/s)  LR: 6.949e-04  Data: 0.024 (0.042)
Train: 112 [ 150/1251 ( 12%)]  Loss: 3.670 (3.60)  Time: 0.299s, 3419.28/s  (0.307s, 3331.23/s)  LR: 6.947e-04  Data: 0.026 (0.036)
Train: 112 [ 200/1251 ( 16%)]  Loss: 3.502 (3.58)  Time: 0.302s, 3393.69/s  (0.306s, 3348.78/s)  LR: 6.945e-04  Data: 0.019 (0.033)
Train: 112 [ 250/1251 ( 20%)]  Loss: 3.590 (3.59)  Time: 0.299s, 3429.32/s  (0.305s, 3358.22/s)  LR: 6.943e-04  Data: 0.022 (0.031)
Train: 112 [ 300/1251 ( 24%)]  Loss: 4.046 (3.65)  Time: 0.306s, 3345.43/s  (0.305s, 3362.77/s)  LR: 6.941e-04  Data: 0.022 (0.029)
Train: 112 [ 350/1251 ( 28%)]  Loss: 3.809 (3.67)  Time: 0.299s, 3426.55/s  (0.304s, 3366.77/s)  LR: 6.939e-04  Data: 0.022 (0.029)
Train: 112 [ 400/1251 ( 32%)]  Loss: 3.597 (3.66)  Time: 0.304s, 3371.45/s  (0.304s, 3370.45/s)  LR: 6.937e-04  Data: 0.022 (0.028)
Train: 112 [ 450/1251 ( 36%)]  Loss: 3.634 (3.66)  Time: 0.306s, 3349.88/s  (0.304s, 3371.53/s)  LR: 6.936e-04  Data: 0.026 (0.027)
Train: 112 [ 500/1251 ( 40%)]  Loss: 3.809 (3.67)  Time: 0.306s, 3347.30/s  (0.304s, 3371.87/s)  LR: 6.934e-04  Data: 0.022 (0.027)
Train: 112 [ 550/1251 ( 44%)]  Loss: 3.883 (3.69)  Time: 0.306s, 3343.15/s  (0.304s, 3372.71/s)  LR: 6.932e-04  Data: 0.031 (0.027)
Train: 112 [ 600/1251 ( 48%)]  Loss: 3.904 (3.71)  Time: 0.306s, 3350.58/s  (0.304s, 3372.25/s)  LR: 6.930e-04  Data: 0.024 (0.026)
Train: 112 [ 650/1251 ( 52%)]  Loss: 3.725 (3.71)  Time: 0.310s, 3304.16/s  (0.304s, 3371.04/s)  LR: 6.928e-04  Data: 0.025 (0.026)
Train: 112 [ 700/1251 ( 56%)]  Loss: 3.311 (3.68)  Time: 0.303s, 3378.77/s  (0.304s, 3371.57/s)  LR: 6.926e-04  Data: 0.024 (0.026)
Train: 112 [ 750/1251 ( 60%)]  Loss: 3.524 (3.67)  Time: 0.304s, 3364.12/s  (0.304s, 3372.07/s)  LR: 6.924e-04  Data: 0.020 (0.026)
Train: 112 [ 800/1251 ( 64%)]  Loss: 3.626 (3.67)  Time: 0.306s, 3348.12/s  (0.304s, 3372.26/s)  LR: 6.922e-04  Data: 0.023 (0.025)
Train: 112 [ 850/1251 ( 68%)]  Loss: 3.422 (3.66)  Time: 0.301s, 3404.58/s  (0.304s, 3371.58/s)  LR: 6.920e-04  Data: 0.023 (0.025)
Train: 112 [ 900/1251 ( 72%)]  Loss: 3.582 (3.65)  Time: 0.304s, 3368.88/s  (0.304s, 3371.51/s)  LR: 6.918e-04  Data: 0.026 (0.025)
Train: 112 [ 950/1251 ( 76%)]  Loss: 3.621 (3.65)  Time: 0.307s, 3336.71/s  (0.304s, 3371.51/s)  LR: 6.916e-04  Data: 0.022 (0.025)
Train: 112 [1000/1251 ( 80%)]  Loss: 3.459 (3.64)  Time: 0.305s, 3353.95/s  (0.304s, 3370.97/s)  LR: 6.914e-04  Data: 0.023 (0.025)
Train: 112 [1050/1251 ( 84%)]  Loss: 3.623 (3.64)  Time: 0.304s, 3369.04/s  (0.304s, 3370.56/s)  LR: 6.912e-04  Data: 0.023 (0.025)
Train: 112 [1100/1251 ( 88%)]  Loss: 3.703 (3.64)  Time: 0.306s, 3349.29/s  (0.304s, 3370.86/s)  LR: 6.911e-04  Data: 0.022 (0.025)
Train: 112 [1150/1251 ( 92%)]  Loss: 3.869 (3.65)  Time: 0.303s, 3380.67/s  (0.304s, 3370.89/s)  LR: 6.909e-04  Data: 0.022 (0.025)
Train: 112 [1200/1251 ( 96%)]  Loss: 3.617 (3.65)  Time: 0.301s, 3399.76/s  (0.304s, 3370.89/s)  LR: 6.907e-04  Data: 0.023 (0.025)
Train: 112 [1250/1251 (100%)]  Loss: 3.639 (3.65)  Time: 0.276s, 3708.22/s  (0.304s, 3372.33/s)  LR: 6.905e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.182 (2.182)  Loss:  0.6304 (0.6304)  Acc@1: 87.3047 (87.3047)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.057 (0.241)  Loss:  0.7354 (1.2258)  Acc@1: 84.5519 (72.2320)  Acc@5: 95.7547 (91.1140)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-111.pth.tar', 72.31599998535157)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-112.pth.tar', 72.23200001220704)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-110.pth.tar', 72.06199993408202)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-109.pth.tar', 71.92600014648437)

Train: 113 [   0/1251 (  0%)]  Loss: 3.520 (3.52)  Time: 2.414s,  424.24/s  (2.414s,  424.24/s)  LR: 6.905e-04  Data: 2.181 (2.181)
Train: 113 [  50/1251 (  4%)]  Loss: 3.539 (3.53)  Time: 0.294s, 3478.97/s  (0.327s, 3129.51/s)  LR: 6.903e-04  Data: 0.021 (0.066)
Train: 113 [ 100/1251 (  8%)]  Loss: 3.534 (3.53)  Time: 0.305s, 3358.62/s  (0.313s, 3272.41/s)  LR: 6.901e-04  Data: 0.024 (0.045)
Train: 113 [ 150/1251 ( 12%)]  Loss: 3.431 (3.51)  Time: 0.304s, 3363.87/s  (0.309s, 3313.92/s)  LR: 6.899e-04  Data: 0.024 (0.037)
Train: 113 [ 200/1251 ( 16%)]  Loss: 3.466 (3.50)  Time: 0.304s, 3368.63/s  (0.307s, 3333.26/s)  LR: 6.897e-04  Data: 0.022 (0.034)
Train: 113 [ 250/1251 ( 20%)]  Loss: 3.464 (3.49)  Time: 0.300s, 3411.14/s  (0.306s, 3343.88/s)  LR: 6.895e-04  Data: 0.020 (0.032)
Train: 113 [ 300/1251 ( 24%)]  Loss: 3.316 (3.47)  Time: 0.306s, 3345.22/s  (0.306s, 3350.46/s)  LR: 6.893e-04  Data: 0.023 (0.030)
Train: 113 [ 350/1251 ( 28%)]  Loss: 3.206 (3.43)  Time: 0.302s, 3386.00/s  (0.305s, 3355.14/s)  LR: 6.891e-04  Data: 0.014 (0.029)
Train: 113 [ 400/1251 ( 32%)]  Loss: 3.381 (3.43)  Time: 0.301s, 3405.92/s  (0.305s, 3358.70/s)  LR: 6.889e-04  Data: 0.013 (0.028)
Train: 113 [ 450/1251 ( 36%)]  Loss: 3.696 (3.46)  Time: 0.300s, 3411.77/s  (0.305s, 3360.44/s)  LR: 6.887e-04  Data: 0.023 (0.028)
Train: 113 [ 500/1251 ( 40%)]  Loss: 3.874 (3.49)  Time: 0.313s, 3272.15/s  (0.305s, 3362.06/s)  LR: 6.885e-04  Data: 0.022 (0.027)
Train: 113 [ 550/1251 ( 44%)]  Loss: 3.599 (3.50)  Time: 0.309s, 3313.15/s  (0.305s, 3361.97/s)  LR: 6.883e-04  Data: 0.031 (0.027)
Train: 113 [ 600/1251 ( 48%)]  Loss: 3.797 (3.52)  Time: 0.303s, 3383.45/s  (0.305s, 3362.77/s)  LR: 6.882e-04  Data: 0.023 (0.027)
Train: 113 [ 650/1251 ( 52%)]  Loss: 3.671 (3.54)  Time: 0.304s, 3369.17/s  (0.305s, 3362.81/s)  LR: 6.880e-04  Data: 0.022 (0.026)
Train: 113 [ 700/1251 ( 56%)]  Loss: 3.796 (3.55)  Time: 0.310s, 3304.24/s  (0.305s, 3362.17/s)  LR: 6.878e-04  Data: 0.023 (0.026)
Train: 113 [ 750/1251 ( 60%)]  Loss: 3.414 (3.54)  Time: 0.310s, 3307.95/s  (0.305s, 3362.60/s)  LR: 6.876e-04  Data: 0.026 (0.026)
Train: 113 [ 800/1251 ( 64%)]  Loss: 3.423 (3.54)  Time: 0.306s, 3349.50/s  (0.305s, 3362.34/s)  LR: 6.874e-04  Data: 0.022 (0.026)
Train: 113 [ 850/1251 ( 68%)]  Loss: 3.666 (3.54)  Time: 0.303s, 3385.04/s  (0.305s, 3362.05/s)  LR: 6.872e-04  Data: 0.023 (0.026)
Train: 113 [ 900/1251 ( 72%)]  Loss: 3.794 (3.56)  Time: 0.308s, 3326.07/s  (0.305s, 3361.19/s)  LR: 6.870e-04  Data: 0.025 (0.025)
Train: 113 [ 950/1251 ( 76%)]  Loss: 3.601 (3.56)  Time: 0.310s, 3301.19/s  (0.305s, 3361.04/s)  LR: 6.868e-04  Data: 0.028 (0.025)
Train: 113 [1000/1251 ( 80%)]  Loss: 3.608 (3.56)  Time: 0.302s, 3386.69/s  (0.305s, 3360.45/s)  LR: 6.866e-04  Data: 0.020 (0.025)
Train: 113 [1050/1251 ( 84%)]  Loss: 3.468 (3.56)  Time: 0.303s, 3380.37/s  (0.305s, 3360.62/s)  LR: 6.864e-04  Data: 0.022 (0.025)
Train: 113 [1100/1251 ( 88%)]  Loss: 3.797 (3.57)  Time: 0.308s, 3329.65/s  (0.305s, 3360.18/s)  LR: 6.862e-04  Data: 0.024 (0.025)
Train: 113 [1150/1251 ( 92%)]  Loss: 3.639 (3.57)  Time: 0.305s, 3359.20/s  (0.305s, 3359.69/s)  LR: 6.860e-04  Data: 0.023 (0.025)
Train: 113 [1200/1251 ( 96%)]  Loss: 3.605 (3.57)  Time: 0.310s, 3303.13/s  (0.305s, 3359.03/s)  LR: 6.858e-04  Data: 0.025 (0.025)
Train: 113 [1250/1251 (100%)]  Loss: 3.779 (3.58)  Time: 0.277s, 3697.56/s  (0.305s, 3360.44/s)  LR: 6.856e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.053 (2.053)  Loss:  0.6626 (0.6626)  Acc@1: 87.5977 (87.5977)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.044 (0.241)  Loss:  0.7637 (1.2408)  Acc@1: 83.9623 (72.1000)  Acc@5: 95.4009 (90.9320)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-111.pth.tar', 72.31599998535157)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-112.pth.tar', 72.23200001220704)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-113.pth.tar', 72.10000001464844)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-110.pth.tar', 72.06199993408202)

Train: 114 [   0/1251 (  0%)]  Loss: 3.911 (3.91)  Time: 2.318s,  441.76/s  (2.318s,  441.76/s)  LR: 6.856e-04  Data: 2.097 (2.097)
Train: 114 [  50/1251 (  4%)]  Loss: 3.828 (3.87)  Time: 0.298s, 3441.47/s  (0.327s, 3132.28/s)  LR: 6.854e-04  Data: 0.023 (0.064)
Train: 114 [ 100/1251 (  8%)]  Loss: 3.813 (3.85)  Time: 0.302s, 3396.07/s  (0.314s, 3265.61/s)  LR: 6.853e-04  Data: 0.025 (0.044)
Train: 114 [ 150/1251 ( 12%)]  Loss: 3.648 (3.80)  Time: 0.299s, 3419.73/s  (0.310s, 3306.89/s)  LR: 6.851e-04  Data: 0.020 (0.037)
Train: 114 [ 200/1251 ( 16%)]  Loss: 3.753 (3.79)  Time: 0.304s, 3372.30/s  (0.308s, 3319.94/s)  LR: 6.849e-04  Data: 0.025 (0.034)
Train: 114 [ 250/1251 ( 20%)]  Loss: 3.692 (3.77)  Time: 0.302s, 3387.22/s  (0.308s, 3327.47/s)  LR: 6.847e-04  Data: 0.019 (0.032)
Train: 114 [ 300/1251 ( 24%)]  Loss: 3.760 (3.77)  Time: 0.313s, 3273.68/s  (0.307s, 3331.54/s)  LR: 6.845e-04  Data: 0.024 (0.030)
Train: 114 [ 350/1251 ( 28%)]  Loss: 3.450 (3.73)  Time: 0.310s, 3307.46/s  (0.307s, 3334.73/s)  LR: 6.843e-04  Data: 0.023 (0.029)
Train: 114 [ 400/1251 ( 32%)]  Loss: 3.809 (3.74)  Time: 0.301s, 3403.17/s  (0.307s, 3335.99/s)  LR: 6.841e-04  Data: 0.024 (0.028)
Train: 114 [ 450/1251 ( 36%)]  Loss: 3.808 (3.75)  Time: 0.302s, 3386.49/s  (0.307s, 3338.58/s)  LR: 6.839e-04  Data: 0.022 (0.028)
Train: 114 [ 500/1251 ( 40%)]  Loss: 3.937 (3.76)  Time: 0.309s, 3317.58/s  (0.307s, 3338.81/s)  LR: 6.837e-04  Data: 0.024 (0.027)
Train: 114 [ 550/1251 ( 44%)]  Loss: 3.575 (3.75)  Time: 0.309s, 3314.90/s  (0.307s, 3339.54/s)  LR: 6.835e-04  Data: 0.026 (0.027)
Train: 114 [ 600/1251 ( 48%)]  Loss: 3.567 (3.73)  Time: 0.305s, 3353.13/s  (0.307s, 3340.11/s)  LR: 6.833e-04  Data: 0.023 (0.027)
Train: 114 [ 650/1251 ( 52%)]  Loss: 3.679 (3.73)  Time: 0.307s, 3339.26/s  (0.307s, 3340.40/s)  LR: 6.831e-04  Data: 0.024 (0.026)
Train: 114 [ 700/1251 ( 56%)]  Loss: 3.347 (3.71)  Time: 0.310s, 3303.58/s  (0.307s, 3340.63/s)  LR: 6.829e-04  Data: 0.026 (0.026)
Train: 114 [ 750/1251 ( 60%)]  Loss: 3.485 (3.69)  Time: 0.309s, 3316.49/s  (0.306s, 3341.39/s)  LR: 6.827e-04  Data: 0.022 (0.026)
Train: 114 [ 800/1251 ( 64%)]  Loss: 3.612 (3.69)  Time: 0.304s, 3372.91/s  (0.306s, 3341.84/s)  LR: 6.825e-04  Data: 0.021 (0.026)
Train: 114 [ 850/1251 ( 68%)]  Loss: 3.621 (3.68)  Time: 0.302s, 3395.41/s  (0.306s, 3341.66/s)  LR: 6.823e-04  Data: 0.022 (0.026)
Train: 114 [ 900/1251 ( 72%)]  Loss: 3.876 (3.69)  Time: 0.308s, 3322.31/s  (0.306s, 3341.98/s)  LR: 6.821e-04  Data: 0.027 (0.025)
Train: 114 [ 950/1251 ( 76%)]  Loss: 3.522 (3.68)  Time: 0.310s, 3308.07/s  (0.306s, 3342.16/s)  LR: 6.820e-04  Data: 0.024 (0.025)
Train: 114 [1000/1251 ( 80%)]  Loss: 3.486 (3.68)  Time: 0.304s, 3372.16/s  (0.306s, 3341.96/s)  LR: 6.818e-04  Data: 0.018 (0.025)
Train: 114 [1050/1251 ( 84%)]  Loss: 3.806 (3.68)  Time: 0.304s, 3366.53/s  (0.306s, 3341.68/s)  LR: 6.816e-04  Data: 0.021 (0.025)
Train: 114 [1100/1251 ( 88%)]  Loss: 3.511 (3.67)  Time: 0.306s, 3347.50/s  (0.306s, 3341.52/s)  LR: 6.814e-04  Data: 0.023 (0.025)
Train: 114 [1150/1251 ( 92%)]  Loss: 3.686 (3.67)  Time: 0.312s, 3285.66/s  (0.306s, 3341.28/s)  LR: 6.812e-04  Data: 0.021 (0.025)
Train: 114 [1200/1251 ( 96%)]  Loss: 3.330 (3.66)  Time: 0.305s, 3361.74/s  (0.306s, 3341.41/s)  LR: 6.810e-04  Data: 0.019 (0.025)
Train: 114 [1250/1251 (100%)]  Loss: 3.717 (3.66)  Time: 0.276s, 3710.76/s  (0.306s, 3343.25/s)  LR: 6.808e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.123 (2.123)  Loss:  0.6284 (0.6284)  Acc@1: 87.3047 (87.3047)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.040 (0.236)  Loss:  0.7891 (1.2303)  Acc@1: 82.6651 (72.3480)  Acc@5: 96.1085 (91.1640)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-114.pth.tar', 72.34799996826172)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-111.pth.tar', 72.31599998535157)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-112.pth.tar', 72.23200001220704)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-113.pth.tar', 72.10000001464844)

Train: 115 [   0/1251 (  0%)]  Loss: 3.687 (3.69)  Time: 2.129s,  480.87/s  (2.129s,  480.87/s)  LR: 6.808e-04  Data: 1.896 (1.896)
Train: 115 [  50/1251 (  4%)]  Loss: 3.478 (3.58)  Time: 0.298s, 3436.10/s  (0.329s, 3113.16/s)  LR: 6.806e-04  Data: 0.024 (0.061)
Train: 115 [ 100/1251 (  8%)]  Loss: 3.603 (3.59)  Time: 0.306s, 3351.58/s  (0.315s, 3245.73/s)  LR: 6.804e-04  Data: 0.024 (0.042)
Train: 115 [ 150/1251 ( 12%)]  Loss: 3.647 (3.60)  Time: 0.309s, 3311.24/s  (0.311s, 3288.37/s)  LR: 6.802e-04  Data: 0.022 (0.036)
Train: 115 [ 200/1251 ( 16%)]  Loss: 3.404 (3.56)  Time: 0.307s, 3337.57/s  (0.310s, 3304.34/s)  LR: 6.800e-04  Data: 0.019 (0.032)
Train: 115 [ 250/1251 ( 20%)]  Loss: 3.902 (3.62)  Time: 0.306s, 3347.17/s  (0.309s, 3312.68/s)  LR: 6.798e-04  Data: 0.021 (0.031)
Train: 115 [ 300/1251 ( 24%)]  Loss: 3.735 (3.64)  Time: 0.306s, 3345.40/s  (0.309s, 3318.19/s)  LR: 6.796e-04  Data: 0.024 (0.029)
Train: 115 [ 350/1251 ( 28%)]  Loss: 3.640 (3.64)  Time: 0.311s, 3290.52/s  (0.308s, 3321.26/s)  LR: 6.794e-04  Data: 0.023 (0.028)
Train: 115 [ 400/1251 ( 32%)]  Loss: 3.666 (3.64)  Time: 0.310s, 3302.17/s  (0.308s, 3322.34/s)  LR: 6.792e-04  Data: 0.025 (0.028)
Train: 115 [ 450/1251 ( 36%)]  Loss: 3.371 (3.61)  Time: 0.309s, 3311.06/s  (0.308s, 3323.66/s)  LR: 6.790e-04  Data: 0.018 (0.027)
Train: 115 [ 500/1251 ( 40%)]  Loss: 3.335 (3.59)  Time: 0.305s, 3353.01/s  (0.308s, 3324.55/s)  LR: 6.788e-04  Data: 0.021 (0.027)
Train: 115 [ 550/1251 ( 44%)]  Loss: 3.570 (3.59)  Time: 0.308s, 3323.48/s  (0.308s, 3325.74/s)  LR: 6.786e-04  Data: 0.025 (0.026)
Train: 115 [ 600/1251 ( 48%)]  Loss: 3.558 (3.58)  Time: 0.310s, 3305.80/s  (0.308s, 3327.59/s)  LR: 6.784e-04  Data: 0.023 (0.026)
Train: 115 [ 650/1251 ( 52%)]  Loss: 3.708 (3.59)  Time: 0.306s, 3347.55/s  (0.308s, 3327.62/s)  LR: 6.783e-04  Data: 0.022 (0.026)
Train: 115 [ 700/1251 ( 56%)]  Loss: 3.250 (3.57)  Time: 0.307s, 3337.38/s  (0.308s, 3327.82/s)  LR: 6.781e-04  Data: 0.024 (0.026)
Train: 115 [ 750/1251 ( 60%)]  Loss: 3.974 (3.60)  Time: 0.305s, 3361.91/s  (0.308s, 3328.44/s)  LR: 6.779e-04  Data: 0.022 (0.025)
Train: 115 [ 800/1251 ( 64%)]  Loss: 3.553 (3.59)  Time: 0.305s, 3358.42/s  (0.308s, 3328.08/s)  LR: 6.777e-04  Data: 0.022 (0.025)
Train: 115 [ 850/1251 ( 68%)]  Loss: 3.468 (3.59)  Time: 0.310s, 3306.04/s  (0.308s, 3327.98/s)  LR: 6.775e-04  Data: 0.021 (0.025)
Train: 115 [ 900/1251 ( 72%)]  Loss: 3.411 (3.58)  Time: 0.310s, 3305.18/s  (0.308s, 3327.99/s)  LR: 6.773e-04  Data: 0.021 (0.025)
Train: 115 [ 950/1251 ( 76%)]  Loss: 3.811 (3.59)  Time: 0.312s, 3284.19/s  (0.308s, 3327.58/s)  LR: 6.771e-04  Data: 0.025 (0.025)
Train: 115 [1000/1251 ( 80%)]  Loss: 3.691 (3.59)  Time: 0.312s, 3286.04/s  (0.308s, 3327.67/s)  LR: 6.769e-04  Data: 0.021 (0.025)
Train: 115 [1050/1251 ( 84%)]  Loss: 3.748 (3.60)  Time: 0.305s, 3356.20/s  (0.308s, 3327.95/s)  LR: 6.767e-04  Data: 0.022 (0.025)
Train: 115 [1100/1251 ( 88%)]  Loss: 3.431 (3.59)  Time: 0.311s, 3287.51/s  (0.308s, 3328.29/s)  LR: 6.765e-04  Data: 0.023 (0.025)
Train: 115 [1150/1251 ( 92%)]  Loss: 3.675 (3.60)  Time: 0.312s, 3284.72/s  (0.308s, 3327.87/s)  LR: 6.763e-04  Data: 0.023 (0.025)
Train: 115 [1200/1251 ( 96%)]  Loss: 3.749 (3.60)  Time: 0.310s, 3305.50/s  (0.308s, 3327.61/s)  LR: 6.761e-04  Data: 0.022 (0.024)
Train: 115 [1250/1251 (100%)]  Loss: 3.466 (3.60)  Time: 0.276s, 3703.55/s  (0.308s, 3329.17/s)  LR: 6.759e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.094 (2.094)  Loss:  0.6328 (0.6328)  Acc@1: 87.5000 (87.5000)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.063 (0.233)  Loss:  0.7817 (1.2349)  Acc@1: 83.1368 (72.3200)  Acc@5: 96.4623 (91.1240)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-114.pth.tar', 72.34799996826172)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-115.pth.tar', 72.32000004394531)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-111.pth.tar', 72.31599998535157)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-112.pth.tar', 72.23200001220704)

Train: 116 [   0/1251 (  0%)]  Loss: 3.568 (3.57)  Time: 2.367s,  432.66/s  (2.367s,  432.66/s)  LR: 6.759e-04  Data: 2.156 (2.156)
Train: 116 [  50/1251 (  4%)]  Loss: 3.819 (3.69)  Time: 0.302s, 3385.92/s  (0.332s, 3087.73/s)  LR: 6.757e-04  Data: 0.025 (0.064)
Train: 116 [ 100/1251 (  8%)]  Loss: 3.806 (3.73)  Time: 0.306s, 3340.99/s  (0.318s, 3220.88/s)  LR: 6.755e-04  Data: 0.024 (0.044)
Train: 116 [ 150/1251 ( 12%)]  Loss: 3.677 (3.72)  Time: 0.306s, 3341.86/s  (0.314s, 3265.39/s)  LR: 6.753e-04  Data: 0.023 (0.037)
Train: 116 [ 200/1251 ( 16%)]  Loss: 3.371 (3.65)  Time: 0.309s, 3318.49/s  (0.312s, 3284.76/s)  LR: 6.751e-04  Data: 0.024 (0.034)
Train: 116 [ 250/1251 ( 20%)]  Loss: 3.444 (3.61)  Time: 0.307s, 3331.01/s  (0.311s, 3292.97/s)  LR: 6.749e-04  Data: 0.023 (0.032)
Train: 116 [ 300/1251 ( 24%)]  Loss: 3.685 (3.62)  Time: 0.303s, 3379.34/s  (0.310s, 3298.80/s)  LR: 6.747e-04  Data: 0.020 (0.030)
Train: 116 [ 350/1251 ( 28%)]  Loss: 3.688 (3.63)  Time: 0.307s, 3334.08/s  (0.310s, 3303.10/s)  LR: 6.745e-04  Data: 0.021 (0.029)
Train: 116 [ 400/1251 ( 32%)]  Loss: 3.726 (3.64)  Time: 0.311s, 3293.74/s  (0.310s, 3305.11/s)  LR: 6.743e-04  Data: 0.023 (0.028)
Train: 116 [ 450/1251 ( 36%)]  Loss: 3.460 (3.62)  Time: 0.309s, 3310.01/s  (0.310s, 3306.34/s)  LR: 6.742e-04  Data: 0.023 (0.028)
Train: 116 [ 500/1251 ( 40%)]  Loss: 3.634 (3.63)  Time: 0.311s, 3287.97/s  (0.310s, 3307.19/s)  LR: 6.740e-04  Data: 0.025 (0.027)
Train: 116 [ 550/1251 ( 44%)]  Loss: 3.972 (3.65)  Time: 0.313s, 3270.01/s  (0.310s, 3307.72/s)  LR: 6.738e-04  Data: 0.027 (0.027)
Train: 116 [ 600/1251 ( 48%)]  Loss: 3.591 (3.65)  Time: 0.310s, 3300.69/s  (0.309s, 3308.91/s)  LR: 6.736e-04  Data: 0.019 (0.026)
Train: 116 [ 650/1251 ( 52%)]  Loss: 3.871 (3.67)  Time: 0.309s, 3314.46/s  (0.309s, 3309.34/s)  LR: 6.734e-04  Data: 0.022 (0.026)
Train: 116 [ 700/1251 ( 56%)]  Loss: 3.396 (3.65)  Time: 0.316s, 3238.31/s  (0.309s, 3309.96/s)  LR: 6.732e-04  Data: 0.023 (0.026)
Train: 116 [ 750/1251 ( 60%)]  Loss: 4.051 (3.67)  Time: 0.313s, 3268.75/s  (0.309s, 3309.97/s)  LR: 6.730e-04  Data: 0.023 (0.026)
Train: 116 [ 800/1251 ( 64%)]  Loss: 3.539 (3.66)  Time: 0.305s, 3356.52/s  (0.309s, 3310.40/s)  LR: 6.728e-04  Data: 0.023 (0.026)
Train: 116 [ 850/1251 ( 68%)]  Loss: 3.735 (3.67)  Time: 0.308s, 3323.32/s  (0.309s, 3310.40/s)  LR: 6.726e-04  Data: 0.024 (0.025)
Train: 116 [ 900/1251 ( 72%)]  Loss: 3.738 (3.67)  Time: 0.313s, 3274.83/s  (0.309s, 3310.82/s)  LR: 6.724e-04  Data: 0.024 (0.025)
Train: 116 [ 950/1251 ( 76%)]  Loss: 3.347 (3.66)  Time: 0.308s, 3328.00/s  (0.309s, 3310.45/s)  LR: 6.722e-04  Data: 0.022 (0.025)
Train: 116 [1000/1251 ( 80%)]  Loss: 3.977 (3.67)  Time: 0.312s, 3287.21/s  (0.309s, 3310.35/s)  LR: 6.720e-04  Data: 0.022 (0.025)
Train: 116 [1050/1251 ( 84%)]  Loss: 3.259 (3.65)  Time: 0.308s, 3326.77/s  (0.309s, 3310.39/s)  LR: 6.718e-04  Data: 0.023 (0.025)
Train: 116 [1100/1251 ( 88%)]  Loss: 3.657 (3.65)  Time: 0.311s, 3295.80/s  (0.309s, 3310.28/s)  LR: 6.716e-04  Data: 0.025 (0.025)
Train: 116 [1150/1251 ( 92%)]  Loss: 3.441 (3.64)  Time: 0.304s, 3373.08/s  (0.309s, 3309.72/s)  LR: 6.714e-04  Data: 0.019 (0.025)
Train: 116 [1200/1251 ( 96%)]  Loss: 3.624 (3.64)  Time: 0.311s, 3292.62/s  (0.309s, 3309.64/s)  LR: 6.712e-04  Data: 0.025 (0.025)
Train: 116 [1250/1251 (100%)]  Loss: 3.827 (3.65)  Time: 0.278s, 3686.13/s  (0.309s, 3311.37/s)  LR: 6.710e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.087 (2.087)  Loss:  0.6865 (0.6865)  Acc@1: 86.6211 (86.6211)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.045 (0.236)  Loss:  0.7544 (1.2134)  Acc@1: 82.4292 (72.7880)  Acc@5: 95.7547 (91.3300)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-116.pth.tar', 72.78799999511719)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-114.pth.tar', 72.34799996826172)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-115.pth.tar', 72.32000004394531)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-111.pth.tar', 72.31599998535157)

Train: 117 [   0/1251 (  0%)]  Loss: 3.590 (3.59)  Time: 2.407s,  425.49/s  (2.407s,  425.49/s)  LR: 6.710e-04  Data: 2.180 (2.180)
Train: 117 [  50/1251 (  4%)]  Loss: 3.695 (3.64)  Time: 0.303s, 3382.43/s  (0.332s, 3082.25/s)  LR: 6.708e-04  Data: 0.022 (0.066)
Train: 117 [ 100/1251 (  8%)]  Loss: 3.588 (3.62)  Time: 0.303s, 3381.71/s  (0.318s, 3220.32/s)  LR: 6.706e-04  Data: 0.026 (0.045)
Train: 117 [ 150/1251 ( 12%)]  Loss: 3.298 (3.54)  Time: 0.306s, 3351.67/s  (0.314s, 3261.52/s)  LR: 6.704e-04  Data: 0.022 (0.037)
Train: 117 [ 200/1251 ( 16%)]  Loss: 3.449 (3.52)  Time: 0.308s, 3326.09/s  (0.313s, 3276.04/s)  LR: 6.702e-04  Data: 0.024 (0.034)
Train: 117 [ 250/1251 ( 20%)]  Loss: 3.643 (3.54)  Time: 0.310s, 3298.77/s  (0.312s, 3285.02/s)  LR: 6.700e-04  Data: 0.022 (0.032)
Train: 117 [ 300/1251 ( 24%)]  Loss: 3.594 (3.55)  Time: 0.309s, 3314.69/s  (0.311s, 3289.09/s)  LR: 6.698e-04  Data: 0.018 (0.030)
Train: 117 [ 350/1251 ( 28%)]  Loss: 3.503 (3.54)  Time: 0.311s, 3290.36/s  (0.311s, 3292.11/s)  LR: 6.696e-04  Data: 0.029 (0.029)
Train: 117 [ 400/1251 ( 32%)]  Loss: 4.023 (3.60)  Time: 0.313s, 3274.06/s  (0.311s, 3293.83/s)  LR: 6.694e-04  Data: 0.022 (0.028)
Train: 117 [ 450/1251 ( 36%)]  Loss: 3.954 (3.63)  Time: 0.308s, 3326.56/s  (0.311s, 3296.32/s)  LR: 6.693e-04  Data: 0.021 (0.028)
Train: 117 [ 500/1251 ( 40%)]  Loss: 3.581 (3.63)  Time: 0.309s, 3308.87/s  (0.310s, 3298.13/s)  LR: 6.691e-04  Data: 0.024 (0.027)
Train: 117 [ 550/1251 ( 44%)]  Loss: 3.574 (3.62)  Time: 0.310s, 3305.99/s  (0.310s, 3299.02/s)  LR: 6.689e-04  Data: 0.023 (0.027)
Train: 117 [ 600/1251 ( 48%)]  Loss: 3.603 (3.62)  Time: 0.315s, 3252.93/s  (0.310s, 3299.63/s)  LR: 6.687e-04  Data: 0.021 (0.026)
Train: 117 [ 650/1251 ( 52%)]  Loss: 3.765 (3.63)  Time: 0.305s, 3360.22/s  (0.310s, 3300.67/s)  LR: 6.685e-04  Data: 0.023 (0.026)
Train: 117 [ 700/1251 ( 56%)]  Loss: 3.417 (3.62)  Time: 0.310s, 3300.62/s  (0.310s, 3301.00/s)  LR: 6.683e-04  Data: 0.022 (0.026)
Train: 117 [ 750/1251 ( 60%)]  Loss: 3.304 (3.60)  Time: 0.308s, 3327.55/s  (0.310s, 3300.93/s)  LR: 6.681e-04  Data: 0.022 (0.026)
Train: 117 [ 800/1251 ( 64%)]  Loss: 3.520 (3.59)  Time: 0.308s, 3325.33/s  (0.310s, 3301.26/s)  LR: 6.679e-04  Data: 0.024 (0.025)
Train: 117 [ 850/1251 ( 68%)]  Loss: 4.099 (3.62)  Time: 0.320s, 3202.70/s  (0.310s, 3301.24/s)  LR: 6.677e-04  Data: 0.022 (0.025)
Train: 117 [ 900/1251 ( 72%)]  Loss: 3.851 (3.63)  Time: 0.316s, 3240.05/s  (0.310s, 3301.34/s)  LR: 6.675e-04  Data: 0.022 (0.025)
Train: 117 [ 950/1251 ( 76%)]  Loss: 2.870 (3.60)  Time: 0.310s, 3303.67/s  (0.310s, 3301.20/s)  LR: 6.673e-04  Data: 0.019 (0.025)
Train: 117 [1000/1251 ( 80%)]  Loss: 3.265 (3.58)  Time: 0.311s, 3294.03/s  (0.310s, 3301.36/s)  LR: 6.671e-04  Data: 0.023 (0.025)
Train: 117 [1050/1251 ( 84%)]  Loss: 3.758 (3.59)  Time: 0.304s, 3368.86/s  (0.310s, 3301.15/s)  LR: 6.669e-04  Data: 0.021 (0.025)
Train: 117 [1100/1251 ( 88%)]  Loss: 3.715 (3.59)  Time: 0.306s, 3350.53/s  (0.310s, 3301.21/s)  LR: 6.667e-04  Data: 0.019 (0.025)
Train: 117 [1150/1251 ( 92%)]  Loss: 3.591 (3.59)  Time: 0.309s, 3310.19/s  (0.310s, 3301.27/s)  LR: 6.665e-04  Data: 0.023 (0.025)
Train: 117 [1200/1251 ( 96%)]  Loss: 3.806 (3.60)  Time: 0.308s, 3320.98/s  (0.310s, 3301.02/s)  LR: 6.663e-04  Data: 0.026 (0.025)
Train: 117 [1250/1251 (100%)]  Loss: 3.705 (3.61)  Time: 0.285s, 3592.58/s  (0.310s, 3302.61/s)  LR: 6.661e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.092 (2.092)  Loss:  0.6548 (0.6548)  Acc@1: 86.1328 (86.1328)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.053 (0.237)  Loss:  0.7520 (1.2274)  Acc@1: 84.0802 (72.4400)  Acc@5: 95.0472 (91.2040)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-116.pth.tar', 72.78799999511719)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-117.pth.tar', 72.44000006591797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-114.pth.tar', 72.34799996826172)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-115.pth.tar', 72.32000004394531)

Train: 118 [   0/1251 (  0%)]  Loss: 3.686 (3.69)  Time: 2.355s,  434.81/s  (2.355s,  434.81/s)  LR: 6.661e-04  Data: 2.135 (2.135)
Train: 118 [  50/1251 (  4%)]  Loss: 3.610 (3.65)  Time: 0.300s, 3417.87/s  (0.333s, 3075.13/s)  LR: 6.659e-04  Data: 0.022 (0.064)
Train: 118 [ 100/1251 (  8%)]  Loss: 3.652 (3.65)  Time: 0.311s, 3295.83/s  (0.319s, 3214.00/s)  LR: 6.657e-04  Data: 0.023 (0.044)
Train: 118 [ 150/1251 ( 12%)]  Loss: 3.542 (3.62)  Time: 0.308s, 3321.07/s  (0.314s, 3257.10/s)  LR: 6.655e-04  Data: 0.024 (0.037)
Train: 118 [ 200/1251 ( 16%)]  Loss: 3.531 (3.60)  Time: 0.302s, 3389.46/s  (0.313s, 3276.40/s)  LR: 6.653e-04  Data: 0.022 (0.033)
Train: 118 [ 250/1251 ( 20%)]  Loss: 3.586 (3.60)  Time: 0.310s, 3304.03/s  (0.312s, 3286.06/s)  LR: 6.651e-04  Data: 0.026 (0.031)
Train: 118 [ 300/1251 ( 24%)]  Loss: 3.596 (3.60)  Time: 0.310s, 3304.73/s  (0.311s, 3292.24/s)  LR: 6.649e-04  Data: 0.023 (0.030)
Train: 118 [ 350/1251 ( 28%)]  Loss: 3.457 (3.58)  Time: 0.305s, 3355.91/s  (0.311s, 3294.18/s)  LR: 6.647e-04  Data: 0.020 (0.029)
Train: 118 [ 400/1251 ( 32%)]  Loss: 3.423 (3.56)  Time: 0.308s, 3326.09/s  (0.311s, 3296.88/s)  LR: 6.645e-04  Data: 0.020 (0.028)
Train: 118 [ 450/1251 ( 36%)]  Loss: 3.638 (3.57)  Time: 0.308s, 3323.41/s  (0.311s, 3297.76/s)  LR: 6.643e-04  Data: 0.023 (0.028)
Train: 118 [ 500/1251 ( 40%)]  Loss: 3.645 (3.58)  Time: 0.313s, 3267.93/s  (0.310s, 3299.12/s)  LR: 6.641e-04  Data: 0.022 (0.027)
Train: 118 [ 550/1251 ( 44%)]  Loss: 4.191 (3.63)  Time: 0.309s, 3308.73/s  (0.310s, 3299.85/s)  LR: 6.639e-04  Data: 0.023 (0.027)
Train: 118 [ 600/1251 ( 48%)]  Loss: 3.232 (3.60)  Time: 0.311s, 3292.12/s  (0.310s, 3299.68/s)  LR: 6.637e-04  Data: 0.026 (0.027)
Train: 118 [ 650/1251 ( 52%)]  Loss: 3.275 (3.58)  Time: 0.312s, 3282.86/s  (0.310s, 3300.07/s)  LR: 6.635e-04  Data: 0.028 (0.026)
Train: 118 [ 700/1251 ( 56%)]  Loss: 3.666 (3.58)  Time: 0.314s, 3258.14/s  (0.310s, 3300.54/s)  LR: 6.634e-04  Data: 0.021 (0.026)
Train: 118 [ 750/1251 ( 60%)]  Loss: 3.472 (3.58)  Time: 0.310s, 3306.37/s  (0.310s, 3301.63/s)  LR: 6.632e-04  Data: 0.025 (0.026)
Train: 118 [ 800/1251 ( 64%)]  Loss: 3.702 (3.58)  Time: 0.303s, 3384.56/s  (0.310s, 3301.86/s)  LR: 6.630e-04  Data: 0.024 (0.026)
Train: 118 [ 850/1251 ( 68%)]  Loss: 3.671 (3.59)  Time: 0.310s, 3303.17/s  (0.310s, 3301.67/s)  LR: 6.628e-04  Data: 0.022 (0.025)
Train: 118 [ 900/1251 ( 72%)]  Loss: 3.744 (3.60)  Time: 0.315s, 3250.19/s  (0.310s, 3301.44/s)  LR: 6.626e-04  Data: 0.021 (0.025)
Train: 118 [ 950/1251 ( 76%)]  Loss: 3.495 (3.59)  Time: 0.311s, 3293.74/s  (0.310s, 3301.78/s)  LR: 6.624e-04  Data: 0.022 (0.025)
Train: 118 [1000/1251 ( 80%)]  Loss: 3.764 (3.60)  Time: 0.309s, 3312.24/s  (0.310s, 3301.54/s)  LR: 6.622e-04  Data: 0.025 (0.025)
Train: 118 [1050/1251 ( 84%)]  Loss: 3.624 (3.60)  Time: 0.311s, 3290.60/s  (0.310s, 3302.05/s)  LR: 6.620e-04  Data: 0.023 (0.025)
Train: 118 [1100/1251 ( 88%)]  Loss: 3.975 (3.62)  Time: 0.307s, 3330.67/s  (0.310s, 3301.87/s)  LR: 6.618e-04  Data: 0.020 (0.025)
Train: 118 [1150/1251 ( 92%)]  Loss: 3.776 (3.62)  Time: 0.307s, 3334.86/s  (0.310s, 3302.26/s)  LR: 6.616e-04  Data: 0.021 (0.025)
Train: 118 [1200/1251 ( 96%)]  Loss: 3.880 (3.63)  Time: 0.309s, 3310.21/s  (0.310s, 3301.85/s)  LR: 6.614e-04  Data: 0.026 (0.025)
Train: 118 [1250/1251 (100%)]  Loss: 3.518 (3.63)  Time: 0.286s, 3582.62/s  (0.310s, 3303.40/s)  LR: 6.612e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.087 (2.087)  Loss:  0.6016 (0.6016)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.063 (0.235)  Loss:  0.6914 (1.1907)  Acc@1: 84.7877 (72.6740)  Acc@5: 95.6368 (91.3620)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-116.pth.tar', 72.78799999511719)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-118.pth.tar', 72.67399998535156)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-117.pth.tar', 72.44000006591797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-114.pth.tar', 72.34799996826172)

Train: 119 [   0/1251 (  0%)]  Loss: 3.823 (3.82)  Time: 2.249s,  455.37/s  (2.249s,  455.37/s)  LR: 6.612e-04  Data: 2.026 (2.026)
Train: 119 [  50/1251 (  4%)]  Loss: 3.767 (3.79)  Time: 0.304s, 3372.63/s  (0.332s, 3084.37/s)  LR: 6.610e-04  Data: 0.021 (0.063)
Train: 119 [ 100/1251 (  8%)]  Loss: 3.282 (3.62)  Time: 0.307s, 3340.70/s  (0.318s, 3216.18/s)  LR: 6.608e-04  Data: 0.023 (0.043)
Train: 119 [ 150/1251 ( 12%)]  Loss: 3.655 (3.63)  Time: 0.310s, 3298.08/s  (0.314s, 3256.94/s)  LR: 6.606e-04  Data: 0.025 (0.036)
Train: 119 [ 200/1251 ( 16%)]  Loss: 3.443 (3.59)  Time: 0.307s, 3337.69/s  (0.313s, 3273.00/s)  LR: 6.604e-04  Data: 0.025 (0.033)
Train: 119 [ 250/1251 ( 20%)]  Loss: 3.517 (3.58)  Time: 0.306s, 3341.05/s  (0.312s, 3283.46/s)  LR: 6.602e-04  Data: 0.019 (0.031)
Train: 119 [ 300/1251 ( 24%)]  Loss: 3.483 (3.57)  Time: 0.312s, 3287.03/s  (0.311s, 3287.89/s)  LR: 6.600e-04  Data: 0.023 (0.030)
Train: 119 [ 350/1251 ( 28%)]  Loss: 3.482 (3.56)  Time: 0.311s, 3290.16/s  (0.311s, 3290.77/s)  LR: 6.598e-04  Data: 0.025 (0.029)
Train: 119 [ 400/1251 ( 32%)]  Loss: 3.679 (3.57)  Time: 0.311s, 3288.39/s  (0.311s, 3294.06/s)  LR: 6.596e-04  Data: 0.025 (0.028)
Train: 119 [ 450/1251 ( 36%)]  Loss: 3.570 (3.57)  Time: 0.310s, 3305.18/s  (0.311s, 3296.04/s)  LR: 6.594e-04  Data: 0.022 (0.027)
Train: 119 [ 500/1251 ( 40%)]  Loss: 3.849 (3.60)  Time: 0.310s, 3307.89/s  (0.311s, 3297.63/s)  LR: 6.592e-04  Data: 0.025 (0.027)
Train: 119 [ 550/1251 ( 44%)]  Loss: 3.594 (3.60)  Time: 0.312s, 3282.26/s  (0.310s, 3298.82/s)  LR: 6.590e-04  Data: 0.023 (0.027)
Train: 119 [ 600/1251 ( 48%)]  Loss: 3.244 (3.57)  Time: 0.314s, 3264.11/s  (0.310s, 3300.13/s)  LR: 6.588e-04  Data: 0.022 (0.026)
Train: 119 [ 650/1251 ( 52%)]  Loss: 3.608 (3.57)  Time: 0.303s, 3384.05/s  (0.310s, 3301.14/s)  LR: 6.586e-04  Data: 0.022 (0.026)
Train: 119 [ 700/1251 ( 56%)]  Loss: 4.038 (3.60)  Time: 0.312s, 3279.78/s  (0.310s, 3301.60/s)  LR: 6.584e-04  Data: 0.027 (0.026)
Train: 119 [ 750/1251 ( 60%)]  Loss: 3.460 (3.59)  Time: 0.311s, 3295.11/s  (0.310s, 3302.02/s)  LR: 6.582e-04  Data: 0.023 (0.026)
Train: 119 [ 800/1251 ( 64%)]  Loss: 3.399 (3.58)  Time: 0.311s, 3296.25/s  (0.310s, 3302.61/s)  LR: 6.580e-04  Data: 0.026 (0.025)
Train: 119 [ 850/1251 ( 68%)]  Loss: 3.624 (3.58)  Time: 0.308s, 3319.85/s  (0.310s, 3303.08/s)  LR: 6.578e-04  Data: 0.022 (0.025)
Train: 119 [ 900/1251 ( 72%)]  Loss: 3.677 (3.59)  Time: 0.313s, 3267.59/s  (0.310s, 3303.49/s)  LR: 6.576e-04  Data: 0.023 (0.025)
Train: 119 [ 950/1251 ( 76%)]  Loss: 3.591 (3.59)  Time: 0.308s, 3322.41/s  (0.310s, 3304.34/s)  LR: 6.574e-04  Data: 0.023 (0.025)
Train: 119 [1000/1251 ( 80%)]  Loss: 3.892 (3.60)  Time: 0.310s, 3300.10/s  (0.310s, 3304.77/s)  LR: 6.572e-04  Data: 0.022 (0.025)
Train: 119 [1050/1251 ( 84%)]  Loss: 3.634 (3.61)  Time: 0.308s, 3321.67/s  (0.310s, 3304.95/s)  LR: 6.570e-04  Data: 0.024 (0.025)
Train: 119 [1100/1251 ( 88%)]  Loss: 3.299 (3.59)  Time: 0.311s, 3295.76/s  (0.310s, 3305.33/s)  LR: 6.568e-04  Data: 0.021 (0.025)
Train: 119 [1150/1251 ( 92%)]  Loss: 3.775 (3.60)  Time: 0.310s, 3302.47/s  (0.310s, 3305.66/s)  LR: 6.566e-04  Data: 0.025 (0.025)
Train: 119 [1200/1251 ( 96%)]  Loss: 3.525 (3.60)  Time: 0.309s, 3312.70/s  (0.310s, 3305.80/s)  LR: 6.564e-04  Data: 0.026 (0.025)
Train: 119 [1250/1251 (100%)]  Loss: 3.411 (3.59)  Time: 0.276s, 3705.57/s  (0.310s, 3308.11/s)  LR: 6.562e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.100 (2.100)  Loss:  0.6504 (0.6504)  Acc@1: 87.2070 (87.2070)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.065 (0.234)  Loss:  0.7197 (1.2169)  Acc@1: 84.0802 (72.5860)  Acc@5: 95.8726 (91.2500)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-116.pth.tar', 72.78799999511719)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-118.pth.tar', 72.67399998535156)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-119.pth.tar', 72.58600006591797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-117.pth.tar', 72.44000006591797)

Train: 120 [   0/1251 (  0%)]  Loss: 3.421 (3.42)  Time: 2.351s,  435.54/s  (2.351s,  435.54/s)  LR: 6.562e-04  Data: 2.132 (2.132)
Train: 120 [  50/1251 (  4%)]  Loss: 3.588 (3.50)  Time: 0.294s, 3481.04/s  (0.330s, 3104.20/s)  LR: 6.560e-04  Data: 0.021 (0.065)
Train: 120 [ 100/1251 (  8%)]  Loss: 3.525 (3.51)  Time: 0.303s, 3380.21/s  (0.316s, 3242.99/s)  LR: 6.558e-04  Data: 0.022 (0.044)
Train: 120 [ 150/1251 ( 12%)]  Loss: 3.671 (3.55)  Time: 0.307s, 3339.18/s  (0.312s, 3286.32/s)  LR: 6.556e-04  Data: 0.017 (0.037)
Train: 120 [ 200/1251 ( 16%)]  Loss: 3.143 (3.47)  Time: 0.307s, 3339.06/s  (0.310s, 3305.04/s)  LR: 6.554e-04  Data: 0.022 (0.034)
Train: 120 [ 250/1251 ( 20%)]  Loss: 3.324 (3.45)  Time: 0.303s, 3375.82/s  (0.309s, 3315.05/s)  LR: 6.552e-04  Data: 0.023 (0.032)
Train: 120 [ 300/1251 ( 24%)]  Loss: 3.840 (3.50)  Time: 0.306s, 3345.50/s  (0.308s, 3321.01/s)  LR: 6.550e-04  Data: 0.022 (0.030)
Train: 120 [ 350/1251 ( 28%)]  Loss: 3.694 (3.53)  Time: 0.310s, 3305.79/s  (0.308s, 3322.98/s)  LR: 6.548e-04  Data: 0.023 (0.029)
Train: 120 [ 400/1251 ( 32%)]  Loss: 3.541 (3.53)  Time: 0.309s, 3314.76/s  (0.308s, 3324.34/s)  LR: 6.546e-04  Data: 0.023 (0.028)
Train: 120 [ 450/1251 ( 36%)]  Loss: 3.647 (3.54)  Time: 0.306s, 3346.60/s  (0.308s, 3325.99/s)  LR: 6.544e-04  Data: 0.023 (0.028)
Train: 120 [ 500/1251 ( 40%)]  Loss: 3.561 (3.54)  Time: 0.310s, 3308.20/s  (0.308s, 3327.69/s)  LR: 6.543e-04  Data: 0.021 (0.027)
Train: 120 [ 550/1251 ( 44%)]  Loss: 3.504 (3.54)  Time: 0.305s, 3359.86/s  (0.308s, 3329.41/s)  LR: 6.541e-04  Data: 0.022 (0.027)
Train: 120 [ 600/1251 ( 48%)]  Loss: 3.765 (3.56)  Time: 0.304s, 3365.36/s  (0.307s, 3330.72/s)  LR: 6.539e-04  Data: 0.023 (0.027)
Train: 120 [ 650/1251 ( 52%)]  Loss: 3.030 (3.52)  Time: 0.299s, 3419.64/s  (0.307s, 3332.16/s)  LR: 6.537e-04  Data: 0.020 (0.026)
Train: 120 [ 700/1251 ( 56%)]  Loss: 3.198 (3.50)  Time: 0.308s, 3320.14/s  (0.307s, 3333.07/s)  LR: 6.535e-04  Data: 0.024 (0.026)
Train: 120 [ 750/1251 ( 60%)]  Loss: 3.454 (3.49)  Time: 0.305s, 3362.23/s  (0.307s, 3333.16/s)  LR: 6.533e-04  Data: 0.025 (0.026)
Train: 120 [ 800/1251 ( 64%)]  Loss: 3.685 (3.51)  Time: 0.305s, 3352.65/s  (0.307s, 3333.27/s)  LR: 6.531e-04  Data: 0.023 (0.026)
Train: 120 [ 850/1251 ( 68%)]  Loss: 3.760 (3.52)  Time: 0.303s, 3375.37/s  (0.307s, 3333.55/s)  LR: 6.529e-04  Data: 0.026 (0.026)
Train: 120 [ 900/1251 ( 72%)]  Loss: 3.856 (3.54)  Time: 0.310s, 3302.53/s  (0.307s, 3333.76/s)  LR: 6.527e-04  Data: 0.025 (0.026)
Train: 120 [ 950/1251 ( 76%)]  Loss: 3.720 (3.55)  Time: 0.304s, 3365.87/s  (0.307s, 3334.61/s)  LR: 6.525e-04  Data: 0.022 (0.025)
Train: 120 [1000/1251 ( 80%)]  Loss: 3.720 (3.55)  Time: 0.307s, 3333.77/s  (0.307s, 3335.23/s)  LR: 6.523e-04  Data: 0.020 (0.025)
Train: 120 [1050/1251 ( 84%)]  Loss: 3.681 (3.56)  Time: 0.303s, 3383.60/s  (0.307s, 3335.70/s)  LR: 6.521e-04  Data: 0.025 (0.025)
Train: 120 [1100/1251 ( 88%)]  Loss: 3.765 (3.57)  Time: 0.313s, 3274.56/s  (0.307s, 3336.63/s)  LR: 6.519e-04  Data: 0.024 (0.025)
Train: 120 [1150/1251 ( 92%)]  Loss: 3.820 (3.58)  Time: 0.311s, 3292.14/s  (0.307s, 3336.99/s)  LR: 6.517e-04  Data: 0.025 (0.025)
Train: 120 [1200/1251 ( 96%)]  Loss: 3.569 (3.58)  Time: 0.303s, 3374.32/s  (0.307s, 3337.43/s)  LR: 6.515e-04  Data: 0.019 (0.025)
Train: 120 [1250/1251 (100%)]  Loss: 3.899 (3.59)  Time: 0.275s, 3724.77/s  (0.307s, 3339.70/s)  LR: 6.513e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.262 (2.262)  Loss:  0.6094 (0.6094)  Acc@1: 88.5742 (88.5742)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.056 (0.241)  Loss:  0.8174 (1.2301)  Acc@1: 82.4292 (72.3640)  Acc@5: 94.8113 (91.1200)
Train: 121 [   0/1251 (  0%)]  Loss: 3.453 (3.45)  Time: 1.935s,  529.11/s  (1.935s,  529.11/s)  LR: 6.513e-04  Data: 1.660 (1.660)
Train: 121 [  50/1251 (  4%)]  Loss: 3.776 (3.61)  Time: 0.288s, 3561.66/s  (0.328s, 3125.80/s)  LR: 6.511e-04  Data: 0.023 (0.068)
Train: 121 [ 100/1251 (  8%)]  Loss: 3.484 (3.57)  Time: 0.304s, 3368.88/s  (0.312s, 3277.22/s)  LR: 6.509e-04  Data: 0.027 (0.046)
Train: 121 [ 150/1251 ( 12%)]  Loss: 3.701 (3.60)  Time: 0.305s, 3352.68/s  (0.309s, 3314.38/s)  LR: 6.507e-04  Data: 0.026 (0.038)
Train: 121 [ 200/1251 ( 16%)]  Loss: 3.690 (3.62)  Time: 0.305s, 3358.62/s  (0.308s, 3328.83/s)  LR: 6.505e-04  Data: 0.020 (0.034)
Train: 121 [ 250/1251 ( 20%)]  Loss: 3.869 (3.66)  Time: 0.299s, 3419.91/s  (0.307s, 3337.06/s)  LR: 6.503e-04  Data: 0.020 (0.032)
Train: 121 [ 300/1251 ( 24%)]  Loss: 3.809 (3.68)  Time: 0.306s, 3342.60/s  (0.306s, 3341.24/s)  LR: 6.501e-04  Data: 0.026 (0.031)
Train: 121 [ 350/1251 ( 28%)]  Loss: 3.623 (3.68)  Time: 0.304s, 3373.46/s  (0.306s, 3344.37/s)  LR: 6.499e-04  Data: 0.023 (0.030)
Train: 121 [ 400/1251 ( 32%)]  Loss: 3.524 (3.66)  Time: 0.310s, 3307.68/s  (0.306s, 3346.70/s)  LR: 6.497e-04  Data: 0.024 (0.029)
Train: 121 [ 450/1251 ( 36%)]  Loss: 3.289 (3.62)  Time: 0.303s, 3381.82/s  (0.306s, 3349.05/s)  LR: 6.495e-04  Data: 0.023 (0.028)
Train: 121 [ 500/1251 ( 40%)]  Loss: 3.765 (3.63)  Time: 0.308s, 3321.04/s  (0.306s, 3350.21/s)  LR: 6.493e-04  Data: 0.024 (0.028)
Train: 121 [ 550/1251 ( 44%)]  Loss: 3.411 (3.62)  Time: 0.304s, 3371.31/s  (0.306s, 3350.31/s)  LR: 6.491e-04  Data: 0.023 (0.027)
Train: 121 [ 600/1251 ( 48%)]  Loss: 3.525 (3.61)  Time: 0.304s, 3370.24/s  (0.306s, 3351.14/s)  LR: 6.489e-04  Data: 0.023 (0.027)
Train: 121 [ 650/1251 ( 52%)]  Loss: 3.536 (3.60)  Time: 0.306s, 3348.85/s  (0.305s, 3352.05/s)  LR: 6.487e-04  Data: 0.022 (0.027)
Train: 121 [ 700/1251 ( 56%)]  Loss: 3.665 (3.61)  Time: 0.305s, 3356.63/s  (0.305s, 3352.87/s)  LR: 6.485e-04  Data: 0.022 (0.026)
Train: 121 [ 750/1251 ( 60%)]  Loss: 4.085 (3.64)  Time: 0.309s, 3311.12/s  (0.305s, 3353.85/s)  LR: 6.483e-04  Data: 0.023 (0.026)
Train: 121 [ 800/1251 ( 64%)]  Loss: 3.408 (3.62)  Time: 0.305s, 3361.70/s  (0.305s, 3354.38/s)  LR: 6.481e-04  Data: 0.027 (0.026)
Train: 121 [ 850/1251 ( 68%)]  Loss: 3.921 (3.64)  Time: 0.301s, 3405.96/s  (0.305s, 3354.70/s)  LR: 6.479e-04  Data: 0.022 (0.026)
Train: 121 [ 900/1251 ( 72%)]  Loss: 3.261 (3.62)  Time: 0.310s, 3301.48/s  (0.305s, 3354.96/s)  LR: 6.477e-04  Data: 0.022 (0.026)
Train: 121 [ 950/1251 ( 76%)]  Loss: 3.681 (3.62)  Time: 0.302s, 3392.92/s  (0.305s, 3355.22/s)  LR: 6.475e-04  Data: 0.022 (0.025)
Train: 121 [1000/1251 ( 80%)]  Loss: 3.808 (3.63)  Time: 0.303s, 3380.79/s  (0.305s, 3355.53/s)  LR: 6.473e-04  Data: 0.026 (0.025)
Train: 121 [1050/1251 ( 84%)]  Loss: 3.344 (3.62)  Time: 0.305s, 3361.70/s  (0.305s, 3355.55/s)  LR: 6.471e-04  Data: 0.025 (0.025)
Train: 121 [1100/1251 ( 88%)]  Loss: 3.610 (3.62)  Time: 0.307s, 3335.62/s  (0.305s, 3355.64/s)  LR: 6.469e-04  Data: 0.021 (0.025)
Train: 121 [1150/1251 ( 92%)]  Loss: 3.670 (3.62)  Time: 0.301s, 3404.28/s  (0.305s, 3355.66/s)  LR: 6.467e-04  Data: 0.021 (0.025)
Train: 121 [1200/1251 ( 96%)]  Loss: 3.631 (3.62)  Time: 0.303s, 3378.01/s  (0.305s, 3355.60/s)  LR: 6.465e-04  Data: 0.026 (0.025)
Train: 121 [1250/1251 (100%)]  Loss: 3.772 (3.63)  Time: 0.276s, 3710.76/s  (0.305s, 3357.52/s)  LR: 6.463e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.136 (2.136)  Loss:  0.6084 (0.6084)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.041 (0.244)  Loss:  0.7227 (1.2317)  Acc@1: 85.7311 (72.6640)  Acc@5: 96.4623 (91.2820)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-116.pth.tar', 72.78799999511719)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-118.pth.tar', 72.67399998535156)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-121.pth.tar', 72.66400000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-119.pth.tar', 72.58600006591797)

Train: 122 [   0/1251 (  0%)]  Loss: 3.830 (3.83)  Time: 2.326s,  440.19/s  (2.326s,  440.19/s)  LR: 6.463e-04  Data: 2.104 (2.104)
Train: 122 [  50/1251 (  4%)]  Loss: 3.323 (3.58)  Time: 0.290s, 3525.79/s  (0.327s, 3133.11/s)  LR: 6.461e-04  Data: 0.024 (0.064)
Train: 122 [ 100/1251 (  8%)]  Loss: 3.544 (3.57)  Time: 0.299s, 3429.16/s  (0.312s, 3280.31/s)  LR: 6.459e-04  Data: 0.025 (0.043)
Train: 122 [ 150/1251 ( 12%)]  Loss: 3.784 (3.62)  Time: 0.302s, 3388.86/s  (0.308s, 3323.99/s)  LR: 6.457e-04  Data: 0.022 (0.037)
Train: 122 [ 200/1251 ( 16%)]  Loss: 3.390 (3.57)  Time: 0.300s, 3408.93/s  (0.306s, 3342.26/s)  LR: 6.455e-04  Data: 0.026 (0.033)
Train: 122 [ 250/1251 ( 20%)]  Loss: 3.736 (3.60)  Time: 0.297s, 3442.35/s  (0.306s, 3351.80/s)  LR: 6.453e-04  Data: 0.022 (0.031)
Train: 122 [ 300/1251 ( 24%)]  Loss: 3.829 (3.63)  Time: 0.304s, 3373.71/s  (0.305s, 3357.94/s)  LR: 6.451e-04  Data: 0.023 (0.030)
Train: 122 [ 350/1251 ( 28%)]  Loss: 3.528 (3.62)  Time: 0.307s, 3333.96/s  (0.305s, 3361.83/s)  LR: 6.449e-04  Data: 0.026 (0.029)
Train: 122 [ 400/1251 ( 32%)]  Loss: 3.821 (3.64)  Time: 0.302s, 3389.10/s  (0.304s, 3365.19/s)  LR: 6.447e-04  Data: 0.022 (0.028)
Train: 122 [ 450/1251 ( 36%)]  Loss: 3.757 (3.65)  Time: 0.301s, 3402.57/s  (0.304s, 3367.61/s)  LR: 6.445e-04  Data: 0.022 (0.028)
Train: 122 [ 500/1251 ( 40%)]  Loss: 3.840 (3.67)  Time: 0.304s, 3366.33/s  (0.304s, 3369.06/s)  LR: 6.443e-04  Data: 0.023 (0.027)
Train: 122 [ 550/1251 ( 44%)]  Loss: 3.894 (3.69)  Time: 0.303s, 3374.80/s  (0.304s, 3369.20/s)  LR: 6.441e-04  Data: 0.023 (0.027)
Train: 122 [ 600/1251 ( 48%)]  Loss: 3.565 (3.68)  Time: 0.302s, 3394.51/s  (0.304s, 3369.72/s)  LR: 6.439e-04  Data: 0.023 (0.026)
Train: 122 [ 650/1251 ( 52%)]  Loss: 3.148 (3.64)  Time: 0.306s, 3344.59/s  (0.304s, 3370.76/s)  LR: 6.437e-04  Data: 0.026 (0.026)
Train: 122 [ 700/1251 ( 56%)]  Loss: 3.580 (3.64)  Time: 0.300s, 3409.29/s  (0.304s, 3372.06/s)  LR: 6.435e-04  Data: 0.022 (0.026)
Train: 122 [ 750/1251 ( 60%)]  Loss: 3.413 (3.62)  Time: 0.304s, 3372.55/s  (0.304s, 3372.95/s)  LR: 6.433e-04  Data: 0.025 (0.026)
Train: 122 [ 800/1251 ( 64%)]  Loss: 3.990 (3.65)  Time: 0.305s, 3356.75/s  (0.303s, 3374.05/s)  LR: 6.431e-04  Data: 0.023 (0.025)
Train: 122 [ 850/1251 ( 68%)]  Loss: 3.786 (3.65)  Time: 0.301s, 3406.56/s  (0.303s, 3375.21/s)  LR: 6.429e-04  Data: 0.024 (0.025)
Train: 122 [ 900/1251 ( 72%)]  Loss: 3.378 (3.64)  Time: 0.306s, 3351.07/s  (0.303s, 3376.23/s)  LR: 6.427e-04  Data: 0.028 (0.025)
Train: 122 [ 950/1251 ( 76%)]  Loss: 3.056 (3.61)  Time: 0.307s, 3339.50/s  (0.303s, 3376.71/s)  LR: 6.425e-04  Data: 0.021 (0.025)
Train: 122 [1000/1251 ( 80%)]  Loss: 3.214 (3.59)  Time: 0.293s, 3490.99/s  (0.303s, 3377.10/s)  LR: 6.423e-04  Data: 0.016 (0.025)
Train: 122 [1050/1251 ( 84%)]  Loss: 3.641 (3.59)  Time: 0.302s, 3394.51/s  (0.303s, 3378.06/s)  LR: 6.421e-04  Data: 0.021 (0.025)
Train: 122 [1100/1251 ( 88%)]  Loss: 3.276 (3.58)  Time: 0.300s, 3408.16/s  (0.303s, 3379.01/s)  LR: 6.419e-04  Data: 0.024 (0.025)
Train: 122 [1150/1251 ( 92%)]  Loss: 3.691 (3.58)  Time: 0.304s, 3363.76/s  (0.303s, 3379.38/s)  LR: 6.417e-04  Data: 0.024 (0.025)
Train: 122 [1200/1251 ( 96%)]  Loss: 3.809 (3.59)  Time: 0.303s, 3377.38/s  (0.303s, 3379.70/s)  LR: 6.415e-04  Data: 0.021 (0.025)
Train: 122 [1250/1251 (100%)]  Loss: 3.971 (3.61)  Time: 0.276s, 3715.34/s  (0.303s, 3381.93/s)  LR: 6.413e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.096 (2.096)  Loss:  0.6538 (0.6538)  Acc@1: 87.5977 (87.5977)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.051 (0.235)  Loss:  0.7056 (1.2108)  Acc@1: 84.1981 (72.8220)  Acc@5: 95.8726 (91.4020)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-122.pth.tar', 72.82199998779296)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-116.pth.tar', 72.78799999511719)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-118.pth.tar', 72.67399998535156)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-121.pth.tar', 72.66400000732422)

Train: 123 [   0/1251 (  0%)]  Loss: 3.316 (3.32)  Time: 2.527s,  405.25/s  (2.527s,  405.25/s)  LR: 6.413e-04  Data: 2.310 (2.310)
Train: 123 [  50/1251 (  4%)]  Loss: 3.823 (3.57)  Time: 0.290s, 3529.39/s  (0.323s, 3174.59/s)  LR: 6.411e-04  Data: 0.022 (0.068)
Train: 123 [ 100/1251 (  8%)]  Loss: 3.356 (3.50)  Time: 0.292s, 3510.17/s  (0.308s, 3325.22/s)  LR: 6.409e-04  Data: 0.025 (0.045)
Train: 123 [ 150/1251 ( 12%)]  Loss: 3.714 (3.55)  Time: 0.301s, 3403.69/s  (0.304s, 3370.58/s)  LR: 6.407e-04  Data: 0.023 (0.038)
Train: 123 [ 200/1251 ( 16%)]  Loss: 3.520 (3.55)  Time: 0.295s, 3467.81/s  (0.302s, 3388.19/s)  LR: 6.405e-04  Data: 0.023 (0.034)
Train: 123 [ 250/1251 ( 20%)]  Loss: 3.638 (3.56)  Time: 0.291s, 3514.20/s  (0.301s, 3397.49/s)  LR: 6.403e-04  Data: 0.021 (0.032)
Train: 123 [ 300/1251 ( 24%)]  Loss: 3.326 (3.53)  Time: 0.300s, 3409.97/s  (0.301s, 3401.50/s)  LR: 6.401e-04  Data: 0.022 (0.030)
Train: 123 [ 350/1251 ( 28%)]  Loss: 3.389 (3.51)  Time: 0.303s, 3375.19/s  (0.301s, 3404.60/s)  LR: 6.399e-04  Data: 0.026 (0.029)
Train: 123 [ 400/1251 ( 32%)]  Loss: 3.772 (3.54)  Time: 0.298s, 3435.23/s  (0.301s, 3406.50/s)  LR: 6.397e-04  Data: 0.022 (0.029)
Train: 123 [ 450/1251 ( 36%)]  Loss: 3.653 (3.55)  Time: 0.296s, 3465.31/s  (0.301s, 3406.09/s)  LR: 6.395e-04  Data: 0.022 (0.028)
Train: 123 [ 500/1251 ( 40%)]  Loss: 3.765 (3.57)  Time: 0.302s, 3391.13/s  (0.301s, 3407.36/s)  LR: 6.393e-04  Data: 0.024 (0.027)
Train: 123 [ 550/1251 ( 44%)]  Loss: 3.632 (3.58)  Time: 0.301s, 3403.53/s  (0.301s, 3407.61/s)  LR: 6.391e-04  Data: 0.025 (0.027)
Train: 123 [ 600/1251 ( 48%)]  Loss: 3.503 (3.57)  Time: 0.305s, 3361.69/s  (0.300s, 3409.13/s)  LR: 6.389e-04  Data: 0.026 (0.027)
Train: 123 [ 650/1251 ( 52%)]  Loss: 3.581 (3.57)  Time: 0.294s, 3485.53/s  (0.300s, 3409.43/s)  LR: 6.387e-04  Data: 0.022 (0.026)
Train: 123 [ 700/1251 ( 56%)]  Loss: 3.665 (3.58)  Time: 0.299s, 3420.46/s  (0.300s, 3409.80/s)  LR: 6.385e-04  Data: 0.025 (0.026)
Train: 123 [ 750/1251 ( 60%)]  Loss: 3.790 (3.59)  Time: 0.303s, 3374.71/s  (0.300s, 3409.65/s)  LR: 6.383e-04  Data: 0.024 (0.026)
Train: 123 [ 800/1251 ( 64%)]  Loss: 3.479 (3.58)  Time: 0.298s, 3433.03/s  (0.300s, 3410.08/s)  LR: 6.381e-04  Data: 0.019 (0.026)
Train: 123 [ 850/1251 ( 68%)]  Loss: 3.299 (3.57)  Time: 0.300s, 3413.28/s  (0.300s, 3410.54/s)  LR: 6.379e-04  Data: 0.022 (0.026)
Train: 123 [ 900/1251 ( 72%)]  Loss: 3.540 (3.57)  Time: 0.298s, 3439.15/s  (0.300s, 3411.17/s)  LR: 6.377e-04  Data: 0.021 (0.026)
Train: 123 [ 950/1251 ( 76%)]  Loss: 3.726 (3.57)  Time: 0.297s, 3443.50/s  (0.300s, 3411.62/s)  LR: 6.375e-04  Data: 0.022 (0.025)
Train: 123 [1000/1251 ( 80%)]  Loss: 3.910 (3.59)  Time: 0.299s, 3422.06/s  (0.300s, 3412.31/s)  LR: 6.373e-04  Data: 0.025 (0.025)
Train: 123 [1050/1251 ( 84%)]  Loss: 3.122 (3.57)  Time: 0.302s, 3390.09/s  (0.300s, 3412.35/s)  LR: 6.371e-04  Data: 0.025 (0.025)
Train: 123 [1100/1251 ( 88%)]  Loss: 3.716 (3.58)  Time: 0.306s, 3342.06/s  (0.300s, 3412.60/s)  LR: 6.369e-04  Data: 0.025 (0.025)
Train: 123 [1150/1251 ( 92%)]  Loss: 3.695 (3.58)  Time: 0.297s, 3444.38/s  (0.300s, 3412.35/s)  LR: 6.367e-04  Data: 0.023 (0.025)
Train: 123 [1200/1251 ( 96%)]  Loss: 3.757 (3.59)  Time: 0.304s, 3370.33/s  (0.300s, 3412.68/s)  LR: 6.365e-04  Data: 0.024 (0.025)
Train: 123 [1250/1251 (100%)]  Loss: 3.673 (3.59)  Time: 0.274s, 3731.69/s  (0.300s, 3414.73/s)  LR: 6.363e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.035 (2.035)  Loss:  0.6206 (0.6206)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.048 (0.237)  Loss:  0.7559 (1.2357)  Acc@1: 84.6698 (72.6640)  Acc@5: 96.3443 (91.2840)
Train: 124 [   0/1251 (  0%)]  Loss: 3.736 (3.74)  Time: 2.388s,  428.73/s  (2.388s,  428.73/s)  LR: 6.363e-04  Data: 2.160 (2.160)
Train: 124 [  50/1251 (  4%)]  Loss: 3.339 (3.54)  Time: 0.288s, 3556.31/s  (0.327s, 3132.60/s)  LR: 6.361e-04  Data: 0.025 (0.065)
Train: 124 [ 100/1251 (  8%)]  Loss: 3.736 (3.60)  Time: 0.292s, 3505.26/s  (0.311s, 3296.30/s)  LR: 6.359e-04  Data: 0.023 (0.044)
Train: 124 [ 150/1251 ( 12%)]  Loss: 3.571 (3.60)  Time: 0.299s, 3421.01/s  (0.306s, 3348.96/s)  LR: 6.357e-04  Data: 0.026 (0.037)
Train: 124 [ 200/1251 ( 16%)]  Loss: 3.628 (3.60)  Time: 0.295s, 3476.88/s  (0.304s, 3372.90/s)  LR: 6.355e-04  Data: 0.022 (0.034)
Train: 124 [ 250/1251 ( 20%)]  Loss: 3.556 (3.59)  Time: 0.299s, 3429.64/s  (0.302s, 3385.59/s)  LR: 6.353e-04  Data: 0.022 (0.031)
Train: 124 [ 300/1251 ( 24%)]  Loss: 4.082 (3.66)  Time: 0.299s, 3420.65/s  (0.302s, 3392.96/s)  LR: 6.351e-04  Data: 0.021 (0.030)
Train: 124 [ 350/1251 ( 28%)]  Loss: 3.438 (3.64)  Time: 0.299s, 3420.41/s  (0.301s, 3400.42/s)  LR: 6.349e-04  Data: 0.022 (0.029)
Train: 124 [ 400/1251 ( 32%)]  Loss: 3.562 (3.63)  Time: 0.303s, 3384.82/s  (0.301s, 3404.30/s)  LR: 6.347e-04  Data: 0.025 (0.028)
Train: 124 [ 450/1251 ( 36%)]  Loss: 3.623 (3.63)  Time: 0.301s, 3399.78/s  (0.300s, 3407.74/s)  LR: 6.345e-04  Data: 0.022 (0.028)
Train: 124 [ 500/1251 ( 40%)]  Loss: 3.503 (3.62)  Time: 0.299s, 3420.77/s  (0.300s, 3409.38/s)  LR: 6.343e-04  Data: 0.024 (0.027)
Train: 124 [ 550/1251 ( 44%)]  Loss: 3.717 (3.62)  Time: 0.304s, 3368.86/s  (0.300s, 3411.26/s)  LR: 6.341e-04  Data: 0.022 (0.027)
Train: 124 [ 600/1251 ( 48%)]  Loss: 3.916 (3.65)  Time: 0.296s, 3458.99/s  (0.300s, 3412.40/s)  LR: 6.339e-04  Data: 0.023 (0.026)
Train: 124 [ 650/1251 ( 52%)]  Loss: 3.481 (3.63)  Time: 0.300s, 3414.88/s  (0.300s, 3412.34/s)  LR: 6.337e-04  Data: 0.023 (0.026)
Train: 124 [ 700/1251 ( 56%)]  Loss: 3.533 (3.63)  Time: 0.304s, 3365.19/s  (0.300s, 3413.71/s)  LR: 6.335e-04  Data: 0.025 (0.026)
Train: 124 [ 750/1251 ( 60%)]  Loss: 3.747 (3.64)  Time: 0.298s, 3437.13/s  (0.300s, 3414.38/s)  LR: 6.333e-04  Data: 0.022 (0.026)
Train: 124 [ 800/1251 ( 64%)]  Loss: 4.009 (3.66)  Time: 0.300s, 3418.45/s  (0.300s, 3415.37/s)  LR: 6.331e-04  Data: 0.023 (0.026)
Train: 124 [ 850/1251 ( 68%)]  Loss: 3.643 (3.66)  Time: 0.301s, 3398.64/s  (0.300s, 3415.88/s)  LR: 6.329e-04  Data: 0.025 (0.025)
Train: 124 [ 900/1251 ( 72%)]  Loss: 3.725 (3.66)  Time: 0.300s, 3410.00/s  (0.300s, 3416.50/s)  LR: 6.327e-04  Data: 0.026 (0.025)
Train: 124 [ 950/1251 ( 76%)]  Loss: 3.380 (3.65)  Time: 0.305s, 3358.91/s  (0.300s, 3417.41/s)  LR: 6.325e-04  Data: 0.023 (0.025)
Train: 124 [1000/1251 ( 80%)]  Loss: 3.591 (3.64)  Time: 0.297s, 3444.73/s  (0.300s, 3417.95/s)  LR: 6.323e-04  Data: 0.017 (0.025)
Train: 124 [1050/1251 ( 84%)]  Loss: 3.737 (3.65)  Time: 0.299s, 3424.99/s  (0.300s, 3417.87/s)  LR: 6.321e-04  Data: 0.026 (0.025)
Train: 124 [1100/1251 ( 88%)]  Loss: 3.532 (3.64)  Time: 0.305s, 3360.42/s  (0.300s, 3418.01/s)  LR: 6.319e-04  Data: 0.024 (0.025)
Train: 124 [1150/1251 ( 92%)]  Loss: 3.439 (3.63)  Time: 0.299s, 3419.17/s  (0.300s, 3417.67/s)  LR: 6.317e-04  Data: 0.025 (0.025)
Train: 124 [1200/1251 ( 96%)]  Loss: 3.480 (3.63)  Time: 0.300s, 3410.11/s  (0.300s, 3417.54/s)  LR: 6.315e-04  Data: 0.018 (0.025)
Train: 124 [1250/1251 (100%)]  Loss: 3.631 (3.63)  Time: 0.276s, 3711.54/s  (0.299s, 3419.83/s)  LR: 6.313e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.063 (2.063)  Loss:  0.6113 (0.6113)  Acc@1: 86.8164 (86.8164)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.046 (0.234)  Loss:  0.6567 (1.1762)  Acc@1: 84.6698 (72.7960)  Acc@5: 96.2264 (91.4280)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-122.pth.tar', 72.82199998779296)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-124.pth.tar', 72.79600006347657)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-116.pth.tar', 72.78799999511719)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-118.pth.tar', 72.67399998535156)

Train: 125 [   0/1251 (  0%)]  Loss: 3.537 (3.54)  Time: 2.207s,  463.97/s  (2.207s,  463.97/s)  LR: 6.313e-04  Data: 1.986 (1.986)
Train: 125 [  50/1251 (  4%)]  Loss: 3.419 (3.48)  Time: 0.278s, 3679.29/s  (0.320s, 3203.47/s)  LR: 6.311e-04  Data: 0.015 (0.061)
Train: 125 [ 100/1251 (  8%)]  Loss: 4.047 (3.67)  Time: 0.297s, 3445.89/s  (0.306s, 3345.07/s)  LR: 6.309e-04  Data: 0.021 (0.042)
Train: 125 [ 150/1251 ( 12%)]  Loss: 3.263 (3.57)  Time: 0.299s, 3429.49/s  (0.303s, 3384.14/s)  LR: 6.307e-04  Data: 0.023 (0.036)
Train: 125 [ 200/1251 ( 16%)]  Loss: 3.433 (3.54)  Time: 0.295s, 3466.86/s  (0.301s, 3401.17/s)  LR: 6.305e-04  Data: 0.019 (0.033)
Train: 125 [ 250/1251 ( 20%)]  Loss: 3.524 (3.54)  Time: 0.303s, 3380.35/s  (0.300s, 3412.20/s)  LR: 6.303e-04  Data: 0.022 (0.031)
Train: 125 [ 300/1251 ( 24%)]  Loss: 3.460 (3.53)  Time: 0.300s, 3416.53/s  (0.300s, 3417.13/s)  LR: 6.301e-04  Data: 0.022 (0.030)
Train: 125 [ 350/1251 ( 28%)]  Loss: 3.734 (3.55)  Time: 0.297s, 3451.15/s  (0.299s, 3422.63/s)  LR: 6.298e-04  Data: 0.023 (0.029)
Train: 125 [ 400/1251 ( 32%)]  Loss: 3.384 (3.53)  Time: 0.297s, 3443.06/s  (0.299s, 3425.95/s)  LR: 6.296e-04  Data: 0.023 (0.028)
Train: 125 [ 450/1251 ( 36%)]  Loss: 3.739 (3.55)  Time: 0.296s, 3458.31/s  (0.299s, 3428.05/s)  LR: 6.294e-04  Data: 0.026 (0.027)
Train: 125 [ 500/1251 ( 40%)]  Loss: 3.627 (3.56)  Time: 0.298s, 3439.60/s  (0.298s, 3430.54/s)  LR: 6.292e-04  Data: 0.026 (0.027)
Train: 125 [ 550/1251 ( 44%)]  Loss: 3.374 (3.55)  Time: 0.300s, 3418.01/s  (0.298s, 3431.16/s)  LR: 6.290e-04  Data: 0.025 (0.027)
Train: 125 [ 600/1251 ( 48%)]  Loss: 3.712 (3.56)  Time: 0.299s, 3428.01/s  (0.298s, 3432.45/s)  LR: 6.288e-04  Data: 0.021 (0.026)
Train: 125 [ 650/1251 ( 52%)]  Loss: 3.572 (3.56)  Time: 0.293s, 3495.37/s  (0.298s, 3433.18/s)  LR: 6.286e-04  Data: 0.021 (0.026)
Train: 125 [ 700/1251 ( 56%)]  Loss: 3.237 (3.54)  Time: 0.303s, 3375.21/s  (0.298s, 3433.73/s)  LR: 6.284e-04  Data: 0.023 (0.026)
Train: 125 [ 750/1251 ( 60%)]  Loss: 3.472 (3.53)  Time: 0.301s, 3404.69/s  (0.298s, 3433.57/s)  LR: 6.282e-04  Data: 0.023 (0.026)
Train: 125 [ 800/1251 ( 64%)]  Loss: 3.200 (3.51)  Time: 0.296s, 3453.75/s  (0.298s, 3433.48/s)  LR: 6.280e-04  Data: 0.024 (0.026)
Train: 125 [ 850/1251 ( 68%)]  Loss: 3.430 (3.51)  Time: 0.296s, 3459.26/s  (0.298s, 3433.72/s)  LR: 6.278e-04  Data: 0.023 (0.026)
Train: 125 [ 900/1251 ( 72%)]  Loss: 3.537 (3.51)  Time: 0.301s, 3396.43/s  (0.298s, 3433.53/s)  LR: 6.276e-04  Data: 0.025 (0.025)
Train: 125 [ 950/1251 ( 76%)]  Loss: 3.642 (3.52)  Time: 0.304s, 3363.00/s  (0.298s, 3433.25/s)  LR: 6.274e-04  Data: 0.024 (0.025)
Train: 125 [1000/1251 ( 80%)]  Loss: 3.760 (3.53)  Time: 0.299s, 3424.80/s  (0.298s, 3433.18/s)  LR: 6.272e-04  Data: 0.021 (0.025)
Train: 125 [1050/1251 ( 84%)]  Loss: 3.524 (3.53)  Time: 0.300s, 3417.18/s  (0.298s, 3432.89/s)  LR: 6.270e-04  Data: 0.023 (0.025)
Train: 125 [1100/1251 ( 88%)]  Loss: 3.557 (3.53)  Time: 0.300s, 3418.20/s  (0.298s, 3433.15/s)  LR: 6.268e-04  Data: 0.021 (0.025)
Train: 125 [1150/1251 ( 92%)]  Loss: 3.865 (3.54)  Time: 0.301s, 3406.78/s  (0.298s, 3433.62/s)  LR: 6.266e-04  Data: 0.022 (0.025)
Train: 125 [1200/1251 ( 96%)]  Loss: 3.385 (3.54)  Time: 0.298s, 3432.69/s  (0.298s, 3433.00/s)  LR: 6.264e-04  Data: 0.019 (0.025)
Train: 125 [1250/1251 (100%)]  Loss: 3.519 (3.54)  Time: 0.276s, 3704.64/s  (0.298s, 3434.69/s)  LR: 6.262e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.191 (2.191)  Loss:  0.6377 (0.6377)  Acc@1: 87.1094 (87.1094)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.046 (0.237)  Loss:  0.7402 (1.2039)  Acc@1: 84.9057 (72.8080)  Acc@5: 96.2264 (91.4300)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-122.pth.tar', 72.82199998779296)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-125.pth.tar', 72.80799990722656)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-124.pth.tar', 72.79600006347657)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-116.pth.tar', 72.78799999511719)

Train: 126 [   0/1251 (  0%)]  Loss: 3.913 (3.91)  Time: 2.369s,  432.20/s  (2.369s,  432.20/s)  LR: 6.262e-04  Data: 2.143 (2.143)
Train: 126 [  50/1251 (  4%)]  Loss: 3.580 (3.75)  Time: 0.288s, 3554.16/s  (0.319s, 3214.93/s)  LR: 6.260e-04  Data: 0.023 (0.066)
Train: 126 [ 100/1251 (  8%)]  Loss: 3.515 (3.67)  Time: 0.297s, 3451.31/s  (0.306s, 3347.60/s)  LR: 6.258e-04  Data: 0.020 (0.045)
Train: 126 [ 150/1251 ( 12%)]  Loss: 3.502 (3.63)  Time: 0.297s, 3448.98/s  (0.302s, 3390.37/s)  LR: 6.256e-04  Data: 0.022 (0.037)
Train: 126 [ 200/1251 ( 16%)]  Loss: 3.598 (3.62)  Time: 0.298s, 3431.36/s  (0.301s, 3403.05/s)  LR: 6.254e-04  Data: 0.018 (0.034)
Train: 126 [ 250/1251 ( 20%)]  Loss: 3.499 (3.60)  Time: 0.299s, 3425.02/s  (0.300s, 3408.54/s)  LR: 6.252e-04  Data: 0.021 (0.032)
Train: 126 [ 300/1251 ( 24%)]  Loss: 3.634 (3.61)  Time: 0.298s, 3435.08/s  (0.300s, 3412.92/s)  LR: 6.250e-04  Data: 0.023 (0.030)
Train: 126 [ 350/1251 ( 28%)]  Loss: 3.597 (3.60)  Time: 0.304s, 3371.76/s  (0.300s, 3415.47/s)  LR: 6.248e-04  Data: 0.025 (0.029)
Train: 126 [ 400/1251 ( 32%)]  Loss: 3.677 (3.61)  Time: 0.299s, 3427.99/s  (0.300s, 3416.25/s)  LR: 6.246e-04  Data: 0.021 (0.028)
Train: 126 [ 450/1251 ( 36%)]  Loss: 3.665 (3.62)  Time: 0.293s, 3491.81/s  (0.300s, 3417.18/s)  LR: 6.244e-04  Data: 0.026 (0.028)
Train: 126 [ 500/1251 ( 40%)]  Loss: 3.502 (3.61)  Time: 0.300s, 3412.23/s  (0.300s, 3418.68/s)  LR: 6.242e-04  Data: 0.022 (0.027)
Train: 126 [ 550/1251 ( 44%)]  Loss: 3.544 (3.60)  Time: 0.300s, 3409.45/s  (0.299s, 3419.30/s)  LR: 6.240e-04  Data: 0.022 (0.027)
Train: 126 [ 600/1251 ( 48%)]  Loss: 3.857 (3.62)  Time: 0.303s, 3374.71/s  (0.299s, 3419.21/s)  LR: 6.238e-04  Data: 0.022 (0.027)
Train: 126 [ 650/1251 ( 52%)]  Loss: 3.531 (3.62)  Time: 0.301s, 3403.53/s  (0.300s, 3419.01/s)  LR: 6.236e-04  Data: 0.022 (0.026)
Train: 126 [ 700/1251 ( 56%)]  Loss: 3.519 (3.61)  Time: 0.303s, 3376.16/s  (0.300s, 3418.94/s)  LR: 6.234e-04  Data: 0.023 (0.026)
Train: 126 [ 750/1251 ( 60%)]  Loss: 3.419 (3.60)  Time: 0.298s, 3432.88/s  (0.300s, 3418.47/s)  LR: 6.232e-04  Data: 0.022 (0.026)
Train: 126 [ 800/1251 ( 64%)]  Loss: 3.219 (3.57)  Time: 0.299s, 3422.38/s  (0.300s, 3418.35/s)  LR: 6.230e-04  Data: 0.029 (0.026)
Train: 126 [ 850/1251 ( 68%)]  Loss: 3.662 (3.58)  Time: 0.303s, 3382.82/s  (0.300s, 3418.08/s)  LR: 6.228e-04  Data: 0.028 (0.026)
Train: 126 [ 900/1251 ( 72%)]  Loss: 3.732 (3.59)  Time: 0.302s, 3385.70/s  (0.300s, 3417.94/s)  LR: 6.226e-04  Data: 0.023 (0.025)
Train: 126 [ 950/1251 ( 76%)]  Loss: 3.569 (3.59)  Time: 0.303s, 3382.20/s  (0.300s, 3418.08/s)  LR: 6.224e-04  Data: 0.023 (0.025)
Train: 126 [1000/1251 ( 80%)]  Loss: 3.875 (3.60)  Time: 0.290s, 3527.52/s  (0.300s, 3417.64/s)  LR: 6.222e-04  Data: 0.013 (0.025)
Train: 126 [1050/1251 ( 84%)]  Loss: 3.621 (3.60)  Time: 0.292s, 3505.62/s  (0.300s, 3417.92/s)  LR: 6.220e-04  Data: 0.021 (0.025)
Train: 126 [1100/1251 ( 88%)]  Loss: 3.461 (3.60)  Time: 0.302s, 3385.75/s  (0.300s, 3417.56/s)  LR: 6.218e-04  Data: 0.023 (0.025)
Train: 126 [1150/1251 ( 92%)]  Loss: 3.432 (3.59)  Time: 0.302s, 3392.97/s  (0.300s, 3417.27/s)  LR: 6.216e-04  Data: 0.032 (0.025)
Train: 126 [1200/1251 ( 96%)]  Loss: 3.507 (3.59)  Time: 0.298s, 3437.04/s  (0.300s, 3416.52/s)  LR: 6.214e-04  Data: 0.023 (0.025)
Train: 126 [1250/1251 (100%)]  Loss: 3.498 (3.58)  Time: 0.276s, 3709.44/s  (0.300s, 3418.28/s)  LR: 6.212e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.057 (2.057)  Loss:  0.6543 (0.6543)  Acc@1: 87.0117 (87.0117)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.041 (0.239)  Loss:  0.7217 (1.2050)  Acc@1: 84.1981 (72.9660)  Acc@5: 96.1085 (91.5100)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-126.pth.tar', 72.96599998779297)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-122.pth.tar', 72.82199998779296)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-125.pth.tar', 72.80799990722656)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-124.pth.tar', 72.79600006347657)

Train: 127 [   0/1251 (  0%)]  Loss: 3.299 (3.30)  Time: 2.220s,  461.34/s  (2.220s,  461.34/s)  LR: 6.212e-04  Data: 1.997 (1.997)
Train: 127 [  50/1251 (  4%)]  Loss: 3.561 (3.43)  Time: 0.287s, 3570.63/s  (0.319s, 3209.43/s)  LR: 6.210e-04  Data: 0.020 (0.062)
Train: 127 [ 100/1251 (  8%)]  Loss: 3.366 (3.41)  Time: 0.294s, 3481.19/s  (0.306s, 3344.66/s)  LR: 6.208e-04  Data: 0.023 (0.043)
Train: 127 [ 150/1251 ( 12%)]  Loss: 3.590 (3.45)  Time: 0.296s, 3461.92/s  (0.303s, 3382.53/s)  LR: 6.206e-04  Data: 0.026 (0.036)
Train: 127 [ 200/1251 ( 16%)]  Loss: 3.508 (3.46)  Time: 0.298s, 3439.09/s  (0.301s, 3402.44/s)  LR: 6.204e-04  Data: 0.023 (0.033)
Train: 127 [ 250/1251 ( 20%)]  Loss: 3.404 (3.45)  Time: 0.301s, 3399.38/s  (0.300s, 3411.84/s)  LR: 6.202e-04  Data: 0.024 (0.031)
Train: 127 [ 300/1251 ( 24%)]  Loss: 3.647 (3.48)  Time: 0.292s, 3511.91/s  (0.300s, 3416.46/s)  LR: 6.200e-04  Data: 0.019 (0.030)
Train: 127 [ 350/1251 ( 28%)]  Loss: 3.608 (3.50)  Time: 0.301s, 3396.81/s  (0.299s, 3420.81/s)  LR: 6.198e-04  Data: 0.027 (0.029)
Train: 127 [ 400/1251 ( 32%)]  Loss: 3.409 (3.49)  Time: 0.300s, 3414.11/s  (0.299s, 3422.54/s)  LR: 6.195e-04  Data: 0.027 (0.028)
Train: 127 [ 450/1251 ( 36%)]  Loss: 3.071 (3.45)  Time: 0.289s, 3547.23/s  (0.299s, 3422.55/s)  LR: 6.193e-04  Data: 0.025 (0.027)
Train: 127 [ 500/1251 ( 40%)]  Loss: 3.679 (3.47)  Time: 0.295s, 3470.96/s  (0.299s, 3423.34/s)  LR: 6.191e-04  Data: 0.023 (0.027)
Train: 127 [ 550/1251 ( 44%)]  Loss: 3.645 (3.48)  Time: 0.314s, 3260.23/s  (0.299s, 3422.36/s)  LR: 6.189e-04  Data: 0.024 (0.027)
Train: 127 [ 600/1251 ( 48%)]  Loss: 3.686 (3.50)  Time: 0.296s, 3454.34/s  (0.299s, 3422.68/s)  LR: 6.187e-04  Data: 0.023 (0.026)
Train: 127 [ 650/1251 ( 52%)]  Loss: 3.279 (3.48)  Time: 0.294s, 3487.75/s  (0.299s, 3423.18/s)  LR: 6.185e-04  Data: 0.021 (0.026)
Train: 127 [ 700/1251 ( 56%)]  Loss: 3.734 (3.50)  Time: 0.302s, 3385.79/s  (0.299s, 3423.84/s)  LR: 6.183e-04  Data: 0.022 (0.026)
Train: 127 [ 750/1251 ( 60%)]  Loss: 3.718 (3.51)  Time: 0.302s, 3394.08/s  (0.299s, 3422.97/s)  LR: 6.181e-04  Data: 0.027 (0.026)
Train: 127 [ 800/1251 ( 64%)]  Loss: 3.617 (3.52)  Time: 0.303s, 3379.91/s  (0.299s, 3422.78/s)  LR: 6.179e-04  Data: 0.012 (0.026)
Train: 127 [ 850/1251 ( 68%)]  Loss: 3.627 (3.52)  Time: 0.298s, 3440.28/s  (0.299s, 3422.64/s)  LR: 6.177e-04  Data: 0.019 (0.025)
Train: 127 [ 900/1251 ( 72%)]  Loss: 3.482 (3.52)  Time: 0.301s, 3402.11/s  (0.299s, 3422.05/s)  LR: 6.175e-04  Data: 0.022 (0.025)
Train: 127 [ 950/1251 ( 76%)]  Loss: 3.627 (3.53)  Time: 0.298s, 3431.56/s  (0.299s, 3421.43/s)  LR: 6.173e-04  Data: 0.017 (0.025)
Train: 127 [1000/1251 ( 80%)]  Loss: 3.283 (3.52)  Time: 0.298s, 3440.47/s  (0.299s, 3421.01/s)  LR: 6.171e-04  Data: 0.021 (0.025)
Train: 127 [1050/1251 ( 84%)]  Loss: 3.730 (3.53)  Time: 0.302s, 3386.15/s  (0.299s, 3420.41/s)  LR: 6.169e-04  Data: 0.021 (0.025)
Train: 127 [1100/1251 ( 88%)]  Loss: 3.718 (3.53)  Time: 0.304s, 3365.76/s  (0.299s, 3419.95/s)  LR: 6.167e-04  Data: 0.025 (0.025)
Train: 127 [1150/1251 ( 92%)]  Loss: 3.797 (3.55)  Time: 0.307s, 3337.18/s  (0.299s, 3419.21/s)  LR: 6.165e-04  Data: 0.026 (0.025)
Train: 127 [1200/1251 ( 96%)]  Loss: 3.501 (3.54)  Time: 0.304s, 3367.77/s  (0.300s, 3418.82/s)  LR: 6.163e-04  Data: 0.023 (0.025)
Train: 127 [1250/1251 (100%)]  Loss: 3.572 (3.54)  Time: 0.276s, 3707.06/s  (0.299s, 3420.70/s)  LR: 6.161e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.010 (2.010)  Loss:  0.5908 (0.5908)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.045 (0.244)  Loss:  0.7236 (1.2052)  Acc@1: 83.8443 (72.8800)  Acc@5: 95.6368 (91.5780)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-126.pth.tar', 72.96599998779297)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-127.pth.tar', 72.88000009277344)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-122.pth.tar', 72.82199998779296)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-125.pth.tar', 72.80799990722656)

Train: 128 [   0/1251 (  0%)]  Loss: 3.926 (3.93)  Time: 2.177s,  470.32/s  (2.177s,  470.32/s)  LR: 6.161e-04  Data: 1.953 (1.953)
Train: 128 [  50/1251 (  4%)]  Loss: 3.866 (3.90)  Time: 0.294s, 3480.23/s  (0.321s, 3192.10/s)  LR: 6.159e-04  Data: 0.026 (0.064)
Train: 128 [ 100/1251 (  8%)]  Loss: 3.838 (3.88)  Time: 0.305s, 3359.82/s  (0.308s, 3325.45/s)  LR: 6.157e-04  Data: 0.021 (0.044)
Train: 128 [ 150/1251 ( 12%)]  Loss: 3.641 (3.82)  Time: 0.304s, 3365.42/s  (0.304s, 3363.87/s)  LR: 6.155e-04  Data: 0.023 (0.037)
Train: 128 [ 200/1251 ( 16%)]  Loss: 3.452 (3.74)  Time: 0.295s, 3472.04/s  (0.303s, 3380.89/s)  LR: 6.153e-04  Data: 0.020 (0.034)
Train: 128 [ 250/1251 ( 20%)]  Loss: 3.673 (3.73)  Time: 0.293s, 3496.15/s  (0.302s, 3392.25/s)  LR: 6.151e-04  Data: 0.025 (0.032)
Train: 128 [ 300/1251 ( 24%)]  Loss: 3.799 (3.74)  Time: 0.299s, 3419.56/s  (0.301s, 3399.49/s)  LR: 6.149e-04  Data: 0.023 (0.030)
Train: 128 [ 350/1251 ( 28%)]  Loss: 3.415 (3.70)  Time: 0.297s, 3451.09/s  (0.301s, 3403.11/s)  LR: 6.147e-04  Data: 0.021 (0.029)
Train: 128 [ 400/1251 ( 32%)]  Loss: 3.834 (3.72)  Time: 0.298s, 3441.60/s  (0.301s, 3405.38/s)  LR: 6.145e-04  Data: 0.021 (0.029)
Train: 128 [ 450/1251 ( 36%)]  Loss: 3.575 (3.70)  Time: 0.302s, 3393.69/s  (0.301s, 3406.97/s)  LR: 6.143e-04  Data: 0.022 (0.028)
Train: 128 [ 500/1251 ( 40%)]  Loss: 3.280 (3.66)  Time: 0.302s, 3390.86/s  (0.300s, 3407.83/s)  LR: 6.141e-04  Data: 0.023 (0.028)
Train: 128 [ 550/1251 ( 44%)]  Loss: 3.604 (3.66)  Time: 0.301s, 3401.12/s  (0.300s, 3408.50/s)  LR: 6.139e-04  Data: 0.022 (0.027)
Train: 128 [ 600/1251 ( 48%)]  Loss: 3.413 (3.64)  Time: 0.296s, 3455.99/s  (0.300s, 3409.32/s)  LR: 6.137e-04  Data: 0.026 (0.027)
Train: 128 [ 650/1251 ( 52%)]  Loss: 3.436 (3.63)  Time: 0.301s, 3405.95/s  (0.300s, 3408.88/s)  LR: 6.135e-04  Data: 0.024 (0.027)
Train: 128 [ 700/1251 ( 56%)]  Loss: 4.094 (3.66)  Time: 0.299s, 3425.03/s  (0.300s, 3409.07/s)  LR: 6.133e-04  Data: 0.023 (0.026)
Train: 128 [ 750/1251 ( 60%)]  Loss: 3.974 (3.68)  Time: 0.305s, 3355.98/s  (0.300s, 3408.84/s)  LR: 6.131e-04  Data: 0.027 (0.026)
Train: 128 [ 800/1251 ( 64%)]  Loss: 3.600 (3.67)  Time: 0.300s, 3418.41/s  (0.300s, 3408.89/s)  LR: 6.129e-04  Data: 0.023 (0.026)
Train: 128 [ 850/1251 ( 68%)]  Loss: 3.736 (3.68)  Time: 0.298s, 3435.07/s  (0.300s, 3408.64/s)  LR: 6.127e-04  Data: 0.021 (0.026)
Train: 128 [ 900/1251 ( 72%)]  Loss: 4.020 (3.69)  Time: 0.304s, 3365.74/s  (0.300s, 3408.20/s)  LR: 6.124e-04  Data: 0.022 (0.026)
Train: 128 [ 950/1251 ( 76%)]  Loss: 3.431 (3.68)  Time: 0.300s, 3418.90/s  (0.301s, 3407.50/s)  LR: 6.122e-04  Data: 0.024 (0.026)
Train: 128 [1000/1251 ( 80%)]  Loss: 3.480 (3.67)  Time: 0.298s, 3441.45/s  (0.301s, 3406.63/s)  LR: 6.120e-04  Data: 0.023 (0.025)
Train: 128 [1050/1251 ( 84%)]  Loss: 3.747 (3.67)  Time: 0.297s, 3453.22/s  (0.301s, 3406.06/s)  LR: 6.118e-04  Data: 0.027 (0.025)
Train: 128 [1100/1251 ( 88%)]  Loss: 4.049 (3.69)  Time: 0.305s, 3352.21/s  (0.301s, 3405.38/s)  LR: 6.116e-04  Data: 0.023 (0.025)
Train: 128 [1150/1251 ( 92%)]  Loss: 3.719 (3.69)  Time: 0.301s, 3402.58/s  (0.301s, 3404.62/s)  LR: 6.114e-04  Data: 0.025 (0.025)
Train: 128 [1200/1251 ( 96%)]  Loss: 3.509 (3.68)  Time: 0.303s, 3383.06/s  (0.301s, 3404.07/s)  LR: 6.112e-04  Data: 0.025 (0.025)
Train: 128 [1250/1251 (100%)]  Loss: 3.484 (3.68)  Time: 0.275s, 3716.98/s  (0.301s, 3405.55/s)  LR: 6.110e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.046 (2.046)  Loss:  0.5938 (0.5938)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.057 (0.238)  Loss:  0.7656 (1.2016)  Acc@1: 84.3160 (73.1180)  Acc@5: 95.6368 (91.5060)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-128.pth.tar', 73.11800016845703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-126.pth.tar', 72.96599998779297)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-127.pth.tar', 72.88000009277344)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-122.pth.tar', 72.82199998779296)

Train: 129 [   0/1251 (  0%)]  Loss: 3.355 (3.35)  Time: 2.449s,  418.21/s  (2.449s,  418.21/s)  LR: 6.110e-04  Data: 2.218 (2.218)
Train: 129 [  50/1251 (  4%)]  Loss: 3.547 (3.45)  Time: 0.288s, 3557.81/s  (0.327s, 3136.01/s)  LR: 6.108e-04  Data: 0.026 (0.070)
Train: 129 [ 100/1251 (  8%)]  Loss: 3.559 (3.49)  Time: 0.300s, 3412.40/s  (0.312s, 3282.44/s)  LR: 6.106e-04  Data: 0.023 (0.047)
Train: 129 [ 150/1251 ( 12%)]  Loss: 3.473 (3.48)  Time: 0.300s, 3415.60/s  (0.308s, 3325.66/s)  LR: 6.104e-04  Data: 0.023 (0.039)
Train: 129 [ 200/1251 ( 16%)]  Loss: 3.477 (3.48)  Time: 0.298s, 3441.81/s  (0.306s, 3343.98/s)  LR: 6.102e-04  Data: 0.024 (0.035)
Train: 129 [ 250/1251 ( 20%)]  Loss: 3.664 (3.51)  Time: 0.305s, 3362.83/s  (0.305s, 3354.19/s)  LR: 6.100e-04  Data: 0.024 (0.033)
Train: 129 [ 300/1251 ( 24%)]  Loss: 2.939 (3.43)  Time: 0.303s, 3378.92/s  (0.305s, 3361.68/s)  LR: 6.098e-04  Data: 0.024 (0.031)
Train: 129 [ 350/1251 ( 28%)]  Loss: 3.309 (3.42)  Time: 0.303s, 3380.62/s  (0.304s, 3366.62/s)  LR: 6.096e-04  Data: 0.023 (0.030)
Train: 129 [ 400/1251 ( 32%)]  Loss: 3.520 (3.43)  Time: 0.305s, 3359.99/s  (0.304s, 3368.52/s)  LR: 6.094e-04  Data: 0.021 (0.029)
Train: 129 [ 450/1251 ( 36%)]  Loss: 3.545 (3.44)  Time: 0.302s, 3389.10/s  (0.304s, 3371.44/s)  LR: 6.092e-04  Data: 0.021 (0.029)
Train: 129 [ 500/1251 ( 40%)]  Loss: 3.539 (3.45)  Time: 0.302s, 3387.08/s  (0.304s, 3373.50/s)  LR: 6.090e-04  Data: 0.021 (0.028)
Train: 129 [ 550/1251 ( 44%)]  Loss: 3.743 (3.47)  Time: 0.306s, 3345.06/s  (0.303s, 3374.80/s)  LR: 6.088e-04  Data: 0.020 (0.028)
Train: 129 [ 600/1251 ( 48%)]  Loss: 3.768 (3.50)  Time: 0.298s, 3436.06/s  (0.303s, 3375.44/s)  LR: 6.086e-04  Data: 0.023 (0.027)
Train: 129 [ 650/1251 ( 52%)]  Loss: 3.647 (3.51)  Time: 0.309s, 3319.01/s  (0.303s, 3374.98/s)  LR: 6.084e-04  Data: 0.028 (0.027)
Train: 129 [ 700/1251 ( 56%)]  Loss: 3.260 (3.49)  Time: 0.304s, 3370.26/s  (0.303s, 3374.35/s)  LR: 6.082e-04  Data: 0.021 (0.027)
Train: 129 [ 750/1251 ( 60%)]  Loss: 3.545 (3.49)  Time: 0.304s, 3366.61/s  (0.303s, 3374.55/s)  LR: 6.080e-04  Data: 0.028 (0.027)
Train: 129 [ 800/1251 ( 64%)]  Loss: 3.588 (3.50)  Time: 0.300s, 3416.19/s  (0.303s, 3374.31/s)  LR: 6.078e-04  Data: 0.025 (0.026)
Train: 129 [ 850/1251 ( 68%)]  Loss: 3.471 (3.50)  Time: 0.302s, 3388.51/s  (0.304s, 3373.75/s)  LR: 6.076e-04  Data: 0.021 (0.026)
Train: 129 [ 900/1251 ( 72%)]  Loss: 3.601 (3.50)  Time: 0.307s, 3340.51/s  (0.304s, 3373.40/s)  LR: 6.074e-04  Data: 0.024 (0.026)
Train: 129 [ 950/1251 ( 76%)]  Loss: 3.984 (3.53)  Time: 0.305s, 3360.00/s  (0.304s, 3373.07/s)  LR: 6.072e-04  Data: 0.022 (0.026)
Train: 129 [1000/1251 ( 80%)]  Loss: 3.295 (3.52)  Time: 0.310s, 3301.87/s  (0.304s, 3372.41/s)  LR: 6.070e-04  Data: 0.021 (0.026)
Train: 129 [1050/1251 ( 84%)]  Loss: 3.712 (3.52)  Time: 0.308s, 3319.35/s  (0.304s, 3371.62/s)  LR: 6.068e-04  Data: 0.024 (0.026)
Train: 129 [1100/1251 ( 88%)]  Loss: 3.519 (3.52)  Time: 0.304s, 3370.97/s  (0.304s, 3371.22/s)  LR: 6.065e-04  Data: 0.024 (0.026)
Train: 129 [1150/1251 ( 92%)]  Loss: 3.358 (3.52)  Time: 0.305s, 3362.21/s  (0.304s, 3370.74/s)  LR: 6.063e-04  Data: 0.022 (0.026)
Train: 129 [1200/1251 ( 96%)]  Loss: 3.478 (3.52)  Time: 0.306s, 3343.19/s  (0.304s, 3370.47/s)  LR: 6.061e-04  Data: 0.022 (0.025)
Train: 129 [1250/1251 (100%)]  Loss: 3.571 (3.52)  Time: 0.276s, 3705.36/s  (0.304s, 3372.32/s)  LR: 6.059e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.039 (2.039)  Loss:  0.6113 (0.6113)  Acc@1: 87.8906 (87.8906)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.040 (0.239)  Loss:  0.7256 (1.1887)  Acc@1: 84.0802 (73.2220)  Acc@5: 95.8726 (91.5920)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-129.pth.tar', 73.22199993652343)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-128.pth.tar', 73.11800016845703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-126.pth.tar', 72.96599998779297)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-127.pth.tar', 72.88000009277344)

Train: 130 [   0/1251 (  0%)]  Loss: 3.748 (3.75)  Time: 2.164s,  473.11/s  (2.164s,  473.11/s)  LR: 6.059e-04  Data: 1.938 (1.938)
Train: 130 [  50/1251 (  4%)]  Loss: 3.451 (3.60)  Time: 0.295s, 3469.90/s  (0.327s, 3132.40/s)  LR: 6.057e-04  Data: 0.020 (0.062)
Train: 130 [ 100/1251 (  8%)]  Loss: 3.661 (3.62)  Time: 0.295s, 3474.12/s  (0.312s, 3278.20/s)  LR: 6.055e-04  Data: 0.019 (0.043)
Train: 130 [ 150/1251 ( 12%)]  Loss: 3.464 (3.58)  Time: 0.303s, 3375.30/s  (0.308s, 3322.15/s)  LR: 6.053e-04  Data: 0.023 (0.037)
Train: 130 [ 200/1251 ( 16%)]  Loss: 3.485 (3.56)  Time: 0.304s, 3369.16/s  (0.306s, 3343.75/s)  LR: 6.051e-04  Data: 0.023 (0.033)
Train: 130 [ 250/1251 ( 20%)]  Loss: 3.684 (3.58)  Time: 0.310s, 3300.38/s  (0.305s, 3354.14/s)  LR: 6.049e-04  Data: 0.025 (0.031)
Train: 130 [ 300/1251 ( 24%)]  Loss: 3.298 (3.54)  Time: 0.306s, 3350.10/s  (0.305s, 3358.09/s)  LR: 6.047e-04  Data: 0.020 (0.030)
Train: 130 [ 350/1251 ( 28%)]  Loss: 3.465 (3.53)  Time: 0.307s, 3338.41/s  (0.305s, 3359.38/s)  LR: 6.045e-04  Data: 0.022 (0.029)
Train: 130 [ 400/1251 ( 32%)]  Loss: 3.461 (3.52)  Time: 0.307s, 3335.04/s  (0.305s, 3360.48/s)  LR: 6.043e-04  Data: 0.026 (0.028)
Train: 130 [ 450/1251 ( 36%)]  Loss: 3.338 (3.51)  Time: 0.303s, 3376.09/s  (0.305s, 3362.01/s)  LR: 6.041e-04  Data: 0.023 (0.028)
Train: 130 [ 500/1251 ( 40%)]  Loss: 3.332 (3.49)  Time: 0.311s, 3295.55/s  (0.305s, 3362.85/s)  LR: 6.039e-04  Data: 0.027 (0.027)
Train: 130 [ 550/1251 ( 44%)]  Loss: 3.458 (3.49)  Time: 0.306s, 3349.56/s  (0.304s, 3364.52/s)  LR: 6.037e-04  Data: 0.020 (0.027)
Train: 130 [ 600/1251 ( 48%)]  Loss: 3.445 (3.48)  Time: 0.300s, 3418.97/s  (0.304s, 3365.41/s)  LR: 6.035e-04  Data: 0.024 (0.027)
Train: 130 [ 650/1251 ( 52%)]  Loss: 3.834 (3.51)  Time: 0.303s, 3381.94/s  (0.304s, 3366.16/s)  LR: 6.033e-04  Data: 0.022 (0.026)
Train: 130 [ 700/1251 ( 56%)]  Loss: 3.686 (3.52)  Time: 0.305s, 3356.10/s  (0.304s, 3365.99/s)  LR: 6.031e-04  Data: 0.025 (0.026)
Train: 130 [ 750/1251 ( 60%)]  Loss: 3.732 (3.53)  Time: 0.302s, 3391.23/s  (0.304s, 3366.18/s)  LR: 6.029e-04  Data: 0.022 (0.026)
Train: 130 [ 800/1251 ( 64%)]  Loss: 3.805 (3.55)  Time: 0.303s, 3382.23/s  (0.304s, 3366.51/s)  LR: 6.027e-04  Data: 0.024 (0.026)
Train: 130 [ 850/1251 ( 68%)]  Loss: 3.386 (3.54)  Time: 0.300s, 3410.63/s  (0.304s, 3366.85/s)  LR: 6.025e-04  Data: 0.025 (0.026)
Train: 130 [ 900/1251 ( 72%)]  Loss: 3.777 (3.55)  Time: 0.303s, 3382.29/s  (0.304s, 3366.45/s)  LR: 6.023e-04  Data: 0.024 (0.026)
Train: 130 [ 950/1251 ( 76%)]  Loss: 3.589 (3.55)  Time: 0.302s, 3386.54/s  (0.304s, 3366.45/s)  LR: 6.021e-04  Data: 0.025 (0.026)
Train: 130 [1000/1251 ( 80%)]  Loss: 3.658 (3.56)  Time: 0.306s, 3346.81/s  (0.304s, 3366.80/s)  LR: 6.019e-04  Data: 0.023 (0.025)
Train: 130 [1050/1251 ( 84%)]  Loss: 3.325 (3.55)  Time: 0.305s, 3358.50/s  (0.304s, 3366.33/s)  LR: 6.017e-04  Data: 0.024 (0.025)
Train: 130 [1100/1251 ( 88%)]  Loss: 3.448 (3.54)  Time: 0.304s, 3372.97/s  (0.304s, 3366.11/s)  LR: 6.014e-04  Data: 0.023 (0.025)
Train: 130 [1150/1251 ( 92%)]  Loss: 3.581 (3.55)  Time: 0.312s, 3280.13/s  (0.304s, 3365.79/s)  LR: 6.012e-04  Data: 0.022 (0.025)
Train: 130 [1200/1251 ( 96%)]  Loss: 3.421 (3.54)  Time: 0.305s, 3358.01/s  (0.304s, 3365.61/s)  LR: 6.010e-04  Data: 0.022 (0.025)
Train: 130 [1250/1251 (100%)]  Loss: 3.370 (3.53)  Time: 0.276s, 3706.47/s  (0.304s, 3367.36/s)  LR: 6.008e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.236 (2.236)  Loss:  0.6143 (0.6143)  Acc@1: 87.5977 (87.5977)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.051 (0.235)  Loss:  0.7192 (1.1868)  Acc@1: 84.5519 (73.2580)  Acc@5: 95.5189 (91.6800)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-130.pth.tar', 73.25800001220703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-129.pth.tar', 73.22199993652343)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-128.pth.tar', 73.11800016845703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-126.pth.tar', 72.96599998779297)

Train: 131 [   0/1251 (  0%)]  Loss: 3.422 (3.42)  Time: 2.166s,  472.76/s  (2.166s,  472.76/s)  LR: 6.008e-04  Data: 1.936 (1.936)
Train: 131 [  50/1251 (  4%)]  Loss: 3.709 (3.57)  Time: 0.300s, 3408.93/s  (0.333s, 3078.20/s)  LR: 6.006e-04  Data: 0.021 (0.072)
Train: 131 [ 100/1251 (  8%)]  Loss: 3.595 (3.58)  Time: 0.305s, 3359.13/s  (0.315s, 3247.09/s)  LR: 6.004e-04  Data: 0.025 (0.048)
Train: 131 [ 150/1251 ( 12%)]  Loss: 3.753 (3.62)  Time: 0.308s, 3324.88/s  (0.310s, 3298.50/s)  LR: 6.002e-04  Data: 0.023 (0.040)
Train: 131 [ 200/1251 ( 16%)]  Loss: 3.326 (3.56)  Time: 0.304s, 3363.82/s  (0.308s, 3322.05/s)  LR: 6.000e-04  Data: 0.023 (0.036)
Train: 131 [ 250/1251 ( 20%)]  Loss: 3.707 (3.59)  Time: 0.304s, 3363.39/s  (0.307s, 3335.43/s)  LR: 5.998e-04  Data: 0.026 (0.033)
Train: 131 [ 300/1251 ( 24%)]  Loss: 3.477 (3.57)  Time: 0.303s, 3378.16/s  (0.306s, 3344.66/s)  LR: 5.996e-04  Data: 0.028 (0.031)
Train: 131 [ 350/1251 ( 28%)]  Loss: 3.484 (3.56)  Time: 0.304s, 3367.60/s  (0.306s, 3348.63/s)  LR: 5.994e-04  Data: 0.021 (0.030)
Train: 131 [ 400/1251 ( 32%)]  Loss: 3.436 (3.55)  Time: 0.301s, 3397.26/s  (0.306s, 3349.96/s)  LR: 5.992e-04  Data: 0.022 (0.029)
Train: 131 [ 450/1251 ( 36%)]  Loss: 3.689 (3.56)  Time: 0.306s, 3349.35/s  (0.306s, 3351.05/s)  LR: 5.990e-04  Data: 0.024 (0.029)
Train: 131 [ 500/1251 ( 40%)]  Loss: 3.352 (3.54)  Time: 0.304s, 3367.28/s  (0.306s, 3351.75/s)  LR: 5.988e-04  Data: 0.025 (0.028)
Train: 131 [ 550/1251 ( 44%)]  Loss: 3.442 (3.53)  Time: 0.309s, 3317.48/s  (0.305s, 3352.20/s)  LR: 5.986e-04  Data: 0.022 (0.028)
Train: 131 [ 600/1251 ( 48%)]  Loss: 3.645 (3.54)  Time: 0.302s, 3390.85/s  (0.305s, 3352.54/s)  LR: 5.984e-04  Data: 0.022 (0.027)
Train: 131 [ 650/1251 ( 52%)]  Loss: 3.216 (3.52)  Time: 0.302s, 3395.25/s  (0.305s, 3352.86/s)  LR: 5.982e-04  Data: 0.022 (0.027)
Train: 131 [ 700/1251 ( 56%)]  Loss: 3.495 (3.52)  Time: 0.304s, 3369.06/s  (0.305s, 3352.97/s)  LR: 5.980e-04  Data: 0.021 (0.027)
Train: 131 [ 750/1251 ( 60%)]  Loss: 3.685 (3.53)  Time: 0.306s, 3351.75/s  (0.305s, 3353.58/s)  LR: 5.978e-04  Data: 0.025 (0.027)
Train: 131 [ 800/1251 ( 64%)]  Loss: 3.559 (3.53)  Time: 0.306s, 3348.44/s  (0.305s, 3353.91/s)  LR: 5.976e-04  Data: 0.024 (0.026)
Train: 131 [ 850/1251 ( 68%)]  Loss: 3.650 (3.54)  Time: 0.302s, 3385.83/s  (0.305s, 3354.00/s)  LR: 5.974e-04  Data: 0.022 (0.026)
Train: 131 [ 900/1251 ( 72%)]  Loss: 3.435 (3.53)  Time: 0.310s, 3304.91/s  (0.305s, 3354.12/s)  LR: 5.972e-04  Data: 0.025 (0.026)
Train: 131 [ 950/1251 ( 76%)]  Loss: 3.036 (3.51)  Time: 0.307s, 3333.08/s  (0.305s, 3354.22/s)  LR: 5.969e-04  Data: 0.021 (0.026)
Train: 131 [1000/1251 ( 80%)]  Loss: 3.729 (3.52)  Time: 0.305s, 3362.72/s  (0.305s, 3354.07/s)  LR: 5.967e-04  Data: 0.023 (0.026)
Train: 131 [1050/1251 ( 84%)]  Loss: 2.951 (3.49)  Time: 0.305s, 3355.20/s  (0.305s, 3353.61/s)  LR: 5.965e-04  Data: 0.022 (0.026)
Train: 131 [1100/1251 ( 88%)]  Loss: 3.519 (3.49)  Time: 0.307s, 3332.01/s  (0.305s, 3353.46/s)  LR: 5.963e-04  Data: 0.023 (0.026)
Train: 131 [1150/1251 ( 92%)]  Loss: 3.582 (3.50)  Time: 0.308s, 3322.87/s  (0.305s, 3352.87/s)  LR: 5.961e-04  Data: 0.024 (0.026)
Train: 131 [1200/1251 ( 96%)]  Loss: 3.533 (3.50)  Time: 0.306s, 3349.13/s  (0.305s, 3352.59/s)  LR: 5.959e-04  Data: 0.019 (0.025)
Train: 131 [1250/1251 (100%)]  Loss: 3.897 (3.51)  Time: 0.276s, 3714.54/s  (0.305s, 3354.32/s)  LR: 5.957e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.001 (2.001)  Loss:  0.6304 (0.6304)  Acc@1: 87.9883 (87.9883)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.056 (0.241)  Loss:  0.7148 (1.2037)  Acc@1: 85.3774 (73.2120)  Acc@5: 96.9340 (91.6020)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-130.pth.tar', 73.25800001220703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-129.pth.tar', 73.22199993652343)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-131.pth.tar', 73.2120001123047)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-128.pth.tar', 73.11800016845703)

Train: 132 [   0/1251 (  0%)]  Loss: 3.399 (3.40)  Time: 2.092s,  489.46/s  (2.092s,  489.46/s)  LR: 5.957e-04  Data: 1.874 (1.874)
Train: 132 [  50/1251 (  4%)]  Loss: 3.386 (3.39)  Time: 0.294s, 3484.66/s  (0.329s, 3110.64/s)  LR: 5.955e-04  Data: 0.025 (0.062)
Train: 132 [ 100/1251 (  8%)]  Loss: 3.771 (3.52)  Time: 0.305s, 3362.55/s  (0.315s, 3248.54/s)  LR: 5.953e-04  Data: 0.027 (0.043)
Train: 132 [ 150/1251 ( 12%)]  Loss: 3.546 (3.53)  Time: 0.303s, 3376.76/s  (0.311s, 3289.34/s)  LR: 5.951e-04  Data: 0.023 (0.037)
Train: 132 [ 200/1251 ( 16%)]  Loss: 3.656 (3.55)  Time: 0.304s, 3372.54/s  (0.309s, 3308.67/s)  LR: 5.949e-04  Data: 0.025 (0.033)
Train: 132 [ 250/1251 ( 20%)]  Loss: 3.914 (3.61)  Time: 0.308s, 3330.03/s  (0.309s, 3317.55/s)  LR: 5.947e-04  Data: 0.022 (0.031)
Train: 132 [ 300/1251 ( 24%)]  Loss: 3.813 (3.64)  Time: 0.307s, 3337.42/s  (0.308s, 3321.02/s)  LR: 5.945e-04  Data: 0.026 (0.030)
Train: 132 [ 350/1251 ( 28%)]  Loss: 3.166 (3.58)  Time: 0.314s, 3265.48/s  (0.308s, 3323.74/s)  LR: 5.943e-04  Data: 0.023 (0.029)
Train: 132 [ 400/1251 ( 32%)]  Loss: 3.060 (3.52)  Time: 0.309s, 3311.02/s  (0.308s, 3324.23/s)  LR: 5.941e-04  Data: 0.024 (0.028)
Train: 132 [ 450/1251 ( 36%)]  Loss: 3.707 (3.54)  Time: 0.309s, 3318.35/s  (0.308s, 3325.31/s)  LR: 5.939e-04  Data: 0.022 (0.028)
Train: 132 [ 500/1251 ( 40%)]  Loss: 3.473 (3.54)  Time: 0.310s, 3299.57/s  (0.308s, 3324.68/s)  LR: 5.937e-04  Data: 0.022 (0.027)
Train: 132 [ 550/1251 ( 44%)]  Loss: 3.586 (3.54)  Time: 0.311s, 3290.06/s  (0.308s, 3324.20/s)  LR: 5.935e-04  Data: 0.021 (0.027)
Train: 132 [ 600/1251 ( 48%)]  Loss: 3.565 (3.54)  Time: 0.305s, 3357.21/s  (0.308s, 3323.08/s)  LR: 5.933e-04  Data: 0.020 (0.027)
Train: 132 [ 650/1251 ( 52%)]  Loss: 3.441 (3.53)  Time: 0.312s, 3286.81/s  (0.308s, 3322.72/s)  LR: 5.931e-04  Data: 0.025 (0.026)
Train: 132 [ 700/1251 ( 56%)]  Loss: 3.470 (3.53)  Time: 0.307s, 3333.22/s  (0.308s, 3322.84/s)  LR: 5.929e-04  Data: 0.025 (0.026)
Train: 132 [ 750/1251 ( 60%)]  Loss: 3.646 (3.54)  Time: 0.308s, 3329.77/s  (0.308s, 3322.69/s)  LR: 5.926e-04  Data: 0.023 (0.026)
Train: 132 [ 800/1251 ( 64%)]  Loss: 3.874 (3.56)  Time: 0.312s, 3283.08/s  (0.308s, 3321.85/s)  LR: 5.924e-04  Data: 0.026 (0.026)
Train: 132 [ 850/1251 ( 68%)]  Loss: 3.503 (3.55)  Time: 0.305s, 3361.63/s  (0.308s, 3321.53/s)  LR: 5.922e-04  Data: 0.022 (0.026)
Train: 132 [ 900/1251 ( 72%)]  Loss: 3.364 (3.54)  Time: 0.310s, 3304.77/s  (0.308s, 3321.03/s)  LR: 5.920e-04  Data: 0.023 (0.026)
Train: 132 [ 950/1251 ( 76%)]  Loss: 3.512 (3.54)  Time: 0.308s, 3320.38/s  (0.308s, 3320.71/s)  LR: 5.918e-04  Data: 0.024 (0.026)
Train: 132 [1000/1251 ( 80%)]  Loss: 3.842 (3.56)  Time: 0.309s, 3312.00/s  (0.308s, 3320.66/s)  LR: 5.916e-04  Data: 0.024 (0.025)
Train: 132 [1050/1251 ( 84%)]  Loss: 3.668 (3.56)  Time: 0.308s, 3327.41/s  (0.308s, 3319.98/s)  LR: 5.914e-04  Data: 0.025 (0.025)
Train: 132 [1100/1251 ( 88%)]  Loss: 3.359 (3.55)  Time: 0.311s, 3290.00/s  (0.308s, 3319.52/s)  LR: 5.912e-04  Data: 0.025 (0.025)
Train: 132 [1150/1251 ( 92%)]  Loss: 3.599 (3.55)  Time: 0.310s, 3303.93/s  (0.309s, 3319.09/s)  LR: 5.910e-04  Data: 0.023 (0.025)
Train: 132 [1200/1251 ( 96%)]  Loss: 3.346 (3.55)  Time: 0.305s, 3354.61/s  (0.309s, 3318.91/s)  LR: 5.908e-04  Data: 0.024 (0.025)
Train: 132 [1250/1251 (100%)]  Loss: 3.890 (3.56)  Time: 0.276s, 3704.21/s  (0.308s, 3321.12/s)  LR: 5.906e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.048 (2.048)  Loss:  0.6030 (0.6030)  Acc@1: 87.5977 (87.5977)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.065 (0.238)  Loss:  0.6528 (1.1657)  Acc@1: 85.8491 (73.2600)  Acc@5: 96.9340 (91.7480)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-132.pth.tar', 73.26000005859375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-130.pth.tar', 73.25800001220703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-129.pth.tar', 73.22199993652343)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-131.pth.tar', 73.2120001123047)

Train: 133 [   0/1251 (  0%)]  Loss: 3.485 (3.49)  Time: 1.970s,  519.76/s  (1.970s,  519.76/s)  LR: 5.906e-04  Data: 1.753 (1.753)
Train: 133 [  50/1251 (  4%)]  Loss: 3.700 (3.59)  Time: 0.304s, 3373.00/s  (0.332s, 3087.07/s)  LR: 5.904e-04  Data: 0.022 (0.060)
Train: 133 [ 100/1251 (  8%)]  Loss: 3.607 (3.60)  Time: 0.307s, 3335.50/s  (0.318s, 3219.27/s)  LR: 5.902e-04  Data: 0.023 (0.042)
Train: 133 [ 150/1251 ( 12%)]  Loss: 3.654 (3.61)  Time: 0.304s, 3372.77/s  (0.314s, 3262.73/s)  LR: 5.900e-04  Data: 0.021 (0.036)
Train: 133 [ 200/1251 ( 16%)]  Loss: 3.729 (3.64)  Time: 0.308s, 3322.78/s  (0.312s, 3282.38/s)  LR: 5.898e-04  Data: 0.023 (0.032)
Train: 133 [ 250/1251 ( 20%)]  Loss: 3.805 (3.66)  Time: 0.310s, 3298.59/s  (0.311s, 3292.04/s)  LR: 5.896e-04  Data: 0.021 (0.031)
Train: 133 [ 300/1251 ( 24%)]  Loss: 3.366 (3.62)  Time: 0.312s, 3282.34/s  (0.310s, 3298.07/s)  LR: 5.894e-04  Data: 0.023 (0.030)
Train: 133 [ 350/1251 ( 28%)]  Loss: 3.503 (3.61)  Time: 0.312s, 3277.69/s  (0.310s, 3302.16/s)  LR: 5.892e-04  Data: 0.026 (0.029)
Train: 133 [ 400/1251 ( 32%)]  Loss: 3.447 (3.59)  Time: 0.306s, 3345.83/s  (0.310s, 3305.05/s)  LR: 5.890e-04  Data: 0.022 (0.028)
Train: 133 [ 450/1251 ( 36%)]  Loss: 3.504 (3.58)  Time: 0.311s, 3289.76/s  (0.310s, 3305.87/s)  LR: 5.888e-04  Data: 0.028 (0.027)
Train: 133 [ 500/1251 ( 40%)]  Loss: 3.702 (3.59)  Time: 0.311s, 3294.23/s  (0.310s, 3307.88/s)  LR: 5.885e-04  Data: 0.022 (0.027)
Train: 133 [ 550/1251 ( 44%)]  Loss: 3.509 (3.58)  Time: 0.313s, 3266.78/s  (0.310s, 3308.05/s)  LR: 5.883e-04  Data: 0.026 (0.027)
Train: 133 [ 600/1251 ( 48%)]  Loss: 3.518 (3.58)  Time: 0.314s, 3262.88/s  (0.310s, 3308.19/s)  LR: 5.881e-04  Data: 0.023 (0.026)
Train: 133 [ 650/1251 ( 52%)]  Loss: 3.748 (3.59)  Time: 0.309s, 3313.99/s  (0.310s, 3307.84/s)  LR: 5.879e-04  Data: 0.024 (0.026)
Train: 133 [ 700/1251 ( 56%)]  Loss: 3.028 (3.55)  Time: 0.309s, 3311.02/s  (0.310s, 3307.44/s)  LR: 5.877e-04  Data: 0.022 (0.026)
Train: 133 [ 750/1251 ( 60%)]  Loss: 3.418 (3.55)  Time: 0.307s, 3332.23/s  (0.310s, 3307.15/s)  LR: 5.875e-04  Data: 0.021 (0.026)
Train: 133 [ 800/1251 ( 64%)]  Loss: 3.455 (3.54)  Time: 0.311s, 3290.69/s  (0.310s, 3307.00/s)  LR: 5.873e-04  Data: 0.027 (0.026)
Train: 133 [ 850/1251 ( 68%)]  Loss: 3.465 (3.54)  Time: 0.311s, 3287.58/s  (0.310s, 3307.19/s)  LR: 5.871e-04  Data: 0.027 (0.026)
Train: 133 [ 900/1251 ( 72%)]  Loss: 3.314 (3.52)  Time: 0.310s, 3299.58/s  (0.310s, 3306.86/s)  LR: 5.869e-04  Data: 0.023 (0.025)
Train: 133 [ 950/1251 ( 76%)]  Loss: 3.612 (3.53)  Time: 0.308s, 3321.40/s  (0.310s, 3306.29/s)  LR: 5.867e-04  Data: 0.023 (0.025)
Train: 133 [1000/1251 ( 80%)]  Loss: 3.482 (3.53)  Time: 0.302s, 3390.48/s  (0.310s, 3306.04/s)  LR: 5.865e-04  Data: 0.018 (0.025)
Train: 133 [1050/1251 ( 84%)]  Loss: 3.404 (3.52)  Time: 0.311s, 3287.34/s  (0.310s, 3305.78/s)  LR: 5.863e-04  Data: 0.022 (0.025)
Train: 133 [1100/1251 ( 88%)]  Loss: 3.837 (3.53)  Time: 0.310s, 3305.26/s  (0.310s, 3305.70/s)  LR: 5.861e-04  Data: 0.026 (0.025)
Train: 133 [1150/1251 ( 92%)]  Loss: 3.597 (3.54)  Time: 0.309s, 3316.10/s  (0.310s, 3305.60/s)  LR: 5.859e-04  Data: 0.021 (0.025)
Train: 133 [1200/1251 ( 96%)]  Loss: 3.387 (3.53)  Time: 0.310s, 3300.46/s  (0.310s, 3305.70/s)  LR: 5.857e-04  Data: 0.026 (0.025)
Train: 133 [1250/1251 (100%)]  Loss: 3.428 (3.53)  Time: 0.276s, 3704.07/s  (0.310s, 3307.53/s)  LR: 5.855e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.032 (2.032)  Loss:  0.6138 (0.6138)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.062 (0.232)  Loss:  0.7051 (1.1650)  Acc@1: 84.9057 (73.7820)  Acc@5: 96.4623 (91.7940)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-133.pth.tar', 73.7820000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-132.pth.tar', 73.26000005859375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-130.pth.tar', 73.25800001220703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-129.pth.tar', 73.22199993652343)

Train: 134 [   0/1251 (  0%)]  Loss: 3.712 (3.71)  Time: 2.240s,  457.18/s  (2.240s,  457.18/s)  LR: 5.855e-04  Data: 2.011 (2.011)
Train: 134 [  50/1251 (  4%)]  Loss: 3.444 (3.58)  Time: 0.305s, 3358.71/s  (0.332s, 3087.06/s)  LR: 5.853e-04  Data: 0.025 (0.062)
Train: 134 [ 100/1251 (  8%)]  Loss: 3.679 (3.61)  Time: 0.311s, 3287.86/s  (0.318s, 3219.69/s)  LR: 5.851e-04  Data: 0.024 (0.043)
Train: 134 [ 150/1251 ( 12%)]  Loss: 3.616 (3.61)  Time: 0.312s, 3285.13/s  (0.314s, 3258.96/s)  LR: 5.848e-04  Data: 0.021 (0.036)
Train: 134 [ 200/1251 ( 16%)]  Loss: 3.496 (3.59)  Time: 0.307s, 3334.33/s  (0.313s, 3273.52/s)  LR: 5.846e-04  Data: 0.023 (0.033)
Train: 134 [ 250/1251 ( 20%)]  Loss: 3.744 (3.62)  Time: 0.311s, 3288.47/s  (0.312s, 3279.97/s)  LR: 5.844e-04  Data: 0.021 (0.031)
Train: 134 [ 300/1251 ( 24%)]  Loss: 3.531 (3.60)  Time: 0.311s, 3290.65/s  (0.312s, 3284.48/s)  LR: 5.842e-04  Data: 0.023 (0.030)
Train: 134 [ 350/1251 ( 28%)]  Loss: 3.708 (3.62)  Time: 0.314s, 3265.37/s  (0.311s, 3288.23/s)  LR: 5.840e-04  Data: 0.026 (0.029)
Train: 134 [ 400/1251 ( 32%)]  Loss: 3.605 (3.62)  Time: 0.303s, 3382.07/s  (0.311s, 3289.34/s)  LR: 5.838e-04  Data: 0.020 (0.028)
Train: 134 [ 450/1251 ( 36%)]  Loss: 3.690 (3.62)  Time: 0.311s, 3297.81/s  (0.311s, 3290.94/s)  LR: 5.836e-04  Data: 0.022 (0.028)
Train: 134 [ 500/1251 ( 40%)]  Loss: 3.547 (3.62)  Time: 0.310s, 3302.27/s  (0.311s, 3290.98/s)  LR: 5.834e-04  Data: 0.022 (0.027)
Train: 134 [ 550/1251 ( 44%)]  Loss: 3.394 (3.60)  Time: 0.317s, 3226.47/s  (0.311s, 3290.68/s)  LR: 5.832e-04  Data: 0.024 (0.027)
Train: 134 [ 600/1251 ( 48%)]  Loss: 3.716 (3.61)  Time: 0.307s, 3335.48/s  (0.311s, 3291.53/s)  LR: 5.830e-04  Data: 0.022 (0.027)
Train: 134 [ 650/1251 ( 52%)]  Loss: 3.581 (3.60)  Time: 0.313s, 3272.56/s  (0.311s, 3291.67/s)  LR: 5.828e-04  Data: 0.021 (0.026)
Train: 134 [ 700/1251 ( 56%)]  Loss: 3.612 (3.60)  Time: 0.311s, 3294.32/s  (0.311s, 3292.20/s)  LR: 5.826e-04  Data: 0.020 (0.026)
Train: 134 [ 750/1251 ( 60%)]  Loss: 3.578 (3.60)  Time: 0.310s, 3300.17/s  (0.311s, 3292.06/s)  LR: 5.824e-04  Data: 0.026 (0.026)
Train: 134 [ 800/1251 ( 64%)]  Loss: 3.664 (3.61)  Time: 0.309s, 3312.80/s  (0.311s, 3292.54/s)  LR: 5.822e-04  Data: 0.022 (0.026)
Train: 134 [ 850/1251 ( 68%)]  Loss: 3.484 (3.60)  Time: 0.305s, 3361.41/s  (0.311s, 3292.62/s)  LR: 5.820e-04  Data: 0.024 (0.026)
Train: 134 [ 900/1251 ( 72%)]  Loss: 3.782 (3.61)  Time: 0.317s, 3229.82/s  (0.311s, 3292.53/s)  LR: 5.818e-04  Data: 0.025 (0.026)
Train: 134 [ 950/1251 ( 76%)]  Loss: 3.930 (3.63)  Time: 0.308s, 3328.76/s  (0.311s, 3292.80/s)  LR: 5.816e-04  Data: 0.022 (0.025)
Train: 134 [1000/1251 ( 80%)]  Loss: 3.694 (3.63)  Time: 0.310s, 3303.37/s  (0.311s, 3293.10/s)  LR: 5.814e-04  Data: 0.024 (0.025)
Train: 134 [1050/1251 ( 84%)]  Loss: 3.802 (3.64)  Time: 0.308s, 3327.90/s  (0.311s, 3293.57/s)  LR: 5.811e-04  Data: 0.025 (0.025)
Train: 134 [1100/1251 ( 88%)]  Loss: 3.325 (3.62)  Time: 0.313s, 3274.03/s  (0.311s, 3293.77/s)  LR: 5.809e-04  Data: 0.021 (0.025)
Train: 134 [1150/1251 ( 92%)]  Loss: 4.033 (3.64)  Time: 0.318s, 3220.63/s  (0.311s, 3293.68/s)  LR: 5.807e-04  Data: 0.019 (0.025)
Train: 134 [1200/1251 ( 96%)]  Loss: 3.723 (3.64)  Time: 0.307s, 3337.47/s  (0.311s, 3294.09/s)  LR: 5.805e-04  Data: 0.022 (0.025)
Train: 134 [1250/1251 (100%)]  Loss: 3.708 (3.65)  Time: 0.282s, 3626.58/s  (0.311s, 3296.18/s)  LR: 5.803e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.011 (2.011)  Loss:  0.6812 (0.6812)  Acc@1: 87.3047 (87.3047)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.046 (0.234)  Loss:  0.7617 (1.2120)  Acc@1: 84.3160 (73.2620)  Acc@5: 95.6368 (91.7600)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-133.pth.tar', 73.7820000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-134.pth.tar', 73.2620000390625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-132.pth.tar', 73.26000005859375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-130.pth.tar', 73.25800001220703)

Train: 135 [   0/1251 (  0%)]  Loss: 3.800 (3.80)  Time: 2.401s,  426.43/s  (2.401s,  426.43/s)  LR: 5.803e-04  Data: 2.183 (2.183)
Train: 135 [  50/1251 (  4%)]  Loss: 3.452 (3.63)  Time: 0.299s, 3423.58/s  (0.333s, 3077.52/s)  LR: 5.801e-04  Data: 0.024 (0.067)
Train: 135 [ 100/1251 (  8%)]  Loss: 3.277 (3.51)  Time: 0.309s, 3318.62/s  (0.319s, 3211.95/s)  LR: 5.799e-04  Data: 0.023 (0.045)
Train: 135 [ 150/1251 ( 12%)]  Loss: 3.656 (3.55)  Time: 0.307s, 3332.13/s  (0.315s, 3254.33/s)  LR: 5.797e-04  Data: 0.021 (0.038)
Train: 135 [ 200/1251 ( 16%)]  Loss: 3.471 (3.53)  Time: 0.308s, 3319.81/s  (0.313s, 3270.79/s)  LR: 5.795e-04  Data: 0.025 (0.034)
Train: 135 [ 250/1251 ( 20%)]  Loss: 3.643 (3.55)  Time: 0.308s, 3324.11/s  (0.312s, 3281.04/s)  LR: 5.793e-04  Data: 0.021 (0.032)
Train: 135 [ 300/1251 ( 24%)]  Loss: 3.255 (3.51)  Time: 0.313s, 3274.54/s  (0.312s, 3284.67/s)  LR: 5.791e-04  Data: 0.022 (0.030)
Train: 135 [ 350/1251 ( 28%)]  Loss: 3.706 (3.53)  Time: 0.309s, 3317.18/s  (0.311s, 3287.49/s)  LR: 5.789e-04  Data: 0.021 (0.029)
Train: 135 [ 400/1251 ( 32%)]  Loss: 3.777 (3.56)  Time: 0.308s, 3327.14/s  (0.311s, 3289.13/s)  LR: 5.787e-04  Data: 0.022 (0.029)
Train: 135 [ 450/1251 ( 36%)]  Loss: 3.832 (3.59)  Time: 0.313s, 3268.90/s  (0.311s, 3291.09/s)  LR: 5.785e-04  Data: 0.024 (0.028)
Train: 135 [ 500/1251 ( 40%)]  Loss: 3.599 (3.59)  Time: 0.307s, 3338.14/s  (0.311s, 3292.14/s)  LR: 5.783e-04  Data: 0.023 (0.028)
Train: 135 [ 550/1251 ( 44%)]  Loss: 3.468 (3.58)  Time: 0.310s, 3302.76/s  (0.311s, 3292.70/s)  LR: 5.781e-04  Data: 0.023 (0.027)
Train: 135 [ 600/1251 ( 48%)]  Loss: 3.666 (3.58)  Time: 0.309s, 3314.39/s  (0.311s, 3293.40/s)  LR: 5.779e-04  Data: 0.023 (0.027)
Train: 135 [ 650/1251 ( 52%)]  Loss: 3.435 (3.57)  Time: 0.309s, 3316.61/s  (0.311s, 3293.83/s)  LR: 5.776e-04  Data: 0.023 (0.027)
Train: 135 [ 700/1251 ( 56%)]  Loss: 3.503 (3.57)  Time: 0.307s, 3331.28/s  (0.311s, 3294.55/s)  LR: 5.774e-04  Data: 0.021 (0.026)
Train: 135 [ 750/1251 ( 60%)]  Loss: 3.604 (3.57)  Time: 0.306s, 3347.71/s  (0.311s, 3294.79/s)  LR: 5.772e-04  Data: 0.021 (0.026)
Train: 135 [ 800/1251 ( 64%)]  Loss: 3.770 (3.58)  Time: 0.312s, 3278.07/s  (0.311s, 3295.15/s)  LR: 5.770e-04  Data: 0.026 (0.026)
Train: 135 [ 850/1251 ( 68%)]  Loss: 3.855 (3.60)  Time: 0.309s, 3315.06/s  (0.311s, 3295.88/s)  LR: 5.768e-04  Data: 0.023 (0.026)
Train: 135 [ 900/1251 ( 72%)]  Loss: 3.417 (3.59)  Time: 0.316s, 3237.82/s  (0.311s, 3296.68/s)  LR: 5.766e-04  Data: 0.022 (0.026)
Train: 135 [ 950/1251 ( 76%)]  Loss: 3.768 (3.60)  Time: 0.310s, 3304.21/s  (0.311s, 3297.09/s)  LR: 5.764e-04  Data: 0.027 (0.026)
Train: 135 [1000/1251 ( 80%)]  Loss: 3.607 (3.60)  Time: 0.305s, 3353.94/s  (0.311s, 3297.86/s)  LR: 5.762e-04  Data: 0.022 (0.025)
Train: 135 [1050/1251 ( 84%)]  Loss: 3.445 (3.59)  Time: 0.314s, 3261.32/s  (0.310s, 3298.18/s)  LR: 5.760e-04  Data: 0.021 (0.025)
Train: 135 [1100/1251 ( 88%)]  Loss: 3.390 (3.58)  Time: 0.309s, 3312.69/s  (0.310s, 3298.62/s)  LR: 5.758e-04  Data: 0.023 (0.025)
Train: 135 [1150/1251 ( 92%)]  Loss: 3.269 (3.57)  Time: 0.312s, 3287.10/s  (0.310s, 3298.62/s)  LR: 5.756e-04  Data: 0.023 (0.025)
Train: 135 [1200/1251 ( 96%)]  Loss: 3.274 (3.56)  Time: 0.306s, 3350.71/s  (0.310s, 3298.90/s)  LR: 5.754e-04  Data: 0.022 (0.025)
Train: 135 [1250/1251 (100%)]  Loss: 3.357 (3.55)  Time: 0.279s, 3664.95/s  (0.310s, 3301.37/s)  LR: 5.752e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.045 (2.045)  Loss:  0.6157 (0.6157)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.063 (0.237)  Loss:  0.6963 (1.1757)  Acc@1: 84.7877 (73.6880)  Acc@5: 96.4623 (92.0140)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-133.pth.tar', 73.7820000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-135.pth.tar', 73.68800011474609)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-134.pth.tar', 73.2620000390625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-132.pth.tar', 73.26000005859375)

Train: 136 [   0/1251 (  0%)]  Loss: 3.160 (3.16)  Time: 2.007s,  510.19/s  (2.007s,  510.19/s)  LR: 5.752e-04  Data: 1.779 (1.779)
Train: 136 [  50/1251 (  4%)]  Loss: 3.533 (3.35)  Time: 0.296s, 3461.30/s  (0.333s, 3078.83/s)  LR: 5.750e-04  Data: 0.021 (0.060)
Train: 136 [ 100/1251 (  8%)]  Loss: 3.443 (3.38)  Time: 0.307s, 3332.20/s  (0.319s, 3214.75/s)  LR: 5.748e-04  Data: 0.021 (0.042)
Train: 136 [ 150/1251 ( 12%)]  Loss: 3.528 (3.42)  Time: 0.306s, 3344.49/s  (0.315s, 3255.91/s)  LR: 5.746e-04  Data: 0.025 (0.036)
Train: 136 [ 200/1251 ( 16%)]  Loss: 3.571 (3.45)  Time: 0.307s, 3331.77/s  (0.313s, 3274.97/s)  LR: 5.743e-04  Data: 0.027 (0.033)
Train: 136 [ 250/1251 ( 20%)]  Loss: 3.319 (3.43)  Time: 0.310s, 3307.91/s  (0.312s, 3284.76/s)  LR: 5.741e-04  Data: 0.023 (0.031)
Train: 136 [ 300/1251 ( 24%)]  Loss: 3.640 (3.46)  Time: 0.308s, 3327.05/s  (0.311s, 3290.58/s)  LR: 5.739e-04  Data: 0.024 (0.030)
Train: 136 [ 350/1251 ( 28%)]  Loss: 3.442 (3.45)  Time: 0.314s, 3256.53/s  (0.311s, 3295.74/s)  LR: 5.737e-04  Data: 0.026 (0.029)
Train: 136 [ 400/1251 ( 32%)]  Loss: 3.422 (3.45)  Time: 0.306s, 3343.88/s  (0.310s, 3299.01/s)  LR: 5.735e-04  Data: 0.025 (0.028)
Train: 136 [ 450/1251 ( 36%)]  Loss: 3.326 (3.44)  Time: 0.311s, 3292.80/s  (0.310s, 3301.62/s)  LR: 5.733e-04  Data: 0.023 (0.028)
Train: 136 [ 500/1251 ( 40%)]  Loss: 3.328 (3.43)  Time: 0.312s, 3286.45/s  (0.310s, 3303.81/s)  LR: 5.731e-04  Data: 0.023 (0.027)
Train: 136 [ 550/1251 ( 44%)]  Loss: 3.651 (3.45)  Time: 0.306s, 3345.68/s  (0.310s, 3306.29/s)  LR: 5.729e-04  Data: 0.027 (0.027)
Train: 136 [ 600/1251 ( 48%)]  Loss: 3.369 (3.44)  Time: 0.303s, 3379.07/s  (0.310s, 3308.51/s)  LR: 5.727e-04  Data: 0.027 (0.027)
Train: 136 [ 650/1251 ( 52%)]  Loss: 3.775 (3.46)  Time: 0.307s, 3339.52/s  (0.309s, 3310.66/s)  LR: 5.725e-04  Data: 0.024 (0.026)
Train: 136 [ 700/1251 ( 56%)]  Loss: 3.572 (3.47)  Time: 0.310s, 3302.91/s  (0.309s, 3311.83/s)  LR: 5.723e-04  Data: 0.023 (0.026)
Train: 136 [ 750/1251 ( 60%)]  Loss: 3.549 (3.48)  Time: 0.311s, 3290.18/s  (0.309s, 3312.95/s)  LR: 5.721e-04  Data: 0.025 (0.026)
Train: 136 [ 800/1251 ( 64%)]  Loss: 3.672 (3.49)  Time: 0.307s, 3338.98/s  (0.309s, 3313.85/s)  LR: 5.719e-04  Data: 0.023 (0.026)
Train: 136 [ 850/1251 ( 68%)]  Loss: 3.487 (3.49)  Time: 0.311s, 3288.90/s  (0.309s, 3314.50/s)  LR: 5.717e-04  Data: 0.023 (0.026)
Train: 136 [ 900/1251 ( 72%)]  Loss: 3.644 (3.50)  Time: 0.307s, 3339.10/s  (0.309s, 3314.98/s)  LR: 5.715e-04  Data: 0.021 (0.025)
Train: 136 [ 950/1251 ( 76%)]  Loss: 3.349 (3.49)  Time: 0.309s, 3318.84/s  (0.309s, 3316.03/s)  LR: 5.713e-04  Data: 0.025 (0.025)
Train: 136 [1000/1251 ( 80%)]  Loss: 3.591 (3.49)  Time: 0.307s, 3333.26/s  (0.309s, 3317.12/s)  LR: 5.710e-04  Data: 0.027 (0.025)
Train: 136 [1050/1251 ( 84%)]  Loss: 3.704 (3.50)  Time: 0.303s, 3374.24/s  (0.309s, 3318.09/s)  LR: 5.708e-04  Data: 0.023 (0.025)
Train: 136 [1100/1251 ( 88%)]  Loss: 3.734 (3.51)  Time: 0.308s, 3322.25/s  (0.309s, 3318.18/s)  LR: 5.706e-04  Data: 0.021 (0.025)
Train: 136 [1150/1251 ( 92%)]  Loss: 3.510 (3.51)  Time: 0.308s, 3324.55/s  (0.309s, 3318.80/s)  LR: 5.704e-04  Data: 0.021 (0.025)
Train: 136 [1200/1251 ( 96%)]  Loss: 3.577 (3.52)  Time: 0.309s, 3309.19/s  (0.308s, 3319.32/s)  LR: 5.702e-04  Data: 0.027 (0.025)
Train: 136 [1250/1251 (100%)]  Loss: 3.652 (3.52)  Time: 0.275s, 3724.29/s  (0.308s, 3321.69/s)  LR: 5.700e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.042 (2.042)  Loss:  0.6104 (0.6104)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.055 (0.231)  Loss:  0.6855 (1.1706)  Acc@1: 85.0236 (73.5760)  Acc@5: 96.3443 (91.8100)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-133.pth.tar', 73.7820000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-135.pth.tar', 73.68800011474609)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-136.pth.tar', 73.57600008789062)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-134.pth.tar', 73.2620000390625)

Train: 137 [   0/1251 (  0%)]  Loss: 3.466 (3.47)  Time: 2.192s,  467.07/s  (2.192s,  467.07/s)  LR: 5.700e-04  Data: 1.962 (1.962)
Train: 137 [  50/1251 (  4%)]  Loss: 3.734 (3.60)  Time: 0.303s, 3382.60/s  (0.330s, 3100.55/s)  LR: 5.698e-04  Data: 0.024 (0.063)
Train: 137 [ 100/1251 (  8%)]  Loss: 3.420 (3.54)  Time: 0.304s, 3369.11/s  (0.315s, 3250.05/s)  LR: 5.696e-04  Data: 0.023 (0.043)
Train: 137 [ 150/1251 ( 12%)]  Loss: 3.360 (3.49)  Time: 0.303s, 3374.29/s  (0.311s, 3293.92/s)  LR: 5.694e-04  Data: 0.026 (0.037)
Train: 137 [ 200/1251 ( 16%)]  Loss: 3.738 (3.54)  Time: 0.302s, 3385.86/s  (0.309s, 3310.80/s)  LR: 5.692e-04  Data: 0.026 (0.033)
Train: 137 [ 250/1251 ( 20%)]  Loss: 3.405 (3.52)  Time: 0.305s, 3354.97/s  (0.308s, 3319.52/s)  LR: 5.690e-04  Data: 0.023 (0.031)
Train: 137 [ 300/1251 ( 24%)]  Loss: 3.338 (3.49)  Time: 0.305s, 3360.49/s  (0.308s, 3325.35/s)  LR: 5.688e-04  Data: 0.023 (0.030)
Train: 137 [ 350/1251 ( 28%)]  Loss: 3.274 (3.47)  Time: 0.307s, 3338.04/s  (0.308s, 3329.22/s)  LR: 5.686e-04  Data: 0.022 (0.029)
Train: 137 [ 400/1251 ( 32%)]  Loss: 3.237 (3.44)  Time: 0.311s, 3294.45/s  (0.307s, 3331.68/s)  LR: 5.684e-04  Data: 0.026 (0.028)
Train: 137 [ 450/1251 ( 36%)]  Loss: 3.761 (3.47)  Time: 0.309s, 3314.07/s  (0.307s, 3332.98/s)  LR: 5.682e-04  Data: 0.023 (0.028)
Train: 137 [ 500/1251 ( 40%)]  Loss: 3.344 (3.46)  Time: 0.306s, 3346.96/s  (0.307s, 3334.86/s)  LR: 5.680e-04  Data: 0.025 (0.027)
Train: 137 [ 550/1251 ( 44%)]  Loss: 3.254 (3.44)  Time: 0.304s, 3371.14/s  (0.307s, 3335.83/s)  LR: 5.677e-04  Data: 0.022 (0.027)
Train: 137 [ 600/1251 ( 48%)]  Loss: 3.970 (3.48)  Time: 0.309s, 3308.86/s  (0.307s, 3337.10/s)  LR: 5.675e-04  Data: 0.024 (0.027)
Train: 137 [ 650/1251 ( 52%)]  Loss: 3.650 (3.50)  Time: 0.309s, 3313.66/s  (0.307s, 3338.48/s)  LR: 5.673e-04  Data: 0.023 (0.026)
Train: 137 [ 700/1251 ( 56%)]  Loss: 3.841 (3.52)  Time: 0.304s, 3370.82/s  (0.307s, 3339.41/s)  LR: 5.671e-04  Data: 0.022 (0.026)
Train: 137 [ 750/1251 ( 60%)]  Loss: 3.478 (3.52)  Time: 0.303s, 3378.24/s  (0.307s, 3340.24/s)  LR: 5.669e-04  Data: 0.027 (0.026)
Train: 137 [ 800/1251 ( 64%)]  Loss: 3.506 (3.52)  Time: 0.305s, 3362.44/s  (0.306s, 3341.37/s)  LR: 5.667e-04  Data: 0.023 (0.026)
Train: 137 [ 850/1251 ( 68%)]  Loss: 3.690 (3.53)  Time: 0.294s, 3488.33/s  (0.306s, 3343.24/s)  LR: 5.665e-04  Data: 0.020 (0.026)
Train: 137 [ 900/1251 ( 72%)]  Loss: 3.378 (3.52)  Time: 0.308s, 3329.48/s  (0.306s, 3344.25/s)  LR: 5.663e-04  Data: 0.024 (0.026)
Train: 137 [ 950/1251 ( 76%)]  Loss: 3.744 (3.53)  Time: 0.306s, 3350.92/s  (0.306s, 3344.64/s)  LR: 5.661e-04  Data: 0.026 (0.025)
Train: 137 [1000/1251 ( 80%)]  Loss: 3.602 (3.53)  Time: 0.302s, 3390.02/s  (0.306s, 3345.61/s)  LR: 5.659e-04  Data: 0.020 (0.025)
Train: 137 [1050/1251 ( 84%)]  Loss: 3.298 (3.52)  Time: 0.308s, 3322.11/s  (0.306s, 3345.91/s)  LR: 5.657e-04  Data: 0.026 (0.025)
Train: 137 [1100/1251 ( 88%)]  Loss: 3.482 (3.52)  Time: 0.309s, 3317.27/s  (0.306s, 3346.22/s)  LR: 5.655e-04  Data: 0.021 (0.025)
Train: 137 [1150/1251 ( 92%)]  Loss: 3.629 (3.52)  Time: 0.306s, 3349.23/s  (0.306s, 3346.76/s)  LR: 5.653e-04  Data: 0.023 (0.025)
Train: 137 [1200/1251 ( 96%)]  Loss: 3.895 (3.54)  Time: 0.303s, 3375.05/s  (0.306s, 3347.04/s)  LR: 5.651e-04  Data: 0.027 (0.025)
Train: 137 [1250/1251 (100%)]  Loss: 3.492 (3.54)  Time: 0.276s, 3713.70/s  (0.306s, 3349.30/s)  LR: 5.649e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.021 (2.021)  Loss:  0.5903 (0.5903)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.052 (0.234)  Loss:  0.7461 (1.1655)  Acc@1: 84.7877 (73.5920)  Acc@5: 96.1085 (91.7600)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-133.pth.tar', 73.7820000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-135.pth.tar', 73.68800011474609)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-137.pth.tar', 73.5920001147461)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-136.pth.tar', 73.57600008789062)

Train: 138 [   0/1251 (  0%)]  Loss: 3.162 (3.16)  Time: 2.826s,  362.36/s  (2.826s,  362.36/s)  LR: 5.648e-04  Data: 2.617 (2.617)
Train: 138 [  50/1251 (  4%)]  Loss: 3.363 (3.26)  Time: 0.291s, 3516.00/s  (0.329s, 3110.59/s)  LR: 5.646e-04  Data: 0.023 (0.075)
Train: 138 [ 100/1251 (  8%)]  Loss: 3.579 (3.37)  Time: 0.299s, 3423.58/s  (0.314s, 3265.87/s)  LR: 5.644e-04  Data: 0.019 (0.049)
Train: 138 [ 150/1251 ( 12%)]  Loss: 4.032 (3.53)  Time: 0.299s, 3425.37/s  (0.309s, 3313.89/s)  LR: 5.642e-04  Data: 0.020 (0.041)
Train: 138 [ 200/1251 ( 16%)]  Loss: 3.724 (3.57)  Time: 0.303s, 3381.92/s  (0.307s, 3335.15/s)  LR: 5.640e-04  Data: 0.024 (0.036)
Train: 138 [ 250/1251 ( 20%)]  Loss: 3.456 (3.55)  Time: 0.303s, 3374.75/s  (0.306s, 3344.87/s)  LR: 5.638e-04  Data: 0.030 (0.034)
Train: 138 [ 300/1251 ( 24%)]  Loss: 3.567 (3.55)  Time: 0.296s, 3459.62/s  (0.306s, 3350.46/s)  LR: 5.636e-04  Data: 0.022 (0.032)
Train: 138 [ 350/1251 ( 28%)]  Loss: 3.735 (3.58)  Time: 0.302s, 3395.68/s  (0.305s, 3355.28/s)  LR: 5.634e-04  Data: 0.024 (0.031)
Train: 138 [ 400/1251 ( 32%)]  Loss: 3.432 (3.56)  Time: 0.303s, 3384.96/s  (0.305s, 3358.72/s)  LR: 5.632e-04  Data: 0.023 (0.030)
Train: 138 [ 450/1251 ( 36%)]  Loss: 3.756 (3.58)  Time: 0.304s, 3362.89/s  (0.305s, 3360.22/s)  LR: 5.630e-04  Data: 0.023 (0.029)
Train: 138 [ 500/1251 ( 40%)]  Loss: 3.496 (3.57)  Time: 0.303s, 3374.33/s  (0.305s, 3362.42/s)  LR: 5.628e-04  Data: 0.023 (0.029)
Train: 138 [ 550/1251 ( 44%)]  Loss: 3.499 (3.57)  Time: 0.301s, 3398.42/s  (0.304s, 3363.05/s)  LR: 5.626e-04  Data: 0.026 (0.028)
Train: 138 [ 600/1251 ( 48%)]  Loss: 3.442 (3.56)  Time: 0.303s, 3376.50/s  (0.304s, 3363.02/s)  LR: 5.624e-04  Data: 0.028 (0.028)
Train: 138 [ 650/1251 ( 52%)]  Loss: 3.223 (3.53)  Time: 0.306s, 3351.40/s  (0.305s, 3362.69/s)  LR: 5.622e-04  Data: 0.020 (0.027)
Train: 138 [ 700/1251 ( 56%)]  Loss: 3.754 (3.55)  Time: 0.305s, 3357.33/s  (0.304s, 3363.66/s)  LR: 5.620e-04  Data: 0.023 (0.027)
Train: 138 [ 750/1251 ( 60%)]  Loss: 3.586 (3.55)  Time: 0.303s, 3383.31/s  (0.304s, 3364.06/s)  LR: 5.617e-04  Data: 0.023 (0.027)
Train: 138 [ 800/1251 ( 64%)]  Loss: 3.641 (3.56)  Time: 0.300s, 3408.33/s  (0.304s, 3365.03/s)  LR: 5.615e-04  Data: 0.022 (0.027)
Train: 138 [ 850/1251 ( 68%)]  Loss: 3.846 (3.57)  Time: 0.305s, 3357.95/s  (0.304s, 3365.15/s)  LR: 5.613e-04  Data: 0.024 (0.026)
Train: 138 [ 900/1251 ( 72%)]  Loss: 3.280 (3.56)  Time: 0.305s, 3351.88/s  (0.304s, 3365.55/s)  LR: 5.611e-04  Data: 0.025 (0.026)
Train: 138 [ 950/1251 ( 76%)]  Loss: 3.271 (3.54)  Time: 0.302s, 3386.36/s  (0.304s, 3365.95/s)  LR: 5.609e-04  Data: 0.023 (0.026)
Train: 138 [1000/1251 ( 80%)]  Loss: 3.395 (3.54)  Time: 0.306s, 3348.52/s  (0.304s, 3366.69/s)  LR: 5.607e-04  Data: 0.026 (0.026)
Train: 138 [1050/1251 ( 84%)]  Loss: 3.411 (3.53)  Time: 0.307s, 3337.23/s  (0.304s, 3367.46/s)  LR: 5.605e-04  Data: 0.024 (0.026)
Train: 138 [1100/1251 ( 88%)]  Loss: 3.282 (3.52)  Time: 0.303s, 3381.47/s  (0.304s, 3367.84/s)  LR: 5.603e-04  Data: 0.021 (0.026)
Train: 138 [1150/1251 ( 92%)]  Loss: 3.497 (3.52)  Time: 0.309s, 3313.34/s  (0.304s, 3368.11/s)  LR: 5.601e-04  Data: 0.024 (0.026)
Train: 138 [1200/1251 ( 96%)]  Loss: 3.816 (3.53)  Time: 0.301s, 3400.89/s  (0.304s, 3368.38/s)  LR: 5.599e-04  Data: 0.023 (0.026)
Train: 138 [1250/1251 (100%)]  Loss: 3.534 (3.53)  Time: 0.276s, 3716.62/s  (0.304s, 3371.07/s)  LR: 5.597e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.161 (2.161)  Loss:  0.6396 (0.6396)  Acc@1: 87.1094 (87.1094)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.041 (0.239)  Loss:  0.7261 (1.1559)  Acc@1: 83.8443 (73.7300)  Acc@5: 96.2264 (91.8380)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-133.pth.tar', 73.7820000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-138.pth.tar', 73.72999983398438)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-135.pth.tar', 73.68800011474609)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-137.pth.tar', 73.5920001147461)

Train: 139 [   0/1251 (  0%)]  Loss: 3.764 (3.76)  Time: 2.440s,  419.76/s  (2.440s,  419.76/s)  LR: 5.597e-04  Data: 2.207 (2.207)
Train: 139 [  50/1251 (  4%)]  Loss: 3.578 (3.67)  Time: 0.297s, 3449.39/s  (0.328s, 3121.00/s)  LR: 5.595e-04  Data: 0.021 (0.066)
Train: 139 [ 100/1251 (  8%)]  Loss: 3.719 (3.69)  Time: 0.294s, 3481.51/s  (0.312s, 3280.43/s)  LR: 5.593e-04  Data: 0.022 (0.045)
Train: 139 [ 150/1251 ( 12%)]  Loss: 3.373 (3.61)  Time: 0.301s, 3401.77/s  (0.308s, 3326.13/s)  LR: 5.591e-04  Data: 0.026 (0.038)
Train: 139 [ 200/1251 ( 16%)]  Loss: 3.684 (3.62)  Time: 0.300s, 3412.78/s  (0.306s, 3348.12/s)  LR: 5.588e-04  Data: 0.026 (0.034)
Train: 139 [ 250/1251 ( 20%)]  Loss: 3.358 (3.58)  Time: 0.303s, 3376.79/s  (0.305s, 3362.77/s)  LR: 5.586e-04  Data: 0.020 (0.032)
Train: 139 [ 300/1251 ( 24%)]  Loss: 3.339 (3.54)  Time: 0.301s, 3402.06/s  (0.304s, 3369.63/s)  LR: 5.584e-04  Data: 0.021 (0.031)
Train: 139 [ 350/1251 ( 28%)]  Loss: 3.523 (3.54)  Time: 0.303s, 3374.03/s  (0.303s, 3375.51/s)  LR: 5.582e-04  Data: 0.027 (0.030)
Train: 139 [ 400/1251 ( 32%)]  Loss: 3.247 (3.51)  Time: 0.302s, 3385.58/s  (0.303s, 3380.27/s)  LR: 5.580e-04  Data: 0.023 (0.029)
Train: 139 [ 450/1251 ( 36%)]  Loss: 3.589 (3.52)  Time: 0.308s, 3327.17/s  (0.303s, 3382.22/s)  LR: 5.578e-04  Data: 0.021 (0.028)
Train: 139 [ 500/1251 ( 40%)]  Loss: 3.586 (3.52)  Time: 0.301s, 3400.68/s  (0.303s, 3383.67/s)  LR: 5.576e-04  Data: 0.023 (0.028)
Train: 139 [ 550/1251 ( 44%)]  Loss: 3.400 (3.51)  Time: 0.300s, 3412.20/s  (0.302s, 3385.47/s)  LR: 5.574e-04  Data: 0.026 (0.027)
Train: 139 [ 600/1251 ( 48%)]  Loss: 3.562 (3.52)  Time: 0.299s, 3429.96/s  (0.302s, 3387.26/s)  LR: 5.572e-04  Data: 0.025 (0.027)
Train: 139 [ 650/1251 ( 52%)]  Loss: 3.375 (3.51)  Time: 0.299s, 3420.78/s  (0.302s, 3388.64/s)  LR: 5.570e-04  Data: 0.024 (0.027)
Train: 139 [ 700/1251 ( 56%)]  Loss: 3.545 (3.51)  Time: 0.306s, 3344.03/s  (0.302s, 3389.19/s)  LR: 5.568e-04  Data: 0.026 (0.026)
Train: 139 [ 750/1251 ( 60%)]  Loss: 3.461 (3.51)  Time: 0.303s, 3376.23/s  (0.302s, 3389.54/s)  LR: 5.566e-04  Data: 0.019 (0.026)
Train: 139 [ 800/1251 ( 64%)]  Loss: 3.695 (3.52)  Time: 0.303s, 3379.14/s  (0.302s, 3389.39/s)  LR: 5.564e-04  Data: 0.023 (0.026)
Train: 139 [ 850/1251 ( 68%)]  Loss: 3.652 (3.52)  Time: 0.310s, 3308.23/s  (0.302s, 3389.21/s)  LR: 5.562e-04  Data: 0.022 (0.026)
Train: 139 [ 900/1251 ( 72%)]  Loss: 3.475 (3.52)  Time: 0.305s, 3358.30/s  (0.302s, 3389.17/s)  LR: 5.560e-04  Data: 0.024 (0.026)
Train: 139 [ 950/1251 ( 76%)]  Loss: 3.653 (3.53)  Time: 0.301s, 3396.93/s  (0.302s, 3389.16/s)  LR: 5.557e-04  Data: 0.022 (0.026)
Train: 139 [1000/1251 ( 80%)]  Loss: 3.496 (3.53)  Time: 0.305s, 3359.22/s  (0.302s, 3389.33/s)  LR: 5.555e-04  Data: 0.022 (0.026)
Train: 139 [1050/1251 ( 84%)]  Loss: 3.125 (3.51)  Time: 0.304s, 3368.82/s  (0.302s, 3389.10/s)  LR: 5.553e-04  Data: 0.023 (0.025)
Train: 139 [1100/1251 ( 88%)]  Loss: 3.850 (3.52)  Time: 0.304s, 3373.54/s  (0.302s, 3389.43/s)  LR: 5.551e-04  Data: 0.024 (0.025)
Train: 139 [1150/1251 ( 92%)]  Loss: 3.657 (3.53)  Time: 0.306s, 3350.68/s  (0.302s, 3389.62/s)  LR: 5.549e-04  Data: 0.023 (0.025)
Train: 139 [1200/1251 ( 96%)]  Loss: 3.206 (3.52)  Time: 0.299s, 3428.05/s  (0.302s, 3390.13/s)  LR: 5.547e-04  Data: 0.023 (0.025)
Train: 139 [1250/1251 (100%)]  Loss: 3.751 (3.53)  Time: 0.276s, 3712.14/s  (0.302s, 3392.36/s)  LR: 5.545e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.086 (2.086)  Loss:  0.6240 (0.6240)  Acc@1: 87.5977 (87.5977)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.056 (0.234)  Loss:  0.7168 (1.1844)  Acc@1: 85.4953 (73.5800)  Acc@5: 96.4623 (91.8420)
Train: 140 [   0/1251 (  0%)]  Loss: 3.536 (3.54)  Time: 2.605s,  393.08/s  (2.605s,  393.08/s)  LR: 5.545e-04  Data: 2.380 (2.380)
Train: 140 [  50/1251 (  4%)]  Loss: 3.399 (3.47)  Time: 0.294s, 3488.60/s  (0.328s, 3123.53/s)  LR: 5.543e-04  Data: 0.022 (0.070)
Train: 140 [ 100/1251 (  8%)]  Loss: 3.592 (3.51)  Time: 0.298s, 3441.86/s  (0.312s, 3282.88/s)  LR: 5.541e-04  Data: 0.023 (0.047)
Train: 140 [ 150/1251 ( 12%)]  Loss: 3.568 (3.52)  Time: 0.296s, 3463.74/s  (0.307s, 3338.14/s)  LR: 5.539e-04  Data: 0.023 (0.039)
Train: 140 [ 200/1251 ( 16%)]  Loss: 3.250 (3.47)  Time: 0.301s, 3400.17/s  (0.305s, 3361.01/s)  LR: 5.537e-04  Data: 0.023 (0.035)
Train: 140 [ 250/1251 ( 20%)]  Loss: 3.386 (3.45)  Time: 0.301s, 3404.01/s  (0.303s, 3375.77/s)  LR: 5.535e-04  Data: 0.020 (0.033)
Train: 140 [ 300/1251 ( 24%)]  Loss: 3.432 (3.45)  Time: 0.298s, 3438.35/s  (0.302s, 3385.69/s)  LR: 5.533e-04  Data: 0.024 (0.031)
Train: 140 [ 350/1251 ( 28%)]  Loss: 3.399 (3.45)  Time: 0.299s, 3428.84/s  (0.302s, 3393.25/s)  LR: 5.530e-04  Data: 0.025 (0.030)
Train: 140 [ 400/1251 ( 32%)]  Loss: 3.592 (3.46)  Time: 0.299s, 3421.70/s  (0.301s, 3399.35/s)  LR: 5.528e-04  Data: 0.020 (0.029)
Train: 140 [ 450/1251 ( 36%)]  Loss: 3.399 (3.46)  Time: 0.301s, 3401.31/s  (0.301s, 3404.43/s)  LR: 5.526e-04  Data: 0.022 (0.029)
Train: 140 [ 500/1251 ( 40%)]  Loss: 3.674 (3.48)  Time: 0.301s, 3401.23/s  (0.301s, 3405.83/s)  LR: 5.524e-04  Data: 0.023 (0.028)
Train: 140 [ 550/1251 ( 44%)]  Loss: 3.526 (3.48)  Time: 0.303s, 3374.91/s  (0.301s, 3406.18/s)  LR: 5.522e-04  Data: 0.026 (0.028)
Train: 140 [ 600/1251 ( 48%)]  Loss: 3.357 (3.47)  Time: 0.297s, 3442.18/s  (0.301s, 3406.32/s)  LR: 5.520e-04  Data: 0.022 (0.027)
Train: 140 [ 650/1251 ( 52%)]  Loss: 3.047 (3.44)  Time: 0.310s, 3307.80/s  (0.301s, 3406.11/s)  LR: 5.518e-04  Data: 0.024 (0.027)
Train: 140 [ 700/1251 ( 56%)]  Loss: 3.472 (3.44)  Time: 0.305s, 3355.41/s  (0.301s, 3406.52/s)  LR: 5.516e-04  Data: 0.028 (0.027)
Train: 140 [ 750/1251 ( 60%)]  Loss: 3.641 (3.45)  Time: 0.301s, 3404.03/s  (0.301s, 3406.92/s)  LR: 5.514e-04  Data: 0.021 (0.027)
Train: 140 [ 800/1251 ( 64%)]  Loss: 3.401 (3.45)  Time: 0.291s, 3517.01/s  (0.300s, 3407.84/s)  LR: 5.512e-04  Data: 0.022 (0.026)
Train: 140 [ 850/1251 ( 68%)]  Loss: 3.716 (3.47)  Time: 0.300s, 3415.14/s  (0.300s, 3408.53/s)  LR: 5.510e-04  Data: 0.022 (0.026)
Train: 140 [ 900/1251 ( 72%)]  Loss: 3.959 (3.49)  Time: 0.302s, 3390.50/s  (0.300s, 3409.15/s)  LR: 5.508e-04  Data: 0.021 (0.026)
Train: 140 [ 950/1251 ( 76%)]  Loss: 3.496 (3.49)  Time: 0.299s, 3421.42/s  (0.300s, 3409.46/s)  LR: 5.506e-04  Data: 0.024 (0.026)
Train: 140 [1000/1251 ( 80%)]  Loss: 3.386 (3.49)  Time: 0.301s, 3398.59/s  (0.300s, 3409.85/s)  LR: 5.504e-04  Data: 0.026 (0.026)
Train: 140 [1050/1251 ( 84%)]  Loss: 3.686 (3.50)  Time: 0.293s, 3492.77/s  (0.300s, 3410.19/s)  LR: 5.501e-04  Data: 0.022 (0.026)
Train: 140 [1100/1251 ( 88%)]  Loss: 3.694 (3.50)  Time: 0.305s, 3356.01/s  (0.300s, 3410.73/s)  LR: 5.499e-04  Data: 0.028 (0.026)
Train: 140 [1150/1251 ( 92%)]  Loss: 3.485 (3.50)  Time: 0.305s, 3361.42/s  (0.300s, 3410.95/s)  LR: 5.497e-04  Data: 0.024 (0.026)
Train: 140 [1200/1251 ( 96%)]  Loss: 3.565 (3.51)  Time: 0.298s, 3434.25/s  (0.300s, 3411.00/s)  LR: 5.495e-04  Data: 0.021 (0.026)
Train: 140 [1250/1251 (100%)]  Loss: 3.787 (3.52)  Time: 0.277s, 3690.97/s  (0.300s, 3412.97/s)  LR: 5.493e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.399 (2.399)  Loss:  0.6470 (0.6470)  Acc@1: 87.4023 (87.4023)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.044 (0.236)  Loss:  0.6890 (1.1619)  Acc@1: 85.7311 (74.0020)  Acc@5: 96.2264 (92.0080)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-140.pth.tar', 74.00200000732421)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-133.pth.tar', 73.7820000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-138.pth.tar', 73.72999983398438)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-135.pth.tar', 73.68800011474609)

Train: 141 [   0/1251 (  0%)]  Loss: 3.606 (3.61)  Time: 2.194s,  466.65/s  (2.194s,  466.65/s)  LR: 5.493e-04  Data: 1.972 (1.972)
Train: 141 [  50/1251 (  4%)]  Loss: 3.872 (3.74)  Time: 0.293s, 3489.42/s  (0.331s, 3090.41/s)  LR: 5.491e-04  Data: 0.022 (0.072)
Train: 141 [ 100/1251 (  8%)]  Loss: 3.816 (3.76)  Time: 0.294s, 3479.28/s  (0.313s, 3270.24/s)  LR: 5.489e-04  Data: 0.026 (0.048)
Train: 141 [ 150/1251 ( 12%)]  Loss: 3.853 (3.79)  Time: 0.297s, 3442.81/s  (0.307s, 3330.38/s)  LR: 5.487e-04  Data: 0.022 (0.040)
Train: 141 [ 200/1251 ( 16%)]  Loss: 3.669 (3.76)  Time: 0.297s, 3446.16/s  (0.305s, 3358.94/s)  LR: 5.485e-04  Data: 0.021 (0.036)
Train: 141 [ 250/1251 ( 20%)]  Loss: 3.660 (3.75)  Time: 0.292s, 3512.07/s  (0.303s, 3376.92/s)  LR: 5.483e-04  Data: 0.024 (0.033)
Train: 141 [ 300/1251 ( 24%)]  Loss: 3.507 (3.71)  Time: 0.302s, 3394.67/s  (0.302s, 3387.27/s)  LR: 5.481e-04  Data: 0.026 (0.032)
Train: 141 [ 350/1251 ( 28%)]  Loss: 3.501 (3.69)  Time: 0.293s, 3492.71/s  (0.302s, 3395.84/s)  LR: 5.479e-04  Data: 0.019 (0.031)
Train: 141 [ 400/1251 ( 32%)]  Loss: 3.682 (3.68)  Time: 0.292s, 3508.76/s  (0.301s, 3400.99/s)  LR: 5.477e-04  Data: 0.022 (0.030)
Train: 141 [ 450/1251 ( 36%)]  Loss: 3.630 (3.68)  Time: 0.304s, 3370.67/s  (0.301s, 3404.36/s)  LR: 5.474e-04  Data: 0.029 (0.029)
Train: 141 [ 500/1251 ( 40%)]  Loss: 3.948 (3.70)  Time: 0.302s, 3389.84/s  (0.300s, 3407.71/s)  LR: 5.472e-04  Data: 0.026 (0.029)
Train: 141 [ 550/1251 ( 44%)]  Loss: 3.204 (3.66)  Time: 0.302s, 3391.21/s  (0.300s, 3409.59/s)  LR: 5.470e-04  Data: 0.023 (0.028)
Train: 141 [ 600/1251 ( 48%)]  Loss: 3.556 (3.65)  Time: 0.296s, 3455.80/s  (0.300s, 3411.40/s)  LR: 5.468e-04  Data: 0.021 (0.028)
Train: 141 [ 650/1251 ( 52%)]  Loss: 3.542 (3.65)  Time: 0.298s, 3434.44/s  (0.300s, 3413.03/s)  LR: 5.466e-04  Data: 0.022 (0.027)
Train: 141 [ 700/1251 ( 56%)]  Loss: 3.630 (3.65)  Time: 0.300s, 3414.05/s  (0.300s, 3414.07/s)  LR: 5.464e-04  Data: 0.023 (0.027)
Train: 141 [ 750/1251 ( 60%)]  Loss: 3.195 (3.62)  Time: 0.298s, 3441.71/s  (0.300s, 3414.68/s)  LR: 5.462e-04  Data: 0.023 (0.027)
Train: 141 [ 800/1251 ( 64%)]  Loss: 3.487 (3.61)  Time: 0.300s, 3416.08/s  (0.300s, 3415.33/s)  LR: 5.460e-04  Data: 0.023 (0.027)
Train: 141 [ 850/1251 ( 68%)]  Loss: 3.737 (3.62)  Time: 0.300s, 3414.82/s  (0.300s, 3416.69/s)  LR: 5.458e-04  Data: 0.023 (0.026)
Train: 141 [ 900/1251 ( 72%)]  Loss: 3.316 (3.60)  Time: 0.297s, 3448.78/s  (0.300s, 3416.91/s)  LR: 5.456e-04  Data: 0.021 (0.026)
Train: 141 [ 950/1251 ( 76%)]  Loss: 3.405 (3.59)  Time: 0.298s, 3437.88/s  (0.300s, 3417.20/s)  LR: 5.454e-04  Data: 0.023 (0.026)
Train: 141 [1000/1251 ( 80%)]  Loss: 3.121 (3.57)  Time: 0.302s, 3385.27/s  (0.300s, 3417.47/s)  LR: 5.452e-04  Data: 0.024 (0.026)
Train: 141 [1050/1251 ( 84%)]  Loss: 3.187 (3.55)  Time: 0.295s, 3467.05/s  (0.300s, 3417.99/s)  LR: 5.450e-04  Data: 0.021 (0.026)
Train: 141 [1100/1251 ( 88%)]  Loss: 3.447 (3.55)  Time: 0.300s, 3417.46/s  (0.300s, 3418.07/s)  LR: 5.448e-04  Data: 0.022 (0.026)
Train: 141 [1150/1251 ( 92%)]  Loss: 3.656 (3.55)  Time: 0.301s, 3401.86/s  (0.300s, 3418.00/s)  LR: 5.445e-04  Data: 0.024 (0.026)
Train: 141 [1200/1251 ( 96%)]  Loss: 3.658 (3.56)  Time: 0.295s, 3467.45/s  (0.300s, 3418.09/s)  LR: 5.443e-04  Data: 0.023 (0.026)
Train: 141 [1250/1251 (100%)]  Loss: 3.600 (3.56)  Time: 0.275s, 3716.89/s  (0.299s, 3420.29/s)  LR: 5.441e-04  Data: 0.000 (0.026)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.023 (2.023)  Loss:  0.5781 (0.5781)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.050 (0.231)  Loss:  0.6909 (1.1556)  Acc@1: 86.0849 (73.7480)  Acc@5: 96.5802 (91.9360)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-140.pth.tar', 74.00200000732421)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-133.pth.tar', 73.7820000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-141.pth.tar', 73.74800016113281)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-138.pth.tar', 73.72999983398438)

Train: 142 [   0/1251 (  0%)]  Loss: 3.362 (3.36)  Time: 2.306s,  444.07/s  (2.306s,  444.07/s)  LR: 5.441e-04  Data: 2.087 (2.087)
Train: 142 [  50/1251 (  4%)]  Loss: 3.782 (3.57)  Time: 0.292s, 3508.40/s  (0.320s, 3203.48/s)  LR: 5.439e-04  Data: 0.023 (0.066)
Train: 142 [ 100/1251 (  8%)]  Loss: 3.963 (3.70)  Time: 0.287s, 3567.91/s  (0.305s, 3353.17/s)  LR: 5.437e-04  Data: 0.023 (0.045)
Train: 142 [ 150/1251 ( 12%)]  Loss: 3.583 (3.67)  Time: 0.289s, 3541.89/s  (0.301s, 3399.33/s)  LR: 5.435e-04  Data: 0.021 (0.038)
Train: 142 [ 200/1251 ( 16%)]  Loss: 3.414 (3.62)  Time: 0.292s, 3504.47/s  (0.299s, 3420.50/s)  LR: 5.433e-04  Data: 0.027 (0.034)
Train: 142 [ 250/1251 ( 20%)]  Loss: 3.613 (3.62)  Time: 0.298s, 3437.55/s  (0.298s, 3432.41/s)  LR: 5.431e-04  Data: 0.026 (0.032)
Train: 142 [ 300/1251 ( 24%)]  Loss: 3.390 (3.59)  Time: 0.304s, 3370.16/s  (0.298s, 3441.17/s)  LR: 5.429e-04  Data: 0.024 (0.031)
Train: 142 [ 350/1251 ( 28%)]  Loss: 3.573 (3.59)  Time: 0.299s, 3419.48/s  (0.297s, 3445.34/s)  LR: 5.427e-04  Data: 0.026 (0.030)
Train: 142 [ 400/1251 ( 32%)]  Loss: 3.739 (3.60)  Time: 0.298s, 3440.76/s  (0.297s, 3448.23/s)  LR: 5.425e-04  Data: 0.022 (0.029)
Train: 142 [ 450/1251 ( 36%)]  Loss: 3.309 (3.57)  Time: 0.299s, 3424.00/s  (0.297s, 3450.20/s)  LR: 5.423e-04  Data: 0.025 (0.028)
Train: 142 [ 500/1251 ( 40%)]  Loss: 3.475 (3.56)  Time: 0.306s, 3348.07/s  (0.297s, 3451.24/s)  LR: 5.421e-04  Data: 0.022 (0.028)
Train: 142 [ 550/1251 ( 44%)]  Loss: 3.135 (3.53)  Time: 0.302s, 3388.56/s  (0.297s, 3450.55/s)  LR: 5.418e-04  Data: 0.020 (0.027)
Train: 142 [ 600/1251 ( 48%)]  Loss: 3.729 (3.54)  Time: 0.294s, 3477.18/s  (0.297s, 3450.35/s)  LR: 5.416e-04  Data: 0.021 (0.027)
Train: 142 [ 650/1251 ( 52%)]  Loss: 3.561 (3.54)  Time: 0.293s, 3499.49/s  (0.297s, 3450.24/s)  LR: 5.414e-04  Data: 0.024 (0.027)
Train: 142 [ 700/1251 ( 56%)]  Loss: 3.580 (3.55)  Time: 0.302s, 3391.45/s  (0.297s, 3450.64/s)  LR: 5.412e-04  Data: 0.024 (0.026)
Train: 142 [ 750/1251 ( 60%)]  Loss: 3.273 (3.53)  Time: 0.307s, 3337.64/s  (0.297s, 3449.74/s)  LR: 5.410e-04  Data: 0.026 (0.026)
Train: 142 [ 800/1251 ( 64%)]  Loss: 3.650 (3.54)  Time: 0.296s, 3456.71/s  (0.297s, 3449.62/s)  LR: 5.408e-04  Data: 0.024 (0.026)
Train: 142 [ 850/1251 ( 68%)]  Loss: 3.541 (3.54)  Time: 0.296s, 3464.05/s  (0.297s, 3448.94/s)  LR: 5.406e-04  Data: 0.025 (0.026)
Train: 142 [ 900/1251 ( 72%)]  Loss: 3.733 (3.55)  Time: 0.301s, 3405.46/s  (0.297s, 3448.02/s)  LR: 5.404e-04  Data: 0.025 (0.026)
Train: 142 [ 950/1251 ( 76%)]  Loss: 3.757 (3.56)  Time: 0.303s, 3380.53/s  (0.297s, 3447.15/s)  LR: 5.402e-04  Data: 0.022 (0.026)
Train: 142 [1000/1251 ( 80%)]  Loss: 3.578 (3.56)  Time: 0.299s, 3419.80/s  (0.297s, 3446.18/s)  LR: 5.400e-04  Data: 0.027 (0.026)
Train: 142 [1050/1251 ( 84%)]  Loss: 3.403 (3.55)  Time: 0.301s, 3403.71/s  (0.297s, 3445.23/s)  LR: 5.398e-04  Data: 0.022 (0.025)
Train: 142 [1100/1251 ( 88%)]  Loss: 3.598 (3.55)  Time: 0.303s, 3384.94/s  (0.297s, 3444.57/s)  LR: 5.396e-04  Data: 0.023 (0.025)
Train: 142 [1150/1251 ( 92%)]  Loss: 3.204 (3.54)  Time: 0.293s, 3490.83/s  (0.297s, 3443.38/s)  LR: 5.394e-04  Data: 0.021 (0.025)
Train: 142 [1200/1251 ( 96%)]  Loss: 3.961 (3.56)  Time: 0.297s, 3453.18/s  (0.297s, 3442.53/s)  LR: 5.391e-04  Data: 0.025 (0.025)
Train: 142 [1250/1251 (100%)]  Loss: 3.303 (3.55)  Time: 0.275s, 3719.12/s  (0.297s, 3443.74/s)  LR: 5.389e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.023 (2.023)  Loss:  0.6270 (0.6270)  Acc@1: 87.6953 (87.6953)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.055 (0.234)  Loss:  0.6968 (1.1403)  Acc@1: 84.0802 (73.8620)  Acc@5: 95.9906 (92.0280)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-140.pth.tar', 74.00200000732421)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-142.pth.tar', 73.86200006591797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-133.pth.tar', 73.7820000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-141.pth.tar', 73.74800016113281)

Train: 143 [   0/1251 (  0%)]  Loss: 3.732 (3.73)  Time: 2.066s,  495.56/s  (2.066s,  495.56/s)  LR: 5.389e-04  Data: 1.848 (1.848)
Train: 143 [  50/1251 (  4%)]  Loss: 3.416 (3.57)  Time: 0.298s, 3430.61/s  (0.327s, 3127.49/s)  LR: 5.387e-04  Data: 0.027 (0.061)
Train: 143 [ 100/1251 (  8%)]  Loss: 3.153 (3.43)  Time: 0.306s, 3349.69/s  (0.313s, 3276.72/s)  LR: 5.385e-04  Data: 0.024 (0.042)
Train: 143 [ 150/1251 ( 12%)]  Loss: 3.717 (3.50)  Time: 0.293s, 3489.40/s  (0.308s, 3329.46/s)  LR: 5.383e-04  Data: 0.022 (0.036)
Train: 143 [ 200/1251 ( 16%)]  Loss: 3.428 (3.49)  Time: 0.296s, 3456.84/s  (0.305s, 3356.20/s)  LR: 5.381e-04  Data: 0.026 (0.033)
Train: 143 [ 250/1251 ( 20%)]  Loss: 3.395 (3.47)  Time: 0.305s, 3359.87/s  (0.304s, 3368.46/s)  LR: 5.379e-04  Data: 0.025 (0.031)
Train: 143 [ 300/1251 ( 24%)]  Loss: 3.528 (3.48)  Time: 0.308s, 3324.19/s  (0.303s, 3379.32/s)  LR: 5.377e-04  Data: 0.019 (0.030)
Train: 143 [ 350/1251 ( 28%)]  Loss: 3.443 (3.48)  Time: 0.297s, 3445.84/s  (0.302s, 3386.97/s)  LR: 5.375e-04  Data: 0.024 (0.029)
Train: 143 [ 400/1251 ( 32%)]  Loss: 3.706 (3.50)  Time: 0.308s, 3328.25/s  (0.302s, 3392.34/s)  LR: 5.373e-04  Data: 0.023 (0.028)
Train: 143 [ 450/1251 ( 36%)]  Loss: 3.683 (3.52)  Time: 0.298s, 3440.35/s  (0.301s, 3397.89/s)  LR: 5.371e-04  Data: 0.026 (0.028)
Train: 143 [ 500/1251 ( 40%)]  Loss: 3.710 (3.54)  Time: 0.302s, 3389.32/s  (0.301s, 3400.82/s)  LR: 5.369e-04  Data: 0.026 (0.027)
Train: 143 [ 550/1251 ( 44%)]  Loss: 3.558 (3.54)  Time: 0.300s, 3414.72/s  (0.301s, 3404.46/s)  LR: 5.366e-04  Data: 0.026 (0.027)
Train: 143 [ 600/1251 ( 48%)]  Loss: 3.604 (3.54)  Time: 0.297s, 3446.38/s  (0.301s, 3406.10/s)  LR: 5.364e-04  Data: 0.022 (0.027)
Train: 143 [ 650/1251 ( 52%)]  Loss: 3.572 (3.55)  Time: 0.298s, 3437.35/s  (0.300s, 3407.68/s)  LR: 5.362e-04  Data: 0.023 (0.026)
Train: 143 [ 700/1251 ( 56%)]  Loss: 3.319 (3.53)  Time: 0.298s, 3436.87/s  (0.300s, 3409.85/s)  LR: 5.360e-04  Data: 0.021 (0.026)
Train: 143 [ 750/1251 ( 60%)]  Loss: 3.661 (3.54)  Time: 0.300s, 3413.02/s  (0.300s, 3410.02/s)  LR: 5.358e-04  Data: 0.022 (0.026)
Train: 143 [ 800/1251 ( 64%)]  Loss: 3.304 (3.53)  Time: 0.303s, 3376.95/s  (0.300s, 3410.03/s)  LR: 5.356e-04  Data: 0.024 (0.026)
Train: 143 [ 850/1251 ( 68%)]  Loss: 3.303 (3.51)  Time: 0.304s, 3373.03/s  (0.300s, 3409.73/s)  LR: 5.354e-04  Data: 0.021 (0.026)
Train: 143 [ 900/1251 ( 72%)]  Loss: 3.607 (3.52)  Time: 0.302s, 3387.45/s  (0.300s, 3409.86/s)  LR: 5.352e-04  Data: 0.026 (0.026)
Train: 143 [ 950/1251 ( 76%)]  Loss: 3.368 (3.51)  Time: 0.303s, 3379.78/s  (0.300s, 3410.40/s)  LR: 5.350e-04  Data: 0.020 (0.025)
Train: 143 [1000/1251 ( 80%)]  Loss: 3.327 (3.50)  Time: 0.298s, 3433.77/s  (0.300s, 3410.60/s)  LR: 5.348e-04  Data: 0.023 (0.025)
Train: 143 [1050/1251 ( 84%)]  Loss: 3.699 (3.51)  Time: 0.296s, 3464.24/s  (0.300s, 3411.02/s)  LR: 5.346e-04  Data: 0.023 (0.025)
Train: 143 [1100/1251 ( 88%)]  Loss: 3.302 (3.50)  Time: 0.301s, 3404.55/s  (0.300s, 3411.47/s)  LR: 5.344e-04  Data: 0.028 (0.025)
Train: 143 [1150/1251 ( 92%)]  Loss: 3.564 (3.50)  Time: 0.297s, 3442.22/s  (0.300s, 3411.83/s)  LR: 5.342e-04  Data: 0.020 (0.025)
Train: 143 [1200/1251 ( 96%)]  Loss: 3.559 (3.51)  Time: 0.298s, 3440.52/s  (0.300s, 3412.06/s)  LR: 5.339e-04  Data: 0.024 (0.025)
Train: 143 [1250/1251 (100%)]  Loss: 3.610 (3.51)  Time: 0.277s, 3693.67/s  (0.300s, 3414.13/s)  LR: 5.337e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.004 (2.004)  Loss:  0.6421 (0.6421)  Acc@1: 87.5000 (87.5000)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.049 (0.237)  Loss:  0.6841 (1.1546)  Acc@1: 84.5519 (73.9480)  Acc@5: 96.3443 (91.9700)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-140.pth.tar', 74.00200000732421)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-143.pth.tar', 73.94800001220703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-142.pth.tar', 73.86200006591797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-133.pth.tar', 73.7820000366211)

Train: 144 [   0/1251 (  0%)]  Loss: 3.560 (3.56)  Time: 2.547s,  402.11/s  (2.547s,  402.11/s)  LR: 5.337e-04  Data: 2.329 (2.329)
Train: 144 [  50/1251 (  4%)]  Loss: 3.370 (3.47)  Time: 0.286s, 3581.60/s  (0.323s, 3174.27/s)  LR: 5.335e-04  Data: 0.025 (0.069)
Train: 144 [ 100/1251 (  8%)]  Loss: 3.309 (3.41)  Time: 0.295s, 3467.94/s  (0.308s, 3324.90/s)  LR: 5.333e-04  Data: 0.023 (0.047)
Train: 144 [ 150/1251 ( 12%)]  Loss: 3.532 (3.44)  Time: 0.291s, 3524.49/s  (0.304s, 3365.66/s)  LR: 5.331e-04  Data: 0.022 (0.039)
Train: 144 [ 200/1251 ( 16%)]  Loss: 3.714 (3.50)  Time: 0.293s, 3494.86/s  (0.303s, 3383.63/s)  LR: 5.329e-04  Data: 0.026 (0.035)
Train: 144 [ 250/1251 ( 20%)]  Loss: 3.104 (3.43)  Time: 0.298s, 3435.10/s  (0.302s, 3392.10/s)  LR: 5.327e-04  Data: 0.023 (0.033)
Train: 144 [ 300/1251 ( 24%)]  Loss: 3.331 (3.42)  Time: 0.299s, 3423.84/s  (0.301s, 3396.67/s)  LR: 5.325e-04  Data: 0.021 (0.031)
Train: 144 [ 350/1251 ( 28%)]  Loss: 3.635 (3.44)  Time: 0.302s, 3395.30/s  (0.301s, 3399.46/s)  LR: 5.323e-04  Data: 0.024 (0.030)
Train: 144 [ 400/1251 ( 32%)]  Loss: 3.603 (3.46)  Time: 0.301s, 3400.35/s  (0.301s, 3401.70/s)  LR: 5.321e-04  Data: 0.023 (0.029)
Train: 144 [ 450/1251 ( 36%)]  Loss: 3.556 (3.47)  Time: 0.300s, 3417.08/s  (0.301s, 3401.60/s)  LR: 5.319e-04  Data: 0.023 (0.029)
Train: 144 [ 500/1251 ( 40%)]  Loss: 3.424 (3.47)  Time: 0.306s, 3351.47/s  (0.301s, 3401.45/s)  LR: 5.317e-04  Data: 0.024 (0.028)
Train: 144 [ 550/1251 ( 44%)]  Loss: 3.477 (3.47)  Time: 0.305s, 3359.88/s  (0.301s, 3401.46/s)  LR: 5.314e-04  Data: 0.023 (0.028)
Train: 144 [ 600/1251 ( 48%)]  Loss: 3.845 (3.50)  Time: 0.300s, 3416.20/s  (0.301s, 3401.82/s)  LR: 5.312e-04  Data: 0.024 (0.027)
Train: 144 [ 650/1251 ( 52%)]  Loss: 3.328 (3.48)  Time: 0.303s, 3376.21/s  (0.301s, 3402.02/s)  LR: 5.310e-04  Data: 0.025 (0.027)
Train: 144 [ 700/1251 ( 56%)]  Loss: 3.932 (3.51)  Time: 0.296s, 3462.23/s  (0.301s, 3401.61/s)  LR: 5.308e-04  Data: 0.024 (0.027)
Train: 144 [ 750/1251 ( 60%)]  Loss: 3.814 (3.53)  Time: 0.303s, 3383.93/s  (0.301s, 3401.43/s)  LR: 5.306e-04  Data: 0.026 (0.027)
Train: 144 [ 800/1251 ( 64%)]  Loss: 3.661 (3.54)  Time: 0.299s, 3419.74/s  (0.301s, 3401.86/s)  LR: 5.304e-04  Data: 0.026 (0.026)
Train: 144 [ 850/1251 ( 68%)]  Loss: 3.797 (3.56)  Time: 0.301s, 3397.32/s  (0.301s, 3401.84/s)  LR: 5.302e-04  Data: 0.024 (0.026)
Train: 144 [ 900/1251 ( 72%)]  Loss: 3.479 (3.55)  Time: 0.303s, 3377.53/s  (0.301s, 3401.88/s)  LR: 5.300e-04  Data: 0.023 (0.026)
Train: 144 [ 950/1251 ( 76%)]  Loss: 3.364 (3.54)  Time: 0.303s, 3375.61/s  (0.301s, 3401.27/s)  LR: 5.298e-04  Data: 0.021 (0.026)
Train: 144 [1000/1251 ( 80%)]  Loss: 3.621 (3.55)  Time: 0.299s, 3426.30/s  (0.301s, 3402.10/s)  LR: 5.296e-04  Data: 0.023 (0.026)
Train: 144 [1050/1251 ( 84%)]  Loss: 3.207 (3.53)  Time: 0.300s, 3408.40/s  (0.301s, 3402.07/s)  LR: 5.294e-04  Data: 0.023 (0.026)
Train: 144 [1100/1251 ( 88%)]  Loss: 3.097 (3.51)  Time: 0.304s, 3367.63/s  (0.301s, 3402.17/s)  LR: 5.292e-04  Data: 0.024 (0.026)
Train: 144 [1150/1251 ( 92%)]  Loss: 4.031 (3.53)  Time: 0.303s, 3384.33/s  (0.301s, 3402.14/s)  LR: 5.290e-04  Data: 0.023 (0.026)
Train: 144 [1200/1251 ( 96%)]  Loss: 3.338 (3.53)  Time: 0.297s, 3444.31/s  (0.301s, 3402.69/s)  LR: 5.287e-04  Data: 0.024 (0.025)
Train: 144 [1250/1251 (100%)]  Loss: 3.788 (3.54)  Time: 0.275s, 3721.49/s  (0.301s, 3404.85/s)  LR: 5.285e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.046 (2.046)  Loss:  0.5938 (0.5938)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.057 (0.236)  Loss:  0.7139 (1.1520)  Acc@1: 83.2547 (73.8080)  Acc@5: 95.5189 (92.0520)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-140.pth.tar', 74.00200000732421)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-143.pth.tar', 73.94800001220703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-142.pth.tar', 73.86200006591797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-144.pth.tar', 73.80799996582031)

Train: 145 [   0/1251 (  0%)]  Loss: 3.344 (3.34)  Time: 2.439s,  419.81/s  (2.439s,  419.81/s)  LR: 5.285e-04  Data: 2.219 (2.219)
Train: 145 [  50/1251 (  4%)]  Loss: 3.474 (3.41)  Time: 0.300s, 3416.41/s  (0.331s, 3096.68/s)  LR: 5.283e-04  Data: 0.024 (0.067)
Train: 145 [ 100/1251 (  8%)]  Loss: 3.312 (3.38)  Time: 0.298s, 3431.57/s  (0.314s, 3257.37/s)  LR: 5.281e-04  Data: 0.022 (0.046)
Train: 145 [ 150/1251 ( 12%)]  Loss: 3.539 (3.42)  Time: 0.294s, 3484.66/s  (0.309s, 3313.30/s)  LR: 5.279e-04  Data: 0.022 (0.038)
Train: 145 [ 200/1251 ( 16%)]  Loss: 3.516 (3.44)  Time: 0.301s, 3401.05/s  (0.307s, 3337.87/s)  LR: 5.277e-04  Data: 0.030 (0.034)
Train: 145 [ 250/1251 ( 20%)]  Loss: 3.714 (3.48)  Time: 0.301s, 3400.32/s  (0.306s, 3351.54/s)  LR: 5.275e-04  Data: 0.021 (0.032)
Train: 145 [ 300/1251 ( 24%)]  Loss: 3.544 (3.49)  Time: 0.302s, 3393.45/s  (0.305s, 3360.64/s)  LR: 5.273e-04  Data: 0.022 (0.031)
Train: 145 [ 350/1251 ( 28%)]  Loss: 3.515 (3.49)  Time: 0.300s, 3408.19/s  (0.304s, 3368.10/s)  LR: 5.271e-04  Data: 0.023 (0.030)
Train: 145 [ 400/1251 ( 32%)]  Loss: 3.471 (3.49)  Time: 0.300s, 3411.47/s  (0.304s, 3371.51/s)  LR: 5.269e-04  Data: 0.022 (0.029)
Train: 145 [ 450/1251 ( 36%)]  Loss: 3.508 (3.49)  Time: 0.301s, 3403.93/s  (0.303s, 3375.16/s)  LR: 5.267e-04  Data: 0.018 (0.028)
Train: 145 [ 500/1251 ( 40%)]  Loss: 3.453 (3.49)  Time: 0.301s, 3399.28/s  (0.303s, 3377.69/s)  LR: 5.265e-04  Data: 0.024 (0.028)
Train: 145 [ 550/1251 ( 44%)]  Loss: 3.316 (3.48)  Time: 0.306s, 3346.22/s  (0.303s, 3379.70/s)  LR: 5.262e-04  Data: 0.026 (0.027)
Train: 145 [ 600/1251 ( 48%)]  Loss: 3.327 (3.46)  Time: 0.300s, 3411.91/s  (0.303s, 3381.68/s)  LR: 5.260e-04  Data: 0.027 (0.027)
Train: 145 [ 650/1251 ( 52%)]  Loss: 3.605 (3.47)  Time: 0.304s, 3372.01/s  (0.303s, 3383.17/s)  LR: 5.258e-04  Data: 0.022 (0.027)
Train: 145 [ 700/1251 ( 56%)]  Loss: 3.471 (3.47)  Time: 0.301s, 3404.24/s  (0.303s, 3384.06/s)  LR: 5.256e-04  Data: 0.021 (0.027)
Train: 145 [ 750/1251 ( 60%)]  Loss: 3.395 (3.47)  Time: 0.301s, 3403.21/s  (0.303s, 3384.58/s)  LR: 5.254e-04  Data: 0.023 (0.026)
Train: 145 [ 800/1251 ( 64%)]  Loss: 3.537 (3.47)  Time: 0.301s, 3402.96/s  (0.302s, 3385.36/s)  LR: 5.252e-04  Data: 0.026 (0.026)
Train: 145 [ 850/1251 ( 68%)]  Loss: 3.270 (3.46)  Time: 0.298s, 3433.50/s  (0.302s, 3385.46/s)  LR: 5.250e-04  Data: 0.025 (0.026)
Train: 145 [ 900/1251 ( 72%)]  Loss: 3.556 (3.47)  Time: 0.301s, 3405.02/s  (0.302s, 3385.22/s)  LR: 5.248e-04  Data: 0.022 (0.026)
Train: 145 [ 950/1251 ( 76%)]  Loss: 3.555 (3.47)  Time: 0.305s, 3355.46/s  (0.303s, 3384.98/s)  LR: 5.246e-04  Data: 0.026 (0.026)
Train: 145 [1000/1251 ( 80%)]  Loss: 3.518 (3.47)  Time: 0.301s, 3397.44/s  (0.303s, 3384.85/s)  LR: 5.244e-04  Data: 0.022 (0.026)
Train: 145 [1050/1251 ( 84%)]  Loss: 3.539 (3.48)  Time: 0.306s, 3351.55/s  (0.303s, 3385.07/s)  LR: 5.242e-04  Data: 0.025 (0.026)
Train: 145 [1100/1251 ( 88%)]  Loss: 3.355 (3.47)  Time: 0.303s, 3381.50/s  (0.302s, 3385.34/s)  LR: 5.240e-04  Data: 0.023 (0.025)
Train: 145 [1150/1251 ( 92%)]  Loss: 3.483 (3.47)  Time: 0.304s, 3365.36/s  (0.302s, 3385.28/s)  LR: 5.237e-04  Data: 0.022 (0.025)
Train: 145 [1200/1251 ( 96%)]  Loss: 3.598 (3.48)  Time: 0.296s, 3461.95/s  (0.302s, 3385.52/s)  LR: 5.235e-04  Data: 0.022 (0.025)
Train: 145 [1250/1251 (100%)]  Loss: 3.135 (3.46)  Time: 0.275s, 3721.82/s  (0.302s, 3387.69/s)  LR: 5.233e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.036 (2.036)  Loss:  0.5718 (0.5718)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.039 (0.238)  Loss:  0.7656 (1.1501)  Acc@1: 82.6651 (74.0280)  Acc@5: 95.1651 (92.0240)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-145.pth.tar', 74.02800009765625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-140.pth.tar', 74.00200000732421)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-143.pth.tar', 73.94800001220703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-142.pth.tar', 73.86200006591797)

Train: 146 [   0/1251 (  0%)]  Loss: 3.385 (3.38)  Time: 2.114s,  484.39/s  (2.114s,  484.39/s)  LR: 5.233e-04  Data: 1.883 (1.883)
Train: 146 [  50/1251 (  4%)]  Loss: 3.455 (3.42)  Time: 0.302s, 3393.15/s  (0.325s, 3151.09/s)  LR: 5.231e-04  Data: 0.018 (0.062)
Train: 146 [ 100/1251 (  8%)]  Loss: 3.322 (3.39)  Time: 0.304s, 3371.56/s  (0.310s, 3302.11/s)  LR: 5.229e-04  Data: 0.028 (0.043)
Train: 146 [ 150/1251 ( 12%)]  Loss: 3.665 (3.46)  Time: 0.294s, 3477.21/s  (0.306s, 3348.39/s)  LR: 5.227e-04  Data: 0.024 (0.036)
Train: 146 [ 200/1251 ( 16%)]  Loss: 3.359 (3.44)  Time: 0.299s, 3427.16/s  (0.304s, 3368.13/s)  LR: 5.225e-04  Data: 0.017 (0.033)
Train: 146 [ 250/1251 ( 20%)]  Loss: 3.385 (3.43)  Time: 0.306s, 3349.65/s  (0.303s, 3377.64/s)  LR: 5.223e-04  Data: 0.023 (0.031)
Train: 146 [ 300/1251 ( 24%)]  Loss: 3.517 (3.44)  Time: 0.302s, 3387.35/s  (0.303s, 3383.61/s)  LR: 5.221e-04  Data: 0.026 (0.030)
Train: 146 [ 350/1251 ( 28%)]  Loss: 3.649 (3.47)  Time: 0.303s, 3374.01/s  (0.302s, 3387.40/s)  LR: 5.219e-04  Data: 0.023 (0.029)
Train: 146 [ 400/1251 ( 32%)]  Loss: 3.166 (3.43)  Time: 0.303s, 3376.37/s  (0.302s, 3388.90/s)  LR: 5.217e-04  Data: 0.026 (0.028)
Train: 146 [ 450/1251 ( 36%)]  Loss: 3.592 (3.45)  Time: 0.299s, 3429.98/s  (0.302s, 3390.86/s)  LR: 5.215e-04  Data: 0.023 (0.028)
Train: 146 [ 500/1251 ( 40%)]  Loss: 3.614 (3.46)  Time: 0.304s, 3373.36/s  (0.302s, 3391.93/s)  LR: 5.212e-04  Data: 0.027 (0.027)
Train: 146 [ 550/1251 ( 44%)]  Loss: 3.296 (3.45)  Time: 0.309s, 3314.50/s  (0.302s, 3391.65/s)  LR: 5.210e-04  Data: 0.024 (0.027)
Train: 146 [ 600/1251 ( 48%)]  Loss: 3.675 (3.47)  Time: 0.301s, 3401.38/s  (0.302s, 3392.66/s)  LR: 5.208e-04  Data: 0.026 (0.027)
Train: 146 [ 650/1251 ( 52%)]  Loss: 3.660 (3.48)  Time: 0.304s, 3368.79/s  (0.302s, 3393.78/s)  LR: 5.206e-04  Data: 0.024 (0.027)
Train: 146 [ 700/1251 ( 56%)]  Loss: 3.513 (3.48)  Time: 0.300s, 3412.92/s  (0.302s, 3394.21/s)  LR: 5.204e-04  Data: 0.024 (0.026)
Train: 146 [ 750/1251 ( 60%)]  Loss: 3.912 (3.51)  Time: 0.298s, 3436.57/s  (0.302s, 3395.16/s)  LR: 5.202e-04  Data: 0.020 (0.026)
Train: 146 [ 800/1251 ( 64%)]  Loss: 3.714 (3.52)  Time: 0.299s, 3429.22/s  (0.302s, 3396.05/s)  LR: 5.200e-04  Data: 0.024 (0.026)
Train: 146 [ 850/1251 ( 68%)]  Loss: 3.572 (3.53)  Time: 0.305s, 3359.70/s  (0.302s, 3395.87/s)  LR: 5.198e-04  Data: 0.022 (0.026)
Train: 146 [ 900/1251 ( 72%)]  Loss: 3.385 (3.52)  Time: 0.308s, 3319.79/s  (0.302s, 3395.97/s)  LR: 5.196e-04  Data: 0.021 (0.026)
Train: 146 [ 950/1251 ( 76%)]  Loss: 3.627 (3.52)  Time: 0.305s, 3358.32/s  (0.302s, 3396.04/s)  LR: 5.194e-04  Data: 0.021 (0.026)
Train: 146 [1000/1251 ( 80%)]  Loss: 3.517 (3.52)  Time: 0.301s, 3407.15/s  (0.302s, 3395.56/s)  LR: 5.192e-04  Data: 0.025 (0.025)
Train: 146 [1050/1251 ( 84%)]  Loss: 3.700 (3.53)  Time: 0.302s, 3392.86/s  (0.302s, 3395.85/s)  LR: 5.190e-04  Data: 0.022 (0.025)
Train: 146 [1100/1251 ( 88%)]  Loss: 3.271 (3.52)  Time: 0.300s, 3409.17/s  (0.302s, 3395.77/s)  LR: 5.188e-04  Data: 0.025 (0.025)
Train: 146 [1150/1251 ( 92%)]  Loss: 3.175 (3.51)  Time: 0.305s, 3361.26/s  (0.302s, 3395.46/s)  LR: 5.185e-04  Data: 0.023 (0.025)
Train: 146 [1200/1251 ( 96%)]  Loss: 3.457 (3.50)  Time: 0.301s, 3402.72/s  (0.302s, 3395.31/s)  LR: 5.183e-04  Data: 0.027 (0.025)
Train: 146 [1250/1251 (100%)]  Loss: 3.738 (3.51)  Time: 0.276s, 3707.26/s  (0.301s, 3396.71/s)  LR: 5.181e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.578 (2.578)  Loss:  0.6577 (0.6577)  Acc@1: 88.0859 (88.0859)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.052 (0.235)  Loss:  0.6826 (1.1631)  Acc@1: 85.8491 (74.2380)  Acc@5: 96.4623 (92.0220)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-146.pth.tar', 74.23800005859376)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-145.pth.tar', 74.02800009765625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-140.pth.tar', 74.00200000732421)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-143.pth.tar', 73.94800001220703)

Train: 147 [   0/1251 (  0%)]  Loss: 3.661 (3.66)  Time: 2.035s,  503.21/s  (2.035s,  503.21/s)  LR: 5.181e-04  Data: 1.810 (1.810)
Train: 147 [  50/1251 (  4%)]  Loss: 3.736 (3.70)  Time: 0.294s, 3482.53/s  (0.322s, 3183.96/s)  LR: 5.179e-04  Data: 0.022 (0.061)
Train: 147 [ 100/1251 (  8%)]  Loss: 3.341 (3.58)  Time: 0.293s, 3500.10/s  (0.309s, 3315.82/s)  LR: 5.177e-04  Data: 0.022 (0.042)
Train: 147 [ 150/1251 ( 12%)]  Loss: 3.343 (3.52)  Time: 0.305s, 3361.95/s  (0.305s, 3353.53/s)  LR: 5.175e-04  Data: 0.025 (0.036)
Train: 147 [ 200/1251 ( 16%)]  Loss: 3.265 (3.47)  Time: 0.293s, 3494.75/s  (0.304s, 3368.08/s)  LR: 5.173e-04  Data: 0.022 (0.033)
Train: 147 [ 250/1251 ( 20%)]  Loss: 3.613 (3.49)  Time: 0.303s, 3378.01/s  (0.303s, 3376.90/s)  LR: 5.171e-04  Data: 0.024 (0.031)
Train: 147 [ 300/1251 ( 24%)]  Loss: 3.634 (3.51)  Time: 0.308s, 3321.66/s  (0.303s, 3380.09/s)  LR: 5.169e-04  Data: 0.026 (0.030)
Train: 147 [ 350/1251 ( 28%)]  Loss: 3.353 (3.49)  Time: 0.291s, 3513.17/s  (0.303s, 3383.51/s)  LR: 5.167e-04  Data: 0.022 (0.029)
Train: 147 [ 400/1251 ( 32%)]  Loss: 3.603 (3.51)  Time: 0.304s, 3367.60/s  (0.303s, 3384.17/s)  LR: 5.165e-04  Data: 0.023 (0.028)
Train: 147 [ 450/1251 ( 36%)]  Loss: 3.565 (3.51)  Time: 0.303s, 3381.70/s  (0.303s, 3384.10/s)  LR: 5.162e-04  Data: 0.021 (0.027)
Train: 147 [ 500/1251 ( 40%)]  Loss: 3.503 (3.51)  Time: 0.308s, 3329.66/s  (0.303s, 3384.05/s)  LR: 5.160e-04  Data: 0.027 (0.027)
Train: 147 [ 550/1251 ( 44%)]  Loss: 3.295 (3.49)  Time: 0.305s, 3354.11/s  (0.303s, 3384.14/s)  LR: 5.158e-04  Data: 0.021 (0.027)
Train: 147 [ 600/1251 ( 48%)]  Loss: 3.425 (3.49)  Time: 0.300s, 3414.88/s  (0.303s, 3383.82/s)  LR: 5.156e-04  Data: 0.025 (0.026)
Train: 147 [ 650/1251 ( 52%)]  Loss: 3.468 (3.49)  Time: 0.307s, 3340.83/s  (0.303s, 3383.49/s)  LR: 5.154e-04  Data: 0.020 (0.026)
Train: 147 [ 700/1251 ( 56%)]  Loss: 3.600 (3.49)  Time: 0.302s, 3392.84/s  (0.303s, 3383.18/s)  LR: 5.152e-04  Data: 0.022 (0.026)
Train: 147 [ 750/1251 ( 60%)]  Loss: 3.505 (3.49)  Time: 0.304s, 3370.69/s  (0.303s, 3382.29/s)  LR: 5.150e-04  Data: 0.023 (0.026)
Train: 147 [ 800/1251 ( 64%)]  Loss: 3.702 (3.51)  Time: 0.300s, 3413.79/s  (0.303s, 3382.16/s)  LR: 5.148e-04  Data: 0.022 (0.026)
Train: 147 [ 850/1251 ( 68%)]  Loss: 3.417 (3.50)  Time: 0.304s, 3367.91/s  (0.303s, 3381.74/s)  LR: 5.146e-04  Data: 0.027 (0.025)
Train: 147 [ 900/1251 ( 72%)]  Loss: 3.712 (3.51)  Time: 0.305s, 3359.56/s  (0.303s, 3381.09/s)  LR: 5.144e-04  Data: 0.025 (0.025)
Train: 147 [ 950/1251 ( 76%)]  Loss: 3.200 (3.50)  Time: 0.302s, 3387.16/s  (0.303s, 3380.47/s)  LR: 5.142e-04  Data: 0.021 (0.025)
Train: 147 [1000/1251 ( 80%)]  Loss: 3.469 (3.50)  Time: 0.299s, 3425.44/s  (0.303s, 3380.21/s)  LR: 5.140e-04  Data: 0.023 (0.025)
Train: 147 [1050/1251 ( 84%)]  Loss: 3.814 (3.51)  Time: 0.310s, 3301.11/s  (0.303s, 3379.79/s)  LR: 5.138e-04  Data: 0.024 (0.025)
Train: 147 [1100/1251 ( 88%)]  Loss: 3.553 (3.51)  Time: 0.307s, 3337.06/s  (0.303s, 3379.73/s)  LR: 5.135e-04  Data: 0.028 (0.025)
Train: 147 [1150/1251 ( 92%)]  Loss: 3.514 (3.51)  Time: 0.298s, 3435.59/s  (0.303s, 3378.85/s)  LR: 5.133e-04  Data: 0.024 (0.025)
Train: 147 [1200/1251 ( 96%)]  Loss: 3.660 (3.52)  Time: 0.305s, 3359.29/s  (0.303s, 3378.19/s)  LR: 5.131e-04  Data: 0.026 (0.025)
Train: 147 [1250/1251 (100%)]  Loss: 3.573 (3.52)  Time: 0.276s, 3709.35/s  (0.303s, 3379.67/s)  LR: 5.129e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.021 (2.021)  Loss:  0.6025 (0.6025)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.056 (0.231)  Loss:  0.7236 (1.1354)  Acc@1: 83.9623 (74.1080)  Acc@5: 95.8726 (92.1660)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-146.pth.tar', 74.23800005859376)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-147.pth.tar', 74.10800014404298)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-145.pth.tar', 74.02800009765625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-140.pth.tar', 74.00200000732421)

Train: 148 [   0/1251 (  0%)]  Loss: 3.653 (3.65)  Time: 1.872s,  546.89/s  (1.872s,  546.89/s)  LR: 5.129e-04  Data: 1.647 (1.647)
Train: 148 [  50/1251 (  4%)]  Loss: 3.777 (3.72)  Time: 0.296s, 3456.34/s  (0.323s, 3167.55/s)  LR: 5.127e-04  Data: 0.022 (0.057)
Train: 148 [ 100/1251 (  8%)]  Loss: 3.340 (3.59)  Time: 0.299s, 3420.50/s  (0.311s, 3290.59/s)  LR: 5.125e-04  Data: 0.024 (0.040)
Train: 148 [ 150/1251 ( 12%)]  Loss: 3.705 (3.62)  Time: 0.298s, 3432.32/s  (0.308s, 3328.23/s)  LR: 5.123e-04  Data: 0.021 (0.035)
Train: 148 [ 200/1251 ( 16%)]  Loss: 3.155 (3.53)  Time: 0.298s, 3441.30/s  (0.306s, 3343.82/s)  LR: 5.121e-04  Data: 0.019 (0.032)
Train: 148 [ 250/1251 ( 20%)]  Loss: 3.546 (3.53)  Time: 0.304s, 3370.51/s  (0.305s, 3352.55/s)  LR: 5.119e-04  Data: 0.025 (0.030)
Train: 148 [ 300/1251 ( 24%)]  Loss: 3.091 (3.47)  Time: 0.305s, 3353.84/s  (0.305s, 3357.79/s)  LR: 5.117e-04  Data: 0.022 (0.029)
Train: 148 [ 350/1251 ( 28%)]  Loss: 3.665 (3.49)  Time: 0.303s, 3374.62/s  (0.305s, 3360.70/s)  LR: 5.115e-04  Data: 0.021 (0.028)
Train: 148 [ 400/1251 ( 32%)]  Loss: 3.485 (3.49)  Time: 0.300s, 3414.96/s  (0.305s, 3361.40/s)  LR: 5.112e-04  Data: 0.018 (0.028)
Train: 148 [ 450/1251 ( 36%)]  Loss: 3.290 (3.47)  Time: 0.303s, 3379.00/s  (0.305s, 3361.53/s)  LR: 5.110e-04  Data: 0.022 (0.027)
Train: 148 [ 500/1251 ( 40%)]  Loss: 3.636 (3.49)  Time: 0.308s, 3319.55/s  (0.305s, 3361.59/s)  LR: 5.108e-04  Data: 0.027 (0.027)
Train: 148 [ 550/1251 ( 44%)]  Loss: 3.129 (3.46)  Time: 0.305s, 3359.76/s  (0.305s, 3361.08/s)  LR: 5.106e-04  Data: 0.026 (0.026)
Train: 148 [ 600/1251 ( 48%)]  Loss: 3.392 (3.45)  Time: 0.306s, 3350.26/s  (0.305s, 3360.31/s)  LR: 5.104e-04  Data: 0.023 (0.026)
Train: 148 [ 650/1251 ( 52%)]  Loss: 3.438 (3.45)  Time: 0.304s, 3367.74/s  (0.305s, 3359.98/s)  LR: 5.102e-04  Data: 0.023 (0.026)
Train: 148 [ 700/1251 ( 56%)]  Loss: 3.324 (3.44)  Time: 0.299s, 3422.45/s  (0.305s, 3359.75/s)  LR: 5.100e-04  Data: 0.021 (0.026)
Train: 148 [ 750/1251 ( 60%)]  Loss: 3.754 (3.46)  Time: 0.301s, 3406.58/s  (0.305s, 3359.66/s)  LR: 5.098e-04  Data: 0.021 (0.026)
Train: 148 [ 800/1251 ( 64%)]  Loss: 3.873 (3.49)  Time: 0.306s, 3345.71/s  (0.305s, 3359.30/s)  LR: 5.096e-04  Data: 0.026 (0.025)
Train: 148 [ 850/1251 ( 68%)]  Loss: 3.163 (3.47)  Time: 0.304s, 3373.04/s  (0.305s, 3359.72/s)  LR: 5.094e-04  Data: 0.021 (0.025)
Train: 148 [ 900/1251 ( 72%)]  Loss: 3.583 (3.47)  Time: 0.305s, 3359.32/s  (0.305s, 3359.04/s)  LR: 5.092e-04  Data: 0.027 (0.025)
Train: 148 [ 950/1251 ( 76%)]  Loss: 3.627 (3.48)  Time: 0.304s, 3366.96/s  (0.305s, 3358.64/s)  LR: 5.090e-04  Data: 0.023 (0.025)
Train: 148 [1000/1251 ( 80%)]  Loss: 3.038 (3.46)  Time: 0.307s, 3340.00/s  (0.305s, 3358.86/s)  LR: 5.088e-04  Data: 0.024 (0.025)
Train: 148 [1050/1251 ( 84%)]  Loss: 3.786 (3.47)  Time: 0.303s, 3375.02/s  (0.305s, 3358.37/s)  LR: 5.085e-04  Data: 0.020 (0.025)
Train: 148 [1100/1251 ( 88%)]  Loss: 3.540 (3.48)  Time: 0.307s, 3336.82/s  (0.305s, 3358.20/s)  LR: 5.083e-04  Data: 0.026 (0.025)
Train: 148 [1150/1251 ( 92%)]  Loss: 3.460 (3.48)  Time: 0.306s, 3342.98/s  (0.305s, 3357.86/s)  LR: 5.081e-04  Data: 0.023 (0.025)
Train: 148 [1200/1251 ( 96%)]  Loss: 3.480 (3.48)  Time: 0.305s, 3360.62/s  (0.305s, 3358.02/s)  LR: 5.079e-04  Data: 0.025 (0.025)
Train: 148 [1250/1251 (100%)]  Loss: 3.690 (3.49)  Time: 0.276s, 3713.30/s  (0.305s, 3359.75/s)  LR: 5.077e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.016 (2.016)  Loss:  0.5728 (0.5728)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.041 (0.231)  Loss:  0.6973 (1.1351)  Acc@1: 83.9623 (74.2580)  Acc@5: 95.9906 (92.2060)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-148.pth.tar', 74.25800001464843)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-146.pth.tar', 74.23800005859376)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-147.pth.tar', 74.10800014404298)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-145.pth.tar', 74.02800009765625)

Train: 149 [   0/1251 (  0%)]  Loss: 2.890 (2.89)  Time: 2.220s,  461.36/s  (2.220s,  461.36/s)  LR: 5.077e-04  Data: 1.982 (1.982)
Train: 149 [  50/1251 (  4%)]  Loss: 3.710 (3.30)  Time: 0.293s, 3499.65/s  (0.324s, 3162.29/s)  LR: 5.075e-04  Data: 0.023 (0.063)
Train: 149 [ 100/1251 (  8%)]  Loss: 3.551 (3.38)  Time: 0.305s, 3352.18/s  (0.311s, 3288.14/s)  LR: 5.073e-04  Data: 0.021 (0.043)
Train: 149 [ 150/1251 ( 12%)]  Loss: 3.320 (3.37)  Time: 0.297s, 3444.72/s  (0.309s, 3319.21/s)  LR: 5.071e-04  Data: 0.022 (0.037)
Train: 149 [ 200/1251 ( 16%)]  Loss: 3.825 (3.46)  Time: 0.302s, 3389.75/s  (0.307s, 3332.15/s)  LR: 5.069e-04  Data: 0.022 (0.033)
Train: 149 [ 250/1251 ( 20%)]  Loss: 3.362 (3.44)  Time: 0.303s, 3385.12/s  (0.307s, 3338.55/s)  LR: 5.067e-04  Data: 0.023 (0.031)
Train: 149 [ 300/1251 ( 24%)]  Loss: 3.252 (3.42)  Time: 0.306s, 3351.82/s  (0.306s, 3342.30/s)  LR: 5.065e-04  Data: 0.025 (0.030)
Train: 149 [ 350/1251 ( 28%)]  Loss: 3.613 (3.44)  Time: 0.307s, 3339.91/s  (0.306s, 3344.54/s)  LR: 5.062e-04  Data: 0.025 (0.029)
Train: 149 [ 400/1251 ( 32%)]  Loss: 3.232 (3.42)  Time: 0.303s, 3374.91/s  (0.306s, 3347.03/s)  LR: 5.060e-04  Data: 0.024 (0.029)
Train: 149 [ 450/1251 ( 36%)]  Loss: 3.256 (3.40)  Time: 0.305s, 3362.38/s  (0.306s, 3347.60/s)  LR: 5.058e-04  Data: 0.023 (0.028)
Train: 149 [ 500/1251 ( 40%)]  Loss: 3.583 (3.42)  Time: 0.303s, 3377.63/s  (0.306s, 3348.26/s)  LR: 5.056e-04  Data: 0.025 (0.028)
Train: 149 [ 550/1251 ( 44%)]  Loss: 3.801 (3.45)  Time: 0.310s, 3305.70/s  (0.306s, 3348.50/s)  LR: 5.054e-04  Data: 0.024 (0.027)
Train: 149 [ 600/1251 ( 48%)]  Loss: 3.333 (3.44)  Time: 0.307s, 3335.23/s  (0.306s, 3348.94/s)  LR: 5.052e-04  Data: 0.021 (0.027)
Train: 149 [ 650/1251 ( 52%)]  Loss: 3.538 (3.45)  Time: 0.308s, 3323.69/s  (0.306s, 3349.35/s)  LR: 5.050e-04  Data: 0.025 (0.027)
Train: 149 [ 700/1251 ( 56%)]  Loss: 3.713 (3.47)  Time: 0.306s, 3350.78/s  (0.306s, 3349.69/s)  LR: 5.048e-04  Data: 0.027 (0.026)
Train: 149 [ 750/1251 ( 60%)]  Loss: 3.373 (3.46)  Time: 0.307s, 3335.82/s  (0.306s, 3349.18/s)  LR: 5.046e-04  Data: 0.024 (0.026)
Train: 149 [ 800/1251 ( 64%)]  Loss: 3.778 (3.48)  Time: 0.306s, 3341.77/s  (0.306s, 3348.38/s)  LR: 5.044e-04  Data: 0.024 (0.026)
Train: 149 [ 850/1251 ( 68%)]  Loss: 3.504 (3.48)  Time: 0.297s, 3445.07/s  (0.306s, 3347.84/s)  LR: 5.042e-04  Data: 0.020 (0.026)
Train: 149 [ 900/1251 ( 72%)]  Loss: 3.411 (3.48)  Time: 0.309s, 3313.76/s  (0.306s, 3347.44/s)  LR: 5.040e-04  Data: 0.023 (0.026)
Train: 149 [ 950/1251 ( 76%)]  Loss: 3.592 (3.48)  Time: 0.306s, 3344.02/s  (0.306s, 3347.07/s)  LR: 5.037e-04  Data: 0.023 (0.025)
Train: 149 [1000/1251 ( 80%)]  Loss: 3.195 (3.47)  Time: 0.304s, 3368.65/s  (0.306s, 3346.27/s)  LR: 5.035e-04  Data: 0.026 (0.025)
Train: 149 [1050/1251 ( 84%)]  Loss: 3.296 (3.46)  Time: 0.305s, 3360.04/s  (0.306s, 3345.96/s)  LR: 5.033e-04  Data: 0.023 (0.025)
Train: 149 [1100/1251 ( 88%)]  Loss: 3.549 (3.46)  Time: 0.307s, 3339.91/s  (0.306s, 3346.55/s)  LR: 5.031e-04  Data: 0.023 (0.025)
Train: 149 [1150/1251 ( 92%)]  Loss: 3.826 (3.48)  Time: 0.309s, 3313.54/s  (0.306s, 3346.78/s)  LR: 5.029e-04  Data: 0.024 (0.025)
Train: 149 [1200/1251 ( 96%)]  Loss: 3.754 (3.49)  Time: 0.315s, 3255.58/s  (0.306s, 3346.49/s)  LR: 5.027e-04  Data: 0.021 (0.025)
Train: 149 [1250/1251 (100%)]  Loss: 3.427 (3.49)  Time: 0.277s, 3695.42/s  (0.306s, 3348.26/s)  LR: 5.025e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.075 (2.075)  Loss:  0.5903 (0.5903)  Acc@1: 87.8906 (87.8906)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.055 (0.235)  Loss:  0.7383 (1.1347)  Acc@1: 83.9623 (74.2600)  Acc@5: 95.5189 (92.1160)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-149.pth.tar', 74.26000014404296)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-148.pth.tar', 74.25800001464843)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-146.pth.tar', 74.23800005859376)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-147.pth.tar', 74.10800014404298)

Train: 150 [   0/1251 (  0%)]  Loss: 3.645 (3.64)  Time: 2.206s,  464.17/s  (2.206s,  464.17/s)  LR: 5.025e-04  Data: 1.974 (1.974)
Train: 150 [  50/1251 (  4%)]  Loss: 3.595 (3.62)  Time: 0.305s, 3355.60/s  (0.327s, 3133.80/s)  LR: 5.023e-04  Data: 0.025 (0.062)
Train: 150 [ 100/1251 (  8%)]  Loss: 3.432 (3.56)  Time: 0.303s, 3381.74/s  (0.314s, 3265.20/s)  LR: 5.021e-04  Data: 0.022 (0.043)
Train: 150 [ 150/1251 ( 12%)]  Loss: 3.631 (3.58)  Time: 0.305s, 3352.63/s  (0.310s, 3302.27/s)  LR: 5.019e-04  Data: 0.026 (0.037)
Train: 150 [ 200/1251 ( 16%)]  Loss: 3.626 (3.59)  Time: 0.308s, 3329.62/s  (0.309s, 3318.60/s)  LR: 5.017e-04  Data: 0.028 (0.033)
Train: 150 [ 250/1251 ( 20%)]  Loss: 3.794 (3.62)  Time: 0.304s, 3368.30/s  (0.308s, 3327.39/s)  LR: 5.015e-04  Data: 0.023 (0.031)
Train: 150 [ 300/1251 ( 24%)]  Loss: 3.631 (3.62)  Time: 0.309s, 3317.00/s  (0.307s, 3331.32/s)  LR: 5.012e-04  Data: 0.021 (0.030)
Train: 150 [ 350/1251 ( 28%)]  Loss: 3.322 (3.58)  Time: 0.309s, 3308.67/s  (0.307s, 3334.90/s)  LR: 5.010e-04  Data: 0.027 (0.029)
Train: 150 [ 400/1251 ( 32%)]  Loss: 3.684 (3.60)  Time: 0.299s, 3419.89/s  (0.307s, 3337.70/s)  LR: 5.008e-04  Data: 0.022 (0.028)
Train: 150 [ 450/1251 ( 36%)]  Loss: 3.674 (3.60)  Time: 0.310s, 3298.66/s  (0.307s, 3339.13/s)  LR: 5.006e-04  Data: 0.025 (0.028)
Train: 150 [ 500/1251 ( 40%)]  Loss: 3.122 (3.56)  Time: 0.310s, 3300.60/s  (0.307s, 3339.31/s)  LR: 5.004e-04  Data: 0.024 (0.027)
Train: 150 [ 550/1251 ( 44%)]  Loss: 3.767 (3.58)  Time: 0.310s, 3305.88/s  (0.307s, 3339.24/s)  LR: 5.002e-04  Data: 0.023 (0.027)
Train: 150 [ 600/1251 ( 48%)]  Loss: 3.287 (3.55)  Time: 0.302s, 3388.24/s  (0.307s, 3339.17/s)  LR: 5.000e-04  Data: 0.022 (0.026)
Train: 150 [ 650/1251 ( 52%)]  Loss: 3.686 (3.56)  Time: 0.307s, 3337.04/s  (0.307s, 3339.13/s)  LR: 4.998e-04  Data: 0.021 (0.026)
Train: 150 [ 700/1251 ( 56%)]  Loss: 3.707 (3.57)  Time: 0.312s, 3286.48/s  (0.307s, 3338.70/s)  LR: 4.996e-04  Data: 0.027 (0.026)
Train: 150 [ 750/1251 ( 60%)]  Loss: 3.216 (3.55)  Time: 0.309s, 3310.14/s  (0.307s, 3338.42/s)  LR: 4.994e-04  Data: 0.026 (0.026)
Train: 150 [ 800/1251 ( 64%)]  Loss: 3.559 (3.55)  Time: 0.304s, 3372.50/s  (0.307s, 3338.34/s)  LR: 4.992e-04  Data: 0.024 (0.026)
Train: 150 [ 850/1251 ( 68%)]  Loss: 3.910 (3.57)  Time: 0.307s, 3333.16/s  (0.307s, 3338.29/s)  LR: 4.990e-04  Data: 0.021 (0.025)
Train: 150 [ 900/1251 ( 72%)]  Loss: 3.600 (3.57)  Time: 0.306s, 3343.62/s  (0.307s, 3337.77/s)  LR: 4.987e-04  Data: 0.022 (0.025)
Train: 150 [ 950/1251 ( 76%)]  Loss: 3.242 (3.56)  Time: 0.311s, 3292.96/s  (0.307s, 3337.32/s)  LR: 4.985e-04  Data: 0.021 (0.025)
Train: 150 [1000/1251 ( 80%)]  Loss: 3.731 (3.56)  Time: 0.314s, 3257.51/s  (0.307s, 3337.18/s)  LR: 4.983e-04  Data: 0.023 (0.025)
Train: 150 [1050/1251 ( 84%)]  Loss: 3.310 (3.55)  Time: 0.310s, 3306.11/s  (0.307s, 3336.61/s)  LR: 4.981e-04  Data: 0.024 (0.025)
Train: 150 [1100/1251 ( 88%)]  Loss: 3.715 (3.56)  Time: 0.304s, 3367.10/s  (0.307s, 3336.44/s)  LR: 4.979e-04  Data: 0.028 (0.025)
Train: 150 [1150/1251 ( 92%)]  Loss: 3.045 (3.54)  Time: 0.304s, 3372.01/s  (0.307s, 3336.41/s)  LR: 4.977e-04  Data: 0.024 (0.025)
Train: 150 [1200/1251 ( 96%)]  Loss: 3.457 (3.54)  Time: 0.309s, 3310.85/s  (0.307s, 3335.94/s)  LR: 4.975e-04  Data: 0.022 (0.025)
Train: 150 [1250/1251 (100%)]  Loss: 3.413 (3.53)  Time: 0.278s, 3681.80/s  (0.307s, 3337.37/s)  LR: 4.973e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.224 (2.224)  Loss:  0.5806 (0.5806)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.062 (0.238)  Loss:  0.7139 (1.1310)  Acc@1: 84.1981 (74.4160)  Acc@5: 96.3443 (92.2940)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-150.pth.tar', 74.4160001171875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-149.pth.tar', 74.26000014404296)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-148.pth.tar', 74.25800001464843)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-146.pth.tar', 74.23800005859376)

Train: 151 [   0/1251 (  0%)]  Loss: 3.763 (3.76)  Time: 2.437s,  420.17/s  (2.437s,  420.17/s)  LR: 4.973e-04  Data: 2.213 (2.213)
Train: 151 [  50/1251 (  4%)]  Loss: 3.520 (3.64)  Time: 0.298s, 3433.16/s  (0.341s, 2999.48/s)  LR: 4.971e-04  Data: 0.024 (0.077)
Train: 151 [ 100/1251 (  8%)]  Loss: 3.414 (3.57)  Time: 0.305s, 3357.41/s  (0.321s, 3186.77/s)  LR: 4.969e-04  Data: 0.025 (0.051)
Train: 151 [ 150/1251 ( 12%)]  Loss: 3.454 (3.54)  Time: 0.302s, 3388.94/s  (0.315s, 3246.26/s)  LR: 4.967e-04  Data: 0.025 (0.041)
Train: 151 [ 200/1251 ( 16%)]  Loss: 3.550 (3.54)  Time: 0.309s, 3316.36/s  (0.313s, 3270.26/s)  LR: 4.965e-04  Data: 0.023 (0.037)
Train: 151 [ 250/1251 ( 20%)]  Loss: 3.551 (3.54)  Time: 0.306s, 3345.87/s  (0.312s, 3285.05/s)  LR: 4.962e-04  Data: 0.022 (0.034)
Train: 151 [ 300/1251 ( 24%)]  Loss: 3.574 (3.55)  Time: 0.311s, 3297.68/s  (0.311s, 3294.70/s)  LR: 4.960e-04  Data: 0.025 (0.032)
Train: 151 [ 350/1251 ( 28%)]  Loss: 3.600 (3.55)  Time: 0.312s, 3277.40/s  (0.310s, 3299.36/s)  LR: 4.958e-04  Data: 0.019 (0.031)
Train: 151 [ 400/1251 ( 32%)]  Loss: 3.460 (3.54)  Time: 0.303s, 3379.54/s  (0.310s, 3302.58/s)  LR: 4.956e-04  Data: 0.020 (0.030)
Train: 151 [ 450/1251 ( 36%)]  Loss: 2.943 (3.48)  Time: 0.298s, 3441.11/s  (0.310s, 3305.73/s)  LR: 4.954e-04  Data: 0.021 (0.029)
Train: 151 [ 500/1251 ( 40%)]  Loss: 3.585 (3.49)  Time: 0.309s, 3312.82/s  (0.310s, 3307.39/s)  LR: 4.952e-04  Data: 0.024 (0.028)
Train: 151 [ 550/1251 ( 44%)]  Loss: 3.511 (3.49)  Time: 0.306s, 3351.32/s  (0.309s, 3309.07/s)  LR: 4.950e-04  Data: 0.024 (0.028)
Train: 151 [ 600/1251 ( 48%)]  Loss: 3.634 (3.50)  Time: 0.308s, 3328.47/s  (0.309s, 3309.48/s)  LR: 4.948e-04  Data: 0.023 (0.028)
Train: 151 [ 650/1251 ( 52%)]  Loss: 3.341 (3.49)  Time: 0.312s, 3286.53/s  (0.309s, 3310.04/s)  LR: 4.946e-04  Data: 0.019 (0.027)
Train: 151 [ 700/1251 ( 56%)]  Loss: 3.554 (3.50)  Time: 0.311s, 3295.67/s  (0.309s, 3310.66/s)  LR: 4.944e-04  Data: 0.024 (0.027)
Train: 151 [ 750/1251 ( 60%)]  Loss: 3.619 (3.50)  Time: 0.308s, 3319.57/s  (0.309s, 3310.76/s)  LR: 4.942e-04  Data: 0.021 (0.027)
Train: 151 [ 800/1251 ( 64%)]  Loss: 3.219 (3.49)  Time: 0.308s, 3328.71/s  (0.309s, 3310.72/s)  LR: 4.940e-04  Data: 0.019 (0.026)
Train: 151 [ 850/1251 ( 68%)]  Loss: 3.552 (3.49)  Time: 0.309s, 3309.70/s  (0.309s, 3311.50/s)  LR: 4.937e-04  Data: 0.024 (0.026)
Train: 151 [ 900/1251 ( 72%)]  Loss: 3.818 (3.51)  Time: 0.309s, 3315.18/s  (0.309s, 3312.63/s)  LR: 4.935e-04  Data: 0.025 (0.026)
Train: 151 [ 950/1251 ( 76%)]  Loss: 3.613 (3.51)  Time: 0.307s, 3332.51/s  (0.309s, 3313.26/s)  LR: 4.933e-04  Data: 0.023 (0.026)
Train: 151 [1000/1251 ( 80%)]  Loss: 3.408 (3.51)  Time: 0.309s, 3310.44/s  (0.309s, 3313.79/s)  LR: 4.931e-04  Data: 0.023 (0.026)
Train: 151 [1050/1251 ( 84%)]  Loss: 3.484 (3.51)  Time: 0.308s, 3324.52/s  (0.309s, 3314.25/s)  LR: 4.929e-04  Data: 0.025 (0.026)
Train: 151 [1100/1251 ( 88%)]  Loss: 3.598 (3.51)  Time: 0.309s, 3317.81/s  (0.309s, 3314.90/s)  LR: 4.927e-04  Data: 0.025 (0.025)
Train: 151 [1150/1251 ( 92%)]  Loss: 3.776 (3.52)  Time: 0.311s, 3297.83/s  (0.309s, 3315.35/s)  LR: 4.925e-04  Data: 0.023 (0.025)
Train: 151 [1200/1251 ( 96%)]  Loss: 3.768 (3.53)  Time: 0.303s, 3376.33/s  (0.309s, 3316.03/s)  LR: 4.923e-04  Data: 0.022 (0.025)
Train: 151 [1250/1251 (100%)]  Loss: 3.627 (3.54)  Time: 0.275s, 3724.59/s  (0.309s, 3318.24/s)  LR: 4.921e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.112 (2.112)  Loss:  0.6245 (0.6245)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.041 (0.238)  Loss:  0.7266 (1.1534)  Acc@1: 84.3160 (74.4260)  Acc@5: 96.4623 (92.3120)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-151.pth.tar', 74.4260000390625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-150.pth.tar', 74.4160001171875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-149.pth.tar', 74.26000014404296)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-148.pth.tar', 74.25800001464843)

Train: 152 [   0/1251 (  0%)]  Loss: 3.776 (3.78)  Time: 2.109s,  485.62/s  (2.109s,  485.62/s)  LR: 4.921e-04  Data: 1.871 (1.871)
Train: 152 [  50/1251 (  4%)]  Loss: 3.558 (3.67)  Time: 0.296s, 3464.16/s  (0.327s, 3128.60/s)  LR: 4.919e-04  Data: 0.024 (0.059)
Train: 152 [ 100/1251 (  8%)]  Loss: 3.488 (3.61)  Time: 0.304s, 3367.20/s  (0.315s, 3252.34/s)  LR: 4.917e-04  Data: 0.025 (0.041)
Train: 152 [ 150/1251 ( 12%)]  Loss: 3.696 (3.63)  Time: 0.310s, 3303.78/s  (0.312s, 3285.11/s)  LR: 4.915e-04  Data: 0.025 (0.035)
Train: 152 [ 200/1251 ( 16%)]  Loss: 3.408 (3.59)  Time: 0.301s, 3397.69/s  (0.310s, 3300.04/s)  LR: 4.912e-04  Data: 0.020 (0.032)
Train: 152 [ 250/1251 ( 20%)]  Loss: 3.593 (3.59)  Time: 0.309s, 3317.30/s  (0.310s, 3305.51/s)  LR: 4.910e-04  Data: 0.021 (0.030)
Train: 152 [ 300/1251 ( 24%)]  Loss: 3.409 (3.56)  Time: 0.312s, 3283.03/s  (0.309s, 3308.88/s)  LR: 4.908e-04  Data: 0.023 (0.029)
Train: 152 [ 350/1251 ( 28%)]  Loss: 3.360 (3.54)  Time: 0.308s, 3329.95/s  (0.309s, 3311.02/s)  LR: 4.906e-04  Data: 0.022 (0.028)
Train: 152 [ 400/1251 ( 32%)]  Loss: 3.307 (3.51)  Time: 0.306s, 3343.09/s  (0.309s, 3312.64/s)  LR: 4.904e-04  Data: 0.020 (0.028)
Train: 152 [ 450/1251 ( 36%)]  Loss: 3.671 (3.53)  Time: 0.309s, 3315.44/s  (0.309s, 3313.74/s)  LR: 4.902e-04  Data: 0.022 (0.027)
Train: 152 [ 500/1251 ( 40%)]  Loss: 3.288 (3.50)  Time: 0.303s, 3382.89/s  (0.309s, 3314.10/s)  LR: 4.900e-04  Data: 0.020 (0.027)
Train: 152 [ 550/1251 ( 44%)]  Loss: 3.487 (3.50)  Time: 0.311s, 3292.67/s  (0.309s, 3314.32/s)  LR: 4.898e-04  Data: 0.024 (0.026)
Train: 152 [ 600/1251 ( 48%)]  Loss: 3.608 (3.51)  Time: 0.306s, 3350.44/s  (0.309s, 3314.79/s)  LR: 4.896e-04  Data: 0.023 (0.026)
Train: 152 [ 650/1251 ( 52%)]  Loss: 3.467 (3.51)  Time: 0.300s, 3415.24/s  (0.309s, 3315.81/s)  LR: 4.894e-04  Data: 0.019 (0.026)
Train: 152 [ 700/1251 ( 56%)]  Loss: 3.657 (3.52)  Time: 0.312s, 3285.75/s  (0.309s, 3316.57/s)  LR: 4.892e-04  Data: 0.024 (0.026)
Train: 152 [ 750/1251 ( 60%)]  Loss: 3.511 (3.52)  Time: 0.311s, 3290.35/s  (0.309s, 3317.03/s)  LR: 4.890e-04  Data: 0.024 (0.025)
Train: 152 [ 800/1251 ( 64%)]  Loss: 3.158 (3.50)  Time: 0.305s, 3358.20/s  (0.309s, 3316.95/s)  LR: 4.887e-04  Data: 0.021 (0.025)
Train: 152 [ 850/1251 ( 68%)]  Loss: 3.483 (3.50)  Time: 0.310s, 3299.84/s  (0.309s, 3317.14/s)  LR: 4.885e-04  Data: 0.023 (0.025)
Train: 152 [ 900/1251 ( 72%)]  Loss: 3.385 (3.49)  Time: 0.308s, 3324.20/s  (0.309s, 3316.54/s)  LR: 4.883e-04  Data: 0.024 (0.025)
Train: 152 [ 950/1251 ( 76%)]  Loss: 3.342 (3.48)  Time: 0.310s, 3298.03/s  (0.309s, 3316.31/s)  LR: 4.881e-04  Data: 0.025 (0.025)
Train: 152 [1000/1251 ( 80%)]  Loss: 3.248 (3.47)  Time: 0.306s, 3341.26/s  (0.309s, 3316.29/s)  LR: 4.879e-04  Data: 0.028 (0.025)
Train: 152 [1050/1251 ( 84%)]  Loss: 3.337 (3.47)  Time: 0.314s, 3264.34/s  (0.309s, 3316.19/s)  LR: 4.877e-04  Data: 0.022 (0.025)
Train: 152 [1100/1251 ( 88%)]  Loss: 3.562 (3.47)  Time: 0.309s, 3310.20/s  (0.309s, 3316.19/s)  LR: 4.875e-04  Data: 0.024 (0.025)
Train: 152 [1150/1251 ( 92%)]  Loss: 3.535 (3.47)  Time: 0.305s, 3359.96/s  (0.309s, 3316.23/s)  LR: 4.873e-04  Data: 0.021 (0.025)
Train: 152 [1200/1251 ( 96%)]  Loss: 3.591 (3.48)  Time: 0.308s, 3327.55/s  (0.309s, 3316.63/s)  LR: 4.871e-04  Data: 0.023 (0.025)
Train: 152 [1250/1251 (100%)]  Loss: 3.829 (3.49)  Time: 0.276s, 3710.11/s  (0.309s, 3319.03/s)  LR: 4.869e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.046 (2.046)  Loss:  0.5811 (0.5811)  Acc@1: 87.6953 (87.6953)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.060 (0.235)  Loss:  0.6484 (1.1161)  Acc@1: 84.6698 (74.5440)  Acc@5: 96.6981 (92.3120)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-152.pth.tar', 74.54400006347656)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-151.pth.tar', 74.4260000390625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-150.pth.tar', 74.4160001171875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-149.pth.tar', 74.26000014404296)

Train: 153 [   0/1251 (  0%)]  Loss: 3.402 (3.40)  Time: 2.726s,  375.64/s  (2.726s,  375.64/s)  LR: 4.869e-04  Data: 2.514 (2.514)
Train: 153 [  50/1251 (  4%)]  Loss: 3.710 (3.56)  Time: 0.294s, 3482.69/s  (0.336s, 3048.38/s)  LR: 4.867e-04  Data: 0.026 (0.072)
Train: 153 [ 100/1251 (  8%)]  Loss: 3.665 (3.59)  Time: 0.305s, 3358.87/s  (0.319s, 3205.54/s)  LR: 4.865e-04  Data: 0.023 (0.048)
Train: 153 [ 150/1251 ( 12%)]  Loss: 3.296 (3.52)  Time: 0.309s, 3316.29/s  (0.315s, 3255.75/s)  LR: 4.862e-04  Data: 0.025 (0.040)
Train: 153 [ 200/1251 ( 16%)]  Loss: 3.139 (3.44)  Time: 0.306s, 3347.09/s  (0.312s, 3279.86/s)  LR: 4.860e-04  Data: 0.025 (0.035)
Train: 153 [ 250/1251 ( 20%)]  Loss: 3.340 (3.43)  Time: 0.309s, 3315.14/s  (0.311s, 3293.21/s)  LR: 4.858e-04  Data: 0.023 (0.033)
Train: 153 [ 300/1251 ( 24%)]  Loss: 3.253 (3.40)  Time: 0.309s, 3317.19/s  (0.310s, 3299.92/s)  LR: 4.856e-04  Data: 0.022 (0.031)
Train: 153 [ 350/1251 ( 28%)]  Loss: 3.379 (3.40)  Time: 0.306s, 3347.37/s  (0.310s, 3305.62/s)  LR: 4.854e-04  Data: 0.025 (0.030)
Train: 153 [ 400/1251 ( 32%)]  Loss: 3.576 (3.42)  Time: 0.303s, 3379.23/s  (0.309s, 3309.76/s)  LR: 4.852e-04  Data: 0.021 (0.029)
Train: 153 [ 450/1251 ( 36%)]  Loss: 3.405 (3.42)  Time: 0.302s, 3388.69/s  (0.309s, 3312.44/s)  LR: 4.850e-04  Data: 0.022 (0.029)
Train: 153 [ 500/1251 ( 40%)]  Loss: 3.844 (3.46)  Time: 0.305s, 3357.05/s  (0.309s, 3314.77/s)  LR: 4.848e-04  Data: 0.025 (0.028)
Train: 153 [ 550/1251 ( 44%)]  Loss: 3.515 (3.46)  Time: 0.298s, 3431.21/s  (0.309s, 3316.21/s)  LR: 4.846e-04  Data: 0.024 (0.028)
Train: 153 [ 600/1251 ( 48%)]  Loss: 3.452 (3.46)  Time: 0.308s, 3329.94/s  (0.309s, 3317.17/s)  LR: 4.844e-04  Data: 0.020 (0.027)
Train: 153 [ 650/1251 ( 52%)]  Loss: 3.660 (3.47)  Time: 0.312s, 3279.55/s  (0.309s, 3318.39/s)  LR: 4.842e-04  Data: 0.023 (0.027)
Train: 153 [ 700/1251 ( 56%)]  Loss: 3.589 (3.48)  Time: 0.310s, 3305.44/s  (0.308s, 3319.52/s)  LR: 4.840e-04  Data: 0.024 (0.027)
Train: 153 [ 750/1251 ( 60%)]  Loss: 3.623 (3.49)  Time: 0.308s, 3325.36/s  (0.308s, 3320.62/s)  LR: 4.837e-04  Data: 0.022 (0.026)
Train: 153 [ 800/1251 ( 64%)]  Loss: 3.252 (3.48)  Time: 0.303s, 3379.30/s  (0.308s, 3321.66/s)  LR: 4.835e-04  Data: 0.026 (0.026)
Train: 153 [ 850/1251 ( 68%)]  Loss: 3.641 (3.49)  Time: 0.306s, 3343.33/s  (0.308s, 3322.13/s)  LR: 4.833e-04  Data: 0.022 (0.026)
Train: 153 [ 900/1251 ( 72%)]  Loss: 3.310 (3.48)  Time: 0.307s, 3339.37/s  (0.308s, 3322.23/s)  LR: 4.831e-04  Data: 0.026 (0.026)
Train: 153 [ 950/1251 ( 76%)]  Loss: 3.246 (3.46)  Time: 0.312s, 3283.57/s  (0.308s, 3323.08/s)  LR: 4.829e-04  Data: 0.027 (0.026)
Train: 153 [1000/1251 ( 80%)]  Loss: 3.365 (3.46)  Time: 0.308s, 3320.34/s  (0.308s, 3324.13/s)  LR: 4.827e-04  Data: 0.023 (0.026)
Train: 153 [1050/1251 ( 84%)]  Loss: 3.509 (3.46)  Time: 0.306s, 3351.35/s  (0.308s, 3324.80/s)  LR: 4.825e-04  Data: 0.024 (0.025)
Train: 153 [1100/1251 ( 88%)]  Loss: 3.591 (3.47)  Time: 0.306s, 3350.99/s  (0.308s, 3325.11/s)  LR: 4.823e-04  Data: 0.022 (0.025)
Train: 153 [1150/1251 ( 92%)]  Loss: 3.704 (3.48)  Time: 0.302s, 3395.98/s  (0.308s, 3325.76/s)  LR: 4.821e-04  Data: 0.019 (0.025)
Train: 153 [1200/1251 ( 96%)]  Loss: 3.533 (3.48)  Time: 0.303s, 3378.93/s  (0.308s, 3325.92/s)  LR: 4.819e-04  Data: 0.022 (0.025)
Train: 153 [1250/1251 (100%)]  Loss: 3.753 (3.49)  Time: 0.275s, 3719.29/s  (0.308s, 3328.77/s)  LR: 4.817e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.074 (2.074)  Loss:  0.6240 (0.6240)  Acc@1: 88.0859 (88.0859)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.041 (0.236)  Loss:  0.6958 (1.1323)  Acc@1: 84.6698 (74.3740)  Acc@5: 96.2264 (92.2460)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-152.pth.tar', 74.54400006347656)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-151.pth.tar', 74.4260000390625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-150.pth.tar', 74.4160001171875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-153.pth.tar', 74.37400006347656)

Train: 154 [   0/1251 (  0%)]  Loss: 3.389 (3.39)  Time: 2.120s,  482.98/s  (2.120s,  482.98/s)  LR: 4.817e-04  Data: 1.901 (1.901)
Train: 154 [  50/1251 (  4%)]  Loss: 3.064 (3.23)  Time: 0.297s, 3442.06/s  (0.324s, 3161.58/s)  LR: 4.815e-04  Data: 0.023 (0.060)
Train: 154 [ 100/1251 (  8%)]  Loss: 3.445 (3.30)  Time: 0.303s, 3383.96/s  (0.312s, 3277.94/s)  LR: 4.812e-04  Data: 0.021 (0.042)
Train: 154 [ 150/1251 ( 12%)]  Loss: 3.053 (3.24)  Time: 0.302s, 3386.57/s  (0.309s, 3312.84/s)  LR: 4.810e-04  Data: 0.024 (0.036)
Train: 154 [ 200/1251 ( 16%)]  Loss: 3.338 (3.26)  Time: 0.302s, 3392.90/s  (0.308s, 3326.94/s)  LR: 4.808e-04  Data: 0.022 (0.033)
Train: 154 [ 250/1251 ( 20%)]  Loss: 3.484 (3.30)  Time: 0.304s, 3365.28/s  (0.307s, 3334.21/s)  LR: 4.806e-04  Data: 0.022 (0.031)
Train: 154 [ 300/1251 ( 24%)]  Loss: 3.636 (3.34)  Time: 0.303s, 3383.12/s  (0.307s, 3338.44/s)  LR: 4.804e-04  Data: 0.024 (0.029)
Train: 154 [ 350/1251 ( 28%)]  Loss: 3.369 (3.35)  Time: 0.306s, 3341.63/s  (0.306s, 3341.00/s)  LR: 4.802e-04  Data: 0.025 (0.029)
Train: 154 [ 400/1251 ( 32%)]  Loss: 3.333 (3.35)  Time: 0.306s, 3346.02/s  (0.306s, 3342.30/s)  LR: 4.800e-04  Data: 0.026 (0.028)
Train: 154 [ 450/1251 ( 36%)]  Loss: 3.348 (3.35)  Time: 0.304s, 3371.65/s  (0.306s, 3344.26/s)  LR: 4.798e-04  Data: 0.024 (0.027)
Train: 154 [ 500/1251 ( 40%)]  Loss: 3.824 (3.39)  Time: 0.304s, 3366.50/s  (0.306s, 3345.06/s)  LR: 4.796e-04  Data: 0.024 (0.027)
Train: 154 [ 550/1251 ( 44%)]  Loss: 3.359 (3.39)  Time: 0.305s, 3356.03/s  (0.306s, 3347.15/s)  LR: 4.794e-04  Data: 0.022 (0.026)
Train: 154 [ 600/1251 ( 48%)]  Loss: 3.178 (3.37)  Time: 0.307s, 3331.41/s  (0.306s, 3347.85/s)  LR: 4.792e-04  Data: 0.025 (0.026)
Train: 154 [ 650/1251 ( 52%)]  Loss: 3.486 (3.38)  Time: 0.304s, 3373.58/s  (0.306s, 3347.61/s)  LR: 4.790e-04  Data: 0.022 (0.026)
Train: 154 [ 700/1251 ( 56%)]  Loss: 3.759 (3.40)  Time: 0.313s, 3276.08/s  (0.306s, 3347.34/s)  LR: 4.788e-04  Data: 0.023 (0.026)
Train: 154 [ 750/1251 ( 60%)]  Loss: 3.541 (3.41)  Time: 0.310s, 3300.38/s  (0.306s, 3347.14/s)  LR: 4.785e-04  Data: 0.025 (0.025)
Train: 154 [ 800/1251 ( 64%)]  Loss: 3.047 (3.39)  Time: 0.307s, 3340.43/s  (0.306s, 3347.23/s)  LR: 4.783e-04  Data: 0.025 (0.025)
Train: 154 [ 850/1251 ( 68%)]  Loss: 3.444 (3.39)  Time: 0.309s, 3312.63/s  (0.306s, 3347.19/s)  LR: 4.781e-04  Data: 0.024 (0.025)
Train: 154 [ 900/1251 ( 72%)]  Loss: 3.408 (3.40)  Time: 0.305s, 3357.58/s  (0.306s, 3346.96/s)  LR: 4.779e-04  Data: 0.020 (0.025)
Train: 154 [ 950/1251 ( 76%)]  Loss: 3.480 (3.40)  Time: 0.311s, 3292.07/s  (0.306s, 3347.17/s)  LR: 4.777e-04  Data: 0.024 (0.025)
Train: 154 [1000/1251 ( 80%)]  Loss: 3.296 (3.39)  Time: 0.304s, 3367.62/s  (0.306s, 3347.23/s)  LR: 4.775e-04  Data: 0.017 (0.025)
Train: 154 [1050/1251 ( 84%)]  Loss: 3.682 (3.41)  Time: 0.306s, 3342.33/s  (0.306s, 3347.61/s)  LR: 4.773e-04  Data: 0.024 (0.025)
Train: 154 [1100/1251 ( 88%)]  Loss: 3.251 (3.40)  Time: 0.306s, 3344.60/s  (0.306s, 3347.62/s)  LR: 4.771e-04  Data: 0.022 (0.025)
Train: 154 [1150/1251 ( 92%)]  Loss: 3.204 (3.39)  Time: 0.304s, 3372.10/s  (0.306s, 3347.99/s)  LR: 4.769e-04  Data: 0.023 (0.025)
Train: 154 [1200/1251 ( 96%)]  Loss: 3.261 (3.39)  Time: 0.296s, 3461.97/s  (0.306s, 3348.45/s)  LR: 4.767e-04  Data: 0.015 (0.025)
Train: 154 [1250/1251 (100%)]  Loss: 3.543 (3.39)  Time: 0.277s, 3700.25/s  (0.306s, 3350.50/s)  LR: 4.765e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.065 (2.065)  Loss:  0.5981 (0.5981)  Acc@1: 87.5000 (87.5000)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.058 (0.234)  Loss:  0.6699 (1.1407)  Acc@1: 85.3774 (74.3860)  Acc@5: 96.6981 (92.3020)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-152.pth.tar', 74.54400006347656)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-151.pth.tar', 74.4260000390625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-150.pth.tar', 74.4160001171875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-154.pth.tar', 74.38600011230469)

Train: 155 [   0/1251 (  0%)]  Loss: 2.855 (2.86)  Time: 2.230s,  459.24/s  (2.230s,  459.24/s)  LR: 4.765e-04  Data: 2.005 (2.005)
Train: 155 [  50/1251 (  4%)]  Loss: 3.351 (3.10)  Time: 0.293s, 3488.97/s  (0.328s, 3123.02/s)  LR: 4.763e-04  Data: 0.022 (0.062)
Train: 155 [ 100/1251 (  8%)]  Loss: 3.529 (3.25)  Time: 0.303s, 3379.10/s  (0.314s, 3264.84/s)  LR: 4.760e-04  Data: 0.023 (0.043)
Train: 155 [ 150/1251 ( 12%)]  Loss: 3.682 (3.35)  Time: 0.303s, 3378.36/s  (0.310s, 3304.96/s)  LR: 4.758e-04  Data: 0.022 (0.036)
Train: 155 [ 200/1251 ( 16%)]  Loss: 3.392 (3.36)  Time: 0.301s, 3403.39/s  (0.308s, 3321.50/s)  LR: 4.756e-04  Data: 0.023 (0.033)
Train: 155 [ 250/1251 ( 20%)]  Loss: 3.037 (3.31)  Time: 0.302s, 3387.76/s  (0.307s, 3332.53/s)  LR: 4.754e-04  Data: 0.021 (0.031)
Train: 155 [ 300/1251 ( 24%)]  Loss: 3.636 (3.35)  Time: 0.309s, 3308.86/s  (0.307s, 3338.29/s)  LR: 4.752e-04  Data: 0.024 (0.030)
Train: 155 [ 350/1251 ( 28%)]  Loss: 3.613 (3.39)  Time: 0.305s, 3354.83/s  (0.306s, 3341.92/s)  LR: 4.750e-04  Data: 0.022 (0.029)
Train: 155 [ 400/1251 ( 32%)]  Loss: 3.807 (3.43)  Time: 0.307s, 3332.18/s  (0.306s, 3343.99/s)  LR: 4.748e-04  Data: 0.022 (0.028)
Train: 155 [ 450/1251 ( 36%)]  Loss: 3.651 (3.46)  Time: 0.307s, 3340.67/s  (0.306s, 3345.94/s)  LR: 4.746e-04  Data: 0.025 (0.027)
Train: 155 [ 500/1251 ( 40%)]  Loss: 3.289 (3.44)  Time: 0.304s, 3370.29/s  (0.306s, 3346.29/s)  LR: 4.744e-04  Data: 0.023 (0.027)
Train: 155 [ 550/1251 ( 44%)]  Loss: 3.604 (3.45)  Time: 0.305s, 3361.13/s  (0.306s, 3348.23/s)  LR: 4.742e-04  Data: 0.022 (0.027)
Train: 155 [ 600/1251 ( 48%)]  Loss: 3.406 (3.45)  Time: 0.303s, 3375.32/s  (0.306s, 3349.35/s)  LR: 4.740e-04  Data: 0.022 (0.026)
Train: 155 [ 650/1251 ( 52%)]  Loss: 3.722 (3.47)  Time: 0.307s, 3335.47/s  (0.306s, 3349.38/s)  LR: 4.738e-04  Data: 0.024 (0.026)
Train: 155 [ 700/1251 ( 56%)]  Loss: 3.469 (3.47)  Time: 0.307s, 3330.51/s  (0.306s, 3349.89/s)  LR: 4.735e-04  Data: 0.025 (0.026)
Train: 155 [ 750/1251 ( 60%)]  Loss: 3.764 (3.49)  Time: 0.312s, 3283.70/s  (0.306s, 3350.71/s)  LR: 4.733e-04  Data: 0.024 (0.026)
Train: 155 [ 800/1251 ( 64%)]  Loss: 3.612 (3.50)  Time: 0.302s, 3390.24/s  (0.306s, 3351.43/s)  LR: 4.731e-04  Data: 0.024 (0.026)
Train: 155 [ 850/1251 ( 68%)]  Loss: 2.989 (3.47)  Time: 0.300s, 3410.40/s  (0.305s, 3351.98/s)  LR: 4.729e-04  Data: 0.020 (0.025)
Train: 155 [ 900/1251 ( 72%)]  Loss: 2.983 (3.44)  Time: 0.304s, 3370.87/s  (0.305s, 3352.41/s)  LR: 4.727e-04  Data: 0.023 (0.025)
Train: 155 [ 950/1251 ( 76%)]  Loss: 3.227 (3.43)  Time: 0.303s, 3381.46/s  (0.305s, 3353.20/s)  LR: 4.725e-04  Data: 0.021 (0.025)
Train: 155 [1000/1251 ( 80%)]  Loss: 3.653 (3.44)  Time: 0.305s, 3362.28/s  (0.305s, 3353.55/s)  LR: 4.723e-04  Data: 0.024 (0.025)
Train: 155 [1050/1251 ( 84%)]  Loss: 3.045 (3.42)  Time: 0.306s, 3349.35/s  (0.305s, 3353.78/s)  LR: 4.721e-04  Data: 0.022 (0.025)
Train: 155 [1100/1251 ( 88%)]  Loss: 3.500 (3.43)  Time: 0.308s, 3323.41/s  (0.305s, 3353.57/s)  LR: 4.719e-04  Data: 0.021 (0.025)
Train: 155 [1150/1251 ( 92%)]  Loss: 3.672 (3.44)  Time: 0.303s, 3384.53/s  (0.305s, 3353.50/s)  LR: 4.717e-04  Data: 0.023 (0.025)
Train: 155 [1200/1251 ( 96%)]  Loss: 3.218 (3.43)  Time: 0.303s, 3384.11/s  (0.305s, 3353.97/s)  LR: 4.715e-04  Data: 0.023 (0.025)
Train: 155 [1250/1251 (100%)]  Loss: 3.691 (3.44)  Time: 0.276s, 3711.37/s  (0.305s, 3356.50/s)  LR: 4.713e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.174 (2.174)  Loss:  0.5938 (0.5938)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.055 (0.238)  Loss:  0.7124 (1.1291)  Acc@1: 84.1981 (74.9440)  Acc@5: 96.8160 (92.3380)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-155.pth.tar', 74.9440001171875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-152.pth.tar', 74.54400006347656)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-151.pth.tar', 74.4260000390625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-150.pth.tar', 74.4160001171875)

Train: 156 [   0/1251 (  0%)]  Loss: 3.267 (3.27)  Time: 2.197s,  466.08/s  (2.197s,  466.08/s)  LR: 4.713e-04  Data: 1.947 (1.947)
Train: 156 [  50/1251 (  4%)]  Loss: 3.667 (3.47)  Time: 0.290s, 3533.29/s  (0.323s, 3167.29/s)  LR: 4.710e-04  Data: 0.024 (0.062)
Train: 156 [ 100/1251 (  8%)]  Loss: 3.550 (3.49)  Time: 0.301s, 3396.67/s  (0.310s, 3298.35/s)  LR: 4.708e-04  Data: 0.021 (0.043)
Train: 156 [ 150/1251 ( 12%)]  Loss: 3.497 (3.50)  Time: 0.303s, 3383.52/s  (0.306s, 3341.05/s)  LR: 4.706e-04  Data: 0.021 (0.036)
Train: 156 [ 200/1251 ( 16%)]  Loss: 3.246 (3.45)  Time: 0.298s, 3433.54/s  (0.305s, 3358.59/s)  LR: 4.704e-04  Data: 0.023 (0.033)
Train: 156 [ 250/1251 ( 20%)]  Loss: 3.435 (3.44)  Time: 0.302s, 3386.20/s  (0.304s, 3366.13/s)  LR: 4.702e-04  Data: 0.022 (0.031)
Train: 156 [ 300/1251 ( 24%)]  Loss: 3.310 (3.42)  Time: 0.302s, 3387.63/s  (0.304s, 3371.35/s)  LR: 4.700e-04  Data: 0.025 (0.030)
Train: 156 [ 350/1251 ( 28%)]  Loss: 3.415 (3.42)  Time: 0.298s, 3439.82/s  (0.304s, 3373.80/s)  LR: 4.698e-04  Data: 0.022 (0.029)
Train: 156 [ 400/1251 ( 32%)]  Loss: 3.513 (3.43)  Time: 0.300s, 3409.47/s  (0.303s, 3378.17/s)  LR: 4.696e-04  Data: 0.020 (0.028)
Train: 156 [ 450/1251 ( 36%)]  Loss: 3.612 (3.45)  Time: 0.303s, 3379.88/s  (0.303s, 3380.87/s)  LR: 4.694e-04  Data: 0.023 (0.027)
Train: 156 [ 500/1251 ( 40%)]  Loss: 3.553 (3.46)  Time: 0.304s, 3364.31/s  (0.303s, 3381.75/s)  LR: 4.692e-04  Data: 0.023 (0.027)
Train: 156 [ 550/1251 ( 44%)]  Loss: 3.560 (3.47)  Time: 0.300s, 3416.92/s  (0.303s, 3382.82/s)  LR: 4.690e-04  Data: 0.021 (0.027)
Train: 156 [ 600/1251 ( 48%)]  Loss: 3.617 (3.48)  Time: 0.299s, 3421.43/s  (0.303s, 3383.30/s)  LR: 4.688e-04  Data: 0.022 (0.026)
Train: 156 [ 650/1251 ( 52%)]  Loss: 3.040 (3.45)  Time: 0.299s, 3425.66/s  (0.303s, 3383.52/s)  LR: 4.686e-04  Data: 0.022 (0.026)
Train: 156 [ 700/1251 ( 56%)]  Loss: 3.633 (3.46)  Time: 0.305s, 3360.69/s  (0.303s, 3384.04/s)  LR: 4.683e-04  Data: 0.026 (0.026)
Train: 156 [ 750/1251 ( 60%)]  Loss: 3.535 (3.47)  Time: 0.305s, 3361.30/s  (0.303s, 3384.48/s)  LR: 4.681e-04  Data: 0.024 (0.026)
Train: 156 [ 800/1251 ( 64%)]  Loss: 3.212 (3.45)  Time: 0.298s, 3440.64/s  (0.302s, 3385.29/s)  LR: 4.679e-04  Data: 0.019 (0.026)
Train: 156 [ 850/1251 ( 68%)]  Loss: 3.179 (3.44)  Time: 0.308s, 3324.84/s  (0.302s, 3386.05/s)  LR: 4.677e-04  Data: 0.023 (0.025)
Train: 156 [ 900/1251 ( 72%)]  Loss: 3.396 (3.43)  Time: 0.313s, 3270.10/s  (0.302s, 3385.78/s)  LR: 4.675e-04  Data: 0.027 (0.025)
Train: 156 [ 950/1251 ( 76%)]  Loss: 3.726 (3.45)  Time: 0.302s, 3388.71/s  (0.302s, 3386.20/s)  LR: 4.673e-04  Data: 0.021 (0.025)
Train: 156 [1000/1251 ( 80%)]  Loss: 3.453 (3.45)  Time: 0.301s, 3402.82/s  (0.302s, 3385.95/s)  LR: 4.671e-04  Data: 0.023 (0.025)
Train: 156 [1050/1251 ( 84%)]  Loss: 3.230 (3.44)  Time: 0.307s, 3334.27/s  (0.302s, 3386.43/s)  LR: 4.669e-04  Data: 0.022 (0.025)
Train: 156 [1100/1251 ( 88%)]  Loss: 3.662 (3.45)  Time: 0.307s, 3332.32/s  (0.302s, 3386.76/s)  LR: 4.667e-04  Data: 0.023 (0.025)
Train: 156 [1150/1251 ( 92%)]  Loss: 3.352 (3.44)  Time: 0.302s, 3388.58/s  (0.302s, 3387.50/s)  LR: 4.665e-04  Data: 0.023 (0.025)
Train: 156 [1200/1251 ( 96%)]  Loss: 3.505 (3.45)  Time: 0.307s, 3340.24/s  (0.302s, 3387.63/s)  LR: 4.663e-04  Data: 0.020 (0.025)
Train: 156 [1250/1251 (100%)]  Loss: 3.253 (3.44)  Time: 0.276s, 3707.74/s  (0.302s, 3390.18/s)  LR: 4.661e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.039 (2.039)  Loss:  0.6089 (0.6089)  Acc@1: 87.6953 (87.6953)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.048 (0.233)  Loss:  0.6704 (1.1299)  Acc@1: 85.0236 (74.7240)  Acc@5: 97.1698 (92.2400)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-155.pth.tar', 74.9440001171875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-156.pth.tar', 74.7239999584961)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-152.pth.tar', 74.54400006347656)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-151.pth.tar', 74.4260000390625)

Train: 157 [   0/1251 (  0%)]  Loss: 3.683 (3.68)  Time: 2.676s,  382.72/s  (2.676s,  382.72/s)  LR: 4.661e-04  Data: 2.463 (2.463)
Train: 157 [  50/1251 (  4%)]  Loss: 3.493 (3.59)  Time: 0.295s, 3466.64/s  (0.328s, 3126.45/s)  LR: 4.659e-04  Data: 0.027 (0.071)
Train: 157 [ 100/1251 (  8%)]  Loss: 3.496 (3.56)  Time: 0.299s, 3421.78/s  (0.311s, 3289.90/s)  LR: 4.656e-04  Data: 0.020 (0.048)
Train: 157 [ 150/1251 ( 12%)]  Loss: 3.619 (3.57)  Time: 0.297s, 3452.65/s  (0.307s, 3337.67/s)  LR: 4.654e-04  Data: 0.021 (0.040)
Train: 157 [ 200/1251 ( 16%)]  Loss: 3.644 (3.59)  Time: 0.296s, 3457.29/s  (0.305s, 3361.61/s)  LR: 4.652e-04  Data: 0.022 (0.036)
Train: 157 [ 250/1251 ( 20%)]  Loss: 2.866 (3.47)  Time: 0.297s, 3450.11/s  (0.304s, 3372.93/s)  LR: 4.650e-04  Data: 0.023 (0.033)
Train: 157 [ 300/1251 ( 24%)]  Loss: 3.437 (3.46)  Time: 0.304s, 3372.78/s  (0.303s, 3380.30/s)  LR: 4.648e-04  Data: 0.024 (0.031)
Train: 157 [ 350/1251 ( 28%)]  Loss: 3.654 (3.49)  Time: 0.301s, 3403.56/s  (0.302s, 3385.36/s)  LR: 4.646e-04  Data: 0.021 (0.030)
Train: 157 [ 400/1251 ( 32%)]  Loss: 3.278 (3.46)  Time: 0.288s, 3559.80/s  (0.302s, 3390.54/s)  LR: 4.644e-04  Data: 0.018 (0.029)
Train: 157 [ 450/1251 ( 36%)]  Loss: 3.397 (3.46)  Time: 0.300s, 3409.66/s  (0.302s, 3394.39/s)  LR: 4.642e-04  Data: 0.029 (0.029)
Train: 157 [ 500/1251 ( 40%)]  Loss: 3.744 (3.48)  Time: 0.299s, 3424.86/s  (0.301s, 3398.68/s)  LR: 4.640e-04  Data: 0.024 (0.028)
Train: 157 [ 550/1251 ( 44%)]  Loss: 3.667 (3.50)  Time: 0.292s, 3505.80/s  (0.301s, 3401.00/s)  LR: 4.638e-04  Data: 0.023 (0.028)
Train: 157 [ 600/1251 ( 48%)]  Loss: 3.562 (3.50)  Time: 0.300s, 3412.40/s  (0.301s, 3403.24/s)  LR: 4.636e-04  Data: 0.024 (0.027)
Train: 157 [ 650/1251 ( 52%)]  Loss: 3.582 (3.51)  Time: 0.302s, 3394.30/s  (0.301s, 3404.26/s)  LR: 4.634e-04  Data: 0.023 (0.027)
Train: 157 [ 700/1251 ( 56%)]  Loss: 3.423 (3.50)  Time: 0.299s, 3420.50/s  (0.301s, 3405.34/s)  LR: 4.632e-04  Data: 0.020 (0.027)
Train: 157 [ 750/1251 ( 60%)]  Loss: 3.211 (3.48)  Time: 0.297s, 3451.73/s  (0.301s, 3407.16/s)  LR: 4.629e-04  Data: 0.022 (0.026)
Train: 157 [ 800/1251 ( 64%)]  Loss: 3.676 (3.50)  Time: 0.302s, 3390.91/s  (0.300s, 3408.26/s)  LR: 4.627e-04  Data: 0.021 (0.026)
Train: 157 [ 850/1251 ( 68%)]  Loss: 3.346 (3.49)  Time: 0.293s, 3495.88/s  (0.300s, 3409.44/s)  LR: 4.625e-04  Data: 0.021 (0.026)
Train: 157 [ 900/1251 ( 72%)]  Loss: 3.302 (3.48)  Time: 0.300s, 3417.51/s  (0.300s, 3410.26/s)  LR: 4.623e-04  Data: 0.021 (0.026)
Train: 157 [ 950/1251 ( 76%)]  Loss: 3.316 (3.47)  Time: 0.302s, 3389.59/s  (0.300s, 3411.27/s)  LR: 4.621e-04  Data: 0.022 (0.026)
Train: 157 [1000/1251 ( 80%)]  Loss: 3.111 (3.45)  Time: 0.302s, 3396.03/s  (0.300s, 3411.66/s)  LR: 4.619e-04  Data: 0.021 (0.026)
Train: 157 [1050/1251 ( 84%)]  Loss: 3.393 (3.45)  Time: 0.294s, 3482.41/s  (0.300s, 3411.94/s)  LR: 4.617e-04  Data: 0.023 (0.025)
Train: 157 [1100/1251 ( 88%)]  Loss: 3.299 (3.44)  Time: 0.302s, 3387.90/s  (0.300s, 3411.58/s)  LR: 4.615e-04  Data: 0.022 (0.025)
Train: 157 [1150/1251 ( 92%)]  Loss: 3.565 (3.45)  Time: 0.303s, 3381.24/s  (0.300s, 3411.18/s)  LR: 4.613e-04  Data: 0.023 (0.025)
Train: 157 [1200/1251 ( 96%)]  Loss: 3.207 (3.44)  Time: 0.301s, 3407.20/s  (0.300s, 3410.63/s)  LR: 4.611e-04  Data: 0.016 (0.025)
Train: 157 [1250/1251 (100%)]  Loss: 3.622 (3.45)  Time: 0.275s, 3727.05/s  (0.300s, 3412.67/s)  LR: 4.609e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.055 (2.055)  Loss:  0.5884 (0.5884)  Acc@1: 87.7930 (87.7930)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.052 (0.233)  Loss:  0.7041 (1.1200)  Acc@1: 84.3160 (74.6160)  Acc@5: 95.5189 (92.3740)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-155.pth.tar', 74.9440001171875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-156.pth.tar', 74.7239999584961)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-157.pth.tar', 74.6160000390625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-152.pth.tar', 74.54400006347656)

Train: 158 [   0/1251 (  0%)]  Loss: 3.481 (3.48)  Time: 2.047s,  500.32/s  (2.047s,  500.32/s)  LR: 4.609e-04  Data: 1.787 (1.787)
Train: 158 [  50/1251 (  4%)]  Loss: 3.552 (3.52)  Time: 0.285s, 3594.26/s  (0.316s, 3235.62/s)  LR: 4.607e-04  Data: 0.027 (0.060)
Train: 158 [ 100/1251 (  8%)]  Loss: 3.418 (3.48)  Time: 0.289s, 3538.09/s  (0.304s, 3366.18/s)  LR: 4.605e-04  Data: 0.022 (0.041)
Train: 158 [ 150/1251 ( 12%)]  Loss: 3.630 (3.52)  Time: 0.293s, 3497.78/s  (0.301s, 3400.44/s)  LR: 4.602e-04  Data: 0.021 (0.035)
Train: 158 [ 200/1251 ( 16%)]  Loss: 3.248 (3.47)  Time: 0.292s, 3506.51/s  (0.300s, 3415.96/s)  LR: 4.600e-04  Data: 0.024 (0.032)
Train: 158 [ 250/1251 ( 20%)]  Loss: 3.504 (3.47)  Time: 0.301s, 3402.48/s  (0.299s, 3421.53/s)  LR: 4.598e-04  Data: 0.025 (0.031)
Train: 158 [ 300/1251 ( 24%)]  Loss: 3.432 (3.47)  Time: 0.295s, 3465.42/s  (0.299s, 3425.50/s)  LR: 4.596e-04  Data: 0.022 (0.029)
Train: 158 [ 350/1251 ( 28%)]  Loss: 3.292 (3.44)  Time: 0.296s, 3460.48/s  (0.299s, 3426.74/s)  LR: 4.594e-04  Data: 0.023 (0.028)
Train: 158 [ 400/1251 ( 32%)]  Loss: 3.472 (3.45)  Time: 0.298s, 3438.44/s  (0.299s, 3425.63/s)  LR: 4.592e-04  Data: 0.029 (0.028)
Train: 158 [ 450/1251 ( 36%)]  Loss: 3.585 (3.46)  Time: 0.303s, 3384.88/s  (0.299s, 3426.57/s)  LR: 4.590e-04  Data: 0.024 (0.027)
Train: 158 [ 500/1251 ( 40%)]  Loss: 3.557 (3.47)  Time: 0.302s, 3393.54/s  (0.299s, 3426.61/s)  LR: 4.588e-04  Data: 0.022 (0.027)
Train: 158 [ 550/1251 ( 44%)]  Loss: 3.502 (3.47)  Time: 0.298s, 3440.16/s  (0.299s, 3426.43/s)  LR: 4.586e-04  Data: 0.020 (0.027)
Train: 158 [ 600/1251 ( 48%)]  Loss: 3.512 (3.48)  Time: 0.297s, 3451.19/s  (0.299s, 3427.00/s)  LR: 4.584e-04  Data: 0.018 (0.026)
Train: 158 [ 650/1251 ( 52%)]  Loss: 3.395 (3.47)  Time: 0.301s, 3404.78/s  (0.299s, 3427.16/s)  LR: 4.582e-04  Data: 0.026 (0.026)
Train: 158 [ 700/1251 ( 56%)]  Loss: 3.576 (3.48)  Time: 0.292s, 3503.04/s  (0.299s, 3427.78/s)  LR: 4.580e-04  Data: 0.019 (0.026)
Train: 158 [ 750/1251 ( 60%)]  Loss: 3.606 (3.49)  Time: 0.298s, 3434.71/s  (0.299s, 3428.05/s)  LR: 4.578e-04  Data: 0.026 (0.026)
Train: 158 [ 800/1251 ( 64%)]  Loss: 3.541 (3.49)  Time: 0.299s, 3424.82/s  (0.299s, 3428.34/s)  LR: 4.575e-04  Data: 0.022 (0.026)
Train: 158 [ 850/1251 ( 68%)]  Loss: 3.440 (3.49)  Time: 0.297s, 3446.89/s  (0.299s, 3427.93/s)  LR: 4.573e-04  Data: 0.023 (0.025)
Train: 158 [ 900/1251 ( 72%)]  Loss: 3.295 (3.48)  Time: 0.302s, 3386.00/s  (0.299s, 3428.10/s)  LR: 4.571e-04  Data: 0.026 (0.025)
Train: 158 [ 950/1251 ( 76%)]  Loss: 3.776 (3.49)  Time: 0.305s, 3359.99/s  (0.299s, 3428.30/s)  LR: 4.569e-04  Data: 0.021 (0.025)
Train: 158 [1000/1251 ( 80%)]  Loss: 3.324 (3.48)  Time: 0.304s, 3371.96/s  (0.299s, 3428.35/s)  LR: 4.567e-04  Data: 0.019 (0.025)
Train: 158 [1050/1251 ( 84%)]  Loss: 3.458 (3.48)  Time: 0.299s, 3427.18/s  (0.299s, 3428.10/s)  LR: 4.565e-04  Data: 0.022 (0.025)
Train: 158 [1100/1251 ( 88%)]  Loss: 3.303 (3.47)  Time: 0.293s, 3497.58/s  (0.299s, 3428.20/s)  LR: 4.563e-04  Data: 0.022 (0.025)
Train: 158 [1150/1251 ( 92%)]  Loss: 2.945 (3.45)  Time: 0.301s, 3400.65/s  (0.299s, 3427.76/s)  LR: 4.561e-04  Data: 0.027 (0.025)
Train: 158 [1200/1251 ( 96%)]  Loss: 3.370 (3.45)  Time: 0.300s, 3413.01/s  (0.299s, 3427.40/s)  LR: 4.559e-04  Data: 0.024 (0.025)
Train: 158 [1250/1251 (100%)]  Loss: 3.578 (3.45)  Time: 0.275s, 3730.13/s  (0.299s, 3428.98/s)  LR: 4.557e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.129 (2.129)  Loss:  0.6309 (0.6309)  Acc@1: 88.0859 (88.0859)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.040 (0.236)  Loss:  0.7183 (1.1219)  Acc@1: 84.0802 (74.9620)  Acc@5: 95.8726 (92.4000)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-158.pth.tar', 74.96199993652344)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-155.pth.tar', 74.9440001171875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-156.pth.tar', 74.7239999584961)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-157.pth.tar', 74.6160000390625)

Train: 159 [   0/1251 (  0%)]  Loss: 3.389 (3.39)  Time: 2.304s,  444.50/s  (2.304s,  444.50/s)  LR: 4.557e-04  Data: 2.078 (2.078)
Train: 159 [  50/1251 (  4%)]  Loss: 3.009 (3.20)  Time: 0.277s, 3694.62/s  (0.322s, 3181.16/s)  LR: 4.555e-04  Data: 0.023 (0.066)
Train: 159 [ 100/1251 (  8%)]  Loss: 3.451 (3.28)  Time: 0.294s, 3479.35/s  (0.307s, 3338.38/s)  LR: 4.553e-04  Data: 0.025 (0.045)
Train: 159 [ 150/1251 ( 12%)]  Loss: 3.162 (3.25)  Time: 0.298s, 3431.91/s  (0.303s, 3383.36/s)  LR: 4.551e-04  Data: 0.013 (0.038)
Train: 159 [ 200/1251 ( 16%)]  Loss: 3.578 (3.32)  Time: 0.294s, 3482.50/s  (0.301s, 3404.58/s)  LR: 4.548e-04  Data: 0.023 (0.034)
Train: 159 [ 250/1251 ( 20%)]  Loss: 3.462 (3.34)  Time: 0.290s, 3529.93/s  (0.300s, 3416.92/s)  LR: 4.546e-04  Data: 0.023 (0.032)
Train: 159 [ 300/1251 ( 24%)]  Loss: 3.380 (3.35)  Time: 0.299s, 3423.72/s  (0.299s, 3423.82/s)  LR: 4.544e-04  Data: 0.023 (0.030)
Train: 159 [ 350/1251 ( 28%)]  Loss: 3.557 (3.37)  Time: 0.292s, 3508.69/s  (0.299s, 3429.09/s)  LR: 4.542e-04  Data: 0.023 (0.029)
Train: 159 [ 400/1251 ( 32%)]  Loss: 3.447 (3.38)  Time: 0.302s, 3385.49/s  (0.298s, 3430.76/s)  LR: 4.540e-04  Data: 0.024 (0.029)
Train: 159 [ 450/1251 ( 36%)]  Loss: 3.227 (3.37)  Time: 0.305s, 3362.03/s  (0.298s, 3433.36/s)  LR: 4.538e-04  Data: 0.028 (0.028)
Train: 159 [ 500/1251 ( 40%)]  Loss: 3.793 (3.41)  Time: 0.300s, 3409.34/s  (0.298s, 3434.67/s)  LR: 4.536e-04  Data: 0.025 (0.027)
Train: 159 [ 550/1251 ( 44%)]  Loss: 2.791 (3.35)  Time: 0.297s, 3446.14/s  (0.298s, 3435.81/s)  LR: 4.534e-04  Data: 0.028 (0.027)
Train: 159 [ 600/1251 ( 48%)]  Loss: 3.314 (3.35)  Time: 0.300s, 3416.57/s  (0.298s, 3436.33/s)  LR: 4.532e-04  Data: 0.024 (0.027)
Train: 159 [ 650/1251 ( 52%)]  Loss: 3.654 (3.37)  Time: 0.302s, 3394.28/s  (0.298s, 3436.05/s)  LR: 4.530e-04  Data: 0.025 (0.026)
Train: 159 [ 700/1251 ( 56%)]  Loss: 3.383 (3.37)  Time: 0.298s, 3435.34/s  (0.298s, 3435.81/s)  LR: 4.528e-04  Data: 0.025 (0.026)
Train: 159 [ 750/1251 ( 60%)]  Loss: 3.289 (3.37)  Time: 0.300s, 3417.20/s  (0.298s, 3435.13/s)  LR: 4.526e-04  Data: 0.023 (0.026)
Train: 159 [ 800/1251 ( 64%)]  Loss: 3.596 (3.38)  Time: 0.297s, 3445.26/s  (0.298s, 3434.86/s)  LR: 4.524e-04  Data: 0.024 (0.026)
Train: 159 [ 850/1251 ( 68%)]  Loss: 3.255 (3.37)  Time: 0.299s, 3428.77/s  (0.298s, 3435.31/s)  LR: 4.522e-04  Data: 0.024 (0.026)
Train: 159 [ 900/1251 ( 72%)]  Loss: 3.513 (3.38)  Time: 0.299s, 3424.92/s  (0.298s, 3435.46/s)  LR: 4.519e-04  Data: 0.026 (0.026)
Train: 159 [ 950/1251 ( 76%)]  Loss: 3.651 (3.40)  Time: 0.297s, 3442.06/s  (0.298s, 3435.52/s)  LR: 4.517e-04  Data: 0.023 (0.025)
Train: 159 [1000/1251 ( 80%)]  Loss: 3.403 (3.40)  Time: 0.292s, 3502.18/s  (0.298s, 3435.77/s)  LR: 4.515e-04  Data: 0.023 (0.025)
Train: 159 [1050/1251 ( 84%)]  Loss: 3.672 (3.41)  Time: 0.299s, 3425.38/s  (0.298s, 3436.09/s)  LR: 4.513e-04  Data: 0.021 (0.025)
Train: 159 [1100/1251 ( 88%)]  Loss: 3.614 (3.42)  Time: 0.305s, 3357.36/s  (0.298s, 3435.63/s)  LR: 4.511e-04  Data: 0.024 (0.025)
Train: 159 [1150/1251 ( 92%)]  Loss: 3.683 (3.43)  Time: 0.300s, 3407.87/s  (0.298s, 3435.04/s)  LR: 4.509e-04  Data: 0.026 (0.025)
Train: 159 [1200/1251 ( 96%)]  Loss: 3.483 (3.43)  Time: 0.302s, 3395.00/s  (0.298s, 3434.37/s)  LR: 4.507e-04  Data: 0.023 (0.025)
Train: 159 [1250/1251 (100%)]  Loss: 3.490 (3.43)  Time: 0.275s, 3723.34/s  (0.298s, 3436.19/s)  LR: 4.505e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.065 (2.065)  Loss:  0.5762 (0.5762)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.049 (0.234)  Loss:  0.6973 (1.1020)  Acc@1: 84.4340 (74.6380)  Acc@5: 96.8160 (92.4840)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-158.pth.tar', 74.96199993652344)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-155.pth.tar', 74.9440001171875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-156.pth.tar', 74.7239999584961)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-159.pth.tar', 74.63800009033203)

Train: 160 [   0/1251 (  0%)]  Loss: 3.414 (3.41)  Time: 2.238s,  457.65/s  (2.238s,  457.65/s)  LR: 4.505e-04  Data: 2.013 (2.013)
Train: 160 [  50/1251 (  4%)]  Loss: 3.363 (3.39)  Time: 0.293s, 3495.35/s  (0.317s, 3229.32/s)  LR: 4.503e-04  Data: 0.024 (0.063)
Train: 160 [ 100/1251 (  8%)]  Loss: 3.497 (3.42)  Time: 0.291s, 3513.71/s  (0.304s, 3365.52/s)  LR: 4.501e-04  Data: 0.022 (0.043)
Train: 160 [ 150/1251 ( 12%)]  Loss: 3.534 (3.45)  Time: 0.297s, 3446.56/s  (0.301s, 3407.44/s)  LR: 4.499e-04  Data: 0.022 (0.036)
Train: 160 [ 200/1251 ( 16%)]  Loss: 3.805 (3.52)  Time: 0.297s, 3450.11/s  (0.299s, 3426.01/s)  LR: 4.497e-04  Data: 0.023 (0.033)
Train: 160 [ 250/1251 ( 20%)]  Loss: 3.437 (3.51)  Time: 0.287s, 3562.83/s  (0.298s, 3436.16/s)  LR: 4.495e-04  Data: 0.020 (0.031)
Train: 160 [ 300/1251 ( 24%)]  Loss: 3.305 (3.48)  Time: 0.293s, 3493.03/s  (0.298s, 3441.72/s)  LR: 4.493e-04  Data: 0.022 (0.030)
Train: 160 [ 350/1251 ( 28%)]  Loss: 3.142 (3.44)  Time: 0.296s, 3465.23/s  (0.297s, 3443.15/s)  LR: 4.490e-04  Data: 0.023 (0.029)
Train: 160 [ 400/1251 ( 32%)]  Loss: 3.468 (3.44)  Time: 0.298s, 3437.25/s  (0.297s, 3444.12/s)  LR: 4.488e-04  Data: 0.026 (0.028)
Train: 160 [ 450/1251 ( 36%)]  Loss: 3.379 (3.43)  Time: 0.294s, 3477.64/s  (0.297s, 3444.79/s)  LR: 4.486e-04  Data: 0.026 (0.027)
Train: 160 [ 500/1251 ( 40%)]  Loss: 3.537 (3.44)  Time: 0.305s, 3361.12/s  (0.297s, 3443.94/s)  LR: 4.484e-04  Data: 0.024 (0.027)
Train: 160 [ 550/1251 ( 44%)]  Loss: 3.225 (3.43)  Time: 0.301s, 3407.35/s  (0.297s, 3444.65/s)  LR: 4.482e-04  Data: 0.025 (0.027)
Train: 160 [ 600/1251 ( 48%)]  Loss: 3.671 (3.44)  Time: 0.299s, 3422.87/s  (0.297s, 3443.59/s)  LR: 4.480e-04  Data: 0.025 (0.026)
Train: 160 [ 650/1251 ( 52%)]  Loss: 3.445 (3.44)  Time: 0.291s, 3515.79/s  (0.297s, 3444.16/s)  LR: 4.478e-04  Data: 0.022 (0.026)
Train: 160 [ 700/1251 ( 56%)]  Loss: 3.333 (3.44)  Time: 0.306s, 3343.90/s  (0.297s, 3443.76/s)  LR: 4.476e-04  Data: 0.023 (0.026)
Train: 160 [ 750/1251 ( 60%)]  Loss: 3.495 (3.44)  Time: 0.292s, 3509.70/s  (0.297s, 3443.19/s)  LR: 4.474e-04  Data: 0.024 (0.026)
Train: 160 [ 800/1251 ( 64%)]  Loss: 3.371 (3.44)  Time: 0.296s, 3456.43/s  (0.297s, 3443.46/s)  LR: 4.472e-04  Data: 0.020 (0.026)
Train: 160 [ 850/1251 ( 68%)]  Loss: 3.476 (3.44)  Time: 0.298s, 3438.37/s  (0.297s, 3442.60/s)  LR: 4.470e-04  Data: 0.022 (0.026)
Train: 160 [ 900/1251 ( 72%)]  Loss: 3.404 (3.44)  Time: 0.303s, 3375.16/s  (0.297s, 3443.10/s)  LR: 4.468e-04  Data: 0.023 (0.025)
Train: 160 [ 950/1251 ( 76%)]  Loss: 3.338 (3.43)  Time: 0.292s, 3512.83/s  (0.297s, 3442.86/s)  LR: 4.466e-04  Data: 0.024 (0.025)
Train: 160 [1000/1251 ( 80%)]  Loss: 3.320 (3.43)  Time: 0.297s, 3447.74/s  (0.297s, 3442.65/s)  LR: 4.464e-04  Data: 0.024 (0.025)
Train: 160 [1050/1251 ( 84%)]  Loss: 3.440 (3.43)  Time: 0.301s, 3399.34/s  (0.297s, 3442.31/s)  LR: 4.461e-04  Data: 0.025 (0.025)
Train: 160 [1100/1251 ( 88%)]  Loss: 3.602 (3.43)  Time: 0.298s, 3430.70/s  (0.297s, 3442.56/s)  LR: 4.459e-04  Data: 0.025 (0.025)
Train: 160 [1150/1251 ( 92%)]  Loss: 2.971 (3.42)  Time: 0.297s, 3451.24/s  (0.297s, 3442.88/s)  LR: 4.457e-04  Data: 0.017 (0.025)
Train: 160 [1200/1251 ( 96%)]  Loss: 3.422 (3.42)  Time: 0.295s, 3468.08/s  (0.297s, 3442.58/s)  LR: 4.455e-04  Data: 0.019 (0.025)
Train: 160 [1250/1251 (100%)]  Loss: 3.473 (3.42)  Time: 0.275s, 3722.07/s  (0.297s, 3444.34/s)  LR: 4.453e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.278 (2.278)  Loss:  0.5894 (0.5894)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.043 (0.244)  Loss:  0.7085 (1.1214)  Acc@1: 84.0802 (74.8900)  Acc@5: 95.7547 (92.5040)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-158.pth.tar', 74.96199993652344)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-155.pth.tar', 74.9440001171875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-160.pth.tar', 74.89000006591797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-156.pth.tar', 74.7239999584961)

Train: 161 [   0/1251 (  0%)]  Loss: 3.553 (3.55)  Time: 2.371s,  431.84/s  (2.371s,  431.84/s)  LR: 4.453e-04  Data: 2.150 (2.150)
Train: 161 [  50/1251 (  4%)]  Loss: 3.157 (3.36)  Time: 0.299s, 3427.70/s  (0.331s, 3092.67/s)  LR: 4.451e-04  Data: 0.022 (0.075)
Train: 161 [ 100/1251 (  8%)]  Loss: 3.280 (3.33)  Time: 0.296s, 3456.91/s  (0.312s, 3284.10/s)  LR: 4.449e-04  Data: 0.025 (0.049)
Train: 161 [ 150/1251 ( 12%)]  Loss: 3.610 (3.40)  Time: 0.296s, 3457.91/s  (0.306s, 3351.23/s)  LR: 4.447e-04  Data: 0.026 (0.041)
Train: 161 [ 200/1251 ( 16%)]  Loss: 3.372 (3.39)  Time: 0.292s, 3506.02/s  (0.303s, 3380.33/s)  LR: 4.445e-04  Data: 0.022 (0.037)
Train: 161 [ 250/1251 ( 20%)]  Loss: 3.234 (3.37)  Time: 0.294s, 3477.91/s  (0.302s, 3392.80/s)  LR: 4.443e-04  Data: 0.023 (0.034)
Train: 161 [ 300/1251 ( 24%)]  Loss: 3.441 (3.38)  Time: 0.299s, 3429.11/s  (0.301s, 3401.65/s)  LR: 4.441e-04  Data: 0.023 (0.032)
Train: 161 [ 350/1251 ( 28%)]  Loss: 3.360 (3.38)  Time: 0.294s, 3480.47/s  (0.300s, 3407.77/s)  LR: 4.439e-04  Data: 0.023 (0.031)
Train: 161 [ 400/1251 ( 32%)]  Loss: 3.466 (3.39)  Time: 0.301s, 3396.96/s  (0.300s, 3411.45/s)  LR: 4.437e-04  Data: 0.017 (0.030)
Train: 161 [ 450/1251 ( 36%)]  Loss: 3.367 (3.38)  Time: 0.297s, 3452.76/s  (0.300s, 3414.83/s)  LR: 4.435e-04  Data: 0.022 (0.029)
Train: 161 [ 500/1251 ( 40%)]  Loss: 3.406 (3.39)  Time: 0.296s, 3464.38/s  (0.300s, 3418.71/s)  LR: 4.432e-04  Data: 0.021 (0.028)
Train: 161 [ 550/1251 ( 44%)]  Loss: 3.338 (3.38)  Time: 0.295s, 3474.82/s  (0.299s, 3420.36/s)  LR: 4.430e-04  Data: 0.022 (0.028)
Train: 161 [ 600/1251 ( 48%)]  Loss: 3.104 (3.36)  Time: 0.302s, 3388.80/s  (0.299s, 3422.02/s)  LR: 4.428e-04  Data: 0.022 (0.027)
Train: 161 [ 650/1251 ( 52%)]  Loss: 3.313 (3.36)  Time: 0.293s, 3495.81/s  (0.299s, 3422.71/s)  LR: 4.426e-04  Data: 0.023 (0.027)
Train: 161 [ 700/1251 ( 56%)]  Loss: 3.646 (3.38)  Time: 0.296s, 3462.57/s  (0.299s, 3424.14/s)  LR: 4.424e-04  Data: 0.022 (0.027)
Train: 161 [ 750/1251 ( 60%)]  Loss: 3.549 (3.39)  Time: 0.301s, 3397.09/s  (0.299s, 3424.46/s)  LR: 4.422e-04  Data: 0.025 (0.027)
Train: 161 [ 800/1251 ( 64%)]  Loss: 3.413 (3.39)  Time: 0.296s, 3464.65/s  (0.299s, 3424.74/s)  LR: 4.420e-04  Data: 0.024 (0.026)
Train: 161 [ 850/1251 ( 68%)]  Loss: 3.167 (3.38)  Time: 0.298s, 3431.39/s  (0.299s, 3425.03/s)  LR: 4.418e-04  Data: 0.024 (0.026)
Train: 161 [ 900/1251 ( 72%)]  Loss: 3.715 (3.39)  Time: 0.303s, 3379.43/s  (0.299s, 3424.48/s)  LR: 4.416e-04  Data: 0.024 (0.026)
Train: 161 [ 950/1251 ( 76%)]  Loss: 3.028 (3.38)  Time: 0.298s, 3435.55/s  (0.299s, 3425.08/s)  LR: 4.414e-04  Data: 0.022 (0.026)
Train: 161 [1000/1251 ( 80%)]  Loss: 3.496 (3.38)  Time: 0.296s, 3454.78/s  (0.299s, 3424.83/s)  LR: 4.412e-04  Data: 0.021 (0.026)
Train: 161 [1050/1251 ( 84%)]  Loss: 3.383 (3.38)  Time: 0.297s, 3447.09/s  (0.299s, 3424.62/s)  LR: 4.410e-04  Data: 0.021 (0.025)
Train: 161 [1100/1251 ( 88%)]  Loss: 3.549 (3.39)  Time: 0.298s, 3433.97/s  (0.299s, 3424.62/s)  LR: 4.408e-04  Data: 0.023 (0.025)
Train: 161 [1150/1251 ( 92%)]  Loss: 3.385 (3.39)  Time: 0.303s, 3383.07/s  (0.299s, 3424.84/s)  LR: 4.406e-04  Data: 0.023 (0.025)
Train: 161 [1200/1251 ( 96%)]  Loss: 3.467 (3.39)  Time: 0.306s, 3343.64/s  (0.299s, 3425.29/s)  LR: 4.404e-04  Data: 0.020 (0.025)
Train: 161 [1250/1251 (100%)]  Loss: 3.508 (3.40)  Time: 0.275s, 3721.71/s  (0.299s, 3427.33/s)  LR: 4.401e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.062 (2.062)  Loss:  0.5752 (0.5752)  Acc@1: 88.1836 (88.1836)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.061 (0.236)  Loss:  0.6792 (1.0982)  Acc@1: 85.7311 (75.0300)  Acc@5: 96.8160 (92.4240)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-161.pth.tar', 75.03000013671875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-158.pth.tar', 74.96199993652344)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-155.pth.tar', 74.9440001171875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-160.pth.tar', 74.89000006591797)

Train: 162 [   0/1251 (  0%)]  Loss: 3.431 (3.43)  Time: 2.555s,  400.76/s  (2.555s,  400.76/s)  LR: 4.401e-04  Data: 2.338 (2.338)
Train: 162 [  50/1251 (  4%)]  Loss: 3.360 (3.40)  Time: 0.289s, 3546.56/s  (0.327s, 3129.40/s)  LR: 4.399e-04  Data: 0.022 (0.068)
Train: 162 [ 100/1251 (  8%)]  Loss: 2.939 (3.24)  Time: 0.306s, 3349.32/s  (0.311s, 3292.25/s)  LR: 4.397e-04  Data: 0.022 (0.046)
Train: 162 [ 150/1251 ( 12%)]  Loss: 3.557 (3.32)  Time: 0.300s, 3416.42/s  (0.306s, 3343.84/s)  LR: 4.395e-04  Data: 0.022 (0.038)
Train: 162 [ 200/1251 ( 16%)]  Loss: 3.216 (3.30)  Time: 0.294s, 3485.61/s  (0.304s, 3368.15/s)  LR: 4.393e-04  Data: 0.024 (0.035)
Train: 162 [ 250/1251 ( 20%)]  Loss: 3.362 (3.31)  Time: 0.297s, 3446.87/s  (0.303s, 3382.76/s)  LR: 4.391e-04  Data: 0.022 (0.032)
Train: 162 [ 300/1251 ( 24%)]  Loss: 3.222 (3.30)  Time: 0.297s, 3443.68/s  (0.302s, 3390.86/s)  LR: 4.389e-04  Data: 0.023 (0.031)
Train: 162 [ 350/1251 ( 28%)]  Loss: 2.997 (3.26)  Time: 0.301s, 3403.95/s  (0.302s, 3396.04/s)  LR: 4.387e-04  Data: 0.022 (0.030)
Train: 162 [ 400/1251 ( 32%)]  Loss: 3.259 (3.26)  Time: 0.298s, 3439.35/s  (0.301s, 3399.94/s)  LR: 4.385e-04  Data: 0.022 (0.029)
Train: 162 [ 450/1251 ( 36%)]  Loss: 3.550 (3.29)  Time: 0.297s, 3452.73/s  (0.301s, 3404.32/s)  LR: 4.383e-04  Data: 0.023 (0.028)
Train: 162 [ 500/1251 ( 40%)]  Loss: 3.063 (3.27)  Time: 0.302s, 3391.35/s  (0.301s, 3406.14/s)  LR: 4.381e-04  Data: 0.025 (0.028)
Train: 162 [ 550/1251 ( 44%)]  Loss: 3.535 (3.29)  Time: 0.303s, 3377.04/s  (0.300s, 3407.92/s)  LR: 4.379e-04  Data: 0.020 (0.027)
Train: 162 [ 600/1251 ( 48%)]  Loss: 3.502 (3.31)  Time: 0.297s, 3445.93/s  (0.300s, 3409.80/s)  LR: 4.377e-04  Data: 0.023 (0.027)
Train: 162 [ 650/1251 ( 52%)]  Loss: 3.260 (3.30)  Time: 0.302s, 3390.29/s  (0.300s, 3409.90/s)  LR: 4.375e-04  Data: 0.024 (0.027)
Train: 162 [ 700/1251 ( 56%)]  Loss: 3.369 (3.31)  Time: 0.298s, 3432.69/s  (0.300s, 3410.12/s)  LR: 4.373e-04  Data: 0.020 (0.026)
Train: 162 [ 750/1251 ( 60%)]  Loss: 3.272 (3.31)  Time: 0.303s, 3381.63/s  (0.300s, 3410.43/s)  LR: 4.370e-04  Data: 0.025 (0.026)
Train: 162 [ 800/1251 ( 64%)]  Loss: 3.459 (3.31)  Time: 0.294s, 3478.42/s  (0.300s, 3410.48/s)  LR: 4.368e-04  Data: 0.023 (0.026)
Train: 162 [ 850/1251 ( 68%)]  Loss: 3.304 (3.31)  Time: 0.296s, 3462.72/s  (0.300s, 3410.40/s)  LR: 4.366e-04  Data: 0.028 (0.026)
Train: 162 [ 900/1251 ( 72%)]  Loss: 3.583 (3.33)  Time: 0.302s, 3386.36/s  (0.300s, 3411.02/s)  LR: 4.364e-04  Data: 0.018 (0.026)
Train: 162 [ 950/1251 ( 76%)]  Loss: 3.558 (3.34)  Time: 0.302s, 3390.62/s  (0.300s, 3411.06/s)  LR: 4.362e-04  Data: 0.022 (0.026)
Train: 162 [1000/1251 ( 80%)]  Loss: 3.441 (3.34)  Time: 0.301s, 3402.63/s  (0.300s, 3410.56/s)  LR: 4.360e-04  Data: 0.016 (0.026)
Train: 162 [1050/1251 ( 84%)]  Loss: 3.484 (3.35)  Time: 0.302s, 3395.53/s  (0.300s, 3409.89/s)  LR: 4.358e-04  Data: 0.030 (0.025)
Train: 162 [1100/1251 ( 88%)]  Loss: 3.470 (3.36)  Time: 0.304s, 3368.13/s  (0.300s, 3410.00/s)  LR: 4.356e-04  Data: 0.025 (0.025)
Train: 162 [1150/1251 ( 92%)]  Loss: 3.509 (3.36)  Time: 0.300s, 3412.15/s  (0.300s, 3409.42/s)  LR: 4.354e-04  Data: 0.021 (0.025)
Train: 162 [1200/1251 ( 96%)]  Loss: 3.464 (3.37)  Time: 0.297s, 3444.36/s  (0.300s, 3408.82/s)  LR: 4.352e-04  Data: 0.025 (0.025)
Train: 162 [1250/1251 (100%)]  Loss: 3.521 (3.37)  Time: 0.275s, 3726.48/s  (0.300s, 3410.69/s)  LR: 4.350e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.087 (2.087)  Loss:  0.5591 (0.5591)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.046 (0.233)  Loss:  0.6592 (1.0883)  Acc@1: 85.2594 (74.8700)  Acc@5: 95.4009 (92.4820)
Train: 163 [   0/1251 (  0%)]  Loss: 3.265 (3.26)  Time: 1.935s,  529.25/s  (1.935s,  529.25/s)  LR: 4.350e-04  Data: 1.696 (1.696)
Train: 163 [  50/1251 (  4%)]  Loss: 3.295 (3.28)  Time: 0.296s, 3454.02/s  (0.318s, 3217.35/s)  LR: 4.348e-04  Data: 0.026 (0.057)
Train: 163 [ 100/1251 (  8%)]  Loss: 3.506 (3.35)  Time: 0.297s, 3452.77/s  (0.307s, 3340.69/s)  LR: 4.346e-04  Data: 0.022 (0.039)
Train: 163 [ 150/1251 ( 12%)]  Loss: 3.360 (3.36)  Time: 0.298s, 3433.59/s  (0.303s, 3379.32/s)  LR: 4.344e-04  Data: 0.025 (0.034)
Train: 163 [ 200/1251 ( 16%)]  Loss: 3.812 (3.45)  Time: 0.299s, 3426.23/s  (0.302s, 3392.14/s)  LR: 4.342e-04  Data: 0.022 (0.031)
Train: 163 [ 250/1251 ( 20%)]  Loss: 3.614 (3.48)  Time: 0.301s, 3402.61/s  (0.301s, 3400.33/s)  LR: 4.339e-04  Data: 0.026 (0.030)
Train: 163 [ 300/1251 ( 24%)]  Loss: 3.622 (3.50)  Time: 0.305s, 3354.38/s  (0.301s, 3404.21/s)  LR: 4.337e-04  Data: 0.024 (0.028)
Train: 163 [ 350/1251 ( 28%)]  Loss: 3.366 (3.48)  Time: 0.300s, 3410.67/s  (0.301s, 3407.13/s)  LR: 4.335e-04  Data: 0.024 (0.028)
Train: 163 [ 400/1251 ( 32%)]  Loss: 3.286 (3.46)  Time: 0.292s, 3502.51/s  (0.300s, 3409.85/s)  LR: 4.333e-04  Data: 0.022 (0.027)
Train: 163 [ 450/1251 ( 36%)]  Loss: 3.321 (3.44)  Time: 0.303s, 3379.43/s  (0.300s, 3409.78/s)  LR: 4.331e-04  Data: 0.021 (0.027)
Train: 163 [ 500/1251 ( 40%)]  Loss: 3.534 (3.45)  Time: 0.298s, 3437.16/s  (0.300s, 3410.23/s)  LR: 4.329e-04  Data: 0.025 (0.026)
Train: 163 [ 550/1251 ( 44%)]  Loss: 3.329 (3.44)  Time: 0.295s, 3467.07/s  (0.300s, 3410.25/s)  LR: 4.327e-04  Data: 0.020 (0.026)
Train: 163 [ 600/1251 ( 48%)]  Loss: 3.509 (3.45)  Time: 0.298s, 3430.58/s  (0.300s, 3410.56/s)  LR: 4.325e-04  Data: 0.023 (0.026)
Train: 163 [ 650/1251 ( 52%)]  Loss: 3.578 (3.46)  Time: 0.305s, 3353.59/s  (0.300s, 3410.23/s)  LR: 4.323e-04  Data: 0.025 (0.026)
Train: 163 [ 700/1251 ( 56%)]  Loss: 3.390 (3.45)  Time: 0.306s, 3347.14/s  (0.300s, 3410.04/s)  LR: 4.321e-04  Data: 0.022 (0.025)
Train: 163 [ 750/1251 ( 60%)]  Loss: 3.721 (3.47)  Time: 0.303s, 3380.68/s  (0.300s, 3410.35/s)  LR: 4.319e-04  Data: 0.024 (0.025)
Train: 163 [ 800/1251 ( 64%)]  Loss: 3.253 (3.46)  Time: 0.302s, 3394.04/s  (0.300s, 3409.47/s)  LR: 4.317e-04  Data: 0.024 (0.025)
Train: 163 [ 850/1251 ( 68%)]  Loss: 3.331 (3.45)  Time: 0.300s, 3415.85/s  (0.300s, 3409.43/s)  LR: 4.315e-04  Data: 0.023 (0.025)
Train: 163 [ 900/1251 ( 72%)]  Loss: 3.298 (3.44)  Time: 0.300s, 3408.08/s  (0.300s, 3409.53/s)  LR: 4.313e-04  Data: 0.021 (0.025)
Train: 163 [ 950/1251 ( 76%)]  Loss: 3.440 (3.44)  Time: 0.303s, 3376.77/s  (0.300s, 3408.75/s)  LR: 4.311e-04  Data: 0.026 (0.025)
Train: 163 [1000/1251 ( 80%)]  Loss: 3.305 (3.43)  Time: 0.300s, 3417.83/s  (0.300s, 3408.73/s)  LR: 4.309e-04  Data: 0.023 (0.025)
Train: 163 [1050/1251 ( 84%)]  Loss: 3.386 (3.43)  Time: 0.302s, 3396.27/s  (0.300s, 3408.31/s)  LR: 4.306e-04  Data: 0.024 (0.025)
Train: 163 [1100/1251 ( 88%)]  Loss: 3.572 (3.44)  Time: 0.306s, 3347.31/s  (0.300s, 3408.19/s)  LR: 4.304e-04  Data: 0.022 (0.025)
Train: 163 [1150/1251 ( 92%)]  Loss: 3.676 (3.45)  Time: 0.300s, 3418.08/s  (0.300s, 3408.22/s)  LR: 4.302e-04  Data: 0.024 (0.025)
Train: 163 [1200/1251 ( 96%)]  Loss: 3.735 (3.46)  Time: 0.306s, 3342.18/s  (0.300s, 3407.84/s)  LR: 4.300e-04  Data: 0.026 (0.024)
Train: 163 [1250/1251 (100%)]  Loss: 3.585 (3.46)  Time: 0.275s, 3723.41/s  (0.300s, 3409.16/s)  LR: 4.298e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.213 (2.213)  Loss:  0.5820 (0.5820)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.046 (0.236)  Loss:  0.7256 (1.1250)  Acc@1: 84.3160 (75.0200)  Acc@5: 96.6981 (92.6240)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-161.pth.tar', 75.03000013671875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-163.pth.tar', 75.0200000390625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-158.pth.tar', 74.96199993652344)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-155.pth.tar', 74.9440001171875)

Train: 164 [   0/1251 (  0%)]  Loss: 3.425 (3.42)  Time: 2.711s,  377.66/s  (2.711s,  377.66/s)  LR: 4.298e-04  Data: 2.503 (2.503)
Train: 164 [  50/1251 (  4%)]  Loss: 3.361 (3.39)  Time: 0.297s, 3446.88/s  (0.329s, 3116.07/s)  LR: 4.296e-04  Data: 0.021 (0.071)
Train: 164 [ 100/1251 (  8%)]  Loss: 3.261 (3.35)  Time: 0.303s, 3381.57/s  (0.313s, 3275.51/s)  LR: 4.294e-04  Data: 0.021 (0.047)
Train: 164 [ 150/1251 ( 12%)]  Loss: 3.324 (3.34)  Time: 0.299s, 3425.74/s  (0.308s, 3326.22/s)  LR: 4.292e-04  Data: 0.024 (0.039)
Train: 164 [ 200/1251 ( 16%)]  Loss: 3.220 (3.32)  Time: 0.301s, 3403.90/s  (0.306s, 3347.53/s)  LR: 4.290e-04  Data: 0.026 (0.035)
Train: 164 [ 250/1251 ( 20%)]  Loss: 3.654 (3.37)  Time: 0.302s, 3387.91/s  (0.305s, 3361.93/s)  LR: 4.288e-04  Data: 0.022 (0.033)
Train: 164 [ 300/1251 ( 24%)]  Loss: 3.303 (3.36)  Time: 0.301s, 3407.62/s  (0.304s, 3368.57/s)  LR: 4.286e-04  Data: 0.023 (0.031)
Train: 164 [ 350/1251 ( 28%)]  Loss: 3.107 (3.33)  Time: 0.304s, 3363.43/s  (0.304s, 3372.96/s)  LR: 4.284e-04  Data: 0.023 (0.030)
Train: 164 [ 400/1251 ( 32%)]  Loss: 2.880 (3.28)  Time: 0.305s, 3362.32/s  (0.303s, 3375.58/s)  LR: 4.282e-04  Data: 0.022 (0.029)
Train: 164 [ 450/1251 ( 36%)]  Loss: 3.347 (3.29)  Time: 0.303s, 3379.61/s  (0.303s, 3378.01/s)  LR: 4.280e-04  Data: 0.025 (0.028)
Train: 164 [ 500/1251 ( 40%)]  Loss: 3.528 (3.31)  Time: 0.300s, 3414.58/s  (0.303s, 3379.33/s)  LR: 4.278e-04  Data: 0.021 (0.028)
Train: 164 [ 550/1251 ( 44%)]  Loss: 3.618 (3.34)  Time: 0.303s, 3376.45/s  (0.303s, 3380.49/s)  LR: 4.276e-04  Data: 0.022 (0.027)
Train: 164 [ 600/1251 ( 48%)]  Loss: 3.904 (3.38)  Time: 0.303s, 3383.98/s  (0.303s, 3381.16/s)  LR: 4.273e-04  Data: 0.024 (0.027)
Train: 164 [ 650/1251 ( 52%)]  Loss: 3.327 (3.38)  Time: 0.303s, 3374.46/s  (0.303s, 3381.62/s)  LR: 4.271e-04  Data: 0.022 (0.027)
Train: 164 [ 700/1251 ( 56%)]  Loss: 3.512 (3.38)  Time: 0.307s, 3330.98/s  (0.303s, 3381.86/s)  LR: 4.269e-04  Data: 0.022 (0.027)
Train: 164 [ 750/1251 ( 60%)]  Loss: 3.573 (3.40)  Time: 0.304s, 3367.97/s  (0.303s, 3381.91/s)  LR: 4.267e-04  Data: 0.025 (0.026)
Train: 164 [ 800/1251 ( 64%)]  Loss: 3.435 (3.40)  Time: 0.301s, 3397.66/s  (0.303s, 3381.28/s)  LR: 4.265e-04  Data: 0.025 (0.026)
Train: 164 [ 850/1251 ( 68%)]  Loss: 3.307 (3.39)  Time: 0.298s, 3434.40/s  (0.303s, 3381.17/s)  LR: 4.263e-04  Data: 0.022 (0.026)
Train: 164 [ 900/1251 ( 72%)]  Loss: 3.151 (3.38)  Time: 0.307s, 3339.77/s  (0.303s, 3380.56/s)  LR: 4.261e-04  Data: 0.021 (0.026)
Train: 164 [ 950/1251 ( 76%)]  Loss: 3.308 (3.38)  Time: 0.301s, 3407.49/s  (0.303s, 3379.79/s)  LR: 4.259e-04  Data: 0.022 (0.026)
Train: 164 [1000/1251 ( 80%)]  Loss: 3.457 (3.38)  Time: 0.307s, 3336.28/s  (0.303s, 3379.31/s)  LR: 4.257e-04  Data: 0.013 (0.026)
Train: 164 [1050/1251 ( 84%)]  Loss: 3.520 (3.39)  Time: 0.303s, 3383.39/s  (0.303s, 3379.08/s)  LR: 4.255e-04  Data: 0.024 (0.025)
Train: 164 [1100/1251 ( 88%)]  Loss: 3.427 (3.39)  Time: 0.305s, 3362.26/s  (0.303s, 3378.61/s)  LR: 4.253e-04  Data: 0.022 (0.025)
Train: 164 [1150/1251 ( 92%)]  Loss: 2.689 (3.36)  Time: 0.299s, 3419.20/s  (0.303s, 3378.41/s)  LR: 4.251e-04  Data: 0.023 (0.025)
Train: 164 [1200/1251 ( 96%)]  Loss: 3.206 (3.35)  Time: 0.308s, 3323.10/s  (0.303s, 3377.98/s)  LR: 4.249e-04  Data: 0.025 (0.025)
Train: 164 [1250/1251 (100%)]  Loss: 3.615 (3.36)  Time: 0.276s, 3708.91/s  (0.303s, 3379.45/s)  LR: 4.247e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.077 (2.077)  Loss:  0.5791 (0.5791)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.058 (0.242)  Loss:  0.6567 (1.1048)  Acc@1: 85.7311 (75.0800)  Acc@5: 96.9340 (92.6260)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-164.pth.tar', 75.08000013671875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-161.pth.tar', 75.03000013671875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-163.pth.tar', 75.0200000390625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-158.pth.tar', 74.96199993652344)

Train: 165 [   0/1251 (  0%)]  Loss: 3.273 (3.27)  Time: 2.037s,  502.59/s  (2.037s,  502.59/s)  LR: 4.247e-04  Data: 1.803 (1.803)
Train: 165 [  50/1251 (  4%)]  Loss: 3.713 (3.49)  Time: 0.289s, 3547.05/s  (0.320s, 3196.29/s)  LR: 4.245e-04  Data: 0.021 (0.061)
Train: 165 [ 100/1251 (  8%)]  Loss: 3.440 (3.48)  Time: 0.300s, 3414.32/s  (0.309s, 3315.65/s)  LR: 4.243e-04  Data: 0.024 (0.042)
Train: 165 [ 150/1251 ( 12%)]  Loss: 3.254 (3.42)  Time: 0.303s, 3380.68/s  (0.306s, 3346.05/s)  LR: 4.241e-04  Data: 0.024 (0.036)
Train: 165 [ 200/1251 ( 16%)]  Loss: 3.400 (3.42)  Time: 0.297s, 3445.53/s  (0.305s, 3360.77/s)  LR: 4.238e-04  Data: 0.022 (0.033)
Train: 165 [ 250/1251 ( 20%)]  Loss: 3.503 (3.43)  Time: 0.308s, 3323.44/s  (0.304s, 3366.32/s)  LR: 4.236e-04  Data: 0.025 (0.031)
Train: 165 [ 300/1251 ( 24%)]  Loss: 3.209 (3.40)  Time: 0.303s, 3375.69/s  (0.304s, 3369.47/s)  LR: 4.234e-04  Data: 0.022 (0.029)
Train: 165 [ 350/1251 ( 28%)]  Loss: 3.370 (3.40)  Time: 0.304s, 3364.77/s  (0.304s, 3372.16/s)  LR: 4.232e-04  Data: 0.024 (0.028)
Train: 165 [ 400/1251 ( 32%)]  Loss: 3.431 (3.40)  Time: 0.298s, 3430.64/s  (0.303s, 3374.52/s)  LR: 4.230e-04  Data: 0.021 (0.028)
Train: 165 [ 450/1251 ( 36%)]  Loss: 3.411 (3.40)  Time: 0.308s, 3322.10/s  (0.303s, 3375.37/s)  LR: 4.228e-04  Data: 0.028 (0.027)
Train: 165 [ 500/1251 ( 40%)]  Loss: 3.326 (3.39)  Time: 0.301s, 3400.61/s  (0.303s, 3375.13/s)  LR: 4.226e-04  Data: 0.024 (0.027)
Train: 165 [ 550/1251 ( 44%)]  Loss: 3.525 (3.40)  Time: 0.305s, 3356.21/s  (0.303s, 3373.98/s)  LR: 4.224e-04  Data: 0.020 (0.027)
Train: 165 [ 600/1251 ( 48%)]  Loss: 3.076 (3.38)  Time: 0.304s, 3373.33/s  (0.304s, 3373.96/s)  LR: 4.222e-04  Data: 0.027 (0.026)
Train: 165 [ 650/1251 ( 52%)]  Loss: 3.543 (3.39)  Time: 0.303s, 3382.03/s  (0.304s, 3373.93/s)  LR: 4.220e-04  Data: 0.023 (0.026)
Train: 165 [ 700/1251 ( 56%)]  Loss: 3.319 (3.39)  Time: 0.308s, 3322.69/s  (0.304s, 3373.44/s)  LR: 4.218e-04  Data: 0.023 (0.026)
Train: 165 [ 750/1251 ( 60%)]  Loss: 3.015 (3.36)  Time: 0.307s, 3336.60/s  (0.304s, 3372.77/s)  LR: 4.216e-04  Data: 0.020 (0.026)
Train: 165 [ 800/1251 ( 64%)]  Loss: 3.259 (3.36)  Time: 0.300s, 3416.17/s  (0.304s, 3372.20/s)  LR: 4.214e-04  Data: 0.021 (0.026)
Train: 165 [ 850/1251 ( 68%)]  Loss: 3.368 (3.36)  Time: 0.303s, 3376.86/s  (0.304s, 3371.38/s)  LR: 4.212e-04  Data: 0.026 (0.025)
Train: 165 [ 900/1251 ( 72%)]  Loss: 3.364 (3.36)  Time: 0.309s, 3309.71/s  (0.304s, 3370.80/s)  LR: 4.210e-04  Data: 0.022 (0.025)
Train: 165 [ 950/1251 ( 76%)]  Loss: 3.366 (3.36)  Time: 0.307s, 3337.59/s  (0.304s, 3369.86/s)  LR: 4.208e-04  Data: 0.021 (0.025)
Train: 165 [1000/1251 ( 80%)]  Loss: 3.378 (3.36)  Time: 0.303s, 3382.05/s  (0.304s, 3369.02/s)  LR: 4.206e-04  Data: 0.024 (0.025)
Train: 165 [1050/1251 ( 84%)]  Loss: 3.276 (3.36)  Time: 0.305s, 3354.60/s  (0.304s, 3368.23/s)  LR: 4.204e-04  Data: 0.029 (0.025)
Train: 165 [1100/1251 ( 88%)]  Loss: 3.728 (3.37)  Time: 0.307s, 3340.72/s  (0.304s, 3366.79/s)  LR: 4.201e-04  Data: 0.023 (0.025)
Train: 165 [1150/1251 ( 92%)]  Loss: 3.397 (3.37)  Time: 0.312s, 3282.56/s  (0.304s, 3366.24/s)  LR: 4.199e-04  Data: 0.023 (0.025)
Train: 165 [1200/1251 ( 96%)]  Loss: 3.315 (3.37)  Time: 0.304s, 3367.22/s  (0.304s, 3365.69/s)  LR: 4.197e-04  Data: 0.023 (0.025)
Train: 165 [1250/1251 (100%)]  Loss: 3.738 (3.38)  Time: 0.276s, 3715.54/s  (0.304s, 3366.93/s)  LR: 4.195e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.082 (2.082)  Loss:  0.5391 (0.5391)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.041 (0.238)  Loss:  0.6436 (1.0825)  Acc@1: 85.3774 (75.3320)  Acc@5: 96.8160 (92.7820)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-165.pth.tar', 75.33200011230468)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-164.pth.tar', 75.08000013671875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-161.pth.tar', 75.03000013671875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-163.pth.tar', 75.0200000390625)

Train: 166 [   0/1251 (  0%)]  Loss: 3.377 (3.38)  Time: 2.350s,  435.76/s  (2.350s,  435.76/s)  LR: 4.195e-04  Data: 2.126 (2.126)
Train: 166 [  50/1251 (  4%)]  Loss: 3.421 (3.40)  Time: 0.297s, 3442.56/s  (0.326s, 3138.66/s)  LR: 4.193e-04  Data: 0.023 (0.064)
Train: 166 [ 100/1251 (  8%)]  Loss: 3.487 (3.43)  Time: 0.300s, 3409.80/s  (0.313s, 3274.75/s)  LR: 4.191e-04  Data: 0.027 (0.043)
Train: 166 [ 150/1251 ( 12%)]  Loss: 3.397 (3.42)  Time: 0.307s, 3335.20/s  (0.309s, 3313.09/s)  LR: 4.189e-04  Data: 0.019 (0.037)
Train: 166 [ 200/1251 ( 16%)]  Loss: 3.468 (3.43)  Time: 0.302s, 3395.79/s  (0.307s, 3332.92/s)  LR: 4.187e-04  Data: 0.016 (0.033)
Train: 166 [ 250/1251 ( 20%)]  Loss: 3.611 (3.46)  Time: 0.306s, 3345.37/s  (0.306s, 3342.71/s)  LR: 4.185e-04  Data: 0.021 (0.031)
Train: 166 [ 300/1251 ( 24%)]  Loss: 3.594 (3.48)  Time: 0.304s, 3370.39/s  (0.306s, 3348.53/s)  LR: 4.183e-04  Data: 0.023 (0.030)
Train: 166 [ 350/1251 ( 28%)]  Loss: 3.526 (3.49)  Time: 0.307s, 3331.60/s  (0.306s, 3350.90/s)  LR: 4.181e-04  Data: 0.021 (0.029)
Train: 166 [ 400/1251 ( 32%)]  Loss: 3.246 (3.46)  Time: 0.301s, 3407.58/s  (0.305s, 3352.30/s)  LR: 4.179e-04  Data: 0.025 (0.028)
Train: 166 [ 450/1251 ( 36%)]  Loss: 2.923 (3.40)  Time: 0.305s, 3355.21/s  (0.305s, 3354.20/s)  LR: 4.177e-04  Data: 0.025 (0.028)
Train: 166 [ 500/1251 ( 40%)]  Loss: 3.592 (3.42)  Time: 0.305s, 3359.57/s  (0.305s, 3355.70/s)  LR: 4.175e-04  Data: 0.025 (0.027)
Train: 166 [ 550/1251 ( 44%)]  Loss: 3.345 (3.42)  Time: 0.307s, 3340.81/s  (0.305s, 3356.41/s)  LR: 4.173e-04  Data: 0.025 (0.027)
Train: 166 [ 600/1251 ( 48%)]  Loss: 3.403 (3.41)  Time: 0.305s, 3357.91/s  (0.305s, 3356.41/s)  LR: 4.171e-04  Data: 0.023 (0.027)
Train: 166 [ 650/1251 ( 52%)]  Loss: 3.804 (3.44)  Time: 0.311s, 3296.21/s  (0.305s, 3357.11/s)  LR: 4.169e-04  Data: 0.026 (0.026)
Train: 166 [ 700/1251 ( 56%)]  Loss: 3.361 (3.44)  Time: 0.308s, 3328.25/s  (0.305s, 3357.00/s)  LR: 4.167e-04  Data: 0.022 (0.026)
Train: 166 [ 750/1251 ( 60%)]  Loss: 3.365 (3.43)  Time: 0.311s, 3291.99/s  (0.305s, 3356.57/s)  LR: 4.165e-04  Data: 0.027 (0.026)
Train: 166 [ 800/1251 ( 64%)]  Loss: 3.351 (3.43)  Time: 0.298s, 3433.67/s  (0.305s, 3356.62/s)  LR: 4.162e-04  Data: 0.023 (0.026)
Train: 166 [ 850/1251 ( 68%)]  Loss: 3.314 (3.42)  Time: 0.309s, 3312.47/s  (0.305s, 3355.44/s)  LR: 4.160e-04  Data: 0.023 (0.026)
Train: 166 [ 900/1251 ( 72%)]  Loss: 3.240 (3.41)  Time: 0.308s, 3320.00/s  (0.305s, 3355.13/s)  LR: 4.158e-04  Data: 0.025 (0.025)
Train: 166 [ 950/1251 ( 76%)]  Loss: 3.663 (3.42)  Time: 0.305s, 3353.36/s  (0.305s, 3355.00/s)  LR: 4.156e-04  Data: 0.024 (0.025)
Train: 166 [1000/1251 ( 80%)]  Loss: 3.534 (3.43)  Time: 0.305s, 3355.46/s  (0.305s, 3354.90/s)  LR: 4.154e-04  Data: 0.026 (0.025)
Train: 166 [1050/1251 ( 84%)]  Loss: 3.414 (3.43)  Time: 0.304s, 3370.93/s  (0.305s, 3354.44/s)  LR: 4.152e-04  Data: 0.025 (0.025)
Train: 166 [1100/1251 ( 88%)]  Loss: 3.523 (3.43)  Time: 0.312s, 3281.83/s  (0.305s, 3354.11/s)  LR: 4.150e-04  Data: 0.024 (0.025)
Train: 166 [1150/1251 ( 92%)]  Loss: 3.124 (3.42)  Time: 0.305s, 3360.29/s  (0.305s, 3354.11/s)  LR: 4.148e-04  Data: 0.021 (0.025)
Train: 166 [1200/1251 ( 96%)]  Loss: 3.682 (3.43)  Time: 0.303s, 3373.99/s  (0.305s, 3353.76/s)  LR: 4.146e-04  Data: 0.020 (0.025)
Train: 166 [1250/1251 (100%)]  Loss: 3.177 (3.42)  Time: 0.276s, 3709.27/s  (0.305s, 3355.44/s)  LR: 4.144e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.071 (2.071)  Loss:  0.5791 (0.5791)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.058 (0.239)  Loss:  0.6729 (1.0980)  Acc@1: 85.6132 (75.1400)  Acc@5: 96.9340 (92.7120)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-165.pth.tar', 75.33200011230468)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-166.pth.tar', 75.1399999560547)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-164.pth.tar', 75.08000013671875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-161.pth.tar', 75.03000013671875)

Train: 167 [   0/1251 (  0%)]  Loss: 3.471 (3.47)  Time: 2.552s,  401.22/s  (2.552s,  401.22/s)  LR: 4.144e-04  Data: 2.321 (2.321)
Train: 167 [  50/1251 (  4%)]  Loss: 3.513 (3.49)  Time: 0.299s, 3427.79/s  (0.332s, 3080.20/s)  LR: 4.142e-04  Data: 0.025 (0.068)
Train: 167 [ 100/1251 (  8%)]  Loss: 3.390 (3.46)  Time: 0.308s, 3327.35/s  (0.317s, 3228.45/s)  LR: 4.140e-04  Data: 0.023 (0.046)
Train: 167 [ 150/1251 ( 12%)]  Loss: 3.292 (3.42)  Time: 0.304s, 3367.99/s  (0.312s, 3278.90/s)  LR: 4.138e-04  Data: 0.022 (0.038)
Train: 167 [ 200/1251 ( 16%)]  Loss: 3.253 (3.38)  Time: 0.301s, 3403.80/s  (0.310s, 3300.18/s)  LR: 4.136e-04  Data: 0.022 (0.034)
Train: 167 [ 250/1251 ( 20%)]  Loss: 3.398 (3.39)  Time: 0.305s, 3357.98/s  (0.309s, 3311.07/s)  LR: 4.134e-04  Data: 0.023 (0.032)
Train: 167 [ 300/1251 ( 24%)]  Loss: 3.237 (3.36)  Time: 0.314s, 3266.15/s  (0.309s, 3317.70/s)  LR: 4.132e-04  Data: 0.025 (0.031)
Train: 167 [ 350/1251 ( 28%)]  Loss: 3.493 (3.38)  Time: 0.308s, 3323.76/s  (0.308s, 3322.19/s)  LR: 4.130e-04  Data: 0.026 (0.030)
Train: 167 [ 400/1251 ( 32%)]  Loss: 3.518 (3.40)  Time: 0.305s, 3358.60/s  (0.308s, 3325.08/s)  LR: 4.128e-04  Data: 0.024 (0.029)
Train: 167 [ 450/1251 ( 36%)]  Loss: 3.320 (3.39)  Time: 0.308s, 3329.79/s  (0.308s, 3327.52/s)  LR: 4.126e-04  Data: 0.023 (0.028)
Train: 167 [ 500/1251 ( 40%)]  Loss: 3.751 (3.42)  Time: 0.307s, 3340.84/s  (0.308s, 3328.76/s)  LR: 4.123e-04  Data: 0.023 (0.028)
Train: 167 [ 550/1251 ( 44%)]  Loss: 3.496 (3.43)  Time: 0.302s, 3391.16/s  (0.307s, 3330.26/s)  LR: 4.121e-04  Data: 0.022 (0.027)
Train: 167 [ 600/1251 ( 48%)]  Loss: 3.025 (3.40)  Time: 0.300s, 3407.87/s  (0.307s, 3331.26/s)  LR: 4.119e-04  Data: 0.024 (0.027)
Train: 167 [ 650/1251 ( 52%)]  Loss: 3.504 (3.40)  Time: 0.303s, 3379.95/s  (0.307s, 3331.54/s)  LR: 4.117e-04  Data: 0.021 (0.027)
Train: 167 [ 700/1251 ( 56%)]  Loss: 3.415 (3.41)  Time: 0.305s, 3352.39/s  (0.307s, 3332.41/s)  LR: 4.115e-04  Data: 0.023 (0.026)
Train: 167 [ 750/1251 ( 60%)]  Loss: 3.409 (3.41)  Time: 0.310s, 3299.45/s  (0.307s, 3333.57/s)  LR: 4.113e-04  Data: 0.024 (0.026)
Train: 167 [ 800/1251 ( 64%)]  Loss: 3.255 (3.40)  Time: 0.305s, 3352.30/s  (0.307s, 3334.35/s)  LR: 4.111e-04  Data: 0.024 (0.026)
Train: 167 [ 850/1251 ( 68%)]  Loss: 3.499 (3.40)  Time: 0.307s, 3340.43/s  (0.307s, 3334.48/s)  LR: 4.109e-04  Data: 0.022 (0.026)
Train: 167 [ 900/1251 ( 72%)]  Loss: 3.432 (3.40)  Time: 0.311s, 3291.34/s  (0.307s, 3334.36/s)  LR: 4.107e-04  Data: 0.021 (0.026)
Train: 167 [ 950/1251 ( 76%)]  Loss: 3.671 (3.42)  Time: 0.310s, 3300.11/s  (0.307s, 3333.72/s)  LR: 4.105e-04  Data: 0.020 (0.026)
Train: 167 [1000/1251 ( 80%)]  Loss: 3.517 (3.42)  Time: 0.305s, 3359.87/s  (0.307s, 3333.45/s)  LR: 4.103e-04  Data: 0.023 (0.025)
Train: 167 [1050/1251 ( 84%)]  Loss: 3.516 (3.43)  Time: 0.307s, 3338.37/s  (0.307s, 3332.57/s)  LR: 4.101e-04  Data: 0.020 (0.025)
Train: 167 [1100/1251 ( 88%)]  Loss: 3.367 (3.42)  Time: 0.313s, 3276.18/s  (0.307s, 3332.14/s)  LR: 4.099e-04  Data: 0.025 (0.025)
Train: 167 [1150/1251 ( 92%)]  Loss: 2.780 (3.40)  Time: 0.308s, 3326.22/s  (0.307s, 3331.65/s)  LR: 4.097e-04  Data: 0.023 (0.025)
Train: 167 [1200/1251 ( 96%)]  Loss: 3.326 (3.39)  Time: 0.306s, 3346.98/s  (0.307s, 3331.23/s)  LR: 4.095e-04  Data: 0.021 (0.025)
Train: 167 [1250/1251 (100%)]  Loss: 3.326 (3.39)  Time: 0.275s, 3726.98/s  (0.307s, 3332.94/s)  LR: 4.093e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.076 (2.076)  Loss:  0.5146 (0.5146)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.049 (0.238)  Loss:  0.6616 (1.0862)  Acc@1: 85.0236 (75.2340)  Acc@5: 96.9340 (92.6600)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-165.pth.tar', 75.33200011230468)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-167.pth.tar', 75.2339999584961)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-166.pth.tar', 75.1399999560547)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-164.pth.tar', 75.08000013671875)

Train: 168 [   0/1251 (  0%)]  Loss: 3.296 (3.30)  Time: 2.249s,  455.27/s  (2.249s,  455.27/s)  LR: 4.093e-04  Data: 2.017 (2.017)
Train: 168 [  50/1251 (  4%)]  Loss: 3.413 (3.35)  Time: 0.303s, 3381.25/s  (0.327s, 3127.40/s)  LR: 4.091e-04  Data: 0.015 (0.061)
Train: 168 [ 100/1251 (  8%)]  Loss: 3.441 (3.38)  Time: 0.305s, 3357.83/s  (0.314s, 3262.11/s)  LR: 4.089e-04  Data: 0.020 (0.042)
Train: 168 [ 150/1251 ( 12%)]  Loss: 3.443 (3.40)  Time: 0.302s, 3395.06/s  (0.310s, 3298.07/s)  LR: 4.087e-04  Data: 0.022 (0.036)
Train: 168 [ 200/1251 ( 16%)]  Loss: 3.486 (3.42)  Time: 0.303s, 3377.89/s  (0.309s, 3311.44/s)  LR: 4.085e-04  Data: 0.024 (0.033)
Train: 168 [ 250/1251 ( 20%)]  Loss: 3.282 (3.39)  Time: 0.305s, 3352.37/s  (0.308s, 3319.47/s)  LR: 4.083e-04  Data: 0.023 (0.031)
Train: 168 [ 300/1251 ( 24%)]  Loss: 3.281 (3.38)  Time: 0.308s, 3319.31/s  (0.308s, 3322.80/s)  LR: 4.080e-04  Data: 0.018 (0.030)
Train: 168 [ 350/1251 ( 28%)]  Loss: 3.284 (3.37)  Time: 0.310s, 3307.70/s  (0.308s, 3325.15/s)  LR: 4.078e-04  Data: 0.024 (0.029)
Train: 168 [ 400/1251 ( 32%)]  Loss: 3.315 (3.36)  Time: 0.303s, 3374.24/s  (0.308s, 3327.86/s)  LR: 4.076e-04  Data: 0.022 (0.028)
Train: 168 [ 450/1251 ( 36%)]  Loss: 3.615 (3.39)  Time: 0.310s, 3303.74/s  (0.308s, 3327.56/s)  LR: 4.074e-04  Data: 0.020 (0.027)
Train: 168 [ 500/1251 ( 40%)]  Loss: 3.509 (3.40)  Time: 0.308s, 3319.77/s  (0.308s, 3327.84/s)  LR: 4.072e-04  Data: 0.026 (0.027)
Train: 168 [ 550/1251 ( 44%)]  Loss: 3.224 (3.38)  Time: 0.310s, 3299.75/s  (0.308s, 3327.95/s)  LR: 4.070e-04  Data: 0.023 (0.027)
Train: 168 [ 600/1251 ( 48%)]  Loss: 3.284 (3.37)  Time: 0.304s, 3369.33/s  (0.308s, 3327.89/s)  LR: 4.068e-04  Data: 0.022 (0.026)
Train: 168 [ 650/1251 ( 52%)]  Loss: 3.149 (3.36)  Time: 0.308s, 3326.94/s  (0.308s, 3327.69/s)  LR: 4.066e-04  Data: 0.024 (0.026)
Train: 168 [ 700/1251 ( 56%)]  Loss: 3.329 (3.36)  Time: 0.313s, 3267.36/s  (0.308s, 3327.20/s)  LR: 4.064e-04  Data: 0.024 (0.026)
Train: 168 [ 750/1251 ( 60%)]  Loss: 3.210 (3.35)  Time: 0.308s, 3328.61/s  (0.308s, 3326.80/s)  LR: 4.062e-04  Data: 0.022 (0.026)
Train: 168 [ 800/1251 ( 64%)]  Loss: 3.194 (3.34)  Time: 0.306s, 3350.36/s  (0.308s, 3327.14/s)  LR: 4.060e-04  Data: 0.023 (0.025)
Train: 168 [ 850/1251 ( 68%)]  Loss: 3.752 (3.36)  Time: 0.306s, 3346.46/s  (0.308s, 3327.11/s)  LR: 4.058e-04  Data: 0.024 (0.025)
Train: 168 [ 900/1251 ( 72%)]  Loss: 3.457 (3.37)  Time: 0.309s, 3315.13/s  (0.308s, 3327.10/s)  LR: 4.056e-04  Data: 0.024 (0.025)
Train: 168 [ 950/1251 ( 76%)]  Loss: 3.814 (3.39)  Time: 0.311s, 3297.54/s  (0.308s, 3326.65/s)  LR: 4.054e-04  Data: 0.027 (0.025)
Train: 168 [1000/1251 ( 80%)]  Loss: 3.388 (3.39)  Time: 0.306s, 3344.18/s  (0.308s, 3326.56/s)  LR: 4.052e-04  Data: 0.023 (0.025)
Train: 168 [1050/1251 ( 84%)]  Loss: 3.469 (3.39)  Time: 0.312s, 3283.81/s  (0.308s, 3326.18/s)  LR: 4.050e-04  Data: 0.023 (0.025)
Train: 168 [1100/1251 ( 88%)]  Loss: 3.403 (3.39)  Time: 0.308s, 3321.81/s  (0.308s, 3325.94/s)  LR: 4.048e-04  Data: 0.025 (0.025)
Train: 168 [1150/1251 ( 92%)]  Loss: 2.978 (3.38)  Time: 0.304s, 3373.50/s  (0.308s, 3325.79/s)  LR: 4.046e-04  Data: 0.026 (0.025)
Train: 168 [1200/1251 ( 96%)]  Loss: 3.439 (3.38)  Time: 0.306s, 3347.95/s  (0.308s, 3325.98/s)  LR: 4.044e-04  Data: 0.024 (0.025)
Train: 168 [1250/1251 (100%)]  Loss: 3.486 (3.38)  Time: 0.276s, 3709.35/s  (0.308s, 3327.91/s)  LR: 4.042e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.088 (2.088)  Loss:  0.5752 (0.5752)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.059 (0.243)  Loss:  0.6538 (1.0938)  Acc@1: 85.6132 (75.2700)  Acc@5: 97.0519 (92.7440)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-165.pth.tar', 75.33200011230468)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-168.pth.tar', 75.27000008544921)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-167.pth.tar', 75.2339999584961)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-166.pth.tar', 75.1399999560547)

Train: 169 [   0/1251 (  0%)]  Loss: 3.196 (3.20)  Time: 2.199s,  465.76/s  (2.199s,  465.76/s)  LR: 4.042e-04  Data: 1.973 (1.973)
Train: 169 [  50/1251 (  4%)]  Loss: 3.298 (3.25)  Time: 0.298s, 3440.45/s  (0.330s, 3102.55/s)  LR: 4.040e-04  Data: 0.027 (0.061)
Train: 169 [ 100/1251 (  8%)]  Loss: 3.266 (3.25)  Time: 0.308s, 3320.62/s  (0.316s, 3236.48/s)  LR: 4.038e-04  Data: 0.022 (0.042)
Train: 169 [ 150/1251 ( 12%)]  Loss: 3.666 (3.36)  Time: 0.305s, 3359.80/s  (0.313s, 3275.05/s)  LR: 4.035e-04  Data: 0.026 (0.036)
Train: 169 [ 200/1251 ( 16%)]  Loss: 3.268 (3.34)  Time: 0.310s, 3303.38/s  (0.311s, 3291.90/s)  LR: 4.033e-04  Data: 0.022 (0.033)
Train: 169 [ 250/1251 ( 20%)]  Loss: 3.729 (3.40)  Time: 0.301s, 3396.61/s  (0.310s, 3302.05/s)  LR: 4.031e-04  Data: 0.023 (0.031)
Train: 169 [ 300/1251 ( 24%)]  Loss: 3.274 (3.39)  Time: 0.309s, 3316.70/s  (0.310s, 3307.95/s)  LR: 4.029e-04  Data: 0.025 (0.029)
Train: 169 [ 350/1251 ( 28%)]  Loss: 3.242 (3.37)  Time: 0.310s, 3301.09/s  (0.309s, 3312.08/s)  LR: 4.027e-04  Data: 0.024 (0.028)
Train: 169 [ 400/1251 ( 32%)]  Loss: 3.155 (3.34)  Time: 0.306s, 3347.55/s  (0.309s, 3314.56/s)  LR: 4.025e-04  Data: 0.020 (0.028)
Train: 169 [ 450/1251 ( 36%)]  Loss: 3.465 (3.36)  Time: 0.308s, 3322.53/s  (0.309s, 3315.41/s)  LR: 4.023e-04  Data: 0.020 (0.027)
Train: 169 [ 500/1251 ( 40%)]  Loss: 3.505 (3.37)  Time: 0.313s, 3274.81/s  (0.309s, 3315.84/s)  LR: 4.021e-04  Data: 0.025 (0.027)
Train: 169 [ 550/1251 ( 44%)]  Loss: 3.511 (3.38)  Time: 0.311s, 3295.55/s  (0.309s, 3315.81/s)  LR: 4.019e-04  Data: 0.023 (0.026)
Train: 169 [ 600/1251 ( 48%)]  Loss: 3.061 (3.36)  Time: 0.306s, 3348.21/s  (0.309s, 3315.89/s)  LR: 4.017e-04  Data: 0.021 (0.026)
Train: 169 [ 650/1251 ( 52%)]  Loss: 3.464 (3.36)  Time: 0.313s, 3272.86/s  (0.309s, 3315.58/s)  LR: 4.015e-04  Data: 0.025 (0.026)
Train: 169 [ 700/1251 ( 56%)]  Loss: 3.171 (3.35)  Time: 0.305s, 3354.43/s  (0.309s, 3315.61/s)  LR: 4.013e-04  Data: 0.018 (0.026)
Train: 169 [ 750/1251 ( 60%)]  Loss: 3.411 (3.36)  Time: 0.309s, 3311.18/s  (0.309s, 3316.38/s)  LR: 4.011e-04  Data: 0.024 (0.025)
Train: 169 [ 800/1251 ( 64%)]  Loss: 3.442 (3.36)  Time: 0.304s, 3368.27/s  (0.309s, 3317.24/s)  LR: 4.009e-04  Data: 0.021 (0.025)
Train: 169 [ 850/1251 ( 68%)]  Loss: 3.545 (3.37)  Time: 0.308s, 3329.50/s  (0.309s, 3317.10/s)  LR: 4.007e-04  Data: 0.021 (0.025)
Train: 169 [ 900/1251 ( 72%)]  Loss: 3.683 (3.39)  Time: 0.315s, 3254.04/s  (0.309s, 3317.11/s)  LR: 4.005e-04  Data: 0.024 (0.025)
Train: 169 [ 950/1251 ( 76%)]  Loss: 3.500 (3.39)  Time: 0.313s, 3276.57/s  (0.309s, 3317.18/s)  LR: 4.003e-04  Data: 0.023 (0.025)
Train: 169 [1000/1251 ( 80%)]  Loss: 3.354 (3.39)  Time: 0.310s, 3298.05/s  (0.309s, 3316.78/s)  LR: 4.001e-04  Data: 0.024 (0.025)
Train: 169 [1050/1251 ( 84%)]  Loss: 3.028 (3.37)  Time: 0.307s, 3337.13/s  (0.309s, 3316.62/s)  LR: 3.999e-04  Data: 0.022 (0.025)
Train: 169 [1100/1251 ( 88%)]  Loss: 3.122 (3.36)  Time: 0.309s, 3318.40/s  (0.309s, 3316.50/s)  LR: 3.997e-04  Data: 0.021 (0.025)
Train: 169 [1150/1251 ( 92%)]  Loss: 3.159 (3.35)  Time: 0.314s, 3257.90/s  (0.309s, 3316.54/s)  LR: 3.995e-04  Data: 0.025 (0.025)
Train: 169 [1200/1251 ( 96%)]  Loss: 3.659 (3.37)  Time: 0.310s, 3298.13/s  (0.309s, 3316.23/s)  LR: 3.993e-04  Data: 0.023 (0.024)
Train: 169 [1250/1251 (100%)]  Loss: 3.483 (3.37)  Time: 0.276s, 3704.87/s  (0.309s, 3318.39/s)  LR: 3.991e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.037 (2.037)  Loss:  0.5596 (0.5596)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.063 (0.236)  Loss:  0.6860 (1.0989)  Acc@1: 85.1415 (75.1940)  Acc@5: 96.6981 (92.5840)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-165.pth.tar', 75.33200011230468)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-168.pth.tar', 75.27000008544921)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-167.pth.tar', 75.2339999584961)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-169.pth.tar', 75.19400000976563)

Train: 170 [   0/1251 (  0%)]  Loss: 3.315 (3.32)  Time: 2.303s,  444.59/s  (2.303s,  444.59/s)  LR: 3.991e-04  Data: 2.078 (2.078)
Train: 170 [  50/1251 (  4%)]  Loss: 3.483 (3.40)  Time: 0.303s, 3377.69/s  (0.331s, 3094.08/s)  LR: 3.989e-04  Data: 0.032 (0.063)
Train: 170 [ 100/1251 (  8%)]  Loss: 3.092 (3.30)  Time: 0.309s, 3318.08/s  (0.317s, 3233.05/s)  LR: 3.987e-04  Data: 0.024 (0.043)
Train: 170 [ 150/1251 ( 12%)]  Loss: 3.424 (3.33)  Time: 0.308s, 3329.45/s  (0.313s, 3271.90/s)  LR: 3.984e-04  Data: 0.029 (0.036)
Train: 170 [ 200/1251 ( 16%)]  Loss: 3.449 (3.35)  Time: 0.309s, 3315.63/s  (0.311s, 3289.93/s)  LR: 3.982e-04  Data: 0.022 (0.033)
Train: 170 [ 250/1251 ( 20%)]  Loss: 3.573 (3.39)  Time: 0.309s, 3313.50/s  (0.310s, 3301.93/s)  LR: 3.980e-04  Data: 0.026 (0.031)
Train: 170 [ 300/1251 ( 24%)]  Loss: 3.333 (3.38)  Time: 0.312s, 3285.28/s  (0.310s, 3307.35/s)  LR: 3.978e-04  Data: 0.029 (0.030)
Train: 170 [ 350/1251 ( 28%)]  Loss: 3.376 (3.38)  Time: 0.309s, 3312.97/s  (0.309s, 3310.41/s)  LR: 3.976e-04  Data: 0.025 (0.029)
Train: 170 [ 400/1251 ( 32%)]  Loss: 3.549 (3.40)  Time: 0.304s, 3363.42/s  (0.309s, 3313.00/s)  LR: 3.974e-04  Data: 0.021 (0.028)
Train: 170 [ 450/1251 ( 36%)]  Loss: 3.223 (3.38)  Time: 0.305s, 3360.53/s  (0.309s, 3316.07/s)  LR: 3.972e-04  Data: 0.025 (0.028)
Train: 170 [ 500/1251 ( 40%)]  Loss: 3.093 (3.36)  Time: 0.307s, 3332.22/s  (0.309s, 3317.93/s)  LR: 3.970e-04  Data: 0.026 (0.027)
Train: 170 [ 550/1251 ( 44%)]  Loss: 3.392 (3.36)  Time: 0.308s, 3325.30/s  (0.308s, 3319.84/s)  LR: 3.968e-04  Data: 0.024 (0.027)
Train: 170 [ 600/1251 ( 48%)]  Loss: 3.350 (3.36)  Time: 0.305s, 3354.87/s  (0.308s, 3320.50/s)  LR: 3.966e-04  Data: 0.023 (0.026)
Train: 170 [ 650/1251 ( 52%)]  Loss: 3.343 (3.36)  Time: 0.313s, 3273.68/s  (0.308s, 3320.96/s)  LR: 3.964e-04  Data: 0.023 (0.026)
Train: 170 [ 700/1251 ( 56%)]  Loss: 3.636 (3.38)  Time: 0.308s, 3327.86/s  (0.308s, 3320.89/s)  LR: 3.962e-04  Data: 0.026 (0.026)
Train: 170 [ 750/1251 ( 60%)]  Loss: 3.459 (3.38)  Time: 0.314s, 3260.56/s  (0.308s, 3321.11/s)  LR: 3.960e-04  Data: 0.022 (0.026)
Train: 170 [ 800/1251 ( 64%)]  Loss: 3.713 (3.40)  Time: 0.307s, 3337.58/s  (0.308s, 3321.16/s)  LR: 3.958e-04  Data: 0.023 (0.026)
Train: 170 [ 850/1251 ( 68%)]  Loss: 3.383 (3.40)  Time: 0.312s, 3286.89/s  (0.308s, 3321.46/s)  LR: 3.956e-04  Data: 0.022 (0.025)
Train: 170 [ 900/1251 ( 72%)]  Loss: 3.354 (3.40)  Time: 0.310s, 3303.43/s  (0.308s, 3321.88/s)  LR: 3.954e-04  Data: 0.022 (0.025)
Train: 170 [ 950/1251 ( 76%)]  Loss: 3.732 (3.41)  Time: 0.306s, 3344.50/s  (0.308s, 3322.19/s)  LR: 3.952e-04  Data: 0.023 (0.025)
Train: 170 [1000/1251 ( 80%)]  Loss: 3.296 (3.41)  Time: 0.304s, 3364.25/s  (0.308s, 3323.07/s)  LR: 3.950e-04  Data: 0.023 (0.025)
Train: 170 [1050/1251 ( 84%)]  Loss: 3.052 (3.39)  Time: 0.304s, 3365.57/s  (0.308s, 3324.12/s)  LR: 3.948e-04  Data: 0.022 (0.025)
Train: 170 [1100/1251 ( 88%)]  Loss: 3.172 (3.38)  Time: 0.308s, 3321.07/s  (0.308s, 3324.33/s)  LR: 3.946e-04  Data: 0.021 (0.025)
Train: 170 [1150/1251 ( 92%)]  Loss: 2.973 (3.37)  Time: 0.306s, 3347.43/s  (0.308s, 3324.72/s)  LR: 3.944e-04  Data: 0.022 (0.025)
Train: 170 [1200/1251 ( 96%)]  Loss: 3.540 (3.37)  Time: 0.305s, 3352.17/s  (0.308s, 3324.64/s)  LR: 3.942e-04  Data: 0.027 (0.025)
Train: 170 [1250/1251 (100%)]  Loss: 3.566 (3.38)  Time: 0.276s, 3709.54/s  (0.308s, 3326.56/s)  LR: 3.940e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.260 (2.260)  Loss:  0.5557 (0.5557)  Acc@1: 89.5508 (89.5508)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.060 (0.239)  Loss:  0.6602 (1.0989)  Acc@1: 86.3208 (75.5400)  Acc@5: 96.3443 (92.7780)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-170.pth.tar', 75.54000000488281)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-165.pth.tar', 75.33200011230468)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-168.pth.tar', 75.27000008544921)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-167.pth.tar', 75.2339999584961)

Train: 171 [   0/1251 (  0%)]  Loss: 3.379 (3.38)  Time: 2.129s,  481.07/s  (2.129s,  481.07/s)  LR: 3.940e-04  Data: 1.889 (1.889)
Train: 171 [  50/1251 (  4%)]  Loss: 3.299 (3.34)  Time: 0.304s, 3373.79/s  (0.337s, 3035.35/s)  LR: 3.938e-04  Data: 0.023 (0.072)
Train: 171 [ 100/1251 (  8%)]  Loss: 3.168 (3.28)  Time: 0.304s, 3372.65/s  (0.319s, 3207.81/s)  LR: 3.936e-04  Data: 0.021 (0.048)
Train: 171 [ 150/1251 ( 12%)]  Loss: 3.484 (3.33)  Time: 0.308s, 3324.68/s  (0.314s, 3262.64/s)  LR: 3.934e-04  Data: 0.023 (0.040)
Train: 171 [ 200/1251 ( 16%)]  Loss: 3.216 (3.31)  Time: 0.311s, 3295.68/s  (0.311s, 3290.71/s)  LR: 3.932e-04  Data: 0.024 (0.036)
Train: 171 [ 250/1251 ( 20%)]  Loss: 3.475 (3.34)  Time: 0.309s, 3314.86/s  (0.310s, 3305.75/s)  LR: 3.930e-04  Data: 0.028 (0.033)
Train: 171 [ 300/1251 ( 24%)]  Loss: 3.321 (3.33)  Time: 0.307s, 3332.47/s  (0.309s, 3315.58/s)  LR: 3.928e-04  Data: 0.023 (0.031)
Train: 171 [ 350/1251 ( 28%)]  Loss: 3.557 (3.36)  Time: 0.303s, 3384.41/s  (0.308s, 3320.45/s)  LR: 3.925e-04  Data: 0.024 (0.030)
Train: 171 [ 400/1251 ( 32%)]  Loss: 2.991 (3.32)  Time: 0.302s, 3395.31/s  (0.308s, 3324.46/s)  LR: 3.923e-04  Data: 0.022 (0.029)
Train: 171 [ 450/1251 ( 36%)]  Loss: 3.550 (3.34)  Time: 0.304s, 3365.55/s  (0.308s, 3327.96/s)  LR: 3.921e-04  Data: 0.023 (0.029)
Train: 171 [ 500/1251 ( 40%)]  Loss: 3.072 (3.32)  Time: 0.303s, 3378.04/s  (0.307s, 3331.25/s)  LR: 3.919e-04  Data: 0.020 (0.028)
Train: 171 [ 550/1251 ( 44%)]  Loss: 3.305 (3.32)  Time: 0.307s, 3338.88/s  (0.307s, 3333.65/s)  LR: 3.917e-04  Data: 0.026 (0.028)
Train: 171 [ 600/1251 ( 48%)]  Loss: 3.654 (3.34)  Time: 0.309s, 3314.14/s  (0.307s, 3335.47/s)  LR: 3.915e-04  Data: 0.023 (0.027)
Train: 171 [ 650/1251 ( 52%)]  Loss: 3.235 (3.34)  Time: 0.310s, 3305.39/s  (0.307s, 3337.11/s)  LR: 3.913e-04  Data: 0.021 (0.027)
Train: 171 [ 700/1251 ( 56%)]  Loss: 3.679 (3.36)  Time: 0.307s, 3336.65/s  (0.307s, 3338.21/s)  LR: 3.911e-04  Data: 0.021 (0.027)
Train: 171 [ 750/1251 ( 60%)]  Loss: 3.021 (3.34)  Time: 0.305s, 3362.36/s  (0.307s, 3339.30/s)  LR: 3.909e-04  Data: 0.026 (0.026)
Train: 171 [ 800/1251 ( 64%)]  Loss: 3.114 (3.32)  Time: 0.300s, 3413.05/s  (0.306s, 3340.96/s)  LR: 3.907e-04  Data: 0.019 (0.026)
Train: 171 [ 850/1251 ( 68%)]  Loss: 3.596 (3.34)  Time: 0.304s, 3369.60/s  (0.306s, 3342.60/s)  LR: 3.905e-04  Data: 0.025 (0.026)
Train: 171 [ 900/1251 ( 72%)]  Loss: 3.588 (3.35)  Time: 0.303s, 3377.93/s  (0.306s, 3342.96/s)  LR: 3.903e-04  Data: 0.020 (0.026)
Train: 171 [ 950/1251 ( 76%)]  Loss: 3.378 (3.35)  Time: 0.305s, 3352.04/s  (0.306s, 3343.57/s)  LR: 3.901e-04  Data: 0.023 (0.026)
Train: 171 [1000/1251 ( 80%)]  Loss: 3.566 (3.36)  Time: 0.299s, 3425.73/s  (0.306s, 3344.51/s)  LR: 3.899e-04  Data: 0.023 (0.026)
Train: 171 [1050/1251 ( 84%)]  Loss: 3.753 (3.38)  Time: 0.307s, 3333.12/s  (0.306s, 3345.59/s)  LR: 3.897e-04  Data: 0.023 (0.025)
Train: 171 [1100/1251 ( 88%)]  Loss: 3.239 (3.38)  Time: 0.307s, 3335.68/s  (0.306s, 3346.51/s)  LR: 3.895e-04  Data: 0.025 (0.025)
Train: 171 [1150/1251 ( 92%)]  Loss: 3.512 (3.38)  Time: 0.300s, 3415.02/s  (0.306s, 3347.67/s)  LR: 3.893e-04  Data: 0.016 (0.025)
Train: 171 [1200/1251 ( 96%)]  Loss: 3.568 (3.39)  Time: 0.303s, 3381.13/s  (0.306s, 3348.38/s)  LR: 3.891e-04  Data: 0.022 (0.025)
Train: 171 [1250/1251 (100%)]  Loss: 3.465 (3.39)  Time: 0.275s, 3723.28/s  (0.306s, 3350.97/s)  LR: 3.889e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.065 (2.065)  Loss:  0.5645 (0.5645)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.056 (0.234)  Loss:  0.6641 (1.0771)  Acc@1: 85.2594 (75.5480)  Acc@5: 96.5802 (92.8340)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-171.pth.tar', 75.54800006103515)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-170.pth.tar', 75.54000000488281)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-165.pth.tar', 75.33200011230468)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-168.pth.tar', 75.27000008544921)

Train: 172 [   0/1251 (  0%)]  Loss: 3.342 (3.34)  Time: 2.009s,  509.68/s  (2.009s,  509.68/s)  LR: 3.889e-04  Data: 1.764 (1.764)
Train: 172 [  50/1251 (  4%)]  Loss: 3.618 (3.48)  Time: 0.294s, 3482.36/s  (0.324s, 3159.43/s)  LR: 3.887e-04  Data: 0.023 (0.057)
Train: 172 [ 100/1251 (  8%)]  Loss: 3.201 (3.39)  Time: 0.301s, 3404.21/s  (0.311s, 3292.33/s)  LR: 3.885e-04  Data: 0.024 (0.040)
Train: 172 [ 150/1251 ( 12%)]  Loss: 3.250 (3.35)  Time: 0.303s, 3382.47/s  (0.307s, 3333.21/s)  LR: 3.883e-04  Data: 0.023 (0.034)
Train: 172 [ 200/1251 ( 16%)]  Loss: 3.692 (3.42)  Time: 0.297s, 3452.25/s  (0.306s, 3350.29/s)  LR: 3.881e-04  Data: 0.023 (0.032)
Train: 172 [ 250/1251 ( 20%)]  Loss: 2.834 (3.32)  Time: 0.302s, 3387.29/s  (0.305s, 3358.60/s)  LR: 3.879e-04  Data: 0.027 (0.030)
Train: 172 [ 300/1251 ( 24%)]  Loss: 3.142 (3.30)  Time: 0.302s, 3396.30/s  (0.304s, 3365.85/s)  LR: 3.877e-04  Data: 0.024 (0.029)
Train: 172 [ 350/1251 ( 28%)]  Loss: 3.296 (3.30)  Time: 0.306s, 3343.00/s  (0.304s, 3370.42/s)  LR: 3.875e-04  Data: 0.027 (0.028)
Train: 172 [ 400/1251 ( 32%)]  Loss: 3.238 (3.29)  Time: 0.301s, 3405.91/s  (0.304s, 3373.49/s)  LR: 3.873e-04  Data: 0.023 (0.028)
Train: 172 [ 450/1251 ( 36%)]  Loss: 3.410 (3.30)  Time: 0.307s, 3334.01/s  (0.303s, 3375.20/s)  LR: 3.871e-04  Data: 0.025 (0.027)
Train: 172 [ 500/1251 ( 40%)]  Loss: 3.156 (3.29)  Time: 0.306s, 3347.50/s  (0.303s, 3376.18/s)  LR: 3.869e-04  Data: 0.024 (0.027)
Train: 172 [ 550/1251 ( 44%)]  Loss: 3.271 (3.29)  Time: 0.299s, 3423.59/s  (0.303s, 3377.14/s)  LR: 3.867e-04  Data: 0.021 (0.026)
Train: 172 [ 600/1251 ( 48%)]  Loss: 3.409 (3.30)  Time: 0.300s, 3414.07/s  (0.303s, 3378.30/s)  LR: 3.865e-04  Data: 0.026 (0.026)
Train: 172 [ 650/1251 ( 52%)]  Loss: 3.529 (3.31)  Time: 0.305s, 3362.53/s  (0.303s, 3379.07/s)  LR: 3.863e-04  Data: 0.025 (0.026)
Train: 172 [ 700/1251 ( 56%)]  Loss: 3.661 (3.34)  Time: 0.304s, 3372.29/s  (0.303s, 3379.44/s)  LR: 3.861e-04  Data: 0.025 (0.026)
Train: 172 [ 750/1251 ( 60%)]  Loss: 3.194 (3.33)  Time: 0.298s, 3439.00/s  (0.303s, 3379.93/s)  LR: 3.859e-04  Data: 0.022 (0.025)
Train: 172 [ 800/1251 ( 64%)]  Loss: 3.350 (3.33)  Time: 0.301s, 3401.17/s  (0.303s, 3380.22/s)  LR: 3.857e-04  Data: 0.022 (0.025)
Train: 172 [ 850/1251 ( 68%)]  Loss: 3.610 (3.34)  Time: 0.302s, 3388.82/s  (0.303s, 3380.61/s)  LR: 3.854e-04  Data: 0.022 (0.025)
Train: 172 [ 900/1251 ( 72%)]  Loss: 3.273 (3.34)  Time: 0.301s, 3406.07/s  (0.303s, 3381.22/s)  LR: 3.852e-04  Data: 0.020 (0.025)
Train: 172 [ 950/1251 ( 76%)]  Loss: 3.542 (3.35)  Time: 0.298s, 3435.01/s  (0.303s, 3381.50/s)  LR: 3.850e-04  Data: 0.023 (0.025)
Train: 172 [1000/1251 ( 80%)]  Loss: 3.498 (3.36)  Time: 0.293s, 3493.11/s  (0.303s, 3383.03/s)  LR: 3.848e-04  Data: 0.023 (0.025)
Train: 172 [1050/1251 ( 84%)]  Loss: 3.665 (3.37)  Time: 0.303s, 3378.50/s  (0.303s, 3383.20/s)  LR: 3.846e-04  Data: 0.022 (0.025)
Train: 172 [1100/1251 ( 88%)]  Loss: 3.729 (3.39)  Time: 0.303s, 3377.67/s  (0.303s, 3383.12/s)  LR: 3.844e-04  Data: 0.019 (0.025)
Train: 172 [1150/1251 ( 92%)]  Loss: 3.188 (3.38)  Time: 0.302s, 3386.61/s  (0.303s, 3383.49/s)  LR: 3.842e-04  Data: 0.021 (0.025)
Train: 172 [1200/1251 ( 96%)]  Loss: 3.310 (3.38)  Time: 0.302s, 3385.86/s  (0.303s, 3383.33/s)  LR: 3.840e-04  Data: 0.022 (0.024)
Train: 172 [1250/1251 (100%)]  Loss: 3.208 (3.37)  Time: 0.275s, 3716.96/s  (0.302s, 3385.29/s)  LR: 3.838e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.018 (2.018)  Loss:  0.5820 (0.5820)  Acc@1: 89.2578 (89.2578)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.053 (0.237)  Loss:  0.6855 (1.0767)  Acc@1: 84.4340 (75.7160)  Acc@5: 96.2264 (92.9400)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-172.pth.tar', 75.7159999609375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-171.pth.tar', 75.54800006103515)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-170.pth.tar', 75.54000000488281)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-165.pth.tar', 75.33200011230468)

Train: 173 [   0/1251 (  0%)]  Loss: 3.520 (3.52)  Time: 2.652s,  386.08/s  (2.652s,  386.08/s)  LR: 3.838e-04  Data: 2.429 (2.429)
Train: 173 [  50/1251 (  4%)]  Loss: 3.053 (3.29)  Time: 0.281s, 3642.93/s  (0.326s, 3137.38/s)  LR: 3.836e-04  Data: 0.023 (0.071)
Train: 173 [ 100/1251 (  8%)]  Loss: 3.561 (3.38)  Time: 0.302s, 3387.22/s  (0.310s, 3299.32/s)  LR: 3.834e-04  Data: 0.026 (0.047)
Train: 173 [ 150/1251 ( 12%)]  Loss: 3.570 (3.43)  Time: 0.293s, 3498.58/s  (0.306s, 3350.37/s)  LR: 3.832e-04  Data: 0.023 (0.039)
Train: 173 [ 200/1251 ( 16%)]  Loss: 3.437 (3.43)  Time: 0.292s, 3504.18/s  (0.304s, 3371.56/s)  LR: 3.830e-04  Data: 0.021 (0.035)
Train: 173 [ 250/1251 ( 20%)]  Loss: 3.625 (3.46)  Time: 0.302s, 3394.79/s  (0.303s, 3382.95/s)  LR: 3.828e-04  Data: 0.023 (0.033)
Train: 173 [ 300/1251 ( 24%)]  Loss: 3.400 (3.45)  Time: 0.304s, 3369.88/s  (0.302s, 3389.26/s)  LR: 3.826e-04  Data: 0.025 (0.031)
Train: 173 [ 350/1251 ( 28%)]  Loss: 3.238 (3.43)  Time: 0.302s, 3389.08/s  (0.302s, 3392.35/s)  LR: 3.824e-04  Data: 0.026 (0.030)
Train: 173 [ 400/1251 ( 32%)]  Loss: 3.473 (3.43)  Time: 0.291s, 3520.32/s  (0.302s, 3395.85/s)  LR: 3.822e-04  Data: 0.021 (0.029)
Train: 173 [ 450/1251 ( 36%)]  Loss: 3.028 (3.39)  Time: 0.297s, 3444.34/s  (0.301s, 3399.78/s)  LR: 3.820e-04  Data: 0.023 (0.028)
Train: 173 [ 500/1251 ( 40%)]  Loss: 3.314 (3.38)  Time: 0.310s, 3301.02/s  (0.301s, 3401.83/s)  LR: 3.818e-04  Data: 0.026 (0.028)
Train: 173 [ 550/1251 ( 44%)]  Loss: 3.331 (3.38)  Time: 0.303s, 3379.30/s  (0.301s, 3403.21/s)  LR: 3.816e-04  Data: 0.026 (0.027)
Train: 173 [ 600/1251 ( 48%)]  Loss: 3.284 (3.37)  Time: 0.296s, 3454.48/s  (0.301s, 3403.67/s)  LR: 3.814e-04  Data: 0.021 (0.027)
Train: 173 [ 650/1251 ( 52%)]  Loss: 3.218 (3.36)  Time: 0.301s, 3404.95/s  (0.301s, 3404.39/s)  LR: 3.812e-04  Data: 0.023 (0.027)
Train: 173 [ 700/1251 ( 56%)]  Loss: 3.350 (3.36)  Time: 0.304s, 3373.68/s  (0.301s, 3404.80/s)  LR: 3.810e-04  Data: 0.024 (0.027)
Train: 173 [ 750/1251 ( 60%)]  Loss: 3.526 (3.37)  Time: 0.299s, 3421.86/s  (0.301s, 3405.05/s)  LR: 3.808e-04  Data: 0.019 (0.026)
Train: 173 [ 800/1251 ( 64%)]  Loss: 3.537 (3.38)  Time: 0.300s, 3409.10/s  (0.301s, 3405.00/s)  LR: 3.806e-04  Data: 0.020 (0.026)
Train: 173 [ 850/1251 ( 68%)]  Loss: 3.259 (3.37)  Time: 0.300s, 3417.89/s  (0.301s, 3405.35/s)  LR: 3.804e-04  Data: 0.022 (0.026)
Train: 173 [ 900/1251 ( 72%)]  Loss: 3.292 (3.37)  Time: 0.301s, 3397.39/s  (0.301s, 3405.91/s)  LR: 3.802e-04  Data: 0.025 (0.026)
Train: 173 [ 950/1251 ( 76%)]  Loss: 3.348 (3.37)  Time: 0.303s, 3378.47/s  (0.301s, 3406.41/s)  LR: 3.800e-04  Data: 0.023 (0.026)
Train: 173 [1000/1251 ( 80%)]  Loss: 3.578 (3.38)  Time: 0.301s, 3397.27/s  (0.301s, 3406.28/s)  LR: 3.798e-04  Data: 0.023 (0.025)
Train: 173 [1050/1251 ( 84%)]  Loss: 3.433 (3.38)  Time: 0.299s, 3424.18/s  (0.301s, 3406.28/s)  LR: 3.796e-04  Data: 0.027 (0.025)
Train: 173 [1100/1251 ( 88%)]  Loss: 3.557 (3.39)  Time: 0.305s, 3361.85/s  (0.301s, 3406.11/s)  LR: 3.794e-04  Data: 0.025 (0.025)
Train: 173 [1150/1251 ( 92%)]  Loss: 3.419 (3.39)  Time: 0.302s, 3393.07/s  (0.301s, 3406.45/s)  LR: 3.792e-04  Data: 0.024 (0.025)
Train: 173 [1200/1251 ( 96%)]  Loss: 3.428 (3.39)  Time: 0.297s, 3448.12/s  (0.301s, 3406.33/s)  LR: 3.790e-04  Data: 0.019 (0.025)
Train: 173 [1250/1251 (100%)]  Loss: 3.333 (3.39)  Time: 0.276s, 3716.66/s  (0.300s, 3408.56/s)  LR: 3.788e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.083 (2.083)  Loss:  0.5601 (0.5601)  Acc@1: 88.5742 (88.5742)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.054 (0.236)  Loss:  0.6221 (1.0808)  Acc@1: 85.2594 (75.6220)  Acc@5: 97.0519 (92.6940)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-172.pth.tar', 75.7159999609375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-173.pth.tar', 75.62199993164063)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-171.pth.tar', 75.54800006103515)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-170.pth.tar', 75.54000000488281)

Train: 174 [   0/1251 (  0%)]  Loss: 3.391 (3.39)  Time: 2.122s,  482.48/s  (2.122s,  482.48/s)  LR: 3.788e-04  Data: 1.903 (1.903)
Train: 174 [  50/1251 (  4%)]  Loss: 3.151 (3.27)  Time: 0.300s, 3408.06/s  (0.324s, 3157.94/s)  LR: 3.786e-04  Data: 0.023 (0.061)
Train: 174 [ 100/1251 (  8%)]  Loss: 3.543 (3.36)  Time: 0.288s, 3553.01/s  (0.309s, 3310.44/s)  LR: 3.784e-04  Data: 0.024 (0.042)
Train: 174 [ 150/1251 ( 12%)]  Loss: 3.248 (3.33)  Time: 0.295s, 3466.59/s  (0.305s, 3358.78/s)  LR: 3.782e-04  Data: 0.022 (0.036)
Train: 174 [ 200/1251 ( 16%)]  Loss: 3.574 (3.38)  Time: 0.297s, 3453.18/s  (0.303s, 3382.39/s)  LR: 3.780e-04  Data: 0.025 (0.032)
Train: 174 [ 250/1251 ( 20%)]  Loss: 3.450 (3.39)  Time: 0.293s, 3494.60/s  (0.302s, 3395.73/s)  LR: 3.778e-04  Data: 0.021 (0.031)
Train: 174 [ 300/1251 ( 24%)]  Loss: 3.049 (3.34)  Time: 0.300s, 3408.06/s  (0.301s, 3404.54/s)  LR: 3.776e-04  Data: 0.023 (0.029)
Train: 174 [ 350/1251 ( 28%)]  Loss: 3.220 (3.33)  Time: 0.308s, 3319.58/s  (0.300s, 3408.56/s)  LR: 3.774e-04  Data: 0.024 (0.028)
Train: 174 [ 400/1251 ( 32%)]  Loss: 3.521 (3.35)  Time: 0.299s, 3426.06/s  (0.300s, 3412.20/s)  LR: 3.772e-04  Data: 0.018 (0.028)
Train: 174 [ 450/1251 ( 36%)]  Loss: 3.574 (3.37)  Time: 0.299s, 3425.09/s  (0.300s, 3415.38/s)  LR: 3.770e-04  Data: 0.020 (0.027)
Train: 174 [ 500/1251 ( 40%)]  Loss: 3.417 (3.38)  Time: 0.303s, 3374.20/s  (0.300s, 3417.73/s)  LR: 3.768e-04  Data: 0.027 (0.027)
Train: 174 [ 550/1251 ( 44%)]  Loss: 3.324 (3.37)  Time: 0.294s, 3487.71/s  (0.300s, 3418.73/s)  LR: 3.766e-04  Data: 0.023 (0.026)
Train: 174 [ 600/1251 ( 48%)]  Loss: 3.525 (3.38)  Time: 0.301s, 3398.01/s  (0.299s, 3420.47/s)  LR: 3.764e-04  Data: 0.022 (0.026)
Train: 174 [ 650/1251 ( 52%)]  Loss: 3.419 (3.39)  Time: 0.297s, 3443.13/s  (0.299s, 3421.62/s)  LR: 3.762e-04  Data: 0.024 (0.026)
Train: 174 [ 700/1251 ( 56%)]  Loss: 3.250 (3.38)  Time: 0.292s, 3507.10/s  (0.299s, 3422.48/s)  LR: 3.760e-04  Data: 0.025 (0.026)
Train: 174 [ 750/1251 ( 60%)]  Loss: 3.391 (3.38)  Time: 0.301s, 3407.31/s  (0.299s, 3422.70/s)  LR: 3.757e-04  Data: 0.023 (0.026)
Train: 174 [ 800/1251 ( 64%)]  Loss: 3.039 (3.36)  Time: 0.301s, 3406.76/s  (0.299s, 3422.68/s)  LR: 3.755e-04  Data: 0.023 (0.026)
Train: 174 [ 850/1251 ( 68%)]  Loss: 3.512 (3.37)  Time: 0.301s, 3407.40/s  (0.299s, 3422.62/s)  LR: 3.753e-04  Data: 0.023 (0.025)
Train: 174 [ 900/1251 ( 72%)]  Loss: 3.167 (3.36)  Time: 0.301s, 3406.40/s  (0.299s, 3422.87/s)  LR: 3.751e-04  Data: 0.027 (0.025)
Train: 174 [ 950/1251 ( 76%)]  Loss: 3.512 (3.36)  Time: 0.299s, 3424.74/s  (0.299s, 3422.92/s)  LR: 3.749e-04  Data: 0.021 (0.025)
Train: 174 [1000/1251 ( 80%)]  Loss: 2.815 (3.34)  Time: 0.298s, 3441.50/s  (0.299s, 3422.77/s)  LR: 3.747e-04  Data: 0.017 (0.025)
Train: 174 [1050/1251 ( 84%)]  Loss: 3.586 (3.35)  Time: 0.300s, 3413.16/s  (0.299s, 3422.59/s)  LR: 3.745e-04  Data: 0.022 (0.025)
Train: 174 [1100/1251 ( 88%)]  Loss: 3.517 (3.36)  Time: 0.299s, 3426.87/s  (0.299s, 3422.75/s)  LR: 3.743e-04  Data: 0.021 (0.025)
Train: 174 [1150/1251 ( 92%)]  Loss: 3.315 (3.35)  Time: 0.295s, 3465.73/s  (0.299s, 3422.87/s)  LR: 3.741e-04  Data: 0.024 (0.025)
Train: 174 [1200/1251 ( 96%)]  Loss: 3.708 (3.37)  Time: 0.297s, 3450.72/s  (0.299s, 3423.16/s)  LR: 3.739e-04  Data: 0.027 (0.025)
Train: 174 [1250/1251 (100%)]  Loss: 3.473 (3.37)  Time: 0.272s, 3758.70/s  (0.299s, 3425.26/s)  LR: 3.737e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.071 (2.071)  Loss:  0.6030 (0.6030)  Acc@1: 88.8672 (88.8672)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.040 (0.239)  Loss:  0.6831 (1.0897)  Acc@1: 84.5519 (75.7960)  Acc@5: 96.9340 (92.9200)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-174.pth.tar', 75.79600001220703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-172.pth.tar', 75.7159999609375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-173.pth.tar', 75.62199993164063)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-171.pth.tar', 75.54800006103515)

Train: 175 [   0/1251 (  0%)]  Loss: 3.176 (3.18)  Time: 2.279s,  449.25/s  (2.279s,  449.25/s)  LR: 3.737e-04  Data: 2.052 (2.052)
Train: 175 [  50/1251 (  4%)]  Loss: 3.163 (3.17)  Time: 0.300s, 3413.21/s  (0.325s, 3147.54/s)  LR: 3.735e-04  Data: 0.021 (0.062)
Train: 175 [ 100/1251 (  8%)]  Loss: 3.556 (3.30)  Time: 0.293s, 3492.97/s  (0.310s, 3306.81/s)  LR: 3.733e-04  Data: 0.023 (0.043)
Train: 175 [ 150/1251 ( 12%)]  Loss: 3.233 (3.28)  Time: 0.296s, 3456.74/s  (0.305s, 3362.30/s)  LR: 3.731e-04  Data: 0.026 (0.036)
Train: 175 [ 200/1251 ( 16%)]  Loss: 2.945 (3.21)  Time: 0.294s, 3486.21/s  (0.302s, 3389.14/s)  LR: 3.729e-04  Data: 0.021 (0.033)
Train: 175 [ 250/1251 ( 20%)]  Loss: 3.300 (3.23)  Time: 0.292s, 3502.91/s  (0.301s, 3403.04/s)  LR: 3.727e-04  Data: 0.024 (0.031)
Train: 175 [ 300/1251 ( 24%)]  Loss: 3.410 (3.25)  Time: 0.294s, 3483.09/s  (0.300s, 3410.49/s)  LR: 3.725e-04  Data: 0.025 (0.029)
Train: 175 [ 350/1251 ( 28%)]  Loss: 3.178 (3.25)  Time: 0.297s, 3453.30/s  (0.300s, 3418.18/s)  LR: 3.723e-04  Data: 0.023 (0.029)
Train: 175 [ 400/1251 ( 32%)]  Loss: 3.448 (3.27)  Time: 0.296s, 3460.61/s  (0.299s, 3423.67/s)  LR: 3.721e-04  Data: 0.023 (0.028)
Train: 175 [ 450/1251 ( 36%)]  Loss: 3.376 (3.28)  Time: 0.291s, 3514.14/s  (0.299s, 3426.52/s)  LR: 3.719e-04  Data: 0.026 (0.027)
Train: 175 [ 500/1251 ( 40%)]  Loss: 3.166 (3.27)  Time: 0.294s, 3486.98/s  (0.299s, 3429.34/s)  LR: 3.717e-04  Data: 0.024 (0.027)
Train: 175 [ 550/1251 ( 44%)]  Loss: 2.970 (3.24)  Time: 0.296s, 3463.45/s  (0.298s, 3431.18/s)  LR: 3.715e-04  Data: 0.023 (0.027)
Train: 175 [ 600/1251 ( 48%)]  Loss: 3.170 (3.24)  Time: 0.299s, 3428.25/s  (0.298s, 3434.92/s)  LR: 3.713e-04  Data: 0.027 (0.026)
Train: 175 [ 650/1251 ( 52%)]  Loss: 3.081 (3.23)  Time: 0.297s, 3442.22/s  (0.298s, 3436.73/s)  LR: 3.711e-04  Data: 0.024 (0.026)
Train: 175 [ 700/1251 ( 56%)]  Loss: 3.474 (3.24)  Time: 0.306s, 3351.75/s  (0.298s, 3438.58/s)  LR: 3.709e-04  Data: 0.025 (0.026)
Train: 175 [ 750/1251 ( 60%)]  Loss: 3.530 (3.26)  Time: 0.296s, 3460.94/s  (0.298s, 3439.69/s)  LR: 3.707e-04  Data: 0.024 (0.026)
Train: 175 [ 800/1251 ( 64%)]  Loss: 3.173 (3.26)  Time: 0.297s, 3445.40/s  (0.298s, 3440.79/s)  LR: 3.705e-04  Data: 0.022 (0.026)
Train: 175 [ 850/1251 ( 68%)]  Loss: 3.557 (3.27)  Time: 0.299s, 3421.99/s  (0.298s, 3441.22/s)  LR: 3.703e-04  Data: 0.023 (0.025)
Train: 175 [ 900/1251 ( 72%)]  Loss: 3.307 (3.27)  Time: 0.297s, 3449.99/s  (0.298s, 3441.85/s)  LR: 3.701e-04  Data: 0.025 (0.025)
Train: 175 [ 950/1251 ( 76%)]  Loss: 3.255 (3.27)  Time: 0.304s, 3372.86/s  (0.298s, 3441.58/s)  LR: 3.699e-04  Data: 0.024 (0.025)
Train: 175 [1000/1251 ( 80%)]  Loss: 3.396 (3.28)  Time: 0.295s, 3471.86/s  (0.298s, 3441.76/s)  LR: 3.697e-04  Data: 0.024 (0.025)
Train: 175 [1050/1251 ( 84%)]  Loss: 3.460 (3.29)  Time: 0.296s, 3459.57/s  (0.297s, 3442.55/s)  LR: 3.695e-04  Data: 0.022 (0.025)
Train: 175 [1100/1251 ( 88%)]  Loss: 3.359 (3.29)  Time: 0.297s, 3442.45/s  (0.297s, 3442.44/s)  LR: 3.693e-04  Data: 0.024 (0.025)
Train: 175 [1150/1251 ( 92%)]  Loss: 3.449 (3.30)  Time: 0.296s, 3459.96/s  (0.297s, 3442.92/s)  LR: 3.691e-04  Data: 0.026 (0.025)
Train: 175 [1200/1251 ( 96%)]  Loss: 3.449 (3.30)  Time: 0.303s, 3384.80/s  (0.297s, 3442.67/s)  LR: 3.689e-04  Data: 0.023 (0.025)
Train: 175 [1250/1251 (100%)]  Loss: 3.139 (3.30)  Time: 0.275s, 3726.64/s  (0.297s, 3444.43/s)  LR: 3.687e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.053 (2.053)  Loss:  0.5464 (0.5464)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.043 (0.236)  Loss:  0.6392 (1.0639)  Acc@1: 85.9670 (75.7700)  Acc@5: 97.1698 (93.0360)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-174.pth.tar', 75.79600001220703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-175.pth.tar', 75.77000010986328)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-172.pth.tar', 75.7159999609375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-173.pth.tar', 75.62199993164063)

Train: 176 [   0/1251 (  0%)]  Loss: 3.264 (3.26)  Time: 2.451s,  417.86/s  (2.451s,  417.86/s)  LR: 3.687e-04  Data: 2.227 (2.227)
Train: 176 [  50/1251 (  4%)]  Loss: 3.340 (3.30)  Time: 0.281s, 3639.80/s  (0.320s, 3204.27/s)  LR: 3.685e-04  Data: 0.023 (0.067)
Train: 176 [ 100/1251 (  8%)]  Loss: 3.216 (3.27)  Time: 0.295s, 3471.77/s  (0.305s, 3356.94/s)  LR: 3.683e-04  Data: 0.022 (0.045)
Train: 176 [ 150/1251 ( 12%)]  Loss: 3.656 (3.37)  Time: 0.302s, 3392.90/s  (0.301s, 3400.31/s)  LR: 3.681e-04  Data: 0.019 (0.038)
Train: 176 [ 200/1251 ( 16%)]  Loss: 3.584 (3.41)  Time: 0.293s, 3490.25/s  (0.299s, 3421.70/s)  LR: 3.679e-04  Data: 0.023 (0.034)
Train: 176 [ 250/1251 ( 20%)]  Loss: 3.371 (3.40)  Time: 0.305s, 3355.45/s  (0.298s, 3433.31/s)  LR: 3.677e-04  Data: 0.023 (0.032)
Train: 176 [ 300/1251 ( 24%)]  Loss: 3.681 (3.44)  Time: 0.299s, 3427.35/s  (0.298s, 3438.79/s)  LR: 3.675e-04  Data: 0.024 (0.031)
Train: 176 [ 350/1251 ( 28%)]  Loss: 3.330 (3.43)  Time: 0.292s, 3501.25/s  (0.297s, 3443.66/s)  LR: 3.673e-04  Data: 0.021 (0.030)
Train: 176 [ 400/1251 ( 32%)]  Loss: 3.118 (3.40)  Time: 0.296s, 3465.05/s  (0.297s, 3445.79/s)  LR: 3.671e-04  Data: 0.022 (0.029)
Train: 176 [ 450/1251 ( 36%)]  Loss: 3.367 (3.39)  Time: 0.296s, 3459.20/s  (0.297s, 3447.88/s)  LR: 3.669e-04  Data: 0.025 (0.028)
Train: 176 [ 500/1251 ( 40%)]  Loss: 3.627 (3.41)  Time: 0.297s, 3452.90/s  (0.297s, 3448.90/s)  LR: 3.667e-04  Data: 0.021 (0.028)
Train: 176 [ 550/1251 ( 44%)]  Loss: 3.215 (3.40)  Time: 0.296s, 3465.11/s  (0.297s, 3450.10/s)  LR: 3.665e-04  Data: 0.025 (0.027)
Train: 176 [ 600/1251 ( 48%)]  Loss: 3.358 (3.39)  Time: 0.295s, 3468.86/s  (0.297s, 3450.31/s)  LR: 3.663e-04  Data: 0.024 (0.027)
Train: 176 [ 650/1251 ( 52%)]  Loss: 3.557 (3.41)  Time: 0.291s, 3520.21/s  (0.297s, 3451.47/s)  LR: 3.661e-04  Data: 0.022 (0.027)
Train: 176 [ 700/1251 ( 56%)]  Loss: 3.471 (3.41)  Time: 0.295s, 3468.58/s  (0.297s, 3451.55/s)  LR: 3.659e-04  Data: 0.021 (0.026)
Train: 176 [ 750/1251 ( 60%)]  Loss: 3.335 (3.41)  Time: 0.295s, 3475.68/s  (0.297s, 3452.56/s)  LR: 3.657e-04  Data: 0.022 (0.026)
Train: 176 [ 800/1251 ( 64%)]  Loss: 3.534 (3.41)  Time: 0.301s, 3403.52/s  (0.297s, 3452.70/s)  LR: 3.655e-04  Data: 0.028 (0.026)
Train: 176 [ 850/1251 ( 68%)]  Loss: 3.436 (3.41)  Time: 0.296s, 3464.47/s  (0.297s, 3452.60/s)  LR: 3.653e-04  Data: 0.024 (0.026)
Train: 176 [ 900/1251 ( 72%)]  Loss: 3.468 (3.42)  Time: 0.304s, 3371.33/s  (0.297s, 3452.36/s)  LR: 3.651e-04  Data: 0.023 (0.026)
Train: 176 [ 950/1251 ( 76%)]  Loss: 3.368 (3.41)  Time: 0.297s, 3444.10/s  (0.297s, 3452.43/s)  LR: 3.649e-04  Data: 0.023 (0.026)
Train: 176 [1000/1251 ( 80%)]  Loss: 3.441 (3.42)  Time: 0.294s, 3482.40/s  (0.297s, 3452.05/s)  LR: 3.647e-04  Data: 0.025 (0.025)
Train: 176 [1050/1251 ( 84%)]  Loss: 3.360 (3.41)  Time: 0.295s, 3470.75/s  (0.297s, 3451.80/s)  LR: 3.645e-04  Data: 0.021 (0.025)
Train: 176 [1100/1251 ( 88%)]  Loss: 3.571 (3.42)  Time: 0.296s, 3453.65/s  (0.297s, 3451.78/s)  LR: 3.643e-04  Data: 0.021 (0.025)
Train: 176 [1150/1251 ( 92%)]  Loss: 3.206 (3.41)  Time: 0.293s, 3500.25/s  (0.297s, 3452.07/s)  LR: 3.641e-04  Data: 0.015 (0.025)
Train: 176 [1200/1251 ( 96%)]  Loss: 3.433 (3.41)  Time: 0.294s, 3483.23/s  (0.297s, 3451.96/s)  LR: 3.639e-04  Data: 0.024 (0.025)
Train: 176 [1250/1251 (100%)]  Loss: 3.234 (3.41)  Time: 0.275s, 3717.53/s  (0.296s, 3453.63/s)  LR: 3.637e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.129 (2.129)  Loss:  0.5151 (0.5151)  Acc@1: 90.1367 (90.1367)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.045 (0.240)  Loss:  0.6440 (1.0630)  Acc@1: 85.1415 (75.7420)  Acc@5: 96.6981 (92.9820)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-174.pth.tar', 75.79600001220703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-175.pth.tar', 75.77000010986328)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-176.pth.tar', 75.74200013916015)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-172.pth.tar', 75.7159999609375)

Train: 177 [   0/1251 (  0%)]  Loss: 3.340 (3.34)  Time: 2.356s,  434.69/s  (2.356s,  434.69/s)  LR: 3.637e-04  Data: 2.136 (2.136)
Train: 177 [  50/1251 (  4%)]  Loss: 3.249 (3.29)  Time: 0.285s, 3590.64/s  (0.321s, 3185.33/s)  LR: 3.635e-04  Data: 0.027 (0.065)
Train: 177 [ 100/1251 (  8%)]  Loss: 3.673 (3.42)  Time: 0.293s, 3496.90/s  (0.306s, 3343.75/s)  LR: 3.633e-04  Data: 0.027 (0.044)
Train: 177 [ 150/1251 ( 12%)]  Loss: 3.243 (3.38)  Time: 0.300s, 3418.04/s  (0.302s, 3387.95/s)  LR: 3.631e-04  Data: 0.024 (0.037)
Train: 177 [ 200/1251 ( 16%)]  Loss: 3.017 (3.30)  Time: 0.297s, 3447.15/s  (0.301s, 3407.09/s)  LR: 3.629e-04  Data: 0.025 (0.034)
Train: 177 [ 250/1251 ( 20%)]  Loss: 3.141 (3.28)  Time: 0.297s, 3452.77/s  (0.300s, 3418.77/s)  LR: 3.627e-04  Data: 0.025 (0.031)
Train: 177 [ 300/1251 ( 24%)]  Loss: 3.477 (3.31)  Time: 0.299s, 3419.90/s  (0.299s, 3423.28/s)  LR: 3.625e-04  Data: 0.025 (0.030)
Train: 177 [ 350/1251 ( 28%)]  Loss: 3.470 (3.33)  Time: 0.303s, 3381.06/s  (0.299s, 3426.36/s)  LR: 3.623e-04  Data: 0.024 (0.029)
Train: 177 [ 400/1251 ( 32%)]  Loss: 3.321 (3.33)  Time: 0.299s, 3424.37/s  (0.299s, 3429.13/s)  LR: 3.621e-04  Data: 0.014 (0.028)
Train: 177 [ 450/1251 ( 36%)]  Loss: 3.435 (3.34)  Time: 0.300s, 3415.80/s  (0.298s, 3430.87/s)  LR: 3.619e-04  Data: 0.022 (0.028)
Train: 177 [ 500/1251 ( 40%)]  Loss: 3.277 (3.33)  Time: 0.299s, 3421.98/s  (0.298s, 3431.67/s)  LR: 3.617e-04  Data: 0.025 (0.027)
Train: 177 [ 550/1251 ( 44%)]  Loss: 3.342 (3.33)  Time: 0.302s, 3395.00/s  (0.298s, 3431.15/s)  LR: 3.615e-04  Data: 0.023 (0.027)
Train: 177 [ 600/1251 ( 48%)]  Loss: 3.690 (3.36)  Time: 0.299s, 3426.33/s  (0.298s, 3430.53/s)  LR: 3.613e-04  Data: 0.024 (0.027)
Train: 177 [ 650/1251 ( 52%)]  Loss: 3.258 (3.35)  Time: 0.294s, 3484.56/s  (0.299s, 3429.96/s)  LR: 3.611e-04  Data: 0.022 (0.026)
Train: 177 [ 700/1251 ( 56%)]  Loss: 3.419 (3.36)  Time: 0.297s, 3449.08/s  (0.299s, 3429.50/s)  LR: 3.609e-04  Data: 0.026 (0.026)
Train: 177 [ 750/1251 ( 60%)]  Loss: 3.753 (3.38)  Time: 0.298s, 3437.51/s  (0.299s, 3429.30/s)  LR: 3.607e-04  Data: 0.021 (0.026)
Train: 177 [ 800/1251 ( 64%)]  Loss: 3.233 (3.37)  Time: 0.295s, 3466.60/s  (0.299s, 3429.10/s)  LR: 3.605e-04  Data: 0.019 (0.026)
Train: 177 [ 850/1251 ( 68%)]  Loss: 3.243 (3.37)  Time: 0.296s, 3456.50/s  (0.299s, 3428.89/s)  LR: 3.603e-04  Data: 0.022 (0.025)
Train: 177 [ 900/1251 ( 72%)]  Loss: 3.378 (3.37)  Time: 0.299s, 3421.19/s  (0.299s, 3428.06/s)  LR: 3.601e-04  Data: 0.021 (0.025)
Train: 177 [ 950/1251 ( 76%)]  Loss: 3.093 (3.35)  Time: 0.297s, 3453.03/s  (0.299s, 3427.14/s)  LR: 3.599e-04  Data: 0.024 (0.025)
Train: 177 [1000/1251 ( 80%)]  Loss: 3.554 (3.36)  Time: 0.299s, 3426.75/s  (0.299s, 3426.58/s)  LR: 3.597e-04  Data: 0.023 (0.025)
Train: 177 [1050/1251 ( 84%)]  Loss: 3.306 (3.36)  Time: 0.298s, 3430.56/s  (0.299s, 3426.61/s)  LR: 3.595e-04  Data: 0.020 (0.025)
Train: 177 [1100/1251 ( 88%)]  Loss: 3.647 (3.37)  Time: 0.299s, 3424.89/s  (0.299s, 3426.18/s)  LR: 3.593e-04  Data: 0.021 (0.025)
Train: 177 [1150/1251 ( 92%)]  Loss: 3.571 (3.38)  Time: 0.306s, 3346.70/s  (0.299s, 3425.92/s)  LR: 3.591e-04  Data: 0.025 (0.025)
Train: 177 [1200/1251 ( 96%)]  Loss: 3.427 (3.38)  Time: 0.301s, 3405.22/s  (0.299s, 3425.91/s)  LR: 3.589e-04  Data: 0.024 (0.025)
Train: 177 [1250/1251 (100%)]  Loss: 3.548 (3.39)  Time: 0.276s, 3707.51/s  (0.299s, 3427.72/s)  LR: 3.587e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.073 (2.073)  Loss:  0.6030 (0.6030)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.056 (0.235)  Loss:  0.6812 (1.1019)  Acc@1: 84.7877 (75.8660)  Acc@5: 96.4623 (92.9380)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-177.pth.tar', 75.86599998535156)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-174.pth.tar', 75.79600001220703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-175.pth.tar', 75.77000010986328)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-176.pth.tar', 75.74200013916015)

Train: 178 [   0/1251 (  0%)]  Loss: 3.212 (3.21)  Time: 1.994s,  513.45/s  (1.994s,  513.45/s)  LR: 3.587e-04  Data: 1.759 (1.759)
Train: 178 [  50/1251 (  4%)]  Loss: 3.539 (3.38)  Time: 0.289s, 3545.33/s  (0.323s, 3172.74/s)  LR: 3.585e-04  Data: 0.022 (0.059)
Train: 178 [ 100/1251 (  8%)]  Loss: 2.926 (3.23)  Time: 0.305s, 3355.61/s  (0.309s, 3319.16/s)  LR: 3.583e-04  Data: 0.025 (0.041)
Train: 178 [ 150/1251 ( 12%)]  Loss: 3.586 (3.32)  Time: 0.291s, 3513.84/s  (0.304s, 3365.74/s)  LR: 3.581e-04  Data: 0.020 (0.035)
Train: 178 [ 200/1251 ( 16%)]  Loss: 3.285 (3.31)  Time: 0.300s, 3412.32/s  (0.302s, 3386.08/s)  LR: 3.579e-04  Data: 0.026 (0.032)
Train: 178 [ 250/1251 ( 20%)]  Loss: 3.482 (3.34)  Time: 0.295s, 3469.25/s  (0.301s, 3398.61/s)  LR: 3.577e-04  Data: 0.021 (0.030)
Train: 178 [ 300/1251 ( 24%)]  Loss: 3.495 (3.36)  Time: 0.300s, 3414.31/s  (0.301s, 3405.95/s)  LR: 3.575e-04  Data: 0.025 (0.029)
Train: 178 [ 350/1251 ( 28%)]  Loss: 3.308 (3.35)  Time: 0.298s, 3439.75/s  (0.300s, 3412.63/s)  LR: 3.573e-04  Data: 0.024 (0.028)
Train: 178 [ 400/1251 ( 32%)]  Loss: 3.579 (3.38)  Time: 0.290s, 3529.40/s  (0.300s, 3417.01/s)  LR: 3.571e-04  Data: 0.027 (0.028)
Train: 178 [ 450/1251 ( 36%)]  Loss: 3.371 (3.38)  Time: 0.298s, 3433.84/s  (0.299s, 3421.25/s)  LR: 3.569e-04  Data: 0.023 (0.027)
Train: 178 [ 500/1251 ( 40%)]  Loss: 3.013 (3.35)  Time: 0.298s, 3435.20/s  (0.299s, 3423.75/s)  LR: 3.567e-04  Data: 0.022 (0.027)
Train: 178 [ 550/1251 ( 44%)]  Loss: 3.210 (3.33)  Time: 0.296s, 3457.60/s  (0.299s, 3426.73/s)  LR: 3.565e-04  Data: 0.021 (0.026)
Train: 178 [ 600/1251 ( 48%)]  Loss: 3.325 (3.33)  Time: 0.298s, 3435.88/s  (0.299s, 3426.89/s)  LR: 3.563e-04  Data: 0.020 (0.026)
Train: 178 [ 650/1251 ( 52%)]  Loss: 3.294 (3.33)  Time: 0.298s, 3440.09/s  (0.299s, 3427.89/s)  LR: 3.561e-04  Data: 0.020 (0.026)
Train: 178 [ 700/1251 ( 56%)]  Loss: 2.828 (3.30)  Time: 0.301s, 3400.24/s  (0.299s, 3429.77/s)  LR: 3.559e-04  Data: 0.023 (0.026)
Train: 178 [ 750/1251 ( 60%)]  Loss: 3.029 (3.28)  Time: 0.292s, 3507.75/s  (0.298s, 3430.67/s)  LR: 3.557e-04  Data: 0.023 (0.026)
Train: 178 [ 800/1251 ( 64%)]  Loss: 3.074 (3.27)  Time: 0.293s, 3492.50/s  (0.298s, 3431.20/s)  LR: 3.555e-04  Data: 0.024 (0.025)
Train: 178 [ 850/1251 ( 68%)]  Loss: 3.382 (3.27)  Time: 0.299s, 3421.17/s  (0.298s, 3431.51/s)  LR: 3.553e-04  Data: 0.021 (0.025)
Train: 178 [ 900/1251 ( 72%)]  Loss: 3.415 (3.28)  Time: 0.301s, 3401.28/s  (0.298s, 3431.83/s)  LR: 3.551e-04  Data: 0.022 (0.025)
Train: 178 [ 950/1251 ( 76%)]  Loss: 3.390 (3.29)  Time: 0.301s, 3406.09/s  (0.298s, 3432.17/s)  LR: 3.549e-04  Data: 0.024 (0.025)
Train: 178 [1000/1251 ( 80%)]  Loss: 3.057 (3.28)  Time: 0.298s, 3438.41/s  (0.298s, 3432.07/s)  LR: 3.547e-04  Data: 0.026 (0.025)
Train: 178 [1050/1251 ( 84%)]  Loss: 3.210 (3.27)  Time: 0.300s, 3417.14/s  (0.298s, 3431.55/s)  LR: 3.545e-04  Data: 0.024 (0.025)
Train: 178 [1100/1251 ( 88%)]  Loss: 3.617 (3.29)  Time: 0.306s, 3342.52/s  (0.298s, 3431.45/s)  LR: 3.543e-04  Data: 0.022 (0.025)
Train: 178 [1150/1251 ( 92%)]  Loss: 2.964 (3.27)  Time: 0.303s, 3381.08/s  (0.298s, 3431.56/s)  LR: 3.541e-04  Data: 0.022 (0.025)
Train: 178 [1200/1251 ( 96%)]  Loss: 3.021 (3.26)  Time: 0.296s, 3454.64/s  (0.298s, 3431.09/s)  LR: 3.539e-04  Data: 0.023 (0.025)
Train: 178 [1250/1251 (100%)]  Loss: 3.174 (3.26)  Time: 0.275s, 3722.89/s  (0.298s, 3432.90/s)  LR: 3.537e-04  Data: 0.000 (0.024)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.057 (2.057)  Loss:  0.5342 (0.5342)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.041 (0.236)  Loss:  0.6602 (1.0659)  Acc@1: 85.6132 (76.1000)  Acc@5: 96.6981 (92.9860)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-178.pth.tar', 76.10000008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-177.pth.tar', 75.86599998535156)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-174.pth.tar', 75.79600001220703)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-175.pth.tar', 75.77000010986328)

Train: 179 [   0/1251 (  0%)]  Loss: 3.074 (3.07)  Time: 2.170s,  471.97/s  (2.170s,  471.97/s)  LR: 3.537e-04  Data: 1.945 (1.945)
Train: 179 [  50/1251 (  4%)]  Loss: 3.271 (3.17)  Time: 0.295s, 3470.34/s  (0.318s, 3220.20/s)  LR: 3.535e-04  Data: 0.027 (0.061)
Train: 179 [ 100/1251 (  8%)]  Loss: 3.132 (3.16)  Time: 0.294s, 3482.43/s  (0.305s, 3356.98/s)  LR: 3.533e-04  Data: 0.024 (0.042)
Train: 179 [ 150/1251 ( 12%)]  Loss: 3.340 (3.20)  Time: 0.289s, 3543.96/s  (0.302s, 3394.57/s)  LR: 3.531e-04  Data: 0.022 (0.036)
Train: 179 [ 200/1251 ( 16%)]  Loss: 3.178 (3.20)  Time: 0.298s, 3436.14/s  (0.300s, 3409.25/s)  LR: 3.529e-04  Data: 0.024 (0.033)
Train: 179 [ 250/1251 ( 20%)]  Loss: 3.328 (3.22)  Time: 0.288s, 3558.08/s  (0.300s, 3416.91/s)  LR: 3.527e-04  Data: 0.021 (0.031)
Train: 179 [ 300/1251 ( 24%)]  Loss: 3.425 (3.25)  Time: 0.302s, 3390.47/s  (0.299s, 3420.09/s)  LR: 3.525e-04  Data: 0.024 (0.029)
Train: 179 [ 350/1251 ( 28%)]  Loss: 3.569 (3.29)  Time: 0.295s, 3475.43/s  (0.299s, 3421.84/s)  LR: 3.523e-04  Data: 0.024 (0.029)
Train: 179 [ 400/1251 ( 32%)]  Loss: 3.357 (3.30)  Time: 0.295s, 3474.85/s  (0.299s, 3423.88/s)  LR: 3.521e-04  Data: 0.026 (0.028)
Train: 179 [ 450/1251 ( 36%)]  Loss: 3.261 (3.29)  Time: 0.297s, 3444.86/s  (0.299s, 3422.44/s)  LR: 3.519e-04  Data: 0.021 (0.028)
Train: 179 [ 500/1251 ( 40%)]  Loss: 3.264 (3.29)  Time: 0.305s, 3353.86/s  (0.299s, 3421.98/s)  LR: 3.517e-04  Data: 0.030 (0.027)
Train: 179 [ 550/1251 ( 44%)]  Loss: 3.521 (3.31)  Time: 0.298s, 3431.73/s  (0.299s, 3421.62/s)  LR: 3.515e-04  Data: 0.022 (0.027)
Train: 179 [ 600/1251 ( 48%)]  Loss: 3.301 (3.31)  Time: 0.301s, 3404.52/s  (0.299s, 3421.12/s)  LR: 3.513e-04  Data: 0.025 (0.026)
Train: 179 [ 650/1251 ( 52%)]  Loss: 3.093 (3.29)  Time: 0.297s, 3446.16/s  (0.299s, 3421.06/s)  LR: 3.511e-04  Data: 0.024 (0.026)
Train: 179 [ 700/1251 ( 56%)]  Loss: 3.246 (3.29)  Time: 0.296s, 3459.54/s  (0.299s, 3420.60/s)  LR: 3.509e-04  Data: 0.023 (0.026)
Train: 179 [ 750/1251 ( 60%)]  Loss: 3.520 (3.31)  Time: 0.304s, 3367.64/s  (0.299s, 3419.68/s)  LR: 3.507e-04  Data: 0.026 (0.026)
Train: 179 [ 800/1251 ( 64%)]  Loss: 3.353 (3.31)  Time: 0.290s, 3535.65/s  (0.299s, 3419.39/s)  LR: 3.505e-04  Data: 0.017 (0.026)
Train: 179 [ 850/1251 ( 68%)]  Loss: 3.415 (3.31)  Time: 0.302s, 3389.47/s  (0.300s, 3418.59/s)  LR: 3.503e-04  Data: 0.024 (0.026)
Train: 179 [ 900/1251 ( 72%)]  Loss: 3.382 (3.32)  Time: 0.297s, 3447.56/s  (0.300s, 3417.29/s)  LR: 3.502e-04  Data: 0.026 (0.025)
Train: 179 [ 950/1251 ( 76%)]  Loss: 2.775 (3.29)  Time: 0.300s, 3411.99/s  (0.300s, 3416.44/s)  LR: 3.500e-04  Data: 0.023 (0.025)
Train: 179 [1000/1251 ( 80%)]  Loss: 3.503 (3.30)  Time: 0.300s, 3412.29/s  (0.300s, 3416.14/s)  LR: 3.498e-04  Data: 0.021 (0.025)
Train: 179 [1050/1251 ( 84%)]  Loss: 3.745 (3.32)  Time: 0.303s, 3383.07/s  (0.300s, 3415.54/s)  LR: 3.496e-04  Data: 0.022 (0.025)
Train: 179 [1100/1251 ( 88%)]  Loss: 3.414 (3.32)  Time: 0.301s, 3405.09/s  (0.300s, 3415.35/s)  LR: 3.494e-04  Data: 0.025 (0.025)
Train: 179 [1150/1251 ( 92%)]  Loss: 3.312 (3.32)  Time: 0.303s, 3377.11/s  (0.300s, 3414.25/s)  LR: 3.492e-04  Data: 0.024 (0.025)
Train: 179 [1200/1251 ( 96%)]  Loss: 3.442 (3.33)  Time: 0.298s, 3431.43/s  (0.300s, 3413.80/s)  LR: 3.490e-04  Data: 0.021 (0.025)
Train: 179 [1250/1251 (100%)]  Loss: 3.250 (3.33)  Time: 0.275s, 3723.02/s  (0.300s, 3415.41/s)  LR: 3.488e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.081 (2.081)  Loss:  0.5903 (0.5903)  Acc@1: 87.9883 (87.9883)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.047 (0.236)  Loss:  0.6338 (1.0562)  Acc@1: 85.9670 (75.9840)  Acc@5: 97.1698 (93.0220)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-178.pth.tar', 76.10000008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-179.pth.tar', 75.98399998046875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-177.pth.tar', 75.86599998535156)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-174.pth.tar', 75.79600001220703)

Train: 180 [   0/1251 (  0%)]  Loss: 3.018 (3.02)  Time: 2.212s,  463.02/s  (2.212s,  463.02/s)  LR: 3.488e-04  Data: 1.994 (1.994)
Train: 180 [  50/1251 (  4%)]  Loss: 3.417 (3.22)  Time: 0.286s, 3579.64/s  (0.324s, 3163.43/s)  LR: 3.486e-04  Data: 0.025 (0.067)
Train: 180 [ 100/1251 (  8%)]  Loss: 3.341 (3.26)  Time: 0.300s, 3410.64/s  (0.309s, 3311.70/s)  LR: 3.484e-04  Data: 0.022 (0.045)
Train: 180 [ 150/1251 ( 12%)]  Loss: 2.976 (3.19)  Time: 0.300s, 3418.52/s  (0.305s, 3353.01/s)  LR: 3.482e-04  Data: 0.024 (0.038)
Train: 180 [ 200/1251 ( 16%)]  Loss: 3.281 (3.21)  Time: 0.301s, 3396.53/s  (0.304s, 3368.53/s)  LR: 3.480e-04  Data: 0.026 (0.034)
Train: 180 [ 250/1251 ( 20%)]  Loss: 3.194 (3.20)  Time: 0.300s, 3415.40/s  (0.303s, 3377.00/s)  LR: 3.478e-04  Data: 0.023 (0.032)
Train: 180 [ 300/1251 ( 24%)]  Loss: 2.988 (3.17)  Time: 0.303s, 3376.61/s  (0.303s, 3380.05/s)  LR: 3.476e-04  Data: 0.026 (0.031)
Train: 180 [ 350/1251 ( 28%)]  Loss: 3.375 (3.20)  Time: 0.303s, 3374.63/s  (0.303s, 3383.46/s)  LR: 3.474e-04  Data: 0.024 (0.029)
Train: 180 [ 400/1251 ( 32%)]  Loss: 3.246 (3.20)  Time: 0.298s, 3438.02/s  (0.302s, 3385.41/s)  LR: 3.472e-04  Data: 0.020 (0.029)
Train: 180 [ 450/1251 ( 36%)]  Loss: 3.573 (3.24)  Time: 0.298s, 3440.05/s  (0.302s, 3386.16/s)  LR: 3.470e-04  Data: 0.021 (0.028)
Train: 180 [ 500/1251 ( 40%)]  Loss: 3.079 (3.23)  Time: 0.304s, 3367.65/s  (0.302s, 3388.89/s)  LR: 3.468e-04  Data: 0.022 (0.028)
Train: 180 [ 550/1251 ( 44%)]  Loss: 2.961 (3.20)  Time: 0.306s, 3345.58/s  (0.302s, 3389.51/s)  LR: 3.466e-04  Data: 0.029 (0.027)
Train: 180 [ 600/1251 ( 48%)]  Loss: 3.333 (3.21)  Time: 0.302s, 3394.20/s  (0.302s, 3390.40/s)  LR: 3.464e-04  Data: 0.025 (0.027)
Train: 180 [ 650/1251 ( 52%)]  Loss: 3.623 (3.24)  Time: 0.299s, 3420.28/s  (0.302s, 3391.42/s)  LR: 3.462e-04  Data: 0.014 (0.027)
Train: 180 [ 700/1251 ( 56%)]  Loss: 3.305 (3.25)  Time: 0.307s, 3332.34/s  (0.302s, 3390.72/s)  LR: 3.460e-04  Data: 0.025 (0.026)
Train: 180 [ 750/1251 ( 60%)]  Loss: 3.334 (3.25)  Time: 0.302s, 3390.85/s  (0.302s, 3390.30/s)  LR: 3.458e-04  Data: 0.024 (0.026)
Train: 180 [ 800/1251 ( 64%)]  Loss: 3.540 (3.27)  Time: 0.301s, 3399.70/s  (0.302s, 3390.74/s)  LR: 3.456e-04  Data: 0.019 (0.026)
Train: 180 [ 850/1251 ( 68%)]  Loss: 3.638 (3.29)  Time: 0.301s, 3399.22/s  (0.302s, 3391.22/s)  LR: 3.454e-04  Data: 0.024 (0.026)
Train: 180 [ 900/1251 ( 72%)]  Loss: 3.554 (3.30)  Time: 0.294s, 3479.06/s  (0.302s, 3391.54/s)  LR: 3.452e-04  Data: 0.023 (0.026)
Train: 180 [ 950/1251 ( 76%)]  Loss: 3.397 (3.31)  Time: 0.300s, 3417.28/s  (0.302s, 3391.94/s)  LR: 3.450e-04  Data: 0.017 (0.025)
Train: 180 [1000/1251 ( 80%)]  Loss: 3.062 (3.30)  Time: 0.303s, 3379.78/s  (0.302s, 3391.77/s)  LR: 3.448e-04  Data: 0.022 (0.025)
Train: 180 [1050/1251 ( 84%)]  Loss: 3.399 (3.30)  Time: 0.301s, 3406.39/s  (0.302s, 3391.19/s)  LR: 3.446e-04  Data: 0.022 (0.025)
Train: 180 [1100/1251 ( 88%)]  Loss: 3.333 (3.30)  Time: 0.304s, 3366.99/s  (0.302s, 3390.37/s)  LR: 3.444e-04  Data: 0.020 (0.025)
Train: 180 [1150/1251 ( 92%)]  Loss: 3.295 (3.30)  Time: 0.305s, 3358.02/s  (0.302s, 3390.07/s)  LR: 3.442e-04  Data: 0.026 (0.025)
Train: 180 [1200/1251 ( 96%)]  Loss: 3.469 (3.31)  Time: 0.306s, 3350.03/s  (0.302s, 3389.83/s)  LR: 3.440e-04  Data: 0.020 (0.025)
Train: 180 [1250/1251 (100%)]  Loss: 3.385 (3.31)  Time: 0.277s, 3703.25/s  (0.302s, 3391.27/s)  LR: 3.438e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.161 (2.161)  Loss:  0.5674 (0.5674)  Acc@1: 88.6719 (88.6719)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.052 (0.238)  Loss:  0.6357 (1.0635)  Acc@1: 85.6132 (76.0200)  Acc@5: 97.1698 (93.0260)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-178.pth.tar', 76.10000008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-180.pth.tar', 76.02000008544921)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-179.pth.tar', 75.98399998046875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-177.pth.tar', 75.86599998535156)

Train: 181 [   0/1251 (  0%)]  Loss: 3.311 (3.31)  Time: 2.209s,  463.53/s  (2.209s,  463.53/s)  LR: 3.438e-04  Data: 1.972 (1.972)
Train: 181 [  50/1251 (  4%)]  Loss: 3.535 (3.42)  Time: 0.292s, 3510.90/s  (0.331s, 3091.07/s)  LR: 3.436e-04  Data: 0.024 (0.073)
Train: 181 [ 100/1251 (  8%)]  Loss: 3.245 (3.36)  Time: 0.302s, 3394.21/s  (0.314s, 3261.60/s)  LR: 3.434e-04  Data: 0.024 (0.048)
Train: 181 [ 150/1251 ( 12%)]  Loss: 3.387 (3.37)  Time: 0.303s, 3381.13/s  (0.309s, 3312.16/s)  LR: 3.432e-04  Data: 0.025 (0.040)
Train: 181 [ 200/1251 ( 16%)]  Loss: 2.971 (3.29)  Time: 0.298s, 3437.07/s  (0.307s, 3335.09/s)  LR: 3.430e-04  Data: 0.024 (0.036)
Train: 181 [ 250/1251 ( 20%)]  Loss: 3.440 (3.31)  Time: 0.300s, 3415.18/s  (0.306s, 3346.88/s)  LR: 3.428e-04  Data: 0.021 (0.033)
Train: 181 [ 300/1251 ( 24%)]  Loss: 2.960 (3.26)  Time: 0.306s, 3343.34/s  (0.305s, 3353.95/s)  LR: 3.426e-04  Data: 0.023 (0.032)
Train: 181 [ 350/1251 ( 28%)]  Loss: 3.505 (3.29)  Time: 0.302s, 3394.86/s  (0.305s, 3357.12/s)  LR: 3.424e-04  Data: 0.026 (0.030)
Train: 181 [ 400/1251 ( 32%)]  Loss: 3.285 (3.29)  Time: 0.302s, 3388.92/s  (0.305s, 3360.58/s)  LR: 3.422e-04  Data: 0.021 (0.030)
Train: 181 [ 450/1251 ( 36%)]  Loss: 3.409 (3.30)  Time: 0.306s, 3343.18/s  (0.305s, 3362.22/s)  LR: 3.420e-04  Data: 0.023 (0.029)
Train: 181 [ 500/1251 ( 40%)]  Loss: 3.647 (3.34)  Time: 0.304s, 3368.97/s  (0.304s, 3363.23/s)  LR: 3.418e-04  Data: 0.021 (0.028)
Train: 181 [ 550/1251 ( 44%)]  Loss: 3.309 (3.33)  Time: 0.303s, 3375.97/s  (0.304s, 3363.98/s)  LR: 3.416e-04  Data: 0.023 (0.028)
Train: 181 [ 600/1251 ( 48%)]  Loss: 3.507 (3.35)  Time: 0.302s, 3389.32/s  (0.304s, 3363.03/s)  LR: 3.414e-04  Data: 0.022 (0.027)
Train: 181 [ 650/1251 ( 52%)]  Loss: 3.367 (3.35)  Time: 0.305s, 3360.35/s  (0.305s, 3362.87/s)  LR: 3.413e-04  Data: 0.025 (0.027)
Train: 181 [ 700/1251 ( 56%)]  Loss: 3.171 (3.34)  Time: 0.301s, 3404.11/s  (0.305s, 3362.30/s)  LR: 3.411e-04  Data: 0.021 (0.027)
Train: 181 [ 750/1251 ( 60%)]  Loss: 3.174 (3.33)  Time: 0.310s, 3299.29/s  (0.305s, 3361.32/s)  LR: 3.409e-04  Data: 0.024 (0.027)
Train: 181 [ 800/1251 ( 64%)]  Loss: 3.406 (3.33)  Time: 0.308s, 3328.60/s  (0.305s, 3360.38/s)  LR: 3.407e-04  Data: 0.024 (0.026)
Train: 181 [ 850/1251 ( 68%)]  Loss: 3.465 (3.34)  Time: 0.306s, 3349.25/s  (0.305s, 3360.08/s)  LR: 3.405e-04  Data: 0.024 (0.026)
Train: 181 [ 900/1251 ( 72%)]  Loss: 3.459 (3.34)  Time: 0.306s, 3342.25/s  (0.305s, 3359.49/s)  LR: 3.403e-04  Data: 0.022 (0.026)
Train: 181 [ 950/1251 ( 76%)]  Loss: 3.614 (3.36)  Time: 0.304s, 3363.39/s  (0.305s, 3358.97/s)  LR: 3.401e-04  Data: 0.023 (0.026)
Train: 181 [1000/1251 ( 80%)]  Loss: 3.358 (3.36)  Time: 0.307s, 3340.10/s  (0.305s, 3358.64/s)  LR: 3.399e-04  Data: 0.020 (0.026)
Train: 181 [1050/1251 ( 84%)]  Loss: 3.227 (3.35)  Time: 0.308s, 3327.70/s  (0.305s, 3358.29/s)  LR: 3.397e-04  Data: 0.024 (0.026)
Train: 181 [1100/1251 ( 88%)]  Loss: 3.307 (3.35)  Time: 0.308s, 3323.76/s  (0.305s, 3356.87/s)  LR: 3.395e-04  Data: 0.018 (0.025)
Train: 181 [1150/1251 ( 92%)]  Loss: 2.687 (3.32)  Time: 0.306s, 3343.00/s  (0.305s, 3356.32/s)  LR: 3.393e-04  Data: 0.025 (0.025)
Train: 181 [1200/1251 ( 96%)]  Loss: 3.272 (3.32)  Time: 0.310s, 3305.81/s  (0.305s, 3355.67/s)  LR: 3.391e-04  Data: 0.027 (0.025)
Train: 181 [1250/1251 (100%)]  Loss: 3.108 (3.31)  Time: 0.277s, 3701.03/s  (0.305s, 3356.98/s)  LR: 3.389e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.106 (2.106)  Loss:  0.5586 (0.5586)  Acc@1: 88.5742 (88.5742)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.061 (0.233)  Loss:  0.6436 (1.0648)  Acc@1: 86.3208 (76.1480)  Acc@5: 97.1698 (93.0880)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-181.pth.tar', 76.14800013427734)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-178.pth.tar', 76.10000008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-180.pth.tar', 76.02000008544921)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-179.pth.tar', 75.98399998046875)

Train: 182 [   0/1251 (  0%)]  Loss: 3.502 (3.50)  Time: 2.362s,  433.50/s  (2.362s,  433.50/s)  LR: 3.389e-04  Data: 2.143 (2.143)
Train: 182 [  50/1251 (  4%)]  Loss: 2.927 (3.21)  Time: 0.303s, 3378.49/s  (0.330s, 3098.43/s)  LR: 3.387e-04  Data: 0.024 (0.064)
Train: 182 [ 100/1251 (  8%)]  Loss: 3.274 (3.23)  Time: 0.302s, 3388.53/s  (0.315s, 3250.67/s)  LR: 3.385e-04  Data: 0.023 (0.044)
Train: 182 [ 150/1251 ( 12%)]  Loss: 3.079 (3.20)  Time: 0.306s, 3343.26/s  (0.311s, 3296.33/s)  LR: 3.383e-04  Data: 0.025 (0.037)
Train: 182 [ 200/1251 ( 16%)]  Loss: 3.518 (3.26)  Time: 0.297s, 3446.40/s  (0.309s, 3317.76/s)  LR: 3.381e-04  Data: 0.021 (0.034)
Train: 182 [ 250/1251 ( 20%)]  Loss: 3.368 (3.28)  Time: 0.304s, 3364.73/s  (0.308s, 3328.66/s)  LR: 3.379e-04  Data: 0.022 (0.031)
Train: 182 [ 300/1251 ( 24%)]  Loss: 3.341 (3.29)  Time: 0.308s, 3327.60/s  (0.307s, 3335.05/s)  LR: 3.377e-04  Data: 0.025 (0.030)
Train: 182 [ 350/1251 ( 28%)]  Loss: 2.969 (3.25)  Time: 0.307s, 3337.19/s  (0.307s, 3339.36/s)  LR: 3.375e-04  Data: 0.028 (0.029)
Train: 182 [ 400/1251 ( 32%)]  Loss: 3.189 (3.24)  Time: 0.307s, 3338.81/s  (0.307s, 3340.51/s)  LR: 3.373e-04  Data: 0.021 (0.028)
Train: 182 [ 450/1251 ( 36%)]  Loss: 3.306 (3.25)  Time: 0.302s, 3389.21/s  (0.307s, 3340.78/s)  LR: 3.371e-04  Data: 0.023 (0.028)
Train: 182 [ 500/1251 ( 40%)]  Loss: 3.081 (3.23)  Time: 0.308s, 3324.17/s  (0.306s, 3342.57/s)  LR: 3.369e-04  Data: 0.027 (0.027)
Train: 182 [ 550/1251 ( 44%)]  Loss: 3.521 (3.26)  Time: 0.312s, 3285.67/s  (0.306s, 3343.28/s)  LR: 3.367e-04  Data: 0.022 (0.027)
Train: 182 [ 600/1251 ( 48%)]  Loss: 3.317 (3.26)  Time: 0.303s, 3380.82/s  (0.306s, 3343.62/s)  LR: 3.365e-04  Data: 0.020 (0.026)
Train: 182 [ 650/1251 ( 52%)]  Loss: 3.302 (3.26)  Time: 0.308s, 3329.48/s  (0.306s, 3343.78/s)  LR: 3.363e-04  Data: 0.023 (0.026)
Train: 182 [ 700/1251 ( 56%)]  Loss: 3.336 (3.27)  Time: 0.310s, 3299.04/s  (0.306s, 3344.79/s)  LR: 3.361e-04  Data: 0.022 (0.026)
Train: 182 [ 750/1251 ( 60%)]  Loss: 3.427 (3.28)  Time: 0.306s, 3351.04/s  (0.306s, 3344.76/s)  LR: 3.359e-04  Data: 0.022 (0.026)
Train: 182 [ 800/1251 ( 64%)]  Loss: 3.550 (3.29)  Time: 0.305s, 3355.87/s  (0.306s, 3344.96/s)  LR: 3.357e-04  Data: 0.025 (0.026)
Train: 182 [ 850/1251 ( 68%)]  Loss: 3.156 (3.29)  Time: 0.311s, 3296.17/s  (0.306s, 3343.98/s)  LR: 3.355e-04  Data: 0.024 (0.025)
Train: 182 [ 900/1251 ( 72%)]  Loss: 3.151 (3.28)  Time: 0.305s, 3353.40/s  (0.306s, 3343.10/s)  LR: 3.354e-04  Data: 0.024 (0.025)
Train: 182 [ 950/1251 ( 76%)]  Loss: 3.447 (3.29)  Time: 0.305s, 3353.62/s  (0.306s, 3342.29/s)  LR: 3.352e-04  Data: 0.020 (0.025)
Train: 182 [1000/1251 ( 80%)]  Loss: 3.174 (3.28)  Time: 0.305s, 3353.11/s  (0.306s, 3341.72/s)  LR: 3.350e-04  Data: 0.024 (0.025)
Train: 182 [1050/1251 ( 84%)]  Loss: 3.596 (3.30)  Time: 0.306s, 3344.42/s  (0.307s, 3340.88/s)  LR: 3.348e-04  Data: 0.027 (0.025)
Train: 182 [1100/1251 ( 88%)]  Loss: 3.032 (3.29)  Time: 0.309s, 3316.84/s  (0.307s, 3339.88/s)  LR: 3.346e-04  Data: 0.023 (0.025)
Train: 182 [1150/1251 ( 92%)]  Loss: 3.019 (3.27)  Time: 0.308s, 3328.22/s  (0.307s, 3338.98/s)  LR: 3.344e-04  Data: 0.026 (0.025)
Train: 182 [1200/1251 ( 96%)]  Loss: 3.358 (3.28)  Time: 0.310s, 3305.78/s  (0.307s, 3338.07/s)  LR: 3.342e-04  Data: 0.024 (0.025)
Train: 182 [1250/1251 (100%)]  Loss: 3.690 (3.29)  Time: 0.275s, 3720.54/s  (0.307s, 3339.20/s)  LR: 3.340e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.116 (2.116)  Loss:  0.5547 (0.5547)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.044 (0.233)  Loss:  0.6689 (1.0400)  Acc@1: 85.4953 (76.4640)  Acc@5: 96.5802 (93.1860)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-182.pth.tar', 76.46400003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-181.pth.tar', 76.14800013427734)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-178.pth.tar', 76.10000008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-180.pth.tar', 76.02000008544921)

Train: 183 [   0/1251 (  0%)]  Loss: 3.419 (3.42)  Time: 2.006s,  510.45/s  (2.006s,  510.45/s)  LR: 3.340e-04  Data: 1.787 (1.787)
Train: 183 [  50/1251 (  4%)]  Loss: 3.144 (3.28)  Time: 0.302s, 3390.26/s  (0.325s, 3148.59/s)  LR: 3.338e-04  Data: 0.022 (0.058)
Train: 183 [ 100/1251 (  8%)]  Loss: 3.338 (3.30)  Time: 0.304s, 3368.88/s  (0.314s, 3258.85/s)  LR: 3.336e-04  Data: 0.026 (0.041)
Train: 183 [ 150/1251 ( 12%)]  Loss: 3.679 (3.39)  Time: 0.307s, 3332.16/s  (0.311s, 3289.31/s)  LR: 3.334e-04  Data: 0.022 (0.035)
Train: 183 [ 200/1251 ( 16%)]  Loss: 3.194 (3.35)  Time: 0.308s, 3322.74/s  (0.310s, 3304.71/s)  LR: 3.332e-04  Data: 0.019 (0.032)
Train: 183 [ 250/1251 ( 20%)]  Loss: 3.191 (3.33)  Time: 0.308s, 3326.79/s  (0.309s, 3311.28/s)  LR: 3.330e-04  Data: 0.026 (0.030)
Train: 183 [ 300/1251 ( 24%)]  Loss: 3.418 (3.34)  Time: 0.313s, 3272.60/s  (0.309s, 3312.79/s)  LR: 3.328e-04  Data: 0.025 (0.029)
Train: 183 [ 350/1251 ( 28%)]  Loss: 3.311 (3.34)  Time: 0.304s, 3366.71/s  (0.309s, 3314.86/s)  LR: 3.326e-04  Data: 0.026 (0.028)
Train: 183 [ 400/1251 ( 32%)]  Loss: 3.336 (3.34)  Time: 0.308s, 3324.25/s  (0.309s, 3316.28/s)  LR: 3.324e-04  Data: 0.023 (0.028)
Train: 183 [ 450/1251 ( 36%)]  Loss: 3.287 (3.33)  Time: 0.305s, 3359.85/s  (0.309s, 3316.43/s)  LR: 3.322e-04  Data: 0.025 (0.027)
Train: 183 [ 500/1251 ( 40%)]  Loss: 3.508 (3.35)  Time: 0.305s, 3354.64/s  (0.309s, 3316.79/s)  LR: 3.320e-04  Data: 0.023 (0.027)
Train: 183 [ 550/1251 ( 44%)]  Loss: 3.235 (3.34)  Time: 0.305s, 3358.64/s  (0.309s, 3316.13/s)  LR: 3.318e-04  Data: 0.026 (0.026)
Train: 183 [ 600/1251 ( 48%)]  Loss: 3.173 (3.33)  Time: 0.304s, 3368.24/s  (0.309s, 3315.97/s)  LR: 3.316e-04  Data: 0.023 (0.026)
Train: 183 [ 650/1251 ( 52%)]  Loss: 3.305 (3.32)  Time: 0.312s, 3281.03/s  (0.309s, 3315.29/s)  LR: 3.314e-04  Data: 0.024 (0.026)
Train: 183 [ 700/1251 ( 56%)]  Loss: 3.770 (3.35)  Time: 0.303s, 3382.60/s  (0.309s, 3315.32/s)  LR: 3.312e-04  Data: 0.025 (0.026)
Train: 183 [ 750/1251 ( 60%)]  Loss: 2.779 (3.32)  Time: 0.309s, 3313.61/s  (0.309s, 3315.79/s)  LR: 3.310e-04  Data: 0.023 (0.025)
Train: 183 [ 800/1251 ( 64%)]  Loss: 3.425 (3.32)  Time: 0.312s, 3283.39/s  (0.309s, 3316.16/s)  LR: 3.308e-04  Data: 0.024 (0.025)
Train: 183 [ 850/1251 ( 68%)]  Loss: 3.329 (3.32)  Time: 0.311s, 3288.01/s  (0.309s, 3316.03/s)  LR: 3.306e-04  Data: 0.023 (0.025)
Train: 183 [ 900/1251 ( 72%)]  Loss: 3.044 (3.31)  Time: 0.313s, 3272.26/s  (0.309s, 3315.72/s)  LR: 3.305e-04  Data: 0.027 (0.025)
Train: 183 [ 950/1251 ( 76%)]  Loss: 3.273 (3.31)  Time: 0.312s, 3283.30/s  (0.309s, 3315.90/s)  LR: 3.303e-04  Data: 0.022 (0.025)
Train: 183 [1000/1251 ( 80%)]  Loss: 3.090 (3.30)  Time: 0.309s, 3312.70/s  (0.309s, 3315.86/s)  LR: 3.301e-04  Data: 0.024 (0.025)
Train: 183 [1050/1251 ( 84%)]  Loss: 3.189 (3.29)  Time: 0.307s, 3334.47/s  (0.309s, 3315.74/s)  LR: 3.299e-04  Data: 0.026 (0.025)
Train: 183 [1100/1251 ( 88%)]  Loss: 3.300 (3.29)  Time: 0.313s, 3273.97/s  (0.309s, 3315.72/s)  LR: 3.297e-04  Data: 0.023 (0.025)
Train: 183 [1150/1251 ( 92%)]  Loss: 3.624 (3.31)  Time: 0.313s, 3275.11/s  (0.309s, 3315.74/s)  LR: 3.295e-04  Data: 0.022 (0.025)
Train: 183 [1200/1251 ( 96%)]  Loss: 3.107 (3.30)  Time: 0.306s, 3341.26/s  (0.309s, 3315.40/s)  LR: 3.293e-04  Data: 0.022 (0.025)
Train: 183 [1250/1251 (100%)]  Loss: 3.378 (3.30)  Time: 0.277s, 3702.23/s  (0.309s, 3317.47/s)  LR: 3.291e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.070 (2.070)  Loss:  0.5508 (0.5508)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.064 (0.240)  Loss:  0.6362 (1.0476)  Acc@1: 85.6132 (76.4740)  Acc@5: 96.9340 (93.1220)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-183.pth.tar', 76.47400008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-182.pth.tar', 76.46400003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-181.pth.tar', 76.14800013427734)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-178.pth.tar', 76.10000008544922)

Train: 184 [   0/1251 (  0%)]  Loss: 3.435 (3.44)  Time: 2.322s,  441.07/s  (2.322s,  441.07/s)  LR: 3.291e-04  Data: 2.095 (2.095)
Train: 184 [  50/1251 (  4%)]  Loss: 3.448 (3.44)  Time: 0.301s, 3396.40/s  (0.331s, 3090.60/s)  LR: 3.289e-04  Data: 0.023 (0.063)
Train: 184 [ 100/1251 (  8%)]  Loss: 2.946 (3.28)  Time: 0.303s, 3374.98/s  (0.317s, 3233.80/s)  LR: 3.287e-04  Data: 0.021 (0.043)
Train: 184 [ 150/1251 ( 12%)]  Loss: 3.100 (3.23)  Time: 0.305s, 3354.64/s  (0.313s, 3272.09/s)  LR: 3.285e-04  Data: 0.026 (0.037)
Train: 184 [ 200/1251 ( 16%)]  Loss: 3.491 (3.28)  Time: 0.307s, 3335.69/s  (0.311s, 3288.06/s)  LR: 3.283e-04  Data: 0.022 (0.033)
Train: 184 [ 250/1251 ( 20%)]  Loss: 3.264 (3.28)  Time: 0.310s, 3299.93/s  (0.311s, 3293.62/s)  LR: 3.281e-04  Data: 0.021 (0.031)
Train: 184 [ 300/1251 ( 24%)]  Loss: 3.463 (3.31)  Time: 0.312s, 3286.64/s  (0.310s, 3297.95/s)  LR: 3.279e-04  Data: 0.025 (0.030)
Train: 184 [ 350/1251 ( 28%)]  Loss: 3.288 (3.30)  Time: 0.305s, 3360.33/s  (0.310s, 3302.08/s)  LR: 3.277e-04  Data: 0.023 (0.029)
Train: 184 [ 400/1251 ( 32%)]  Loss: 3.255 (3.30)  Time: 0.305s, 3357.05/s  (0.310s, 3303.96/s)  LR: 3.275e-04  Data: 0.023 (0.028)
Train: 184 [ 450/1251 ( 36%)]  Loss: 3.436 (3.31)  Time: 0.309s, 3314.75/s  (0.310s, 3305.52/s)  LR: 3.273e-04  Data: 0.022 (0.028)
Train: 184 [ 500/1251 ( 40%)]  Loss: 3.499 (3.33)  Time: 0.309s, 3318.22/s  (0.310s, 3306.45/s)  LR: 3.271e-04  Data: 0.022 (0.027)
Train: 184 [ 550/1251 ( 44%)]  Loss: 3.426 (3.34)  Time: 0.309s, 3315.11/s  (0.310s, 3307.25/s)  LR: 3.269e-04  Data: 0.022 (0.027)
Train: 184 [ 600/1251 ( 48%)]  Loss: 3.496 (3.35)  Time: 0.306s, 3343.08/s  (0.310s, 3307.51/s)  LR: 3.267e-04  Data: 0.024 (0.026)
Train: 184 [ 650/1251 ( 52%)]  Loss: 3.384 (3.35)  Time: 0.307s, 3331.03/s  (0.310s, 3307.52/s)  LR: 3.265e-04  Data: 0.021 (0.026)
Train: 184 [ 700/1251 ( 56%)]  Loss: 3.105 (3.34)  Time: 0.315s, 3247.51/s  (0.310s, 3306.90/s)  LR: 3.264e-04  Data: 0.023 (0.026)
Train: 184 [ 750/1251 ( 60%)]  Loss: 3.380 (3.34)  Time: 0.312s, 3280.29/s  (0.310s, 3306.66/s)  LR: 3.262e-04  Data: 0.022 (0.026)
Train: 184 [ 800/1251 ( 64%)]  Loss: 3.183 (3.33)  Time: 0.311s, 3292.55/s  (0.310s, 3306.60/s)  LR: 3.260e-04  Data: 0.021 (0.026)
Train: 184 [ 850/1251 ( 68%)]  Loss: 3.379 (3.33)  Time: 0.308s, 3321.22/s  (0.310s, 3306.37/s)  LR: 3.258e-04  Data: 0.024 (0.025)
Train: 184 [ 900/1251 ( 72%)]  Loss: 3.370 (3.33)  Time: 0.309s, 3315.76/s  (0.310s, 3306.04/s)  LR: 3.256e-04  Data: 0.028 (0.025)
Train: 184 [ 950/1251 ( 76%)]  Loss: 3.229 (3.33)  Time: 0.309s, 3317.08/s  (0.310s, 3306.03/s)  LR: 3.254e-04  Data: 0.019 (0.025)
Train: 184 [1000/1251 ( 80%)]  Loss: 3.428 (3.33)  Time: 0.310s, 3303.16/s  (0.310s, 3305.96/s)  LR: 3.252e-04  Data: 0.025 (0.025)
Train: 184 [1050/1251 ( 84%)]  Loss: 3.419 (3.34)  Time: 0.312s, 3280.75/s  (0.310s, 3306.07/s)  LR: 3.250e-04  Data: 0.023 (0.025)
Train: 184 [1100/1251 ( 88%)]  Loss: 3.161 (3.33)  Time: 0.308s, 3325.86/s  (0.310s, 3306.28/s)  LR: 3.248e-04  Data: 0.021 (0.025)
Train: 184 [1150/1251 ( 92%)]  Loss: 3.156 (3.32)  Time: 0.315s, 3255.27/s  (0.310s, 3306.17/s)  LR: 3.246e-04  Data: 0.023 (0.025)
Train: 184 [1200/1251 ( 96%)]  Loss: 3.449 (3.33)  Time: 0.309s, 3315.52/s  (0.310s, 3305.97/s)  LR: 3.244e-04  Data: 0.016 (0.025)
Train: 184 [1250/1251 (100%)]  Loss: 3.384 (3.33)  Time: 0.276s, 3705.71/s  (0.310s, 3307.72/s)  LR: 3.242e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.095 (2.095)  Loss:  0.5488 (0.5488)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.053 (0.235)  Loss:  0.6357 (1.0604)  Acc@1: 85.3774 (76.1220)  Acc@5: 97.4057 (93.0860)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-183.pth.tar', 76.47400008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-182.pth.tar', 76.46400003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-181.pth.tar', 76.14800013427734)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-184.pth.tar', 76.12199998291015)

Train: 185 [   0/1251 (  0%)]  Loss: 3.035 (3.03)  Time: 2.478s,  413.23/s  (2.478s,  413.23/s)  LR: 3.242e-04  Data: 2.251 (2.251)
Train: 185 [  50/1251 (  4%)]  Loss: 3.368 (3.20)  Time: 0.302s, 3395.41/s  (0.334s, 3061.97/s)  LR: 3.240e-04  Data: 0.026 (0.066)
Train: 185 [ 100/1251 (  8%)]  Loss: 3.502 (3.30)  Time: 0.308s, 3325.01/s  (0.320s, 3204.05/s)  LR: 3.238e-04  Data: 0.027 (0.045)
Train: 185 [ 150/1251 ( 12%)]  Loss: 3.202 (3.28)  Time: 0.304s, 3364.73/s  (0.315s, 3249.06/s)  LR: 3.236e-04  Data: 0.023 (0.038)
Train: 185 [ 200/1251 ( 16%)]  Loss: 3.262 (3.27)  Time: 0.307s, 3336.95/s  (0.313s, 3268.91/s)  LR: 3.234e-04  Data: 0.021 (0.034)
Train: 185 [ 250/1251 ( 20%)]  Loss: 3.126 (3.25)  Time: 0.307s, 3332.07/s  (0.312s, 3279.65/s)  LR: 3.232e-04  Data: 0.020 (0.032)
Train: 185 [ 300/1251 ( 24%)]  Loss: 3.521 (3.29)  Time: 0.309s, 3313.29/s  (0.312s, 3286.54/s)  LR: 3.230e-04  Data: 0.024 (0.030)
Train: 185 [ 350/1251 ( 28%)]  Loss: 3.530 (3.32)  Time: 0.312s, 3286.28/s  (0.311s, 3290.68/s)  LR: 3.228e-04  Data: 0.022 (0.029)
Train: 185 [ 400/1251 ( 32%)]  Loss: 3.452 (3.33)  Time: 0.312s, 3286.89/s  (0.311s, 3292.69/s)  LR: 3.227e-04  Data: 0.019 (0.029)
Train: 185 [ 450/1251 ( 36%)]  Loss: 3.068 (3.31)  Time: 0.310s, 3305.68/s  (0.311s, 3293.94/s)  LR: 3.225e-04  Data: 0.022 (0.028)
Train: 185 [ 500/1251 ( 40%)]  Loss: 3.435 (3.32)  Time: 0.309s, 3319.16/s  (0.311s, 3296.06/s)  LR: 3.223e-04  Data: 0.022 (0.027)
Train: 185 [ 550/1251 ( 44%)]  Loss: 3.186 (3.31)  Time: 0.310s, 3304.54/s  (0.311s, 3297.17/s)  LR: 3.221e-04  Data: 0.022 (0.027)
Train: 185 [ 600/1251 ( 48%)]  Loss: 3.312 (3.31)  Time: 0.307s, 3337.56/s  (0.310s, 3298.45/s)  LR: 3.219e-04  Data: 0.026 (0.027)
Train: 185 [ 650/1251 ( 52%)]  Loss: 3.549 (3.32)  Time: 0.310s, 3299.62/s  (0.310s, 3299.60/s)  LR: 3.217e-04  Data: 0.024 (0.026)
Train: 185 [ 700/1251 ( 56%)]  Loss: 3.080 (3.31)  Time: 0.308s, 3322.67/s  (0.310s, 3300.39/s)  LR: 3.215e-04  Data: 0.022 (0.026)
Train: 185 [ 750/1251 ( 60%)]  Loss: 3.382 (3.31)  Time: 0.307s, 3333.85/s  (0.310s, 3300.75/s)  LR: 3.213e-04  Data: 0.025 (0.026)
Train: 185 [ 800/1251 ( 64%)]  Loss: 3.291 (3.31)  Time: 0.311s, 3291.34/s  (0.310s, 3301.70/s)  LR: 3.211e-04  Data: 0.025 (0.026)
Train: 185 [ 850/1251 ( 68%)]  Loss: 3.359 (3.31)  Time: 0.316s, 3245.29/s  (0.310s, 3301.93/s)  LR: 3.209e-04  Data: 0.026 (0.026)
Train: 185 [ 900/1251 ( 72%)]  Loss: 3.173 (3.31)  Time: 0.311s, 3290.75/s  (0.310s, 3301.96/s)  LR: 3.207e-04  Data: 0.022 (0.025)
Train: 185 [ 950/1251 ( 76%)]  Loss: 3.088 (3.30)  Time: 0.308s, 3320.98/s  (0.310s, 3301.84/s)  LR: 3.205e-04  Data: 0.024 (0.025)
Train: 185 [1000/1251 ( 80%)]  Loss: 3.465 (3.30)  Time: 0.307s, 3338.69/s  (0.310s, 3302.11/s)  LR: 3.203e-04  Data: 0.018 (0.025)
Train: 185 [1050/1251 ( 84%)]  Loss: 3.321 (3.30)  Time: 0.308s, 3327.11/s  (0.310s, 3302.45/s)  LR: 3.201e-04  Data: 0.020 (0.025)
Train: 185 [1100/1251 ( 88%)]  Loss: 3.464 (3.31)  Time: 0.311s, 3294.51/s  (0.310s, 3302.63/s)  LR: 3.199e-04  Data: 0.025 (0.025)
Train: 185 [1150/1251 ( 92%)]  Loss: 3.081 (3.30)  Time: 0.312s, 3285.13/s  (0.310s, 3302.58/s)  LR: 3.197e-04  Data: 0.022 (0.025)
Train: 185 [1200/1251 ( 96%)]  Loss: 3.249 (3.30)  Time: 0.316s, 3240.94/s  (0.310s, 3302.36/s)  LR: 3.196e-04  Data: 0.028 (0.025)
Train: 185 [1250/1251 (100%)]  Loss: 3.368 (3.30)  Time: 0.276s, 3708.06/s  (0.310s, 3304.50/s)  LR: 3.194e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.160 (2.160)  Loss:  0.5269 (0.5269)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.042 (0.236)  Loss:  0.6665 (1.0469)  Acc@1: 85.4953 (76.4560)  Acc@5: 96.3443 (93.3380)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-183.pth.tar', 76.47400008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-182.pth.tar', 76.46400003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-185.pth.tar', 76.45600003417968)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-181.pth.tar', 76.14800013427734)

Train: 186 [   0/1251 (  0%)]  Loss: 3.129 (3.13)  Time: 2.166s,  472.65/s  (2.166s,  472.65/s)  LR: 3.194e-04  Data: 1.946 (1.946)
Train: 186 [  50/1251 (  4%)]  Loss: 3.330 (3.23)  Time: 0.298s, 3433.99/s  (0.328s, 3118.94/s)  LR: 3.192e-04  Data: 0.022 (0.062)
Train: 186 [ 100/1251 (  8%)]  Loss: 3.640 (3.37)  Time: 0.307s, 3338.41/s  (0.315s, 3247.65/s)  LR: 3.190e-04  Data: 0.019 (0.043)
Train: 186 [ 150/1251 ( 12%)]  Loss: 3.219 (3.33)  Time: 0.304s, 3367.06/s  (0.312s, 3284.39/s)  LR: 3.188e-04  Data: 0.026 (0.036)
Train: 186 [ 200/1251 ( 16%)]  Loss: 3.682 (3.40)  Time: 0.307s, 3336.70/s  (0.310s, 3300.23/s)  LR: 3.186e-04  Data: 0.023 (0.033)
Train: 186 [ 250/1251 ( 20%)]  Loss: 2.940 (3.32)  Time: 0.316s, 3239.46/s  (0.309s, 3309.47/s)  LR: 3.184e-04  Data: 0.026 (0.031)
Train: 186 [ 300/1251 ( 24%)]  Loss: 3.496 (3.35)  Time: 0.312s, 3286.82/s  (0.309s, 3314.12/s)  LR: 3.182e-04  Data: 0.022 (0.030)
Train: 186 [ 350/1251 ( 28%)]  Loss: 3.525 (3.37)  Time: 0.308s, 3320.08/s  (0.309s, 3318.16/s)  LR: 3.180e-04  Data: 0.023 (0.029)
Train: 186 [ 400/1251 ( 32%)]  Loss: 3.300 (3.36)  Time: 0.304s, 3369.21/s  (0.308s, 3321.06/s)  LR: 3.178e-04  Data: 0.026 (0.028)
Train: 186 [ 450/1251 ( 36%)]  Loss: 3.159 (3.34)  Time: 0.303s, 3374.54/s  (0.308s, 3322.65/s)  LR: 3.176e-04  Data: 0.022 (0.027)
Train: 186 [ 500/1251 ( 40%)]  Loss: 3.106 (3.32)  Time: 0.305s, 3353.78/s  (0.308s, 3323.54/s)  LR: 3.174e-04  Data: 0.026 (0.027)
Train: 186 [ 550/1251 ( 44%)]  Loss: 3.205 (3.31)  Time: 0.310s, 3307.04/s  (0.308s, 3324.33/s)  LR: 3.172e-04  Data: 0.022 (0.027)
Train: 186 [ 600/1251 ( 48%)]  Loss: 3.315 (3.31)  Time: 0.308s, 3325.25/s  (0.308s, 3324.69/s)  LR: 3.170e-04  Data: 0.020 (0.026)
Train: 186 [ 650/1251 ( 52%)]  Loss: 2.953 (3.29)  Time: 0.309s, 3314.85/s  (0.308s, 3325.85/s)  LR: 3.168e-04  Data: 0.022 (0.026)
Train: 186 [ 700/1251 ( 56%)]  Loss: 3.156 (3.28)  Time: 0.313s, 3268.78/s  (0.308s, 3326.02/s)  LR: 3.166e-04  Data: 0.022 (0.026)
Train: 186 [ 750/1251 ( 60%)]  Loss: 3.449 (3.29)  Time: 0.308s, 3329.06/s  (0.308s, 3326.91/s)  LR: 3.165e-04  Data: 0.020 (0.026)
Train: 186 [ 800/1251 ( 64%)]  Loss: 3.265 (3.29)  Time: 0.309s, 3308.80/s  (0.308s, 3327.76/s)  LR: 3.163e-04  Data: 0.026 (0.026)
Train: 186 [ 850/1251 ( 68%)]  Loss: 3.155 (3.28)  Time: 0.306s, 3342.36/s  (0.308s, 3328.40/s)  LR: 3.161e-04  Data: 0.024 (0.025)
Train: 186 [ 900/1251 ( 72%)]  Loss: 3.235 (3.28)  Time: 0.310s, 3299.22/s  (0.308s, 3328.79/s)  LR: 3.159e-04  Data: 0.025 (0.025)
Train: 186 [ 950/1251 ( 76%)]  Loss: 3.517 (3.29)  Time: 0.300s, 3411.13/s  (0.308s, 3329.23/s)  LR: 3.157e-04  Data: 0.021 (0.025)
Train: 186 [1000/1251 ( 80%)]  Loss: 3.361 (3.29)  Time: 0.306s, 3350.39/s  (0.308s, 3329.76/s)  LR: 3.155e-04  Data: 0.022 (0.025)
Train: 186 [1050/1251 ( 84%)]  Loss: 3.075 (3.28)  Time: 0.310s, 3301.98/s  (0.307s, 3330.38/s)  LR: 3.153e-04  Data: 0.024 (0.025)
Train: 186 [1100/1251 ( 88%)]  Loss: 3.337 (3.28)  Time: 0.311s, 3296.05/s  (0.307s, 3330.40/s)  LR: 3.151e-04  Data: 0.024 (0.025)
Train: 186 [1150/1251 ( 92%)]  Loss: 3.496 (3.29)  Time: 0.314s, 3258.83/s  (0.308s, 3330.04/s)  LR: 3.149e-04  Data: 0.026 (0.025)
Train: 186 [1200/1251 ( 96%)]  Loss: 3.142 (3.29)  Time: 0.309s, 3311.46/s  (0.308s, 3330.03/s)  LR: 3.147e-04  Data: 0.024 (0.025)
Train: 186 [1250/1251 (100%)]  Loss: 3.327 (3.29)  Time: 0.276s, 3713.88/s  (0.307s, 3332.09/s)  LR: 3.145e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.068 (2.068)  Loss:  0.5288 (0.5288)  Acc@1: 88.8672 (88.8672)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.046 (0.236)  Loss:  0.6206 (1.0311)  Acc@1: 86.4387 (76.5320)  Acc@5: 96.9340 (93.2800)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-186.pth.tar', 76.53200005615234)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-183.pth.tar', 76.47400008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-182.pth.tar', 76.46400003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-185.pth.tar', 76.45600003417968)

Train: 187 [   0/1251 (  0%)]  Loss: 3.283 (3.28)  Time: 2.100s,  487.61/s  (2.100s,  487.61/s)  LR: 3.145e-04  Data: 1.880 (1.880)
Train: 187 [  50/1251 (  4%)]  Loss: 3.649 (3.47)  Time: 0.300s, 3414.43/s  (0.326s, 3145.78/s)  LR: 3.143e-04  Data: 0.021 (0.061)
Train: 187 [ 100/1251 (  8%)]  Loss: 3.413 (3.45)  Time: 0.304s, 3366.37/s  (0.313s, 3271.91/s)  LR: 3.141e-04  Data: 0.025 (0.042)
Train: 187 [ 150/1251 ( 12%)]  Loss: 3.043 (3.35)  Time: 0.302s, 3387.31/s  (0.309s, 3310.13/s)  LR: 3.139e-04  Data: 0.020 (0.036)
Train: 187 [ 200/1251 ( 16%)]  Loss: 3.076 (3.29)  Time: 0.309s, 3318.60/s  (0.308s, 3325.67/s)  LR: 3.137e-04  Data: 0.021 (0.033)
Train: 187 [ 250/1251 ( 20%)]  Loss: 3.341 (3.30)  Time: 0.306s, 3345.52/s  (0.307s, 3332.93/s)  LR: 3.136e-04  Data: 0.025 (0.031)
Train: 187 [ 300/1251 ( 24%)]  Loss: 3.293 (3.30)  Time: 0.313s, 3270.52/s  (0.307s, 3335.64/s)  LR: 3.134e-04  Data: 0.027 (0.030)
Train: 187 [ 350/1251 ( 28%)]  Loss: 3.135 (3.28)  Time: 0.308s, 3324.18/s  (0.307s, 3336.89/s)  LR: 3.132e-04  Data: 0.019 (0.029)
Train: 187 [ 400/1251 ( 32%)]  Loss: 3.388 (3.29)  Time: 0.310s, 3302.05/s  (0.307s, 3337.67/s)  LR: 3.130e-04  Data: 0.025 (0.028)
Train: 187 [ 450/1251 ( 36%)]  Loss: 2.896 (3.25)  Time: 0.306s, 3341.12/s  (0.307s, 3338.89/s)  LR: 3.128e-04  Data: 0.023 (0.028)
Train: 187 [ 500/1251 ( 40%)]  Loss: 3.381 (3.26)  Time: 0.306s, 3347.61/s  (0.307s, 3340.40/s)  LR: 3.126e-04  Data: 0.025 (0.027)
Train: 187 [ 550/1251 ( 44%)]  Loss: 3.305 (3.27)  Time: 0.310s, 3298.75/s  (0.306s, 3341.86/s)  LR: 3.124e-04  Data: 0.026 (0.027)
Train: 187 [ 600/1251 ( 48%)]  Loss: 3.639 (3.30)  Time: 0.313s, 3272.10/s  (0.306s, 3343.13/s)  LR: 3.122e-04  Data: 0.024 (0.026)
Train: 187 [ 650/1251 ( 52%)]  Loss: 3.617 (3.32)  Time: 0.311s, 3288.80/s  (0.306s, 3343.37/s)  LR: 3.120e-04  Data: 0.024 (0.026)
Train: 187 [ 700/1251 ( 56%)]  Loss: 3.500 (3.33)  Time: 0.306s, 3348.21/s  (0.306s, 3343.70/s)  LR: 3.118e-04  Data: 0.023 (0.026)
Train: 187 [ 750/1251 ( 60%)]  Loss: 3.612 (3.35)  Time: 0.312s, 3283.35/s  (0.306s, 3343.99/s)  LR: 3.116e-04  Data: 0.023 (0.026)
Train: 187 [ 800/1251 ( 64%)]  Loss: 3.350 (3.35)  Time: 0.309s, 3310.38/s  (0.306s, 3344.19/s)  LR: 3.114e-04  Data: 0.024 (0.026)
Train: 187 [ 850/1251 ( 68%)]  Loss: 2.880 (3.32)  Time: 0.309s, 3319.13/s  (0.306s, 3344.76/s)  LR: 3.112e-04  Data: 0.023 (0.026)
Train: 187 [ 900/1251 ( 72%)]  Loss: 3.117 (3.31)  Time: 0.308s, 3328.59/s  (0.306s, 3344.79/s)  LR: 3.111e-04  Data: 0.023 (0.025)
Train: 187 [ 950/1251 ( 76%)]  Loss: 3.328 (3.31)  Time: 0.308s, 3321.35/s  (0.306s, 3344.72/s)  LR: 3.109e-04  Data: 0.025 (0.025)
Train: 187 [1000/1251 ( 80%)]  Loss: 3.404 (3.32)  Time: 0.304s, 3367.06/s  (0.306s, 3344.71/s)  LR: 3.107e-04  Data: 0.021 (0.025)
Train: 187 [1050/1251 ( 84%)]  Loss: 3.362 (3.32)  Time: 0.306s, 3346.97/s  (0.306s, 3345.32/s)  LR: 3.105e-04  Data: 0.022 (0.025)
Train: 187 [1100/1251 ( 88%)]  Loss: 3.164 (3.31)  Time: 0.306s, 3346.90/s  (0.306s, 3345.46/s)  LR: 3.103e-04  Data: 0.022 (0.025)
Train: 187 [1150/1251 ( 92%)]  Loss: 3.269 (3.31)  Time: 0.308s, 3327.71/s  (0.306s, 3345.93/s)  LR: 3.101e-04  Data: 0.024 (0.025)
Train: 187 [1200/1251 ( 96%)]  Loss: 3.220 (3.31)  Time: 0.308s, 3322.23/s  (0.306s, 3346.41/s)  LR: 3.099e-04  Data: 0.025 (0.025)
Train: 187 [1250/1251 (100%)]  Loss: 3.296 (3.31)  Time: 0.275s, 3717.04/s  (0.306s, 3348.39/s)  LR: 3.097e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.046 (2.046)  Loss:  0.5669 (0.5669)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.059 (0.234)  Loss:  0.6582 (1.0439)  Acc@1: 85.7311 (76.7340)  Acc@5: 96.5802 (93.3800)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-187.pth.tar', 76.73400000732421)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-186.pth.tar', 76.53200005615234)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-183.pth.tar', 76.47400008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-182.pth.tar', 76.46400003417969)

Train: 188 [   0/1251 (  0%)]  Loss: 3.139 (3.14)  Time: 2.202s,  464.97/s  (2.202s,  464.97/s)  LR: 3.097e-04  Data: 1.961 (1.961)
Train: 188 [  50/1251 (  4%)]  Loss: 3.487 (3.31)  Time: 0.298s, 3440.11/s  (0.332s, 3088.55/s)  LR: 3.095e-04  Data: 0.020 (0.068)
Train: 188 [ 100/1251 (  8%)]  Loss: 3.218 (3.28)  Time: 0.301s, 3404.66/s  (0.315s, 3246.42/s)  LR: 3.093e-04  Data: 0.023 (0.046)
Train: 188 [ 150/1251 ( 12%)]  Loss: 3.311 (3.29)  Time: 0.303s, 3382.85/s  (0.311s, 3294.37/s)  LR: 3.091e-04  Data: 0.025 (0.038)
Train: 188 [ 200/1251 ( 16%)]  Loss: 3.560 (3.34)  Time: 0.302s, 3390.60/s  (0.309s, 3315.27/s)  LR: 3.089e-04  Data: 0.021 (0.035)
Train: 188 [ 250/1251 ( 20%)]  Loss: 3.212 (3.32)  Time: 0.310s, 3306.40/s  (0.308s, 3328.77/s)  LR: 3.087e-04  Data: 0.025 (0.032)
Train: 188 [ 300/1251 ( 24%)]  Loss: 3.182 (3.30)  Time: 0.305s, 3352.15/s  (0.307s, 3336.67/s)  LR: 3.086e-04  Data: 0.023 (0.031)
Train: 188 [ 350/1251 ( 28%)]  Loss: 3.298 (3.30)  Time: 0.306s, 3345.75/s  (0.306s, 3341.20/s)  LR: 3.084e-04  Data: 0.024 (0.030)
Train: 188 [ 400/1251 ( 32%)]  Loss: 3.439 (3.32)  Time: 0.306s, 3343.73/s  (0.306s, 3344.95/s)  LR: 3.082e-04  Data: 0.027 (0.029)
Train: 188 [ 450/1251 ( 36%)]  Loss: 2.978 (3.28)  Time: 0.302s, 3396.29/s  (0.306s, 3347.15/s)  LR: 3.080e-04  Data: 0.022 (0.028)
Train: 188 [ 500/1251 ( 40%)]  Loss: 3.457 (3.30)  Time: 0.303s, 3378.54/s  (0.306s, 3350.07/s)  LR: 3.078e-04  Data: 0.023 (0.028)
Train: 188 [ 550/1251 ( 44%)]  Loss: 3.406 (3.31)  Time: 0.303s, 3374.49/s  (0.305s, 3352.56/s)  LR: 3.076e-04  Data: 0.023 (0.027)
Train: 188 [ 600/1251 ( 48%)]  Loss: 3.463 (3.32)  Time: 0.302s, 3394.31/s  (0.305s, 3354.61/s)  LR: 3.074e-04  Data: 0.024 (0.027)
Train: 188 [ 650/1251 ( 52%)]  Loss: 3.061 (3.30)  Time: 0.303s, 3379.53/s  (0.305s, 3354.75/s)  LR: 3.072e-04  Data: 0.023 (0.027)
Train: 188 [ 700/1251 ( 56%)]  Loss: 3.116 (3.29)  Time: 0.307s, 3335.43/s  (0.305s, 3356.05/s)  LR: 3.070e-04  Data: 0.023 (0.027)
Train: 188 [ 750/1251 ( 60%)]  Loss: 3.128 (3.28)  Time: 0.306s, 3346.39/s  (0.305s, 3357.13/s)  LR: 3.068e-04  Data: 0.025 (0.026)
Train: 188 [ 800/1251 ( 64%)]  Loss: 3.202 (3.27)  Time: 0.302s, 3394.65/s  (0.305s, 3358.43/s)  LR: 3.066e-04  Data: 0.021 (0.026)
Train: 188 [ 850/1251 ( 68%)]  Loss: 3.661 (3.30)  Time: 0.310s, 3304.98/s  (0.305s, 3358.96/s)  LR: 3.064e-04  Data: 0.022 (0.026)
Train: 188 [ 900/1251 ( 72%)]  Loss: 3.322 (3.30)  Time: 0.301s, 3398.99/s  (0.305s, 3359.69/s)  LR: 3.063e-04  Data: 0.022 (0.026)
Train: 188 [ 950/1251 ( 76%)]  Loss: 3.309 (3.30)  Time: 0.305s, 3361.30/s  (0.305s, 3360.21/s)  LR: 3.061e-04  Data: 0.026 (0.026)
Train: 188 [1000/1251 ( 80%)]  Loss: 3.213 (3.29)  Time: 0.306s, 3346.00/s  (0.305s, 3360.54/s)  LR: 3.059e-04  Data: 0.025 (0.026)
Train: 188 [1050/1251 ( 84%)]  Loss: 3.022 (3.28)  Time: 0.304s, 3364.71/s  (0.305s, 3361.37/s)  LR: 3.057e-04  Data: 0.024 (0.025)
Train: 188 [1100/1251 ( 88%)]  Loss: 3.563 (3.29)  Time: 0.302s, 3386.96/s  (0.305s, 3361.60/s)  LR: 3.055e-04  Data: 0.023 (0.025)
Train: 188 [1150/1251 ( 92%)]  Loss: 3.382 (3.30)  Time: 0.304s, 3372.54/s  (0.305s, 3361.55/s)  LR: 3.053e-04  Data: 0.021 (0.025)
Train: 188 [1200/1251 ( 96%)]  Loss: 3.220 (3.29)  Time: 0.304s, 3367.75/s  (0.305s, 3361.97/s)  LR: 3.051e-04  Data: 0.024 (0.025)
Train: 188 [1250/1251 (100%)]  Loss: 3.319 (3.29)  Time: 0.276s, 3715.94/s  (0.304s, 3363.90/s)  LR: 3.049e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.050 (2.050)  Loss:  0.5361 (0.5361)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.049 (0.232)  Loss:  0.5972 (1.0389)  Acc@1: 86.2028 (76.5860)  Acc@5: 97.1698 (93.1800)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-187.pth.tar', 76.73400000732421)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-188.pth.tar', 76.58600008300782)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-186.pth.tar', 76.53200005615234)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-183.pth.tar', 76.47400008544922)

Train: 189 [   0/1251 (  0%)]  Loss: 3.094 (3.09)  Time: 2.141s,  478.39/s  (2.141s,  478.39/s)  LR: 3.049e-04  Data: 1.891 (1.891)
Train: 189 [  50/1251 (  4%)]  Loss: 2.922 (3.01)  Time: 0.294s, 3484.17/s  (0.327s, 3130.49/s)  LR: 3.047e-04  Data: 0.024 (0.063)
Train: 189 [ 100/1251 (  8%)]  Loss: 3.284 (3.10)  Time: 0.301s, 3402.75/s  (0.313s, 3275.51/s)  LR: 3.045e-04  Data: 0.021 (0.044)
Train: 189 [ 150/1251 ( 12%)]  Loss: 3.207 (3.13)  Time: 0.301s, 3406.43/s  (0.309s, 3319.20/s)  LR: 3.043e-04  Data: 0.023 (0.037)
Train: 189 [ 200/1251 ( 16%)]  Loss: 3.439 (3.19)  Time: 0.299s, 3421.62/s  (0.307s, 3340.74/s)  LR: 3.042e-04  Data: 0.029 (0.034)
Train: 189 [ 250/1251 ( 20%)]  Loss: 3.011 (3.16)  Time: 0.302s, 3386.35/s  (0.305s, 3353.76/s)  LR: 3.040e-04  Data: 0.025 (0.032)
Train: 189 [ 300/1251 ( 24%)]  Loss: 3.308 (3.18)  Time: 0.300s, 3409.95/s  (0.305s, 3362.18/s)  LR: 3.038e-04  Data: 0.023 (0.030)
Train: 189 [ 350/1251 ( 28%)]  Loss: 3.379 (3.21)  Time: 0.306s, 3343.15/s  (0.304s, 3367.50/s)  LR: 3.036e-04  Data: 0.022 (0.029)
Train: 189 [ 400/1251 ( 32%)]  Loss: 3.327 (3.22)  Time: 0.304s, 3366.17/s  (0.304s, 3371.78/s)  LR: 3.034e-04  Data: 0.023 (0.028)
Train: 189 [ 450/1251 ( 36%)]  Loss: 3.404 (3.24)  Time: 0.304s, 3370.86/s  (0.303s, 3374.68/s)  LR: 3.032e-04  Data: 0.023 (0.028)
Train: 189 [ 500/1251 ( 40%)]  Loss: 3.380 (3.25)  Time: 0.304s, 3365.56/s  (0.303s, 3375.83/s)  LR: 3.030e-04  Data: 0.026 (0.028)
Train: 189 [ 550/1251 ( 44%)]  Loss: 3.014 (3.23)  Time: 0.304s, 3364.18/s  (0.303s, 3376.98/s)  LR: 3.028e-04  Data: 0.022 (0.027)
Train: 189 [ 600/1251 ( 48%)]  Loss: 3.123 (3.22)  Time: 0.299s, 3424.74/s  (0.303s, 3378.71/s)  LR: 3.026e-04  Data: 0.023 (0.027)
Train: 189 [ 650/1251 ( 52%)]  Loss: 3.373 (3.23)  Time: 0.300s, 3418.81/s  (0.303s, 3381.01/s)  LR: 3.024e-04  Data: 0.021 (0.027)
Train: 189 [ 700/1251 ( 56%)]  Loss: 3.579 (3.26)  Time: 0.305s, 3354.92/s  (0.303s, 3382.07/s)  LR: 3.022e-04  Data: 0.027 (0.026)
Train: 189 [ 750/1251 ( 60%)]  Loss: 3.072 (3.24)  Time: 0.301s, 3396.45/s  (0.303s, 3383.26/s)  LR: 3.021e-04  Data: 0.024 (0.026)
Train: 189 [ 800/1251 ( 64%)]  Loss: 3.611 (3.27)  Time: 0.306s, 3349.25/s  (0.303s, 3384.37/s)  LR: 3.019e-04  Data: 0.027 (0.026)
Train: 189 [ 850/1251 ( 68%)]  Loss: 3.212 (3.26)  Time: 0.302s, 3391.79/s  (0.303s, 3384.82/s)  LR: 3.017e-04  Data: 0.024 (0.026)
Train: 189 [ 900/1251 ( 72%)]  Loss: 3.394 (3.27)  Time: 0.298s, 3431.64/s  (0.302s, 3385.37/s)  LR: 3.015e-04  Data: 0.022 (0.026)
Train: 189 [ 950/1251 ( 76%)]  Loss: 2.953 (3.25)  Time: 0.306s, 3348.65/s  (0.302s, 3385.81/s)  LR: 3.013e-04  Data: 0.026 (0.026)
Train: 189 [1000/1251 ( 80%)]  Loss: 3.124 (3.25)  Time: 0.300s, 3409.98/s  (0.302s, 3386.05/s)  LR: 3.011e-04  Data: 0.024 (0.025)
Train: 189 [1050/1251 ( 84%)]  Loss: 3.249 (3.25)  Time: 0.302s, 3393.96/s  (0.302s, 3386.39/s)  LR: 3.009e-04  Data: 0.026 (0.025)
Train: 189 [1100/1251 ( 88%)]  Loss: 3.516 (3.26)  Time: 0.306s, 3349.46/s  (0.302s, 3386.77/s)  LR: 3.007e-04  Data: 0.023 (0.025)
Train: 189 [1150/1251 ( 92%)]  Loss: 3.176 (3.26)  Time: 0.308s, 3329.66/s  (0.302s, 3387.14/s)  LR: 3.005e-04  Data: 0.025 (0.025)
Train: 189 [1200/1251 ( 96%)]  Loss: 3.504 (3.27)  Time: 0.298s, 3432.88/s  (0.302s, 3387.68/s)  LR: 3.003e-04  Data: 0.020 (0.025)
Train: 189 [1250/1251 (100%)]  Loss: 3.323 (3.27)  Time: 0.274s, 3740.39/s  (0.302s, 3390.03/s)  LR: 3.001e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.023 (2.023)  Loss:  0.5386 (0.5386)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.041 (0.236)  Loss:  0.6167 (1.0485)  Acc@1: 87.1462 (76.5140)  Acc@5: 96.8160 (93.2540)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-187.pth.tar', 76.73400000732421)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-188.pth.tar', 76.58600008300782)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-186.pth.tar', 76.53200005615234)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-189.pth.tar', 76.51400010498047)

Train: 190 [   0/1251 (  0%)]  Loss: 3.292 (3.29)  Time: 2.467s,  415.15/s  (2.467s,  415.15/s)  LR: 3.001e-04  Data: 2.245 (2.245)
Train: 190 [  50/1251 (  4%)]  Loss: 3.285 (3.29)  Time: 0.295s, 3469.91/s  (0.323s, 3166.20/s)  LR: 3.000e-04  Data: 0.024 (0.068)
Train: 190 [ 100/1251 (  8%)]  Loss: 3.314 (3.30)  Time: 0.296s, 3453.95/s  (0.309s, 3314.01/s)  LR: 2.998e-04  Data: 0.027 (0.046)
Train: 190 [ 150/1251 ( 12%)]  Loss: 3.207 (3.27)  Time: 0.302s, 3388.41/s  (0.305s, 3355.57/s)  LR: 2.996e-04  Data: 0.027 (0.039)
Train: 190 [ 200/1251 ( 16%)]  Loss: 3.405 (3.30)  Time: 0.300s, 3410.68/s  (0.303s, 3377.20/s)  LR: 2.994e-04  Data: 0.029 (0.035)
Train: 190 [ 250/1251 ( 20%)]  Loss: 3.245 (3.29)  Time: 0.302s, 3396.24/s  (0.302s, 3387.17/s)  LR: 2.992e-04  Data: 0.024 (0.033)
Train: 190 [ 300/1251 ( 24%)]  Loss: 3.365 (3.30)  Time: 0.297s, 3443.97/s  (0.302s, 3391.40/s)  LR: 2.990e-04  Data: 0.022 (0.031)
Train: 190 [ 350/1251 ( 28%)]  Loss: 3.228 (3.29)  Time: 0.298s, 3440.28/s  (0.302s, 3395.73/s)  LR: 2.988e-04  Data: 0.026 (0.030)
Train: 190 [ 400/1251 ( 32%)]  Loss: 3.421 (3.31)  Time: 0.299s, 3428.21/s  (0.301s, 3398.20/s)  LR: 2.986e-04  Data: 0.026 (0.029)
Train: 190 [ 450/1251 ( 36%)]  Loss: 3.117 (3.29)  Time: 0.303s, 3378.38/s  (0.301s, 3399.96/s)  LR: 2.984e-04  Data: 0.021 (0.029)
Train: 190 [ 500/1251 ( 40%)]  Loss: 3.460 (3.30)  Time: 0.290s, 3535.85/s  (0.301s, 3402.43/s)  LR: 2.982e-04  Data: 0.028 (0.028)
Train: 190 [ 550/1251 ( 44%)]  Loss: 3.197 (3.29)  Time: 0.295s, 3475.67/s  (0.301s, 3403.73/s)  LR: 2.981e-04  Data: 0.022 (0.028)
Train: 190 [ 600/1251 ( 48%)]  Loss: 3.345 (3.30)  Time: 0.295s, 3465.51/s  (0.301s, 3405.82/s)  LR: 2.979e-04  Data: 0.027 (0.027)
Train: 190 [ 650/1251 ( 52%)]  Loss: 3.155 (3.29)  Time: 0.295s, 3471.17/s  (0.300s, 3407.71/s)  LR: 2.977e-04  Data: 0.022 (0.027)
Train: 190 [ 700/1251 ( 56%)]  Loss: 3.117 (3.28)  Time: 0.299s, 3427.48/s  (0.300s, 3409.02/s)  LR: 2.975e-04  Data: 0.030 (0.027)
Train: 190 [ 750/1251 ( 60%)]  Loss: 3.166 (3.27)  Time: 0.296s, 3458.72/s  (0.300s, 3410.15/s)  LR: 2.973e-04  Data: 0.024 (0.027)
Train: 190 [ 800/1251 ( 64%)]  Loss: 3.171 (3.26)  Time: 0.300s, 3414.05/s  (0.300s, 3410.93/s)  LR: 2.971e-04  Data: 0.023 (0.026)
Train: 190 [ 850/1251 ( 68%)]  Loss: 3.376 (3.27)  Time: 0.298s, 3433.79/s  (0.300s, 3411.64/s)  LR: 2.969e-04  Data: 0.021 (0.026)
Train: 190 [ 900/1251 ( 72%)]  Loss: 3.365 (3.28)  Time: 0.302s, 3390.98/s  (0.300s, 3412.24/s)  LR: 2.967e-04  Data: 0.028 (0.026)
Train: 190 [ 950/1251 ( 76%)]  Loss: 3.387 (3.28)  Time: 0.299s, 3421.40/s  (0.300s, 3412.19/s)  LR: 2.965e-04  Data: 0.021 (0.026)
Train: 190 [1000/1251 ( 80%)]  Loss: 3.466 (3.29)  Time: 0.297s, 3451.21/s  (0.300s, 3412.55/s)  LR: 2.963e-04  Data: 0.021 (0.026)
Train: 190 [1050/1251 ( 84%)]  Loss: 3.288 (3.29)  Time: 0.300s, 3409.60/s  (0.300s, 3413.05/s)  LR: 2.962e-04  Data: 0.023 (0.026)
Train: 190 [1100/1251 ( 88%)]  Loss: 3.515 (3.30)  Time: 0.301s, 3398.10/s  (0.300s, 3412.91/s)  LR: 2.960e-04  Data: 0.023 (0.026)
Train: 190 [1150/1251 ( 92%)]  Loss: 3.291 (3.30)  Time: 0.301s, 3396.41/s  (0.300s, 3413.10/s)  LR: 2.958e-04  Data: 0.020 (0.025)
Train: 190 [1200/1251 ( 96%)]  Loss: 3.126 (3.29)  Time: 0.299s, 3424.66/s  (0.300s, 3414.10/s)  LR: 2.956e-04  Data: 0.027 (0.025)
Train: 190 [1250/1251 (100%)]  Loss: 2.997 (3.28)  Time: 0.276s, 3712.32/s  (0.300s, 3416.75/s)  LR: 2.954e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.281 (2.281)  Loss:  0.5068 (0.5068)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.053 (0.236)  Loss:  0.6348 (1.0259)  Acc@1: 85.1415 (76.7480)  Acc@5: 96.8160 (93.3500)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-190.pth.tar', 76.74800000976562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-187.pth.tar', 76.73400000732421)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-188.pth.tar', 76.58600008300782)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-186.pth.tar', 76.53200005615234)

Train: 191 [   0/1251 (  0%)]  Loss: 2.864 (2.86)  Time: 2.526s,  405.40/s  (2.526s,  405.40/s)  LR: 2.954e-04  Data: 2.304 (2.304)
Train: 191 [  50/1251 (  4%)]  Loss: 3.031 (2.95)  Time: 0.283s, 3617.29/s  (0.330s, 3100.14/s)  LR: 2.952e-04  Data: 0.026 (0.079)
Train: 191 [ 100/1251 (  8%)]  Loss: 3.431 (3.11)  Time: 0.290s, 3528.81/s  (0.311s, 3297.77/s)  LR: 2.950e-04  Data: 0.022 (0.052)
Train: 191 [ 150/1251 ( 12%)]  Loss: 3.257 (3.15)  Time: 0.293s, 3491.77/s  (0.304s, 3363.23/s)  LR: 2.948e-04  Data: 0.026 (0.042)
Train: 191 [ 200/1251 ( 16%)]  Loss: 2.947 (3.11)  Time: 0.291s, 3520.86/s  (0.302s, 3389.31/s)  LR: 2.946e-04  Data: 0.025 (0.038)
Train: 191 [ 250/1251 ( 20%)]  Loss: 3.148 (3.11)  Time: 0.300s, 3416.17/s  (0.301s, 3404.98/s)  LR: 2.945e-04  Data: 0.021 (0.035)
Train: 191 [ 300/1251 ( 24%)]  Loss: 3.385 (3.15)  Time: 0.293s, 3489.62/s  (0.300s, 3415.20/s)  LR: 2.943e-04  Data: 0.021 (0.033)
Train: 191 [ 350/1251 ( 28%)]  Loss: 3.473 (3.19)  Time: 0.293s, 3496.89/s  (0.299s, 3421.01/s)  LR: 2.941e-04  Data: 0.025 (0.032)
Train: 191 [ 400/1251 ( 32%)]  Loss: 3.431 (3.22)  Time: 0.295s, 3466.83/s  (0.299s, 3425.55/s)  LR: 2.939e-04  Data: 0.025 (0.031)
Train: 191 [ 450/1251 ( 36%)]  Loss: 3.252 (3.22)  Time: 0.292s, 3510.16/s  (0.299s, 3428.92/s)  LR: 2.937e-04  Data: 0.024 (0.030)
Train: 191 [ 500/1251 ( 40%)]  Loss: 3.195 (3.22)  Time: 0.299s, 3429.85/s  (0.298s, 3431.49/s)  LR: 2.935e-04  Data: 0.027 (0.029)
Train: 191 [ 550/1251 ( 44%)]  Loss: 2.956 (3.20)  Time: 0.303s, 3376.46/s  (0.298s, 3431.38/s)  LR: 2.933e-04  Data: 0.025 (0.029)
Train: 191 [ 600/1251 ( 48%)]  Loss: 3.295 (3.20)  Time: 0.299s, 3425.50/s  (0.298s, 3432.73/s)  LR: 2.931e-04  Data: 0.021 (0.028)
Train: 191 [ 650/1251 ( 52%)]  Loss: 3.244 (3.21)  Time: 0.294s, 3486.34/s  (0.298s, 3433.60/s)  LR: 2.929e-04  Data: 0.022 (0.028)
Train: 191 [ 700/1251 ( 56%)]  Loss: 3.020 (3.20)  Time: 0.298s, 3440.60/s  (0.298s, 3434.43/s)  LR: 2.927e-04  Data: 0.024 (0.028)
Train: 191 [ 750/1251 ( 60%)]  Loss: 3.102 (3.19)  Time: 0.300s, 3417.14/s  (0.298s, 3433.90/s)  LR: 2.926e-04  Data: 0.023 (0.027)
Train: 191 [ 800/1251 ( 64%)]  Loss: 3.319 (3.20)  Time: 0.293s, 3496.93/s  (0.298s, 3433.54/s)  LR: 2.924e-04  Data: 0.023 (0.027)
Train: 191 [ 850/1251 ( 68%)]  Loss: 3.577 (3.22)  Time: 0.294s, 3481.97/s  (0.298s, 3434.25/s)  LR: 2.922e-04  Data: 0.025 (0.027)
Train: 191 [ 900/1251 ( 72%)]  Loss: 3.418 (3.23)  Time: 0.301s, 3406.16/s  (0.298s, 3433.46/s)  LR: 2.920e-04  Data: 0.022 (0.027)
Train: 191 [ 950/1251 ( 76%)]  Loss: 3.130 (3.22)  Time: 0.303s, 3376.87/s  (0.298s, 3433.35/s)  LR: 2.918e-04  Data: 0.025 (0.026)
Train: 191 [1000/1251 ( 80%)]  Loss: 3.100 (3.22)  Time: 0.293s, 3489.63/s  (0.298s, 3432.91/s)  LR: 2.916e-04  Data: 0.022 (0.026)
Train: 191 [1050/1251 ( 84%)]  Loss: 3.191 (3.22)  Time: 0.301s, 3400.62/s  (0.298s, 3433.06/s)  LR: 2.914e-04  Data: 0.021 (0.026)
Train: 191 [1100/1251 ( 88%)]  Loss: 3.107 (3.21)  Time: 0.294s, 3481.55/s  (0.298s, 3433.49/s)  LR: 2.912e-04  Data: 0.023 (0.026)
Train: 191 [1150/1251 ( 92%)]  Loss: 3.566 (3.23)  Time: 0.301s, 3399.42/s  (0.298s, 3433.76/s)  LR: 2.911e-04  Data: 0.023 (0.026)
Train: 191 [1200/1251 ( 96%)]  Loss: 3.334 (3.23)  Time: 0.298s, 3434.98/s  (0.298s, 3433.74/s)  LR: 2.909e-04  Data: 0.025 (0.026)
Train: 191 [1250/1251 (100%)]  Loss: 3.029 (3.22)  Time: 0.274s, 3731.76/s  (0.298s, 3436.17/s)  LR: 2.907e-04  Data: 0.000 (0.026)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.031 (2.031)  Loss:  0.5210 (0.5210)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.062 (0.239)  Loss:  0.6489 (1.0371)  Acc@1: 84.5519 (76.4280)  Acc@5: 96.9340 (93.3080)
Train: 192 [   0/1251 (  0%)]  Loss: 3.479 (3.48)  Time: 2.429s,  421.60/s  (2.429s,  421.60/s)  LR: 2.907e-04  Data: 2.180 (2.180)
Train: 192 [  50/1251 (  4%)]  Loss: 3.131 (3.30)  Time: 0.291s, 3520.44/s  (0.327s, 3131.81/s)  LR: 2.905e-04  Data: 0.022 (0.067)
Train: 192 [ 100/1251 (  8%)]  Loss: 3.355 (3.32)  Time: 0.295s, 3468.95/s  (0.310s, 3302.80/s)  LR: 2.903e-04  Data: 0.025 (0.046)
Train: 192 [ 150/1251 ( 12%)]  Loss: 3.367 (3.33)  Time: 0.290s, 3534.39/s  (0.305s, 3361.40/s)  LR: 2.901e-04  Data: 0.019 (0.038)
Train: 192 [ 200/1251 ( 16%)]  Loss: 3.245 (3.32)  Time: 0.298s, 3440.38/s  (0.303s, 3384.77/s)  LR: 2.899e-04  Data: 0.022 (0.035)
Train: 192 [ 250/1251 ( 20%)]  Loss: 3.232 (3.30)  Time: 0.293s, 3492.60/s  (0.301s, 3400.29/s)  LR: 2.897e-04  Data: 0.024 (0.032)
Train: 192 [ 300/1251 ( 24%)]  Loss: 3.520 (3.33)  Time: 0.293s, 3496.08/s  (0.300s, 3411.82/s)  LR: 2.895e-04  Data: 0.021 (0.031)
Train: 192 [ 350/1251 ( 28%)]  Loss: 3.593 (3.37)  Time: 0.295s, 3470.32/s  (0.299s, 3419.77/s)  LR: 2.894e-04  Data: 0.025 (0.030)
Train: 192 [ 400/1251 ( 32%)]  Loss: 2.799 (3.30)  Time: 0.296s, 3458.81/s  (0.299s, 3424.00/s)  LR: 2.892e-04  Data: 0.026 (0.029)
Train: 192 [ 450/1251 ( 36%)]  Loss: 3.489 (3.32)  Time: 0.296s, 3455.96/s  (0.299s, 3427.86/s)  LR: 2.890e-04  Data: 0.021 (0.028)
Train: 192 [ 500/1251 ( 40%)]  Loss: 3.422 (3.33)  Time: 0.299s, 3426.96/s  (0.298s, 3430.82/s)  LR: 2.888e-04  Data: 0.024 (0.028)
Train: 192 [ 550/1251 ( 44%)]  Loss: 3.336 (3.33)  Time: 0.298s, 3433.79/s  (0.298s, 3432.47/s)  LR: 2.886e-04  Data: 0.021 (0.028)
Train: 192 [ 600/1251 ( 48%)]  Loss: 3.474 (3.34)  Time: 0.301s, 3404.32/s  (0.298s, 3434.19/s)  LR: 2.884e-04  Data: 0.024 (0.027)
Train: 192 [ 650/1251 ( 52%)]  Loss: 2.951 (3.31)  Time: 0.303s, 3381.77/s  (0.298s, 3434.94/s)  LR: 2.882e-04  Data: 0.023 (0.027)
Train: 192 [ 700/1251 ( 56%)]  Loss: 3.456 (3.32)  Time: 0.303s, 3381.85/s  (0.298s, 3435.30/s)  LR: 2.880e-04  Data: 0.026 (0.027)
Train: 192 [ 750/1251 ( 60%)]  Loss: 3.476 (3.33)  Time: 0.302s, 3394.01/s  (0.298s, 3435.35/s)  LR: 2.878e-04  Data: 0.025 (0.026)
Train: 192 [ 800/1251 ( 64%)]  Loss: 3.414 (3.34)  Time: 0.297s, 3448.52/s  (0.298s, 3436.12/s)  LR: 2.877e-04  Data: 0.025 (0.026)
Train: 192 [ 850/1251 ( 68%)]  Loss: 3.490 (3.35)  Time: 0.298s, 3439.16/s  (0.298s, 3436.28/s)  LR: 2.875e-04  Data: 0.021 (0.026)
Train: 192 [ 900/1251 ( 72%)]  Loss: 3.280 (3.34)  Time: 0.300s, 3413.72/s  (0.298s, 3436.21/s)  LR: 2.873e-04  Data: 0.024 (0.026)
Train: 192 [ 950/1251 ( 76%)]  Loss: 3.588 (3.35)  Time: 0.292s, 3509.48/s  (0.298s, 3436.84/s)  LR: 2.871e-04  Data: 0.023 (0.026)
Train: 192 [1000/1251 ( 80%)]  Loss: 3.260 (3.35)  Time: 0.294s, 3478.66/s  (0.298s, 3437.06/s)  LR: 2.869e-04  Data: 0.022 (0.026)
Train: 192 [1050/1251 ( 84%)]  Loss: 3.272 (3.35)  Time: 0.299s, 3423.32/s  (0.298s, 3437.52/s)  LR: 2.867e-04  Data: 0.024 (0.026)
Train: 192 [1100/1251 ( 88%)]  Loss: 3.324 (3.35)  Time: 0.302s, 3385.61/s  (0.298s, 3437.84/s)  LR: 2.865e-04  Data: 0.022 (0.026)
Train: 192 [1150/1251 ( 92%)]  Loss: 3.607 (3.36)  Time: 0.299s, 3421.06/s  (0.298s, 3438.55/s)  LR: 2.863e-04  Data: 0.022 (0.025)
Train: 192 [1200/1251 ( 96%)]  Loss: 3.179 (3.35)  Time: 0.298s, 3437.26/s  (0.298s, 3438.87/s)  LR: 2.862e-04  Data: 0.025 (0.025)
Train: 192 [1250/1251 (100%)]  Loss: 3.102 (3.34)  Time: 0.274s, 3740.39/s  (0.298s, 3441.25/s)  LR: 2.860e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.014 (2.014)  Loss:  0.5737 (0.5737)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.050 (0.236)  Loss:  0.7080 (1.0476)  Acc@1: 84.6698 (76.8080)  Acc@5: 96.3443 (93.5380)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-192.pth.tar', 76.80799993408203)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-190.pth.tar', 76.74800000976562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-187.pth.tar', 76.73400000732421)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-188.pth.tar', 76.58600008300782)

Train: 193 [   0/1251 (  0%)]  Loss: 3.464 (3.46)  Time: 2.280s,  449.12/s  (2.280s,  449.12/s)  LR: 2.860e-04  Data: 2.055 (2.055)
Train: 193 [  50/1251 (  4%)]  Loss: 3.345 (3.40)  Time: 0.288s, 3550.94/s  (0.320s, 3201.51/s)  LR: 2.858e-04  Data: 0.021 (0.067)
Train: 193 [ 100/1251 (  8%)]  Loss: 3.242 (3.35)  Time: 0.288s, 3558.64/s  (0.305s, 3354.96/s)  LR: 2.856e-04  Data: 0.023 (0.045)
Train: 193 [ 150/1251 ( 12%)]  Loss: 3.012 (3.27)  Time: 0.297s, 3445.65/s  (0.301s, 3398.00/s)  LR: 2.854e-04  Data: 0.025 (0.038)
Train: 193 [ 200/1251 ( 16%)]  Loss: 3.240 (3.26)  Time: 0.287s, 3562.85/s  (0.300s, 3418.35/s)  LR: 2.852e-04  Data: 0.021 (0.035)
Train: 193 [ 250/1251 ( 20%)]  Loss: 3.471 (3.30)  Time: 0.292s, 3506.53/s  (0.298s, 3430.58/s)  LR: 2.850e-04  Data: 0.022 (0.032)
Train: 193 [ 300/1251 ( 24%)]  Loss: 3.126 (3.27)  Time: 0.289s, 3538.06/s  (0.298s, 3438.79/s)  LR: 2.848e-04  Data: 0.022 (0.031)
Train: 193 [ 350/1251 ( 28%)]  Loss: 3.396 (3.29)  Time: 0.291s, 3520.53/s  (0.297s, 3445.15/s)  LR: 2.847e-04  Data: 0.024 (0.030)
Train: 193 [ 400/1251 ( 32%)]  Loss: 2.982 (3.25)  Time: 0.296s, 3462.82/s  (0.297s, 3448.90/s)  LR: 2.845e-04  Data: 0.024 (0.029)
Train: 193 [ 450/1251 ( 36%)]  Loss: 3.276 (3.26)  Time: 0.293s, 3492.43/s  (0.297s, 3450.99/s)  LR: 2.843e-04  Data: 0.025 (0.029)
Train: 193 [ 500/1251 ( 40%)]  Loss: 2.964 (3.23)  Time: 0.296s, 3462.64/s  (0.297s, 3453.59/s)  LR: 2.841e-04  Data: 0.022 (0.028)
Train: 193 [ 550/1251 ( 44%)]  Loss: 3.522 (3.25)  Time: 0.289s, 3543.40/s  (0.296s, 3455.30/s)  LR: 2.839e-04  Data: 0.021 (0.028)
Train: 193 [ 600/1251 ( 48%)]  Loss: 3.042 (3.24)  Time: 0.286s, 3579.46/s  (0.296s, 3457.95/s)  LR: 2.837e-04  Data: 0.021 (0.027)
Train: 193 [ 650/1251 ( 52%)]  Loss: 3.215 (3.24)  Time: 0.299s, 3422.59/s  (0.296s, 3458.51/s)  LR: 2.835e-04  Data: 0.024 (0.027)
Train: 193 [ 700/1251 ( 56%)]  Loss: 3.093 (3.23)  Time: 0.297s, 3445.02/s  (0.296s, 3460.76/s)  LR: 2.833e-04  Data: 0.026 (0.027)
Train: 193 [ 750/1251 ( 60%)]  Loss: 3.336 (3.23)  Time: 0.299s, 3419.10/s  (0.296s, 3461.79/s)  LR: 2.832e-04  Data: 0.025 (0.027)
Train: 193 [ 800/1251 ( 64%)]  Loss: 3.194 (3.23)  Time: 0.295s, 3474.41/s  (0.296s, 3461.50/s)  LR: 2.830e-04  Data: 0.024 (0.026)
Train: 193 [ 850/1251 ( 68%)]  Loss: 3.269 (3.23)  Time: 0.303s, 3379.05/s  (0.296s, 3460.25/s)  LR: 2.828e-04  Data: 0.021 (0.026)
Train: 193 [ 900/1251 ( 72%)]  Loss: 3.438 (3.24)  Time: 0.303s, 3380.98/s  (0.296s, 3459.61/s)  LR: 2.826e-04  Data: 0.021 (0.026)
Train: 193 [ 950/1251 ( 76%)]  Loss: 3.301 (3.25)  Time: 0.298s, 3430.84/s  (0.296s, 3459.28/s)  LR: 2.824e-04  Data: 0.021 (0.026)
Train: 193 [1000/1251 ( 80%)]  Loss: 3.130 (3.24)  Time: 0.293s, 3490.11/s  (0.296s, 3458.55/s)  LR: 2.822e-04  Data: 0.024 (0.026)
Train: 193 [1050/1251 ( 84%)]  Loss: 3.058 (3.23)  Time: 0.299s, 3424.99/s  (0.296s, 3457.72/s)  LR: 2.820e-04  Data: 0.024 (0.026)
Train: 193 [1100/1251 ( 88%)]  Loss: 3.249 (3.23)  Time: 0.299s, 3419.34/s  (0.296s, 3456.60/s)  LR: 2.819e-04  Data: 0.025 (0.026)
Train: 193 [1150/1251 ( 92%)]  Loss: 3.155 (3.23)  Time: 0.299s, 3429.34/s  (0.296s, 3455.56/s)  LR: 2.817e-04  Data: 0.022 (0.025)
Train: 193 [1200/1251 ( 96%)]  Loss: 3.035 (3.22)  Time: 0.295s, 3467.51/s  (0.296s, 3454.54/s)  LR: 2.815e-04  Data: 0.028 (0.025)
Train: 193 [1250/1251 (100%)]  Loss: 3.324 (3.23)  Time: 0.276s, 3711.17/s  (0.296s, 3455.40/s)  LR: 2.813e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.021 (2.021)  Loss:  0.5049 (0.5049)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.047 (0.232)  Loss:  0.6216 (1.0298)  Acc@1: 85.1415 (76.5300)  Acc@5: 97.0519 (93.3720)
Train: 194 [   0/1251 (  0%)]  Loss: 3.420 (3.42)  Time: 2.125s,  481.97/s  (2.125s,  481.97/s)  LR: 2.813e-04  Data: 1.891 (1.891)
Train: 194 [  50/1251 (  4%)]  Loss: 3.132 (3.28)  Time: 0.291s, 3519.98/s  (0.318s, 3222.29/s)  LR: 2.811e-04  Data: 0.021 (0.061)
Train: 194 [ 100/1251 (  8%)]  Loss: 2.818 (3.12)  Time: 0.294s, 3487.40/s  (0.305s, 3360.09/s)  LR: 2.809e-04  Data: 0.022 (0.042)
Train: 194 [ 150/1251 ( 12%)]  Loss: 2.844 (3.05)  Time: 0.295s, 3475.64/s  (0.301s, 3402.47/s)  LR: 2.807e-04  Data: 0.022 (0.036)
Train: 194 [ 200/1251 ( 16%)]  Loss: 3.450 (3.13)  Time: 0.286s, 3574.86/s  (0.299s, 3421.14/s)  LR: 2.805e-04  Data: 0.023 (0.033)
Train: 194 [ 250/1251 ( 20%)]  Loss: 3.414 (3.18)  Time: 0.296s, 3464.40/s  (0.298s, 3431.06/s)  LR: 2.804e-04  Data: 0.027 (0.031)
Train: 194 [ 300/1251 ( 24%)]  Loss: 3.033 (3.16)  Time: 0.293s, 3489.23/s  (0.298s, 3437.11/s)  LR: 2.802e-04  Data: 0.022 (0.030)
Train: 194 [ 350/1251 ( 28%)]  Loss: 3.378 (3.19)  Time: 0.303s, 3375.32/s  (0.298s, 3440.06/s)  LR: 2.800e-04  Data: 0.025 (0.029)
Train: 194 [ 400/1251 ( 32%)]  Loss: 3.156 (3.18)  Time: 0.293s, 3493.00/s  (0.298s, 3440.57/s)  LR: 2.798e-04  Data: 0.024 (0.028)
Train: 194 [ 450/1251 ( 36%)]  Loss: 3.409 (3.21)  Time: 0.291s, 3518.59/s  (0.297s, 3442.84/s)  LR: 2.796e-04  Data: 0.026 (0.028)
Train: 194 [ 500/1251 ( 40%)]  Loss: 3.455 (3.23)  Time: 0.300s, 3414.43/s  (0.297s, 3444.31/s)  LR: 2.794e-04  Data: 0.022 (0.027)
Train: 194 [ 550/1251 ( 44%)]  Loss: 3.259 (3.23)  Time: 0.302s, 3386.07/s  (0.297s, 3445.23/s)  LR: 2.792e-04  Data: 0.021 (0.027)
Train: 194 [ 600/1251 ( 48%)]  Loss: 3.595 (3.26)  Time: 0.295s, 3465.70/s  (0.297s, 3445.34/s)  LR: 2.791e-04  Data: 0.022 (0.026)
Train: 194 [ 650/1251 ( 52%)]  Loss: 3.318 (3.26)  Time: 0.295s, 3472.68/s  (0.297s, 3446.08/s)  LR: 2.789e-04  Data: 0.020 (0.026)
Train: 194 [ 700/1251 ( 56%)]  Loss: 3.513 (3.28)  Time: 0.295s, 3474.25/s  (0.297s, 3446.46/s)  LR: 2.787e-04  Data: 0.023 (0.026)
Train: 194 [ 750/1251 ( 60%)]  Loss: 3.402 (3.29)  Time: 0.301s, 3405.52/s  (0.297s, 3447.28/s)  LR: 2.785e-04  Data: 0.022 (0.026)
Train: 194 [ 800/1251 ( 64%)]  Loss: 3.391 (3.29)  Time: 0.294s, 3481.28/s  (0.297s, 3447.35/s)  LR: 2.783e-04  Data: 0.023 (0.026)
Train: 194 [ 850/1251 ( 68%)]  Loss: 3.329 (3.30)  Time: 0.298s, 3440.99/s  (0.297s, 3447.05/s)  LR: 2.781e-04  Data: 0.022 (0.026)
Train: 194 [ 900/1251 ( 72%)]  Loss: 3.433 (3.30)  Time: 0.306s, 3348.78/s  (0.297s, 3446.61/s)  LR: 2.779e-04  Data: 0.025 (0.026)
Train: 194 [ 950/1251 ( 76%)]  Loss: 3.218 (3.30)  Time: 0.299s, 3422.88/s  (0.297s, 3446.45/s)  LR: 2.778e-04  Data: 0.022 (0.025)
Train: 194 [1000/1251 ( 80%)]  Loss: 3.256 (3.30)  Time: 0.301s, 3402.91/s  (0.297s, 3446.74/s)  LR: 2.776e-04  Data: 0.024 (0.025)
Train: 194 [1050/1251 ( 84%)]  Loss: 3.204 (3.29)  Time: 0.299s, 3429.11/s  (0.297s, 3446.81/s)  LR: 2.774e-04  Data: 0.024 (0.025)
Train: 194 [1100/1251 ( 88%)]  Loss: 3.230 (3.29)  Time: 0.302s, 3394.34/s  (0.297s, 3446.02/s)  LR: 2.772e-04  Data: 0.024 (0.025)
Train: 194 [1150/1251 ( 92%)]  Loss: 3.347 (3.29)  Time: 0.300s, 3411.26/s  (0.297s, 3445.17/s)  LR: 2.770e-04  Data: 0.025 (0.025)
Train: 194 [1200/1251 ( 96%)]  Loss: 3.317 (3.29)  Time: 0.300s, 3416.25/s  (0.297s, 3444.89/s)  LR: 2.768e-04  Data: 0.022 (0.025)
Train: 194 [1250/1251 (100%)]  Loss: 3.214 (3.29)  Time: 0.275s, 3720.56/s  (0.297s, 3446.42/s)  LR: 2.766e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.020 (2.020)  Loss:  0.5234 (0.5234)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.046 (0.239)  Loss:  0.6738 (1.0448)  Acc@1: 83.9623 (76.7560)  Acc@5: 96.9340 (93.4820)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-192.pth.tar', 76.80799993408203)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-194.pth.tar', 76.75600001464844)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-190.pth.tar', 76.74800000976562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-187.pth.tar', 76.73400000732421)

Train: 195 [   0/1251 (  0%)]  Loss: 3.450 (3.45)  Time: 2.469s,  414.70/s  (2.469s,  414.70/s)  LR: 2.766e-04  Data: 2.250 (2.250)
Train: 195 [  50/1251 (  4%)]  Loss: 3.084 (3.27)  Time: 0.300s, 3418.58/s  (0.322s, 3182.77/s)  LR: 2.765e-04  Data: 0.026 (0.068)
Train: 195 [ 100/1251 (  8%)]  Loss: 3.103 (3.21)  Time: 0.296s, 3456.07/s  (0.308s, 3326.17/s)  LR: 2.763e-04  Data: 0.023 (0.046)
Train: 195 [ 150/1251 ( 12%)]  Loss: 3.386 (3.26)  Time: 0.304s, 3373.31/s  (0.304s, 3369.72/s)  LR: 2.761e-04  Data: 0.024 (0.039)
Train: 195 [ 200/1251 ( 16%)]  Loss: 3.240 (3.25)  Time: 0.299s, 3419.11/s  (0.302s, 3390.46/s)  LR: 2.759e-04  Data: 0.024 (0.035)
Train: 195 [ 250/1251 ( 20%)]  Loss: 3.493 (3.29)  Time: 0.296s, 3465.27/s  (0.301s, 3402.36/s)  LR: 2.757e-04  Data: 0.020 (0.033)
Train: 195 [ 300/1251 ( 24%)]  Loss: 3.262 (3.29)  Time: 0.300s, 3409.19/s  (0.300s, 3408.37/s)  LR: 2.755e-04  Data: 0.026 (0.031)
Train: 195 [ 350/1251 ( 28%)]  Loss: 3.483 (3.31)  Time: 0.292s, 3503.18/s  (0.300s, 3412.80/s)  LR: 2.753e-04  Data: 0.016 (0.030)
Train: 195 [ 400/1251 ( 32%)]  Loss: 3.432 (3.33)  Time: 0.300s, 3412.07/s  (0.300s, 3417.06/s)  LR: 2.752e-04  Data: 0.027 (0.029)
Train: 195 [ 450/1251 ( 36%)]  Loss: 3.202 (3.31)  Time: 0.301s, 3397.52/s  (0.299s, 3419.97/s)  LR: 2.750e-04  Data: 0.026 (0.029)
Train: 195 [ 500/1251 ( 40%)]  Loss: 3.286 (3.31)  Time: 0.299s, 3422.12/s  (0.299s, 3420.59/s)  LR: 2.748e-04  Data: 0.022 (0.028)
Train: 195 [ 550/1251 ( 44%)]  Loss: 3.223 (3.30)  Time: 0.303s, 3383.11/s  (0.299s, 3420.22/s)  LR: 2.746e-04  Data: 0.021 (0.028)
Train: 195 [ 600/1251 ( 48%)]  Loss: 3.202 (3.30)  Time: 0.295s, 3467.15/s  (0.299s, 3420.87/s)  LR: 2.744e-04  Data: 0.023 (0.027)
Train: 195 [ 650/1251 ( 52%)]  Loss: 3.444 (3.31)  Time: 0.298s, 3438.39/s  (0.299s, 3420.47/s)  LR: 2.742e-04  Data: 0.023 (0.027)
Train: 195 [ 700/1251 ( 56%)]  Loss: 3.375 (3.31)  Time: 0.299s, 3426.88/s  (0.299s, 3420.07/s)  LR: 2.740e-04  Data: 0.023 (0.027)
Train: 195 [ 750/1251 ( 60%)]  Loss: 2.957 (3.29)  Time: 0.299s, 3427.26/s  (0.299s, 3419.50/s)  LR: 2.739e-04  Data: 0.022 (0.027)
Train: 195 [ 800/1251 ( 64%)]  Loss: 3.104 (3.28)  Time: 0.301s, 3403.67/s  (0.299s, 3419.16/s)  LR: 2.737e-04  Data: 0.022 (0.026)
Train: 195 [ 850/1251 ( 68%)]  Loss: 3.186 (3.27)  Time: 0.303s, 3375.44/s  (0.300s, 3418.66/s)  LR: 2.735e-04  Data: 0.025 (0.026)
Train: 195 [ 900/1251 ( 72%)]  Loss: 3.121 (3.26)  Time: 0.299s, 3422.32/s  (0.300s, 3417.76/s)  LR: 2.733e-04  Data: 0.023 (0.026)
Train: 195 [ 950/1251 ( 76%)]  Loss: 3.238 (3.26)  Time: 0.302s, 3385.22/s  (0.300s, 3416.91/s)  LR: 2.731e-04  Data: 0.023 (0.026)
Train: 195 [1000/1251 ( 80%)]  Loss: 3.147 (3.26)  Time: 0.300s, 3416.02/s  (0.300s, 3415.72/s)  LR: 2.729e-04  Data: 0.025 (0.026)
Train: 195 [1050/1251 ( 84%)]  Loss: 3.143 (3.25)  Time: 0.306s, 3351.34/s  (0.300s, 3414.88/s)  LR: 2.727e-04  Data: 0.021 (0.026)
Train: 195 [1100/1251 ( 88%)]  Loss: 3.398 (3.26)  Time: 0.307s, 3339.12/s  (0.300s, 3414.31/s)  LR: 2.726e-04  Data: 0.022 (0.026)
Train: 195 [1150/1251 ( 92%)]  Loss: 3.196 (3.26)  Time: 0.302s, 3387.19/s  (0.300s, 3412.71/s)  LR: 2.724e-04  Data: 0.026 (0.026)
Train: 195 [1200/1251 ( 96%)]  Loss: 3.303 (3.26)  Time: 0.301s, 3403.49/s  (0.300s, 3411.51/s)  LR: 2.722e-04  Data: 0.024 (0.025)
Train: 195 [1250/1251 (100%)]  Loss: 3.558 (3.27)  Time: 0.275s, 3723.05/s  (0.300s, 3412.78/s)  LR: 2.720e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.058 (2.058)  Loss:  0.5542 (0.5542)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.056 (0.235)  Loss:  0.6328 (1.0270)  Acc@1: 85.9670 (76.8700)  Acc@5: 97.1698 (93.4860)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-195.pth.tar', 76.87000010986328)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-192.pth.tar', 76.80799993408203)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-194.pth.tar', 76.75600001464844)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-190.pth.tar', 76.74800000976562)

Train: 196 [   0/1251 (  0%)]  Loss: 3.293 (3.29)  Time: 2.376s,  430.92/s  (2.376s,  430.92/s)  LR: 2.720e-04  Data: 2.145 (2.145)
Train: 196 [  50/1251 (  4%)]  Loss: 3.495 (3.39)  Time: 0.294s, 3485.50/s  (0.326s, 3143.25/s)  LR: 2.718e-04  Data: 0.026 (0.066)
Train: 196 [ 100/1251 (  8%)]  Loss: 3.030 (3.27)  Time: 0.297s, 3450.82/s  (0.310s, 3299.57/s)  LR: 2.716e-04  Data: 0.024 (0.045)
Train: 196 [ 150/1251 ( 12%)]  Loss: 3.241 (3.26)  Time: 0.303s, 3374.89/s  (0.306s, 3341.49/s)  LR: 2.715e-04  Data: 0.022 (0.038)
Train: 196 [ 200/1251 ( 16%)]  Loss: 3.138 (3.24)  Time: 0.295s, 3468.14/s  (0.304s, 3363.56/s)  LR: 2.713e-04  Data: 0.022 (0.034)
Train: 196 [ 250/1251 ( 20%)]  Loss: 3.110 (3.22)  Time: 0.298s, 3430.62/s  (0.303s, 3376.17/s)  LR: 2.711e-04  Data: 0.022 (0.032)
Train: 196 [ 300/1251 ( 24%)]  Loss: 2.909 (3.17)  Time: 0.295s, 3471.71/s  (0.303s, 3382.05/s)  LR: 2.709e-04  Data: 0.027 (0.031)
Train: 196 [ 350/1251 ( 28%)]  Loss: 3.362 (3.20)  Time: 0.302s, 3391.90/s  (0.302s, 3386.92/s)  LR: 2.707e-04  Data: 0.023 (0.030)
Train: 196 [ 400/1251 ( 32%)]  Loss: 3.132 (3.19)  Time: 0.298s, 3437.24/s  (0.302s, 3388.97/s)  LR: 2.705e-04  Data: 0.024 (0.029)
Train: 196 [ 450/1251 ( 36%)]  Loss: 2.817 (3.15)  Time: 0.302s, 3387.45/s  (0.302s, 3391.31/s)  LR: 2.703e-04  Data: 0.022 (0.028)
Train: 196 [ 500/1251 ( 40%)]  Loss: 3.144 (3.15)  Time: 0.301s, 3403.37/s  (0.302s, 3391.98/s)  LR: 2.702e-04  Data: 0.025 (0.028)
Train: 196 [ 550/1251 ( 44%)]  Loss: 2.839 (3.13)  Time: 0.298s, 3438.25/s  (0.302s, 3392.47/s)  LR: 2.700e-04  Data: 0.022 (0.027)
Train: 196 [ 600/1251 ( 48%)]  Loss: 3.373 (3.14)  Time: 0.299s, 3425.03/s  (0.302s, 3393.80/s)  LR: 2.698e-04  Data: 0.022 (0.027)
Train: 196 [ 650/1251 ( 52%)]  Loss: 3.410 (3.16)  Time: 0.304s, 3372.68/s  (0.302s, 3393.94/s)  LR: 2.696e-04  Data: 0.021 (0.027)
Train: 196 [ 700/1251 ( 56%)]  Loss: 3.174 (3.16)  Time: 0.301s, 3400.31/s  (0.302s, 3393.67/s)  LR: 2.694e-04  Data: 0.022 (0.027)
Train: 196 [ 750/1251 ( 60%)]  Loss: 3.292 (3.17)  Time: 0.303s, 3384.97/s  (0.302s, 3393.10/s)  LR: 2.692e-04  Data: 0.022 (0.026)
Train: 196 [ 800/1251 ( 64%)]  Loss: 3.236 (3.18)  Time: 0.305s, 3362.78/s  (0.302s, 3392.14/s)  LR: 2.691e-04  Data: 0.024 (0.026)
Train: 196 [ 850/1251 ( 68%)]  Loss: 3.354 (3.19)  Time: 0.307s, 3333.19/s  (0.302s, 3392.36/s)  LR: 2.689e-04  Data: 0.026 (0.026)
Train: 196 [ 900/1251 ( 72%)]  Loss: 3.414 (3.20)  Time: 0.299s, 3424.42/s  (0.302s, 3392.04/s)  LR: 2.687e-04  Data: 0.023 (0.026)
Train: 196 [ 950/1251 ( 76%)]  Loss: 3.357 (3.21)  Time: 0.305s, 3359.87/s  (0.302s, 3391.83/s)  LR: 2.685e-04  Data: 0.025 (0.026)
Train: 196 [1000/1251 ( 80%)]  Loss: 3.208 (3.21)  Time: 0.301s, 3407.09/s  (0.302s, 3391.17/s)  LR: 2.683e-04  Data: 0.022 (0.026)
Train: 196 [1050/1251 ( 84%)]  Loss: 3.205 (3.21)  Time: 0.308s, 3322.48/s  (0.302s, 3390.82/s)  LR: 2.681e-04  Data: 0.022 (0.025)
Train: 196 [1100/1251 ( 88%)]  Loss: 3.363 (3.21)  Time: 0.306s, 3350.92/s  (0.302s, 3390.31/s)  LR: 2.680e-04  Data: 0.027 (0.025)
Train: 196 [1150/1251 ( 92%)]  Loss: 3.548 (3.23)  Time: 0.300s, 3417.21/s  (0.302s, 3389.42/s)  LR: 2.678e-04  Data: 0.025 (0.025)
Train: 196 [1200/1251 ( 96%)]  Loss: 3.149 (3.22)  Time: 0.303s, 3382.05/s  (0.302s, 3388.70/s)  LR: 2.676e-04  Data: 0.025 (0.025)
Train: 196 [1250/1251 (100%)]  Loss: 3.208 (3.22)  Time: 0.275s, 3725.64/s  (0.302s, 3389.93/s)  LR: 2.674e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.055 (2.055)  Loss:  0.5132 (0.5132)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.056 (0.234)  Loss:  0.6426 (1.0192)  Acc@1: 86.3208 (76.7860)  Acc@5: 96.6981 (93.3840)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-195.pth.tar', 76.87000010986328)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-192.pth.tar', 76.80799993408203)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-196.pth.tar', 76.78600000488281)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-194.pth.tar', 76.75600001464844)

Train: 197 [   0/1251 (  0%)]  Loss: 3.459 (3.46)  Time: 2.370s,  432.07/s  (2.370s,  432.07/s)  LR: 2.674e-04  Data: 2.137 (2.137)
Train: 197 [  50/1251 (  4%)]  Loss: 3.204 (3.33)  Time: 0.301s, 3401.42/s  (0.324s, 3162.67/s)  LR: 2.672e-04  Data: 0.023 (0.065)
Train: 197 [ 100/1251 (  8%)]  Loss: 3.218 (3.29)  Time: 0.300s, 3416.36/s  (0.310s, 3308.18/s)  LR: 2.670e-04  Data: 0.025 (0.045)
Train: 197 [ 150/1251 ( 12%)]  Loss: 3.007 (3.22)  Time: 0.301s, 3407.57/s  (0.306s, 3351.10/s)  LR: 2.669e-04  Data: 0.022 (0.038)
Train: 197 [ 200/1251 ( 16%)]  Loss: 3.170 (3.21)  Time: 0.297s, 3451.08/s  (0.304s, 3368.73/s)  LR: 2.667e-04  Data: 0.019 (0.034)
Train: 197 [ 250/1251 ( 20%)]  Loss: 3.197 (3.21)  Time: 0.301s, 3406.26/s  (0.303s, 3378.61/s)  LR: 2.665e-04  Data: 0.025 (0.032)
Train: 197 [ 300/1251 ( 24%)]  Loss: 2.944 (3.17)  Time: 0.300s, 3414.63/s  (0.302s, 3386.37/s)  LR: 2.663e-04  Data: 0.024 (0.031)
Train: 197 [ 350/1251 ( 28%)]  Loss: 3.257 (3.18)  Time: 0.295s, 3466.86/s  (0.302s, 3390.91/s)  LR: 2.661e-04  Data: 0.023 (0.030)
Train: 197 [ 400/1251 ( 32%)]  Loss: 3.358 (3.20)  Time: 0.301s, 3400.18/s  (0.302s, 3392.92/s)  LR: 2.659e-04  Data: 0.025 (0.029)
Train: 197 [ 450/1251 ( 36%)]  Loss: 3.367 (3.22)  Time: 0.304s, 3372.74/s  (0.302s, 3395.58/s)  LR: 2.658e-04  Data: 0.022 (0.028)
Train: 197 [ 500/1251 ( 40%)]  Loss: 3.157 (3.21)  Time: 0.300s, 3410.85/s  (0.301s, 3397.51/s)  LR: 2.656e-04  Data: 0.024 (0.028)
Train: 197 [ 550/1251 ( 44%)]  Loss: 3.331 (3.22)  Time: 0.297s, 3451.43/s  (0.301s, 3398.26/s)  LR: 2.654e-04  Data: 0.021 (0.028)
Train: 197 [ 600/1251 ( 48%)]  Loss: 3.176 (3.22)  Time: 0.306s, 3343.59/s  (0.301s, 3398.56/s)  LR: 2.652e-04  Data: 0.024 (0.027)
Train: 197 [ 650/1251 ( 52%)]  Loss: 3.392 (3.23)  Time: 0.303s, 3380.64/s  (0.301s, 3398.21/s)  LR: 2.650e-04  Data: 0.022 (0.027)
Train: 197 [ 700/1251 ( 56%)]  Loss: 3.577 (3.25)  Time: 0.305s, 3355.54/s  (0.301s, 3397.53/s)  LR: 2.648e-04  Data: 0.023 (0.027)
Train: 197 [ 750/1251 ( 60%)]  Loss: 3.212 (3.25)  Time: 0.307s, 3331.42/s  (0.301s, 3397.78/s)  LR: 2.647e-04  Data: 0.027 (0.026)
Train: 197 [ 800/1251 ( 64%)]  Loss: 3.175 (3.25)  Time: 0.298s, 3438.13/s  (0.301s, 3397.27/s)  LR: 2.645e-04  Data: 0.024 (0.026)
Train: 197 [ 850/1251 ( 68%)]  Loss: 3.111 (3.24)  Time: 0.307s, 3334.84/s  (0.302s, 3396.31/s)  LR: 2.643e-04  Data: 0.026 (0.026)
Train: 197 [ 900/1251 ( 72%)]  Loss: 2.768 (3.21)  Time: 0.303s, 3374.31/s  (0.302s, 3396.02/s)  LR: 2.641e-04  Data: 0.025 (0.026)
Train: 197 [ 950/1251 ( 76%)]  Loss: 2.871 (3.20)  Time: 0.302s, 3389.32/s  (0.302s, 3396.01/s)  LR: 2.639e-04  Data: 0.023 (0.026)
Train: 197 [1000/1251 ( 80%)]  Loss: 3.194 (3.20)  Time: 0.299s, 3419.94/s  (0.302s, 3395.25/s)  LR: 2.637e-04  Data: 0.023 (0.026)
Train: 197 [1050/1251 ( 84%)]  Loss: 3.005 (3.19)  Time: 0.306s, 3348.49/s  (0.302s, 3394.49/s)  LR: 2.636e-04  Data: 0.025 (0.026)
Train: 197 [1100/1251 ( 88%)]  Loss: 3.466 (3.20)  Time: 0.302s, 3390.64/s  (0.302s, 3393.71/s)  LR: 2.634e-04  Data: 0.023 (0.026)
Train: 197 [1150/1251 ( 92%)]  Loss: 3.227 (3.20)  Time: 0.306s, 3345.36/s  (0.302s, 3392.96/s)  LR: 2.632e-04  Data: 0.023 (0.025)
Train: 197 [1200/1251 ( 96%)]  Loss: 3.680 (3.22)  Time: 0.303s, 3375.70/s  (0.302s, 3392.22/s)  LR: 2.630e-04  Data: 0.023 (0.025)
Train: 197 [1250/1251 (100%)]  Loss: 3.255 (3.22)  Time: 0.275s, 3717.73/s  (0.302s, 3392.99/s)  LR: 2.628e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.042 (2.042)  Loss:  0.5552 (0.5552)  Acc@1: 89.7461 (89.7461)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.052 (0.238)  Loss:  0.6699 (1.0317)  Acc@1: 84.9057 (77.0640)  Acc@5: 96.4623 (93.5600)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-197.pth.tar', 77.0640000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-195.pth.tar', 76.87000010986328)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-192.pth.tar', 76.80799993408203)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-196.pth.tar', 76.78600000488281)

Train: 198 [   0/1251 (  0%)]  Loss: 3.020 (3.02)  Time: 2.033s,  503.60/s  (2.033s,  503.60/s)  LR: 2.628e-04  Data: 1.808 (1.808)
Train: 198 [  50/1251 (  4%)]  Loss: 3.161 (3.09)  Time: 0.299s, 3427.88/s  (0.320s, 3202.96/s)  LR: 2.626e-04  Data: 0.029 (0.061)
Train: 198 [ 100/1251 (  8%)]  Loss: 3.573 (3.25)  Time: 0.296s, 3462.52/s  (0.308s, 3319.68/s)  LR: 2.625e-04  Data: 0.021 (0.042)
Train: 198 [ 150/1251 ( 12%)]  Loss: 3.368 (3.28)  Time: 0.303s, 3383.93/s  (0.306s, 3347.64/s)  LR: 2.623e-04  Data: 0.024 (0.036)
Train: 198 [ 200/1251 ( 16%)]  Loss: 3.334 (3.29)  Time: 0.299s, 3426.54/s  (0.305s, 3360.67/s)  LR: 2.621e-04  Data: 0.023 (0.033)
Train: 198 [ 250/1251 ( 20%)]  Loss: 3.112 (3.26)  Time: 0.302s, 3388.22/s  (0.304s, 3368.77/s)  LR: 2.619e-04  Data: 0.021 (0.031)
Train: 198 [ 300/1251 ( 24%)]  Loss: 3.488 (3.29)  Time: 0.304s, 3364.45/s  (0.304s, 3372.06/s)  LR: 2.617e-04  Data: 0.026 (0.030)
Train: 198 [ 350/1251 ( 28%)]  Loss: 3.497 (3.32)  Time: 0.305s, 3361.85/s  (0.304s, 3373.14/s)  LR: 2.615e-04  Data: 0.022 (0.029)
Train: 198 [ 400/1251 ( 32%)]  Loss: 3.138 (3.30)  Time: 0.301s, 3396.94/s  (0.304s, 3373.93/s)  LR: 2.614e-04  Data: 0.028 (0.028)
Train: 198 [ 450/1251 ( 36%)]  Loss: 3.074 (3.28)  Time: 0.305s, 3356.89/s  (0.304s, 3373.55/s)  LR: 2.612e-04  Data: 0.024 (0.028)
Train: 198 [ 500/1251 ( 40%)]  Loss: 3.321 (3.28)  Time: 0.301s, 3406.25/s  (0.303s, 3374.03/s)  LR: 2.610e-04  Data: 0.025 (0.027)
Train: 198 [ 550/1251 ( 44%)]  Loss: 3.182 (3.27)  Time: 0.307s, 3330.59/s  (0.304s, 3373.31/s)  LR: 2.608e-04  Data: 0.025 (0.027)
Train: 198 [ 600/1251 ( 48%)]  Loss: 3.092 (3.26)  Time: 0.303s, 3384.58/s  (0.304s, 3373.95/s)  LR: 2.606e-04  Data: 0.026 (0.027)
Train: 198 [ 650/1251 ( 52%)]  Loss: 3.056 (3.24)  Time: 0.303s, 3381.40/s  (0.303s, 3374.55/s)  LR: 2.605e-04  Data: 0.023 (0.027)
Train: 198 [ 700/1251 ( 56%)]  Loss: 3.361 (3.25)  Time: 0.301s, 3404.15/s  (0.303s, 3374.67/s)  LR: 2.603e-04  Data: 0.027 (0.026)
Train: 198 [ 750/1251 ( 60%)]  Loss: 3.264 (3.25)  Time: 0.307s, 3336.89/s  (0.303s, 3374.38/s)  LR: 2.601e-04  Data: 0.017 (0.026)
Train: 198 [ 800/1251 ( 64%)]  Loss: 2.985 (3.24)  Time: 0.300s, 3411.88/s  (0.303s, 3374.29/s)  LR: 2.599e-04  Data: 0.027 (0.026)
Train: 198 [ 850/1251 ( 68%)]  Loss: 3.427 (3.25)  Time: 0.304s, 3364.96/s  (0.304s, 3373.39/s)  LR: 2.597e-04  Data: 0.027 (0.026)
Train: 198 [ 900/1251 ( 72%)]  Loss: 3.284 (3.25)  Time: 0.306s, 3351.64/s  (0.304s, 3373.16/s)  LR: 2.595e-04  Data: 0.026 (0.026)
Train: 198 [ 950/1251 ( 76%)]  Loss: 2.772 (3.23)  Time: 0.309s, 3308.92/s  (0.304s, 3372.23/s)  LR: 2.594e-04  Data: 0.024 (0.026)
Train: 198 [1000/1251 ( 80%)]  Loss: 3.069 (3.22)  Time: 0.302s, 3391.77/s  (0.304s, 3371.20/s)  LR: 2.592e-04  Data: 0.022 (0.026)
Train: 198 [1050/1251 ( 84%)]  Loss: 2.982 (3.21)  Time: 0.305s, 3354.17/s  (0.304s, 3370.51/s)  LR: 2.590e-04  Data: 0.027 (0.025)
Train: 198 [1100/1251 ( 88%)]  Loss: 3.378 (3.21)  Time: 0.308s, 3324.31/s  (0.304s, 3369.75/s)  LR: 2.588e-04  Data: 0.020 (0.025)
Train: 198 [1150/1251 ( 92%)]  Loss: 3.476 (3.23)  Time: 0.306s, 3351.37/s  (0.304s, 3369.28/s)  LR: 2.586e-04  Data: 0.021 (0.025)
Train: 198 [1200/1251 ( 96%)]  Loss: 3.155 (3.22)  Time: 0.306s, 3345.47/s  (0.304s, 3368.87/s)  LR: 2.585e-04  Data: 0.023 (0.025)
Train: 198 [1250/1251 (100%)]  Loss: 3.328 (3.23)  Time: 0.276s, 3715.69/s  (0.304s, 3370.76/s)  LR: 2.583e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.064 (2.064)  Loss:  0.5303 (0.5303)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.056 (0.235)  Loss:  0.6445 (1.0237)  Acc@1: 85.9670 (76.9240)  Acc@5: 96.6981 (93.4440)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-197.pth.tar', 77.0640000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-198.pth.tar', 76.92399998046875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-195.pth.tar', 76.87000010986328)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-192.pth.tar', 76.80799993408203)

Train: 199 [   0/1251 (  0%)]  Loss: 3.301 (3.30)  Time: 2.209s,  463.59/s  (2.209s,  463.59/s)  LR: 2.583e-04  Data: 1.989 (1.989)
Train: 199 [  50/1251 (  4%)]  Loss: 3.178 (3.24)  Time: 0.297s, 3443.64/s  (0.323s, 3170.53/s)  LR: 2.581e-04  Data: 0.022 (0.063)
Train: 199 [ 100/1251 (  8%)]  Loss: 3.184 (3.22)  Time: 0.296s, 3462.01/s  (0.310s, 3298.57/s)  LR: 2.579e-04  Data: 0.024 (0.043)
Train: 199 [ 150/1251 ( 12%)]  Loss: 3.220 (3.22)  Time: 0.304s, 3364.39/s  (0.307s, 3335.59/s)  LR: 2.577e-04  Data: 0.027 (0.037)
Train: 199 [ 200/1251 ( 16%)]  Loss: 3.121 (3.20)  Time: 0.302s, 3392.45/s  (0.306s, 3350.60/s)  LR: 2.575e-04  Data: 0.021 (0.033)
Train: 199 [ 250/1251 ( 20%)]  Loss: 3.451 (3.24)  Time: 0.304s, 3364.84/s  (0.305s, 3356.45/s)  LR: 2.574e-04  Data: 0.024 (0.031)
Train: 199 [ 300/1251 ( 24%)]  Loss: 2.906 (3.19)  Time: 0.307s, 3330.45/s  (0.305s, 3356.73/s)  LR: 2.572e-04  Data: 0.026 (0.030)
Train: 199 [ 350/1251 ( 28%)]  Loss: 3.296 (3.21)  Time: 0.303s, 3378.83/s  (0.305s, 3358.12/s)  LR: 2.570e-04  Data: 0.021 (0.029)
Train: 199 [ 400/1251 ( 32%)]  Loss: 3.057 (3.19)  Time: 0.299s, 3426.82/s  (0.305s, 3358.96/s)  LR: 2.568e-04  Data: 0.026 (0.028)
Train: 199 [ 450/1251 ( 36%)]  Loss: 3.193 (3.19)  Time: 0.305s, 3356.09/s  (0.305s, 3359.28/s)  LR: 2.566e-04  Data: 0.024 (0.028)
Train: 199 [ 500/1251 ( 40%)]  Loss: 3.335 (3.20)  Time: 0.311s, 3288.69/s  (0.305s, 3359.23/s)  LR: 2.565e-04  Data: 0.028 (0.027)
Train: 199 [ 550/1251 ( 44%)]  Loss: 3.210 (3.20)  Time: 0.307s, 3338.07/s  (0.305s, 3359.14/s)  LR: 2.563e-04  Data: 0.021 (0.027)
Train: 199 [ 600/1251 ( 48%)]  Loss: 3.507 (3.23)  Time: 0.303s, 3384.62/s  (0.305s, 3358.91/s)  LR: 2.561e-04  Data: 0.022 (0.027)
Train: 199 [ 650/1251 ( 52%)]  Loss: 2.944 (3.21)  Time: 0.307s, 3339.12/s  (0.305s, 3358.20/s)  LR: 2.559e-04  Data: 0.022 (0.027)
Train: 199 [ 700/1251 ( 56%)]  Loss: 3.150 (3.20)  Time: 0.306s, 3342.74/s  (0.305s, 3357.91/s)  LR: 2.557e-04  Data: 0.021 (0.026)
Train: 199 [ 750/1251 ( 60%)]  Loss: 2.938 (3.19)  Time: 0.307s, 3337.77/s  (0.305s, 3357.42/s)  LR: 2.556e-04  Data: 0.022 (0.026)
Train: 199 [ 800/1251 ( 64%)]  Loss: 2.907 (3.17)  Time: 0.307s, 3334.40/s  (0.305s, 3356.58/s)  LR: 2.554e-04  Data: 0.022 (0.026)
Train: 199 [ 850/1251 ( 68%)]  Loss: 2.949 (3.16)  Time: 0.307s, 3340.82/s  (0.305s, 3356.34/s)  LR: 2.552e-04  Data: 0.024 (0.026)
Train: 199 [ 900/1251 ( 72%)]  Loss: 3.022 (3.15)  Time: 0.299s, 3426.19/s  (0.305s, 3355.94/s)  LR: 2.550e-04  Data: 0.018 (0.026)
Train: 199 [ 950/1251 ( 76%)]  Loss: 3.029 (3.14)  Time: 0.308s, 3321.95/s  (0.305s, 3355.36/s)  LR: 2.548e-04  Data: 0.021 (0.026)
Train: 199 [1000/1251 ( 80%)]  Loss: 3.609 (3.17)  Time: 0.310s, 3307.72/s  (0.305s, 3355.32/s)  LR: 2.547e-04  Data: 0.024 (0.025)
Train: 199 [1050/1251 ( 84%)]  Loss: 3.101 (3.16)  Time: 0.309s, 3310.07/s  (0.305s, 3354.90/s)  LR: 2.545e-04  Data: 0.020 (0.025)
Train: 199 [1100/1251 ( 88%)]  Loss: 3.361 (3.17)  Time: 0.306s, 3341.90/s  (0.305s, 3354.76/s)  LR: 2.543e-04  Data: 0.021 (0.025)
Train: 199 [1150/1251 ( 92%)]  Loss: 3.147 (3.17)  Time: 0.312s, 3283.94/s  (0.305s, 3354.53/s)  LR: 2.541e-04  Data: 0.023 (0.025)
Train: 199 [1200/1251 ( 96%)]  Loss: 3.372 (3.18)  Time: 0.306s, 3349.34/s  (0.305s, 3354.15/s)  LR: 2.539e-04  Data: 0.020 (0.025)
Train: 199 [1250/1251 (100%)]  Loss: 3.242 (3.18)  Time: 0.275s, 3723.62/s  (0.305s, 3355.75/s)  LR: 2.538e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.032 (2.032)  Loss:  0.5244 (0.5244)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.050 (0.240)  Loss:  0.6108 (1.0149)  Acc@1: 85.0236 (77.0720)  Acc@5: 97.4057 (93.5580)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-199.pth.tar', 77.0719999584961)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-197.pth.tar', 77.0640000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-198.pth.tar', 76.92399998046875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-195.pth.tar', 76.87000010986328)

Train: 200 [   0/1251 (  0%)]  Loss: 3.303 (3.30)  Time: 2.096s,  488.48/s  (2.096s,  488.48/s)  LR: 2.537e-04  Data: 1.831 (1.831)
Train: 200 [  50/1251 (  4%)]  Loss: 3.142 (3.22)  Time: 0.293s, 3494.85/s  (0.327s, 3132.40/s)  LR: 2.536e-04  Data: 0.023 (0.062)
Train: 200 [ 100/1251 (  8%)]  Loss: 3.328 (3.26)  Time: 0.298s, 3441.59/s  (0.313s, 3270.29/s)  LR: 2.534e-04  Data: 0.022 (0.043)
Train: 200 [ 150/1251 ( 12%)]  Loss: 3.258 (3.26)  Time: 0.304s, 3373.38/s  (0.310s, 3308.41/s)  LR: 2.532e-04  Data: 0.028 (0.036)
Train: 200 [ 200/1251 ( 16%)]  Loss: 3.368 (3.28)  Time: 0.306s, 3344.47/s  (0.308s, 3324.69/s)  LR: 2.530e-04  Data: 0.025 (0.033)
Train: 200 [ 250/1251 ( 20%)]  Loss: 2.878 (3.21)  Time: 0.306s, 3345.02/s  (0.307s, 3332.99/s)  LR: 2.528e-04  Data: 0.023 (0.031)
Train: 200 [ 300/1251 ( 24%)]  Loss: 3.024 (3.19)  Time: 0.310s, 3303.08/s  (0.307s, 3337.19/s)  LR: 2.527e-04  Data: 0.022 (0.030)
Train: 200 [ 350/1251 ( 28%)]  Loss: 3.057 (3.17)  Time: 0.310s, 3307.89/s  (0.307s, 3340.90/s)  LR: 2.525e-04  Data: 0.023 (0.029)
Train: 200 [ 400/1251 ( 32%)]  Loss: 3.260 (3.18)  Time: 0.306s, 3343.10/s  (0.306s, 3342.17/s)  LR: 2.523e-04  Data: 0.022 (0.028)
Train: 200 [ 450/1251 ( 36%)]  Loss: 3.487 (3.21)  Time: 0.305s, 3361.82/s  (0.306s, 3342.84/s)  LR: 2.521e-04  Data: 0.022 (0.028)
Train: 200 [ 500/1251 ( 40%)]  Loss: 3.027 (3.19)  Time: 0.302s, 3392.20/s  (0.306s, 3343.29/s)  LR: 2.519e-04  Data: 0.023 (0.027)
Train: 200 [ 550/1251 ( 44%)]  Loss: 3.361 (3.21)  Time: 0.307s, 3334.43/s  (0.306s, 3343.77/s)  LR: 2.518e-04  Data: 0.021 (0.027)
Train: 200 [ 600/1251 ( 48%)]  Loss: 2.966 (3.19)  Time: 0.309s, 3309.40/s  (0.306s, 3343.97/s)  LR: 2.516e-04  Data: 0.024 (0.027)
Train: 200 [ 650/1251 ( 52%)]  Loss: 3.203 (3.19)  Time: 0.310s, 3303.05/s  (0.306s, 3343.21/s)  LR: 2.514e-04  Data: 0.022 (0.026)
Train: 200 [ 700/1251 ( 56%)]  Loss: 3.327 (3.20)  Time: 0.307s, 3340.41/s  (0.306s, 3343.18/s)  LR: 2.512e-04  Data: 0.025 (0.026)
Train: 200 [ 750/1251 ( 60%)]  Loss: 2.963 (3.18)  Time: 0.306s, 3351.19/s  (0.306s, 3343.66/s)  LR: 2.510e-04  Data: 0.024 (0.026)
Train: 200 [ 800/1251 ( 64%)]  Loss: 2.882 (3.17)  Time: 0.306s, 3347.13/s  (0.306s, 3343.93/s)  LR: 2.509e-04  Data: 0.021 (0.026)
Train: 200 [ 850/1251 ( 68%)]  Loss: 3.115 (3.16)  Time: 0.302s, 3389.11/s  (0.306s, 3343.23/s)  LR: 2.507e-04  Data: 0.020 (0.026)
Train: 200 [ 900/1251 ( 72%)]  Loss: 3.214 (3.17)  Time: 0.309s, 3309.45/s  (0.306s, 3342.78/s)  LR: 2.505e-04  Data: 0.023 (0.026)
Train: 200 [ 950/1251 ( 76%)]  Loss: 3.601 (3.19)  Time: 0.308s, 3319.47/s  (0.306s, 3342.78/s)  LR: 2.503e-04  Data: 0.022 (0.025)
Train: 200 [1000/1251 ( 80%)]  Loss: 3.285 (3.19)  Time: 0.299s, 3421.35/s  (0.306s, 3342.58/s)  LR: 2.501e-04  Data: 0.022 (0.025)
Train: 200 [1050/1251 ( 84%)]  Loss: 3.017 (3.18)  Time: 0.308s, 3323.88/s  (0.306s, 3342.87/s)  LR: 2.500e-04  Data: 0.024 (0.025)
Train: 200 [1100/1251 ( 88%)]  Loss: 3.162 (3.18)  Time: 0.307s, 3339.87/s  (0.306s, 3342.80/s)  LR: 2.498e-04  Data: 0.021 (0.025)
Train: 200 [1150/1251 ( 92%)]  Loss: 3.153 (3.18)  Time: 0.308s, 3322.86/s  (0.306s, 3342.40/s)  LR: 2.496e-04  Data: 0.019 (0.025)
Train: 200 [1200/1251 ( 96%)]  Loss: 3.275 (3.19)  Time: 0.305s, 3356.65/s  (0.306s, 3342.26/s)  LR: 2.494e-04  Data: 0.027 (0.025)
Train: 200 [1250/1251 (100%)]  Loss: 3.283 (3.19)  Time: 0.277s, 3701.63/s  (0.306s, 3344.06/s)  LR: 2.493e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.354 (2.354)  Loss:  0.5332 (0.5332)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.055 (0.237)  Loss:  0.6353 (1.0081)  Acc@1: 84.3160 (77.1340)  Acc@5: 96.9340 (93.5460)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-200.pth.tar', 77.1340000390625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-199.pth.tar', 77.0719999584961)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-197.pth.tar', 77.0640000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-198.pth.tar', 76.92399998046875)

Train: 201 [   0/1251 (  0%)]  Loss: 3.058 (3.06)  Time: 2.375s,  431.16/s  (2.375s,  431.16/s)  LR: 2.492e-04  Data: 2.148 (2.148)
Train: 201 [  50/1251 (  4%)]  Loss: 3.328 (3.19)  Time: 0.295s, 3467.62/s  (0.334s, 3069.83/s)  LR: 2.491e-04  Data: 0.021 (0.075)
Train: 201 [ 100/1251 (  8%)]  Loss: 3.499 (3.29)  Time: 0.304s, 3367.38/s  (0.316s, 3241.71/s)  LR: 2.489e-04  Data: 0.027 (0.050)
Train: 201 [ 150/1251 ( 12%)]  Loss: 3.437 (3.33)  Time: 0.305s, 3360.75/s  (0.311s, 3288.68/s)  LR: 2.487e-04  Data: 0.019 (0.041)
Train: 201 [ 200/1251 ( 16%)]  Loss: 2.945 (3.25)  Time: 0.308s, 3320.57/s  (0.310s, 3307.87/s)  LR: 2.485e-04  Data: 0.024 (0.036)
Train: 201 [ 250/1251 ( 20%)]  Loss: 3.276 (3.26)  Time: 0.309s, 3315.88/s  (0.309s, 3317.35/s)  LR: 2.484e-04  Data: 0.023 (0.034)
Train: 201 [ 300/1251 ( 24%)]  Loss: 3.049 (3.23)  Time: 0.310s, 3299.28/s  (0.308s, 3321.93/s)  LR: 2.482e-04  Data: 0.022 (0.032)
Train: 201 [ 350/1251 ( 28%)]  Loss: 3.248 (3.23)  Time: 0.307s, 3330.46/s  (0.308s, 3325.57/s)  LR: 2.480e-04  Data: 0.023 (0.031)
Train: 201 [ 400/1251 ( 32%)]  Loss: 3.196 (3.23)  Time: 0.302s, 3388.98/s  (0.308s, 3328.00/s)  LR: 2.478e-04  Data: 0.025 (0.030)
Train: 201 [ 450/1251 ( 36%)]  Loss: 3.297 (3.23)  Time: 0.312s, 3285.52/s  (0.308s, 3328.37/s)  LR: 2.476e-04  Data: 0.020 (0.029)
Train: 201 [ 500/1251 ( 40%)]  Loss: 3.349 (3.24)  Time: 0.310s, 3306.65/s  (0.308s, 3329.67/s)  LR: 2.475e-04  Data: 0.025 (0.029)
Train: 201 [ 550/1251 ( 44%)]  Loss: 3.338 (3.25)  Time: 0.307s, 3334.37/s  (0.307s, 3330.81/s)  LR: 2.473e-04  Data: 0.024 (0.028)
Train: 201 [ 600/1251 ( 48%)]  Loss: 3.253 (3.25)  Time: 0.307s, 3333.96/s  (0.307s, 3331.07/s)  LR: 2.471e-04  Data: 0.021 (0.028)
Train: 201 [ 650/1251 ( 52%)]  Loss: 2.975 (3.23)  Time: 0.307s, 3340.00/s  (0.307s, 3331.01/s)  LR: 2.469e-04  Data: 0.022 (0.027)
Train: 201 [ 700/1251 ( 56%)]  Loss: 3.516 (3.25)  Time: 0.310s, 3306.60/s  (0.307s, 3331.33/s)  LR: 2.467e-04  Data: 0.023 (0.027)
Train: 201 [ 750/1251 ( 60%)]  Loss: 3.065 (3.24)  Time: 0.310s, 3302.72/s  (0.307s, 3330.77/s)  LR: 2.466e-04  Data: 0.022 (0.027)
Train: 201 [ 800/1251 ( 64%)]  Loss: 3.450 (3.25)  Time: 0.306s, 3347.20/s  (0.307s, 3330.66/s)  LR: 2.464e-04  Data: 0.024 (0.027)
Train: 201 [ 850/1251 ( 68%)]  Loss: 3.501 (3.27)  Time: 0.303s, 3375.07/s  (0.307s, 3330.38/s)  LR: 2.462e-04  Data: 0.023 (0.027)
Train: 201 [ 900/1251 ( 72%)]  Loss: 2.972 (3.25)  Time: 0.307s, 3339.54/s  (0.307s, 3330.22/s)  LR: 2.460e-04  Data: 0.025 (0.026)
Train: 201 [ 950/1251 ( 76%)]  Loss: 3.244 (3.25)  Time: 0.311s, 3295.73/s  (0.307s, 3330.52/s)  LR: 2.459e-04  Data: 0.027 (0.026)
Train: 201 [1000/1251 ( 80%)]  Loss: 3.181 (3.25)  Time: 0.304s, 3364.50/s  (0.307s, 3330.63/s)  LR: 2.457e-04  Data: 0.026 (0.026)
Train: 201 [1050/1251 ( 84%)]  Loss: 3.350 (3.25)  Time: 0.310s, 3298.55/s  (0.307s, 3330.25/s)  LR: 2.455e-04  Data: 0.023 (0.026)
Train: 201 [1100/1251 ( 88%)]  Loss: 3.127 (3.25)  Time: 0.310s, 3304.85/s  (0.308s, 3329.90/s)  LR: 2.453e-04  Data: 0.026 (0.026)
Train: 201 [1150/1251 ( 92%)]  Loss: 3.390 (3.25)  Time: 0.308s, 3328.90/s  (0.307s, 3330.40/s)  LR: 2.451e-04  Data: 0.023 (0.026)
Train: 201 [1200/1251 ( 96%)]  Loss: 3.090 (3.25)  Time: 0.303s, 3377.91/s  (0.307s, 3330.57/s)  LR: 2.450e-04  Data: 0.019 (0.026)
Train: 201 [1250/1251 (100%)]  Loss: 3.167 (3.24)  Time: 0.277s, 3697.65/s  (0.307s, 3332.24/s)  LR: 2.448e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.048 (2.048)  Loss:  0.5854 (0.5854)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.065 (0.235)  Loss:  0.6582 (1.0430)  Acc@1: 85.2594 (77.0440)  Acc@5: 96.9340 (93.4860)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-200.pth.tar', 77.1340000390625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-199.pth.tar', 77.0719999584961)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-197.pth.tar', 77.0640000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-201.pth.tar', 77.04400006103516)

Train: 202 [   0/1251 (  0%)]  Loss: 3.224 (3.22)  Time: 2.616s,  391.47/s  (2.616s,  391.47/s)  LR: 2.448e-04  Data: 2.403 (2.403)
Train: 202 [  50/1251 (  4%)]  Loss: 3.350 (3.29)  Time: 0.297s, 3445.59/s  (0.332s, 3083.73/s)  LR: 2.446e-04  Data: 0.022 (0.070)
Train: 202 [ 100/1251 (  8%)]  Loss: 2.929 (3.17)  Time: 0.306s, 3348.53/s  (0.317s, 3233.06/s)  LR: 2.444e-04  Data: 0.023 (0.047)
Train: 202 [ 150/1251 ( 12%)]  Loss: 3.076 (3.14)  Time: 0.308s, 3321.06/s  (0.312s, 3278.22/s)  LR: 2.442e-04  Data: 0.027 (0.039)
Train: 202 [ 200/1251 ( 16%)]  Loss: 3.240 (3.16)  Time: 0.303s, 3375.75/s  (0.310s, 3298.12/s)  LR: 2.441e-04  Data: 0.024 (0.035)
Train: 202 [ 250/1251 ( 20%)]  Loss: 3.146 (3.16)  Time: 0.305s, 3362.35/s  (0.309s, 3310.32/s)  LR: 2.439e-04  Data: 0.023 (0.033)
Train: 202 [ 300/1251 ( 24%)]  Loss: 3.134 (3.16)  Time: 0.313s, 3268.37/s  (0.309s, 3315.93/s)  LR: 2.437e-04  Data: 0.026 (0.031)
Train: 202 [ 350/1251 ( 28%)]  Loss: 3.135 (3.15)  Time: 0.307s, 3330.64/s  (0.308s, 3319.86/s)  LR: 2.435e-04  Data: 0.023 (0.030)
Train: 202 [ 400/1251 ( 32%)]  Loss: 2.851 (3.12)  Time: 0.307s, 3337.53/s  (0.308s, 3322.23/s)  LR: 2.434e-04  Data: 0.026 (0.029)
Train: 202 [ 450/1251 ( 36%)]  Loss: 3.191 (3.13)  Time: 0.306s, 3343.75/s  (0.308s, 3325.06/s)  LR: 2.432e-04  Data: 0.022 (0.029)
Train: 202 [ 500/1251 ( 40%)]  Loss: 3.148 (3.13)  Time: 0.305s, 3357.09/s  (0.308s, 3327.75/s)  LR: 2.430e-04  Data: 0.019 (0.028)
Train: 202 [ 550/1251 ( 44%)]  Loss: 3.179 (3.13)  Time: 0.307s, 3336.77/s  (0.308s, 3329.51/s)  LR: 2.428e-04  Data: 0.023 (0.028)
Train: 202 [ 600/1251 ( 48%)]  Loss: 2.969 (3.12)  Time: 0.304s, 3372.69/s  (0.307s, 3330.73/s)  LR: 2.426e-04  Data: 0.023 (0.027)
Train: 202 [ 650/1251 ( 52%)]  Loss: 3.043 (3.12)  Time: 0.304s, 3373.33/s  (0.307s, 3331.35/s)  LR: 2.425e-04  Data: 0.025 (0.027)
Train: 202 [ 700/1251 ( 56%)]  Loss: 3.412 (3.14)  Time: 0.308s, 3329.58/s  (0.307s, 3332.07/s)  LR: 2.423e-04  Data: 0.023 (0.027)
Train: 202 [ 750/1251 ( 60%)]  Loss: 2.979 (3.13)  Time: 0.307s, 3340.82/s  (0.307s, 3333.22/s)  LR: 2.421e-04  Data: 0.021 (0.027)
Train: 202 [ 800/1251 ( 64%)]  Loss: 2.834 (3.11)  Time: 0.307s, 3332.49/s  (0.307s, 3333.48/s)  LR: 2.419e-04  Data: 0.021 (0.026)
Train: 202 [ 850/1251 ( 68%)]  Loss: 3.426 (3.13)  Time: 0.309s, 3311.43/s  (0.307s, 3333.70/s)  LR: 2.418e-04  Data: 0.026 (0.026)
Train: 202 [ 900/1251 ( 72%)]  Loss: 3.127 (3.13)  Time: 0.309s, 3316.59/s  (0.307s, 3333.84/s)  LR: 2.416e-04  Data: 0.027 (0.026)
Train: 202 [ 950/1251 ( 76%)]  Loss: 3.572 (3.15)  Time: 0.313s, 3274.59/s  (0.307s, 3333.97/s)  LR: 2.414e-04  Data: 0.023 (0.026)
Train: 202 [1000/1251 ( 80%)]  Loss: 3.361 (3.16)  Time: 0.311s, 3295.77/s  (0.307s, 3333.81/s)  LR: 2.412e-04  Data: 0.025 (0.026)
Train: 202 [1050/1251 ( 84%)]  Loss: 3.328 (3.17)  Time: 0.310s, 3299.01/s  (0.307s, 3333.38/s)  LR: 2.410e-04  Data: 0.026 (0.026)
Train: 202 [1100/1251 ( 88%)]  Loss: 3.304 (3.17)  Time: 0.310s, 3298.82/s  (0.307s, 3333.82/s)  LR: 2.409e-04  Data: 0.025 (0.025)
Train: 202 [1150/1251 ( 92%)]  Loss: 3.175 (3.17)  Time: 0.307s, 3331.44/s  (0.307s, 3334.08/s)  LR: 2.407e-04  Data: 0.026 (0.025)
Train: 202 [1200/1251 ( 96%)]  Loss: 3.153 (3.17)  Time: 0.303s, 3378.08/s  (0.307s, 3334.24/s)  LR: 2.405e-04  Data: 0.026 (0.025)
Train: 202 [1250/1251 (100%)]  Loss: 3.069 (3.17)  Time: 0.277s, 3701.12/s  (0.307s, 3336.29/s)  LR: 2.403e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.015 (2.015)  Loss:  0.5156 (0.5156)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.059 (0.237)  Loss:  0.6533 (1.0164)  Acc@1: 85.3774 (77.1960)  Acc@5: 96.6981 (93.6340)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-202.pth.tar', 77.19599998291015)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-200.pth.tar', 77.1340000390625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-199.pth.tar', 77.0719999584961)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-197.pth.tar', 77.0640000366211)

Train: 203 [   0/1251 (  0%)]  Loss: 3.414 (3.41)  Time: 2.311s,  443.15/s  (2.311s,  443.15/s)  LR: 2.403e-04  Data: 2.083 (2.083)
Train: 203 [  50/1251 (  4%)]  Loss: 3.262 (3.34)  Time: 0.293s, 3494.30/s  (0.329s, 3114.94/s)  LR: 2.402e-04  Data: 0.020 (0.064)
Train: 203 [ 100/1251 (  8%)]  Loss: 3.083 (3.25)  Time: 0.300s, 3414.96/s  (0.315s, 3254.23/s)  LR: 2.400e-04  Data: 0.021 (0.044)
Train: 203 [ 150/1251 ( 12%)]  Loss: 3.316 (3.27)  Time: 0.305s, 3359.26/s  (0.311s, 3297.47/s)  LR: 2.398e-04  Data: 0.024 (0.037)
Train: 203 [ 200/1251 ( 16%)]  Loss: 3.265 (3.27)  Time: 0.299s, 3428.12/s  (0.309s, 3314.06/s)  LR: 2.396e-04  Data: 0.025 (0.034)
Train: 203 [ 250/1251 ( 20%)]  Loss: 3.304 (3.27)  Time: 0.308s, 3328.37/s  (0.308s, 3324.02/s)  LR: 2.395e-04  Data: 0.023 (0.032)
Train: 203 [ 300/1251 ( 24%)]  Loss: 3.462 (3.30)  Time: 0.311s, 3289.79/s  (0.308s, 3328.57/s)  LR: 2.393e-04  Data: 0.021 (0.030)
Train: 203 [ 350/1251 ( 28%)]  Loss: 3.348 (3.31)  Time: 0.302s, 3393.86/s  (0.307s, 3333.57/s)  LR: 2.391e-04  Data: 0.021 (0.029)
Train: 203 [ 400/1251 ( 32%)]  Loss: 3.356 (3.31)  Time: 0.307s, 3331.91/s  (0.307s, 3336.42/s)  LR: 2.389e-04  Data: 0.023 (0.029)
Train: 203 [ 450/1251 ( 36%)]  Loss: 3.426 (3.32)  Time: 0.303s, 3383.72/s  (0.307s, 3338.65/s)  LR: 2.387e-04  Data: 0.022 (0.028)
Train: 203 [ 500/1251 ( 40%)]  Loss: 3.250 (3.32)  Time: 0.307s, 3331.89/s  (0.307s, 3340.70/s)  LR: 2.386e-04  Data: 0.024 (0.027)
Train: 203 [ 550/1251 ( 44%)]  Loss: 3.316 (3.32)  Time: 0.308s, 3322.76/s  (0.306s, 3342.06/s)  LR: 2.384e-04  Data: 0.022 (0.027)
Train: 203 [ 600/1251 ( 48%)]  Loss: 3.304 (3.32)  Time: 0.305s, 3361.39/s  (0.306s, 3343.63/s)  LR: 2.382e-04  Data: 0.022 (0.027)
Train: 203 [ 650/1251 ( 52%)]  Loss: 3.090 (3.30)  Time: 0.309s, 3312.02/s  (0.306s, 3344.59/s)  LR: 2.380e-04  Data: 0.020 (0.027)
Train: 203 [ 700/1251 ( 56%)]  Loss: 3.136 (3.29)  Time: 0.311s, 3293.35/s  (0.306s, 3344.54/s)  LR: 2.379e-04  Data: 0.026 (0.026)
Train: 203 [ 750/1251 ( 60%)]  Loss: 3.144 (3.28)  Time: 0.302s, 3386.73/s  (0.306s, 3345.16/s)  LR: 2.377e-04  Data: 0.019 (0.026)
Train: 203 [ 800/1251 ( 64%)]  Loss: 3.039 (3.27)  Time: 0.311s, 3288.92/s  (0.306s, 3345.70/s)  LR: 2.375e-04  Data: 0.023 (0.026)
Train: 203 [ 850/1251 ( 68%)]  Loss: 3.456 (3.28)  Time: 0.302s, 3386.23/s  (0.306s, 3346.40/s)  LR: 2.373e-04  Data: 0.024 (0.026)
Train: 203 [ 900/1251 ( 72%)]  Loss: 3.208 (3.27)  Time: 0.303s, 3380.35/s  (0.306s, 3347.68/s)  LR: 2.372e-04  Data: 0.025 (0.026)
Train: 203 [ 950/1251 ( 76%)]  Loss: 3.411 (3.28)  Time: 0.300s, 3417.44/s  (0.306s, 3347.96/s)  LR: 2.370e-04  Data: 0.023 (0.026)
Train: 203 [1000/1251 ( 80%)]  Loss: 2.977 (3.27)  Time: 0.306s, 3349.92/s  (0.306s, 3349.09/s)  LR: 2.368e-04  Data: 0.026 (0.025)
Train: 203 [1050/1251 ( 84%)]  Loss: 3.372 (3.27)  Time: 0.308s, 3326.19/s  (0.306s, 3349.53/s)  LR: 2.366e-04  Data: 0.024 (0.025)
Train: 203 [1100/1251 ( 88%)]  Loss: 3.362 (3.27)  Time: 0.306s, 3343.30/s  (0.306s, 3349.98/s)  LR: 2.365e-04  Data: 0.026 (0.025)
Train: 203 [1150/1251 ( 92%)]  Loss: 3.130 (3.27)  Time: 0.308s, 3323.76/s  (0.306s, 3350.27/s)  LR: 2.363e-04  Data: 0.021 (0.025)
Train: 203 [1200/1251 ( 96%)]  Loss: 3.427 (3.27)  Time: 0.302s, 3390.39/s  (0.306s, 3350.74/s)  LR: 2.361e-04  Data: 0.020 (0.025)
Train: 203 [1250/1251 (100%)]  Loss: 3.124 (3.27)  Time: 0.277s, 3700.60/s  (0.305s, 3352.87/s)  LR: 2.359e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.050 (2.050)  Loss:  0.5469 (0.5469)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.055 (0.234)  Loss:  0.6509 (1.0374)  Acc@1: 85.4953 (77.1000)  Acc@5: 97.0519 (93.6240)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-202.pth.tar', 77.19599998291015)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-200.pth.tar', 77.1340000390625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-203.pth.tar', 77.10000003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-199.pth.tar', 77.0719999584961)

Train: 204 [   0/1251 (  0%)]  Loss: 3.292 (3.29)  Time: 2.335s,  438.46/s  (2.335s,  438.46/s)  LR: 2.359e-04  Data: 2.118 (2.118)
Train: 204 [  50/1251 (  4%)]  Loss: 2.906 (3.10)  Time: 0.299s, 3422.01/s  (0.330s, 3104.20/s)  LR: 2.357e-04  Data: 0.022 (0.065)
Train: 204 [ 100/1251 (  8%)]  Loss: 2.813 (3.00)  Time: 0.303s, 3379.36/s  (0.315s, 3255.31/s)  LR: 2.356e-04  Data: 0.022 (0.044)
Train: 204 [ 150/1251 ( 12%)]  Loss: 3.176 (3.05)  Time: 0.302s, 3388.02/s  (0.310s, 3304.56/s)  LR: 2.354e-04  Data: 0.023 (0.037)
Train: 204 [ 200/1251 ( 16%)]  Loss: 3.435 (3.12)  Time: 0.297s, 3452.37/s  (0.308s, 3328.45/s)  LR: 2.352e-04  Data: 0.021 (0.034)
Train: 204 [ 250/1251 ( 20%)]  Loss: 3.564 (3.20)  Time: 0.304s, 3371.98/s  (0.307s, 3338.36/s)  LR: 2.350e-04  Data: 0.021 (0.032)
Train: 204 [ 300/1251 ( 24%)]  Loss: 2.942 (3.16)  Time: 0.300s, 3416.85/s  (0.306s, 3345.50/s)  LR: 2.349e-04  Data: 0.022 (0.030)
Train: 204 [ 350/1251 ( 28%)]  Loss: 3.478 (3.20)  Time: 0.308s, 3327.91/s  (0.306s, 3351.14/s)  LR: 2.347e-04  Data: 0.022 (0.029)
Train: 204 [ 400/1251 ( 32%)]  Loss: 3.370 (3.22)  Time: 0.307s, 3337.84/s  (0.305s, 3354.33/s)  LR: 2.345e-04  Data: 0.023 (0.028)
Train: 204 [ 450/1251 ( 36%)]  Loss: 3.167 (3.21)  Time: 0.301s, 3397.28/s  (0.305s, 3356.71/s)  LR: 2.343e-04  Data: 0.024 (0.028)
Train: 204 [ 500/1251 ( 40%)]  Loss: 3.118 (3.21)  Time: 0.307s, 3337.83/s  (0.305s, 3358.85/s)  LR: 2.342e-04  Data: 0.020 (0.027)
Train: 204 [ 550/1251 ( 44%)]  Loss: 3.232 (3.21)  Time: 0.303s, 3377.93/s  (0.305s, 3360.38/s)  LR: 2.340e-04  Data: 0.023 (0.027)
Train: 204 [ 600/1251 ( 48%)]  Loss: 3.252 (3.21)  Time: 0.303s, 3383.40/s  (0.305s, 3362.71/s)  LR: 2.338e-04  Data: 0.023 (0.027)
Train: 204 [ 650/1251 ( 52%)]  Loss: 3.458 (3.23)  Time: 0.304s, 3369.38/s  (0.304s, 3363.79/s)  LR: 2.336e-04  Data: 0.025 (0.026)
Train: 204 [ 700/1251 ( 56%)]  Loss: 3.157 (3.22)  Time: 0.301s, 3405.88/s  (0.304s, 3365.62/s)  LR: 2.335e-04  Data: 0.024 (0.026)
Train: 204 [ 750/1251 ( 60%)]  Loss: 3.265 (3.23)  Time: 0.306s, 3351.40/s  (0.304s, 3366.73/s)  LR: 2.333e-04  Data: 0.025 (0.026)
Train: 204 [ 800/1251 ( 64%)]  Loss: 3.203 (3.23)  Time: 0.303s, 3384.25/s  (0.304s, 3367.81/s)  LR: 2.331e-04  Data: 0.021 (0.026)
Train: 204 [ 850/1251 ( 68%)]  Loss: 3.121 (3.22)  Time: 0.301s, 3399.79/s  (0.304s, 3368.39/s)  LR: 2.329e-04  Data: 0.026 (0.026)
Train: 204 [ 900/1251 ( 72%)]  Loss: 3.238 (3.22)  Time: 0.307s, 3331.43/s  (0.304s, 3369.42/s)  LR: 2.328e-04  Data: 0.023 (0.026)
Train: 204 [ 950/1251 ( 76%)]  Loss: 2.937 (3.21)  Time: 0.310s, 3304.05/s  (0.304s, 3370.24/s)  LR: 2.326e-04  Data: 0.023 (0.026)
Train: 204 [1000/1251 ( 80%)]  Loss: 3.430 (3.22)  Time: 0.302s, 3386.54/s  (0.304s, 3371.11/s)  LR: 2.324e-04  Data: 0.026 (0.025)
Train: 204 [1050/1251 ( 84%)]  Loss: 3.192 (3.22)  Time: 0.302s, 3392.82/s  (0.304s, 3372.32/s)  LR: 2.322e-04  Data: 0.022 (0.025)
Train: 204 [1100/1251 ( 88%)]  Loss: 3.129 (3.21)  Time: 0.306s, 3344.14/s  (0.304s, 3373.10/s)  LR: 2.321e-04  Data: 0.022 (0.025)
Train: 204 [1150/1251 ( 92%)]  Loss: 3.173 (3.21)  Time: 0.303s, 3382.21/s  (0.304s, 3373.61/s)  LR: 2.319e-04  Data: 0.021 (0.025)
Train: 204 [1200/1251 ( 96%)]  Loss: 3.144 (3.21)  Time: 0.302s, 3393.77/s  (0.303s, 3374.64/s)  LR: 2.317e-04  Data: 0.027 (0.025)
Train: 204 [1250/1251 (100%)]  Loss: 2.833 (3.19)  Time: 0.276s, 3710.19/s  (0.303s, 3377.06/s)  LR: 2.315e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.009 (2.009)  Loss:  0.5034 (0.5034)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.041 (0.234)  Loss:  0.6123 (1.0123)  Acc@1: 86.3208 (77.2080)  Acc@5: 96.9340 (93.6620)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-204.pth.tar', 77.2080000048828)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-202.pth.tar', 77.19599998291015)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-200.pth.tar', 77.1340000390625)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-203.pth.tar', 77.10000003417969)

Train: 205 [   0/1251 (  0%)]  Loss: 3.201 (3.20)  Time: 2.261s,  452.92/s  (2.261s,  452.92/s)  LR: 2.315e-04  Data: 2.034 (2.034)
Train: 205 [  50/1251 (  4%)]  Loss: 2.954 (3.08)  Time: 0.292s, 3510.26/s  (0.322s, 3177.65/s)  LR: 2.314e-04  Data: 0.022 (0.064)
Train: 205 [ 100/1251 (  8%)]  Loss: 3.137 (3.10)  Time: 0.301s, 3404.32/s  (0.309s, 3314.71/s)  LR: 2.312e-04  Data: 0.027 (0.044)
Train: 205 [ 150/1251 ( 12%)]  Loss: 3.262 (3.14)  Time: 0.299s, 3426.93/s  (0.305s, 3356.87/s)  LR: 2.310e-04  Data: 0.023 (0.037)
Train: 205 [ 200/1251 ( 16%)]  Loss: 3.197 (3.15)  Time: 0.296s, 3462.57/s  (0.304s, 3372.17/s)  LR: 2.308e-04  Data: 0.025 (0.034)
Train: 205 [ 250/1251 ( 20%)]  Loss: 3.175 (3.15)  Time: 0.303s, 3385.09/s  (0.303s, 3380.42/s)  LR: 2.307e-04  Data: 0.026 (0.032)
Train: 205 [ 300/1251 ( 24%)]  Loss: 3.016 (3.13)  Time: 0.302s, 3392.41/s  (0.303s, 3384.98/s)  LR: 2.305e-04  Data: 0.023 (0.031)
Train: 205 [ 350/1251 ( 28%)]  Loss: 3.013 (3.12)  Time: 0.300s, 3413.33/s  (0.302s, 3387.51/s)  LR: 2.303e-04  Data: 0.022 (0.030)
Train: 205 [ 400/1251 ( 32%)]  Loss: 3.341 (3.14)  Time: 0.297s, 3442.93/s  (0.302s, 3388.32/s)  LR: 2.301e-04  Data: 0.022 (0.029)
Train: 205 [ 450/1251 ( 36%)]  Loss: 2.861 (3.12)  Time: 0.300s, 3416.89/s  (0.302s, 3389.94/s)  LR: 2.300e-04  Data: 0.021 (0.028)
Train: 205 [ 500/1251 ( 40%)]  Loss: 3.151 (3.12)  Time: 0.299s, 3424.28/s  (0.302s, 3389.57/s)  LR: 2.298e-04  Data: 0.022 (0.028)
Train: 205 [ 550/1251 ( 44%)]  Loss: 3.184 (3.12)  Time: 0.301s, 3399.29/s  (0.302s, 3390.71/s)  LR: 2.296e-04  Data: 0.023 (0.027)
Train: 205 [ 600/1251 ( 48%)]  Loss: 2.698 (3.09)  Time: 0.304s, 3364.39/s  (0.302s, 3390.99/s)  LR: 2.294e-04  Data: 0.025 (0.027)
Train: 205 [ 650/1251 ( 52%)]  Loss: 3.168 (3.10)  Time: 0.302s, 3387.36/s  (0.302s, 3391.18/s)  LR: 2.293e-04  Data: 0.028 (0.027)
Train: 205 [ 700/1251 ( 56%)]  Loss: 3.104 (3.10)  Time: 0.304s, 3365.79/s  (0.302s, 3390.97/s)  LR: 2.291e-04  Data: 0.022 (0.027)
Train: 205 [ 750/1251 ( 60%)]  Loss: 3.204 (3.10)  Time: 0.304s, 3368.30/s  (0.302s, 3391.14/s)  LR: 2.289e-04  Data: 0.023 (0.026)
Train: 205 [ 800/1251 ( 64%)]  Loss: 3.028 (3.10)  Time: 0.303s, 3374.72/s  (0.302s, 3391.17/s)  LR: 2.288e-04  Data: 0.025 (0.026)
Train: 205 [ 850/1251 ( 68%)]  Loss: 2.714 (3.08)  Time: 0.305s, 3355.04/s  (0.302s, 3391.24/s)  LR: 2.286e-04  Data: 0.020 (0.026)
Train: 205 [ 900/1251 ( 72%)]  Loss: 3.395 (3.09)  Time: 0.299s, 3422.43/s  (0.302s, 3390.08/s)  LR: 2.284e-04  Data: 0.022 (0.026)
Train: 205 [ 950/1251 ( 76%)]  Loss: 3.216 (3.10)  Time: 0.301s, 3402.34/s  (0.302s, 3389.80/s)  LR: 2.282e-04  Data: 0.021 (0.026)
Train: 205 [1000/1251 ( 80%)]  Loss: 3.434 (3.12)  Time: 0.302s, 3387.01/s  (0.302s, 3389.84/s)  LR: 2.281e-04  Data: 0.027 (0.026)
Train: 205 [1050/1251 ( 84%)]  Loss: 3.314 (3.13)  Time: 0.305s, 3358.17/s  (0.302s, 3390.16/s)  LR: 2.279e-04  Data: 0.023 (0.026)
Train: 205 [1100/1251 ( 88%)]  Loss: 3.305 (3.13)  Time: 0.298s, 3433.07/s  (0.302s, 3389.57/s)  LR: 2.277e-04  Data: 0.023 (0.025)
Train: 205 [1150/1251 ( 92%)]  Loss: 3.187 (3.14)  Time: 0.306s, 3344.46/s  (0.302s, 3389.38/s)  LR: 2.275e-04  Data: 0.023 (0.025)
Train: 205 [1200/1251 ( 96%)]  Loss: 2.996 (3.13)  Time: 0.302s, 3387.70/s  (0.302s, 3389.02/s)  LR: 2.274e-04  Data: 0.024 (0.025)
Train: 205 [1250/1251 (100%)]  Loss: 3.233 (3.13)  Time: 0.275s, 3725.86/s  (0.302s, 3390.68/s)  LR: 2.272e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.041 (2.041)  Loss:  0.5371 (0.5371)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.046 (0.235)  Loss:  0.6455 (1.0132)  Acc@1: 85.8491 (77.4340)  Acc@5: 96.9340 (93.6700)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-205.pth.tar', 77.43399992919922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-204.pth.tar', 77.2080000048828)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-202.pth.tar', 77.19599998291015)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-200.pth.tar', 77.1340000390625)

Train: 206 [   0/1251 (  0%)]  Loss: 3.396 (3.40)  Time: 2.187s,  468.14/s  (2.187s,  468.14/s)  LR: 2.272e-04  Data: 1.968 (1.968)
Train: 206 [  50/1251 (  4%)]  Loss: 3.236 (3.32)  Time: 0.294s, 3488.88/s  (0.319s, 3214.60/s)  LR: 2.270e-04  Data: 0.024 (0.062)
Train: 206 [ 100/1251 (  8%)]  Loss: 2.985 (3.21)  Time: 0.294s, 3483.88/s  (0.307s, 3337.67/s)  LR: 2.268e-04  Data: 0.022 (0.043)
Train: 206 [ 150/1251 ( 12%)]  Loss: 3.212 (3.21)  Time: 0.301s, 3400.27/s  (0.303s, 3374.96/s)  LR: 2.267e-04  Data: 0.022 (0.037)
Train: 206 [ 200/1251 ( 16%)]  Loss: 3.015 (3.17)  Time: 0.297s, 3442.35/s  (0.302s, 3389.22/s)  LR: 2.265e-04  Data: 0.016 (0.033)
Train: 206 [ 250/1251 ( 20%)]  Loss: 3.002 (3.14)  Time: 0.299s, 3425.90/s  (0.302s, 3396.14/s)  LR: 2.263e-04  Data: 0.025 (0.031)
Train: 206 [ 300/1251 ( 24%)]  Loss: 2.904 (3.11)  Time: 0.296s, 3458.48/s  (0.301s, 3398.55/s)  LR: 2.261e-04  Data: 0.027 (0.030)
Train: 206 [ 350/1251 ( 28%)]  Loss: 3.086 (3.10)  Time: 0.302s, 3394.70/s  (0.301s, 3401.29/s)  LR: 2.260e-04  Data: 0.020 (0.029)
Train: 206 [ 400/1251 ( 32%)]  Loss: 2.859 (3.08)  Time: 0.303s, 3379.60/s  (0.301s, 3402.41/s)  LR: 2.258e-04  Data: 0.027 (0.029)
Train: 206 [ 450/1251 ( 36%)]  Loss: 3.255 (3.10)  Time: 0.299s, 3419.73/s  (0.301s, 3402.72/s)  LR: 2.256e-04  Data: 0.021 (0.028)
Train: 206 [ 500/1251 ( 40%)]  Loss: 2.862 (3.07)  Time: 0.303s, 3382.91/s  (0.301s, 3401.67/s)  LR: 2.255e-04  Data: 0.023 (0.028)
Train: 206 [ 550/1251 ( 44%)]  Loss: 3.107 (3.08)  Time: 0.306s, 3346.24/s  (0.301s, 3400.59/s)  LR: 2.253e-04  Data: 0.022 (0.027)
Train: 206 [ 600/1251 ( 48%)]  Loss: 3.196 (3.09)  Time: 0.301s, 3406.98/s  (0.301s, 3400.04/s)  LR: 2.251e-04  Data: 0.021 (0.027)
Train: 206 [ 650/1251 ( 52%)]  Loss: 3.204 (3.09)  Time: 0.304s, 3371.47/s  (0.301s, 3399.59/s)  LR: 2.249e-04  Data: 0.024 (0.027)
Train: 206 [ 700/1251 ( 56%)]  Loss: 3.083 (3.09)  Time: 0.301s, 3397.78/s  (0.301s, 3399.48/s)  LR: 2.248e-04  Data: 0.026 (0.026)
Train: 206 [ 750/1251 ( 60%)]  Loss: 3.026 (3.09)  Time: 0.301s, 3405.44/s  (0.301s, 3399.51/s)  LR: 2.246e-04  Data: 0.025 (0.026)
Train: 206 [ 800/1251 ( 64%)]  Loss: 2.983 (3.08)  Time: 0.303s, 3375.71/s  (0.301s, 3399.93/s)  LR: 2.244e-04  Data: 0.024 (0.026)
Train: 206 [ 850/1251 ( 68%)]  Loss: 3.210 (3.09)  Time: 0.306s, 3350.05/s  (0.301s, 3399.60/s)  LR: 2.242e-04  Data: 0.023 (0.026)
Train: 206 [ 900/1251 ( 72%)]  Loss: 3.244 (3.10)  Time: 0.304s, 3363.26/s  (0.301s, 3400.26/s)  LR: 2.241e-04  Data: 0.023 (0.026)
Train: 206 [ 950/1251 ( 76%)]  Loss: 3.235 (3.11)  Time: 0.306s, 3350.76/s  (0.301s, 3400.13/s)  LR: 2.239e-04  Data: 0.026 (0.026)
Train: 206 [1000/1251 ( 80%)]  Loss: 3.150 (3.11)  Time: 0.296s, 3454.06/s  (0.301s, 3400.56/s)  LR: 2.237e-04  Data: 0.022 (0.026)
Train: 206 [1050/1251 ( 84%)]  Loss: 3.218 (3.11)  Time: 0.303s, 3382.63/s  (0.301s, 3400.75/s)  LR: 2.236e-04  Data: 0.023 (0.025)
Train: 206 [1100/1251 ( 88%)]  Loss: 3.170 (3.11)  Time: 0.300s, 3412.95/s  (0.301s, 3401.48/s)  LR: 2.234e-04  Data: 0.023 (0.025)
Train: 206 [1150/1251 ( 92%)]  Loss: 2.958 (3.11)  Time: 0.299s, 3426.32/s  (0.301s, 3401.84/s)  LR: 2.232e-04  Data: 0.018 (0.025)
Train: 206 [1200/1251 ( 96%)]  Loss: 3.145 (3.11)  Time: 0.299s, 3429.21/s  (0.301s, 3402.02/s)  LR: 2.230e-04  Data: 0.021 (0.025)
Train: 206 [1250/1251 (100%)]  Loss: 3.384 (3.12)  Time: 0.275s, 3723.59/s  (0.301s, 3404.12/s)  LR: 2.229e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.030 (2.030)  Loss:  0.5249 (0.5249)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.048 (0.232)  Loss:  0.5815 (1.0123)  Acc@1: 87.0283 (77.4040)  Acc@5: 98.2311 (93.7240)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-205.pth.tar', 77.43399992919922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-206.pth.tar', 77.40400005371093)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-204.pth.tar', 77.2080000048828)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-202.pth.tar', 77.19599998291015)

Train: 207 [   0/1251 (  0%)]  Loss: 3.285 (3.28)  Time: 2.509s,  408.11/s  (2.509s,  408.11/s)  LR: 2.229e-04  Data: 2.283 (2.283)
Train: 207 [  50/1251 (  4%)]  Loss: 2.771 (3.03)  Time: 0.289s, 3541.34/s  (0.324s, 3161.88/s)  LR: 2.227e-04  Data: 0.019 (0.069)
Train: 207 [ 100/1251 (  8%)]  Loss: 3.033 (3.03)  Time: 0.301s, 3401.87/s  (0.309s, 3316.82/s)  LR: 2.225e-04  Data: 0.022 (0.047)
Train: 207 [ 150/1251 ( 12%)]  Loss: 3.142 (3.06)  Time: 0.299s, 3420.71/s  (0.304s, 3364.11/s)  LR: 2.223e-04  Data: 0.017 (0.039)
Train: 207 [ 200/1251 ( 16%)]  Loss: 3.348 (3.12)  Time: 0.296s, 3462.42/s  (0.302s, 3385.45/s)  LR: 2.222e-04  Data: 0.024 (0.035)
Train: 207 [ 250/1251 ( 20%)]  Loss: 3.069 (3.11)  Time: 0.298s, 3439.85/s  (0.301s, 3399.02/s)  LR: 2.220e-04  Data: 0.024 (0.033)
Train: 207 [ 300/1251 ( 24%)]  Loss: 3.121 (3.11)  Time: 0.301s, 3396.60/s  (0.301s, 3407.04/s)  LR: 2.218e-04  Data: 0.023 (0.031)
Train: 207 [ 350/1251 ( 28%)]  Loss: 2.953 (3.09)  Time: 0.301s, 3400.58/s  (0.300s, 3409.99/s)  LR: 2.217e-04  Data: 0.022 (0.030)
Train: 207 [ 400/1251 ( 32%)]  Loss: 2.902 (3.07)  Time: 0.300s, 3410.86/s  (0.300s, 3411.84/s)  LR: 2.215e-04  Data: 0.028 (0.029)
Train: 207 [ 450/1251 ( 36%)]  Loss: 3.109 (3.07)  Time: 0.304s, 3364.35/s  (0.300s, 3414.52/s)  LR: 2.213e-04  Data: 0.025 (0.029)
Train: 207 [ 500/1251 ( 40%)]  Loss: 3.076 (3.07)  Time: 0.294s, 3479.77/s  (0.300s, 3415.40/s)  LR: 2.211e-04  Data: 0.020 (0.028)
Train: 207 [ 550/1251 ( 44%)]  Loss: 2.867 (3.06)  Time: 0.305s, 3355.05/s  (0.300s, 3415.14/s)  LR: 2.210e-04  Data: 0.023 (0.028)
Train: 207 [ 600/1251 ( 48%)]  Loss: 2.937 (3.05)  Time: 0.297s, 3446.67/s  (0.300s, 3415.02/s)  LR: 2.208e-04  Data: 0.024 (0.027)
Train: 207 [ 650/1251 ( 52%)]  Loss: 2.760 (3.03)  Time: 0.301s, 3404.45/s  (0.300s, 3415.16/s)  LR: 2.206e-04  Data: 0.024 (0.027)
Train: 207 [ 700/1251 ( 56%)]  Loss: 2.837 (3.01)  Time: 0.299s, 3429.94/s  (0.300s, 3415.82/s)  LR: 2.205e-04  Data: 0.026 (0.027)
Train: 207 [ 750/1251 ( 60%)]  Loss: 3.160 (3.02)  Time: 0.301s, 3405.95/s  (0.300s, 3416.26/s)  LR: 2.203e-04  Data: 0.023 (0.027)
Train: 207 [ 800/1251 ( 64%)]  Loss: 3.343 (3.04)  Time: 0.296s, 3454.65/s  (0.300s, 3416.42/s)  LR: 2.201e-04  Data: 0.023 (0.026)
Train: 207 [ 850/1251 ( 68%)]  Loss: 2.981 (3.04)  Time: 0.295s, 3468.85/s  (0.300s, 3416.62/s)  LR: 2.199e-04  Data: 0.019 (0.026)
Train: 207 [ 900/1251 ( 72%)]  Loss: 2.944 (3.03)  Time: 0.304s, 3371.86/s  (0.300s, 3416.78/s)  LR: 2.198e-04  Data: 0.024 (0.026)
Train: 207 [ 950/1251 ( 76%)]  Loss: 3.196 (3.04)  Time: 0.298s, 3441.92/s  (0.300s, 3417.71/s)  LR: 2.196e-04  Data: 0.022 (0.026)
Train: 207 [1000/1251 ( 80%)]  Loss: 3.073 (3.04)  Time: 0.300s, 3410.89/s  (0.300s, 3418.20/s)  LR: 2.194e-04  Data: 0.025 (0.026)
Train: 207 [1050/1251 ( 84%)]  Loss: 2.986 (3.04)  Time: 0.296s, 3461.22/s  (0.299s, 3419.60/s)  LR: 2.193e-04  Data: 0.025 (0.026)
Train: 207 [1100/1251 ( 88%)]  Loss: 3.045 (3.04)  Time: 0.300s, 3416.75/s  (0.299s, 3420.22/s)  LR: 2.191e-04  Data: 0.021 (0.026)
Train: 207 [1150/1251 ( 92%)]  Loss: 2.905 (3.04)  Time: 0.299s, 3430.00/s  (0.299s, 3420.82/s)  LR: 2.189e-04  Data: 0.021 (0.026)
Train: 207 [1200/1251 ( 96%)]  Loss: 3.386 (3.05)  Time: 0.290s, 3529.95/s  (0.299s, 3421.85/s)  LR: 2.187e-04  Data: 0.022 (0.025)
Train: 207 [1250/1251 (100%)]  Loss: 3.299 (3.06)  Time: 0.276s, 3712.06/s  (0.299s, 3424.58/s)  LR: 2.186e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.034 (2.034)  Loss:  0.5361 (0.5361)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.048 (0.239)  Loss:  0.6436 (1.0052)  Acc@1: 85.7311 (77.5660)  Acc@5: 96.8160 (93.7200)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-207.pth.tar', 77.56600000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-205.pth.tar', 77.43399992919922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-206.pth.tar', 77.40400005371093)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-204.pth.tar', 77.2080000048828)

Train: 208 [   0/1251 (  0%)]  Loss: 3.340 (3.34)  Time: 2.358s,  434.33/s  (2.358s,  434.33/s)  LR: 2.186e-04  Data: 2.140 (2.140)
Train: 208 [  50/1251 (  4%)]  Loss: 3.110 (3.22)  Time: 0.287s, 3564.65/s  (0.320s, 3203.35/s)  LR: 2.184e-04  Data: 0.022 (0.067)
Train: 208 [ 100/1251 (  8%)]  Loss: 3.170 (3.21)  Time: 0.297s, 3446.97/s  (0.305s, 3361.27/s)  LR: 2.182e-04  Data: 0.027 (0.046)
Train: 208 [ 150/1251 ( 12%)]  Loss: 3.241 (3.22)  Time: 0.291s, 3518.48/s  (0.301s, 3403.39/s)  LR: 2.181e-04  Data: 0.024 (0.038)
Train: 208 [ 200/1251 ( 16%)]  Loss: 3.180 (3.21)  Time: 0.294s, 3488.07/s  (0.299s, 3424.62/s)  LR: 2.179e-04  Data: 0.020 (0.034)
Train: 208 [ 250/1251 ( 20%)]  Loss: 2.938 (3.16)  Time: 0.301s, 3405.23/s  (0.298s, 3435.35/s)  LR: 2.177e-04  Data: 0.027 (0.032)
Train: 208 [ 300/1251 ( 24%)]  Loss: 3.304 (3.18)  Time: 0.292s, 3508.32/s  (0.298s, 3440.01/s)  LR: 2.175e-04  Data: 0.023 (0.031)
Train: 208 [ 350/1251 ( 28%)]  Loss: 3.115 (3.17)  Time: 0.298s, 3434.07/s  (0.298s, 3441.87/s)  LR: 2.174e-04  Data: 0.023 (0.030)
Train: 208 [ 400/1251 ( 32%)]  Loss: 3.256 (3.18)  Time: 0.293s, 3498.82/s  (0.297s, 3445.18/s)  LR: 2.172e-04  Data: 0.021 (0.029)
Train: 208 [ 450/1251 ( 36%)]  Loss: 3.337 (3.20)  Time: 0.297s, 3448.56/s  (0.297s, 3446.44/s)  LR: 2.170e-04  Data: 0.024 (0.028)
Train: 208 [ 500/1251 ( 40%)]  Loss: 2.934 (3.18)  Time: 0.292s, 3501.16/s  (0.297s, 3446.92/s)  LR: 2.169e-04  Data: 0.022 (0.028)
Train: 208 [ 550/1251 ( 44%)]  Loss: 3.372 (3.19)  Time: 0.297s, 3450.63/s  (0.297s, 3447.11/s)  LR: 2.167e-04  Data: 0.024 (0.027)
Train: 208 [ 600/1251 ( 48%)]  Loss: 3.368 (3.21)  Time: 0.297s, 3444.78/s  (0.297s, 3447.33/s)  LR: 2.165e-04  Data: 0.023 (0.027)
Train: 208 [ 650/1251 ( 52%)]  Loss: 3.375 (3.22)  Time: 0.301s, 3397.30/s  (0.297s, 3448.58/s)  LR: 2.163e-04  Data: 0.022 (0.027)
Train: 208 [ 700/1251 ( 56%)]  Loss: 3.258 (3.22)  Time: 0.298s, 3433.90/s  (0.297s, 3448.18/s)  LR: 2.162e-04  Data: 0.024 (0.027)
Train: 208 [ 750/1251 ( 60%)]  Loss: 3.089 (3.21)  Time: 0.287s, 3566.64/s  (0.297s, 3448.52/s)  LR: 2.160e-04  Data: 0.026 (0.026)
Train: 208 [ 800/1251 ( 64%)]  Loss: 3.109 (3.21)  Time: 0.298s, 3442.00/s  (0.297s, 3448.02/s)  LR: 2.158e-04  Data: 0.024 (0.026)
Train: 208 [ 850/1251 ( 68%)]  Loss: 3.110 (3.20)  Time: 0.298s, 3441.78/s  (0.297s, 3447.44/s)  LR: 2.157e-04  Data: 0.021 (0.026)
Train: 208 [ 900/1251 ( 72%)]  Loss: 3.225 (3.20)  Time: 0.296s, 3462.71/s  (0.297s, 3446.95/s)  LR: 2.155e-04  Data: 0.022 (0.026)
Train: 208 [ 950/1251 ( 76%)]  Loss: 3.065 (3.19)  Time: 0.297s, 3442.10/s  (0.297s, 3445.93/s)  LR: 2.153e-04  Data: 0.024 (0.026)
Train: 208 [1000/1251 ( 80%)]  Loss: 3.250 (3.20)  Time: 0.297s, 3452.26/s  (0.297s, 3445.23/s)  LR: 2.152e-04  Data: 0.024 (0.026)
Train: 208 [1050/1251 ( 84%)]  Loss: 3.127 (3.19)  Time: 0.299s, 3426.03/s  (0.297s, 3445.40/s)  LR: 2.150e-04  Data: 0.028 (0.026)
Train: 208 [1100/1251 ( 88%)]  Loss: 3.168 (3.19)  Time: 0.295s, 3467.51/s  (0.297s, 3445.34/s)  LR: 2.148e-04  Data: 0.022 (0.026)
Train: 208 [1150/1251 ( 92%)]  Loss: 2.816 (3.18)  Time: 0.298s, 3434.23/s  (0.297s, 3444.99/s)  LR: 2.146e-04  Data: 0.020 (0.025)
Train: 208 [1200/1251 ( 96%)]  Loss: 3.173 (3.18)  Time: 0.300s, 3417.63/s  (0.297s, 3444.63/s)  LR: 2.145e-04  Data: 0.023 (0.025)
Train: 208 [1250/1251 (100%)]  Loss: 3.226 (3.18)  Time: 0.273s, 3744.41/s  (0.297s, 3446.55/s)  LR: 2.143e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.010 (2.010)  Loss:  0.5273 (0.5273)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.045 (0.234)  Loss:  0.6323 (1.0059)  Acc@1: 85.9670 (77.3360)  Acc@5: 96.4623 (93.7100)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-207.pth.tar', 77.56600000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-205.pth.tar', 77.43399992919922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-206.pth.tar', 77.40400005371093)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-208.pth.tar', 77.33600010986328)

Train: 209 [   0/1251 (  0%)]  Loss: 2.970 (2.97)  Time: 2.255s,  454.17/s  (2.255s,  454.17/s)  LR: 2.143e-04  Data: 2.019 (2.019)
Train: 209 [  50/1251 (  4%)]  Loss: 3.014 (2.99)  Time: 0.289s, 3541.63/s  (0.320s, 3203.96/s)  LR: 2.141e-04  Data: 0.023 (0.064)
Train: 209 [ 100/1251 (  8%)]  Loss: 3.194 (3.06)  Time: 0.296s, 3455.77/s  (0.306s, 3342.98/s)  LR: 2.140e-04  Data: 0.023 (0.044)
Train: 209 [ 150/1251 ( 12%)]  Loss: 3.510 (3.17)  Time: 0.297s, 3442.10/s  (0.302s, 3394.42/s)  LR: 2.138e-04  Data: 0.025 (0.037)
Train: 209 [ 200/1251 ( 16%)]  Loss: 3.124 (3.16)  Time: 0.298s, 3437.54/s  (0.300s, 3416.06/s)  LR: 2.136e-04  Data: 0.024 (0.034)
Train: 209 [ 250/1251 ( 20%)]  Loss: 3.304 (3.19)  Time: 0.294s, 3486.90/s  (0.299s, 3426.76/s)  LR: 2.135e-04  Data: 0.023 (0.032)
Train: 209 [ 300/1251 ( 24%)]  Loss: 3.264 (3.20)  Time: 0.290s, 3536.52/s  (0.298s, 3435.00/s)  LR: 2.133e-04  Data: 0.023 (0.030)
Train: 209 [ 350/1251 ( 28%)]  Loss: 3.318 (3.21)  Time: 0.296s, 3456.54/s  (0.298s, 3438.49/s)  LR: 2.131e-04  Data: 0.025 (0.029)
Train: 209 [ 400/1251 ( 32%)]  Loss: 3.389 (3.23)  Time: 0.291s, 3522.57/s  (0.298s, 3441.81/s)  LR: 2.129e-04  Data: 0.021 (0.029)
Train: 209 [ 450/1251 ( 36%)]  Loss: 3.081 (3.22)  Time: 0.299s, 3429.86/s  (0.297s, 3443.79/s)  LR: 2.128e-04  Data: 0.026 (0.028)
Train: 209 [ 500/1251 ( 40%)]  Loss: 2.732 (3.17)  Time: 0.294s, 3486.66/s  (0.297s, 3444.83/s)  LR: 2.126e-04  Data: 0.027 (0.028)
Train: 209 [ 550/1251 ( 44%)]  Loss: 3.312 (3.18)  Time: 0.305s, 3359.50/s  (0.297s, 3445.96/s)  LR: 2.124e-04  Data: 0.025 (0.027)
Train: 209 [ 600/1251 ( 48%)]  Loss: 2.899 (3.16)  Time: 0.297s, 3443.08/s  (0.297s, 3445.96/s)  LR: 2.123e-04  Data: 0.021 (0.027)
Train: 209 [ 650/1251 ( 52%)]  Loss: 2.845 (3.14)  Time: 0.302s, 3389.30/s  (0.297s, 3445.90/s)  LR: 2.121e-04  Data: 0.023 (0.027)
Train: 209 [ 700/1251 ( 56%)]  Loss: 3.395 (3.16)  Time: 0.301s, 3400.20/s  (0.297s, 3446.30/s)  LR: 2.119e-04  Data: 0.024 (0.027)
Train: 209 [ 750/1251 ( 60%)]  Loss: 3.269 (3.16)  Time: 0.296s, 3461.50/s  (0.297s, 3445.77/s)  LR: 2.118e-04  Data: 0.022 (0.026)
Train: 209 [ 800/1251 ( 64%)]  Loss: 3.429 (3.18)  Time: 0.297s, 3443.90/s  (0.297s, 3445.14/s)  LR: 2.116e-04  Data: 0.022 (0.026)
Train: 209 [ 850/1251 ( 68%)]  Loss: 3.269 (3.18)  Time: 0.301s, 3398.52/s  (0.297s, 3444.36/s)  LR: 2.114e-04  Data: 0.022 (0.026)
Train: 209 [ 900/1251 ( 72%)]  Loss: 3.176 (3.18)  Time: 0.296s, 3459.37/s  (0.297s, 3443.65/s)  LR: 2.113e-04  Data: 0.027 (0.026)
Train: 209 [ 950/1251 ( 76%)]  Loss: 2.991 (3.17)  Time: 0.298s, 3439.48/s  (0.297s, 3443.45/s)  LR: 2.111e-04  Data: 0.024 (0.026)
Train: 209 [1000/1251 ( 80%)]  Loss: 2.878 (3.16)  Time: 0.300s, 3413.67/s  (0.297s, 3443.36/s)  LR: 2.109e-04  Data: 0.026 (0.026)
Train: 209 [1050/1251 ( 84%)]  Loss: 3.142 (3.16)  Time: 0.300s, 3417.62/s  (0.297s, 3442.52/s)  LR: 2.108e-04  Data: 0.025 (0.026)
Train: 209 [1100/1251 ( 88%)]  Loss: 3.070 (3.16)  Time: 0.297s, 3449.61/s  (0.298s, 3441.58/s)  LR: 2.106e-04  Data: 0.021 (0.025)
Train: 209 [1150/1251 ( 92%)]  Loss: 2.995 (3.15)  Time: 0.307s, 3340.43/s  (0.298s, 3440.57/s)  LR: 2.104e-04  Data: 0.022 (0.025)
Train: 209 [1200/1251 ( 96%)]  Loss: 2.761 (3.13)  Time: 0.299s, 3428.61/s  (0.298s, 3439.67/s)  LR: 2.102e-04  Data: 0.025 (0.025)
Train: 209 [1250/1251 (100%)]  Loss: 3.173 (3.13)  Time: 0.275s, 3719.62/s  (0.298s, 3440.84/s)  LR: 2.101e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.002 (2.002)  Loss:  0.5444 (0.5444)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.041 (0.241)  Loss:  0.6416 (0.9976)  Acc@1: 84.7877 (77.5400)  Acc@5: 96.6981 (93.7560)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-207.pth.tar', 77.56600000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-209.pth.tar', 77.53999998535156)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-205.pth.tar', 77.43399992919922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-206.pth.tar', 77.40400005371093)

Train: 210 [   0/1251 (  0%)]  Loss: 3.290 (3.29)  Time: 2.126s,  481.58/s  (2.126s,  481.58/s)  LR: 2.101e-04  Data: 1.908 (1.908)
Train: 210 [  50/1251 (  4%)]  Loss: 3.144 (3.22)  Time: 0.291s, 3516.45/s  (0.320s, 3195.90/s)  LR: 2.099e-04  Data: 0.030 (0.062)
Train: 210 [ 100/1251 (  8%)]  Loss: 3.495 (3.31)  Time: 0.302s, 3389.97/s  (0.307s, 3335.10/s)  LR: 2.097e-04  Data: 0.025 (0.043)
Train: 210 [ 150/1251 ( 12%)]  Loss: 2.981 (3.23)  Time: 0.300s, 3413.23/s  (0.303s, 3376.54/s)  LR: 2.096e-04  Data: 0.026 (0.037)
Train: 210 [ 200/1251 ( 16%)]  Loss: 3.064 (3.19)  Time: 0.298s, 3439.77/s  (0.302s, 3394.43/s)  LR: 2.094e-04  Data: 0.022 (0.033)
Train: 210 [ 250/1251 ( 20%)]  Loss: 3.168 (3.19)  Time: 0.303s, 3379.20/s  (0.301s, 3403.03/s)  LR: 2.092e-04  Data: 0.022 (0.031)
Train: 210 [ 300/1251 ( 24%)]  Loss: 3.235 (3.20)  Time: 0.301s, 3402.32/s  (0.300s, 3409.33/s)  LR: 2.091e-04  Data: 0.019 (0.030)
Train: 210 [ 350/1251 ( 28%)]  Loss: 3.295 (3.21)  Time: 0.298s, 3440.43/s  (0.300s, 3411.64/s)  LR: 2.089e-04  Data: 0.023 (0.029)
Train: 210 [ 400/1251 ( 32%)]  Loss: 3.266 (3.22)  Time: 0.301s, 3398.55/s  (0.300s, 3414.10/s)  LR: 2.087e-04  Data: 0.023 (0.028)
Train: 210 [ 450/1251 ( 36%)]  Loss: 3.103 (3.20)  Time: 0.309s, 3318.59/s  (0.300s, 3414.51/s)  LR: 2.086e-04  Data: 0.024 (0.028)
Train: 210 [ 500/1251 ( 40%)]  Loss: 3.219 (3.21)  Time: 0.295s, 3467.80/s  (0.300s, 3415.96/s)  LR: 2.084e-04  Data: 0.022 (0.027)
Train: 210 [ 550/1251 ( 44%)]  Loss: 3.061 (3.19)  Time: 0.300s, 3413.69/s  (0.300s, 3416.91/s)  LR: 2.082e-04  Data: 0.021 (0.027)
Train: 210 [ 600/1251 ( 48%)]  Loss: 3.082 (3.18)  Time: 0.299s, 3425.63/s  (0.300s, 3416.59/s)  LR: 2.081e-04  Data: 0.024 (0.027)
Train: 210 [ 650/1251 ( 52%)]  Loss: 3.057 (3.18)  Time: 0.303s, 3381.22/s  (0.300s, 3416.45/s)  LR: 2.079e-04  Data: 0.023 (0.027)
Train: 210 [ 700/1251 ( 56%)]  Loss: 2.932 (3.16)  Time: 0.300s, 3413.82/s  (0.300s, 3416.37/s)  LR: 2.077e-04  Data: 0.024 (0.026)
Train: 210 [ 750/1251 ( 60%)]  Loss: 2.908 (3.14)  Time: 0.304s, 3369.94/s  (0.300s, 3416.19/s)  LR: 2.076e-04  Data: 0.028 (0.026)
Train: 210 [ 800/1251 ( 64%)]  Loss: 3.261 (3.15)  Time: 0.301s, 3398.61/s  (0.300s, 3415.98/s)  LR: 2.074e-04  Data: 0.022 (0.026)
Train: 210 [ 850/1251 ( 68%)]  Loss: 3.007 (3.14)  Time: 0.296s, 3453.75/s  (0.300s, 3416.58/s)  LR: 2.072e-04  Data: 0.016 (0.026)
Train: 210 [ 900/1251 ( 72%)]  Loss: 3.323 (3.15)  Time: 0.300s, 3408.84/s  (0.300s, 3416.36/s)  LR: 2.070e-04  Data: 0.023 (0.026)
Train: 210 [ 950/1251 ( 76%)]  Loss: 2.900 (3.14)  Time: 0.301s, 3399.90/s  (0.300s, 3415.95/s)  LR: 2.069e-04  Data: 0.023 (0.026)
Train: 210 [1000/1251 ( 80%)]  Loss: 2.942 (3.13)  Time: 0.298s, 3434.83/s  (0.300s, 3415.95/s)  LR: 2.067e-04  Data: 0.021 (0.026)
Train: 210 [1050/1251 ( 84%)]  Loss: 2.729 (3.11)  Time: 0.305s, 3356.93/s  (0.300s, 3415.39/s)  LR: 2.065e-04  Data: 0.028 (0.025)
Train: 210 [1100/1251 ( 88%)]  Loss: 2.879 (3.10)  Time: 0.302s, 3391.05/s  (0.300s, 3414.40/s)  LR: 2.064e-04  Data: 0.026 (0.025)
Train: 210 [1150/1251 ( 92%)]  Loss: 3.095 (3.10)  Time: 0.302s, 3388.98/s  (0.300s, 3413.16/s)  LR: 2.062e-04  Data: 0.022 (0.025)
Train: 210 [1200/1251 ( 96%)]  Loss: 3.406 (3.11)  Time: 0.292s, 3506.42/s  (0.300s, 3412.73/s)  LR: 2.060e-04  Data: 0.021 (0.025)
Train: 210 [1250/1251 (100%)]  Loss: 3.098 (3.11)  Time: 0.276s, 3711.49/s  (0.300s, 3413.94/s)  LR: 2.059e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.229 (2.229)  Loss:  0.5259 (0.5259)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.055 (0.236)  Loss:  0.6274 (1.0009)  Acc@1: 85.1415 (77.6780)  Acc@5: 97.4057 (93.8520)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-210.pth.tar', 77.67800013916016)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-207.pth.tar', 77.56600000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-209.pth.tar', 77.53999998535156)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-205.pth.tar', 77.43399992919922)

Train: 211 [   0/1251 (  0%)]  Loss: 2.939 (2.94)  Time: 2.402s,  426.25/s  (2.402s,  426.25/s)  LR: 2.059e-04  Data: 2.186 (2.186)
Train: 211 [  50/1251 (  4%)]  Loss: 2.999 (2.97)  Time: 0.303s, 3379.65/s  (0.332s, 3084.47/s)  LR: 2.057e-04  Data: 0.026 (0.077)
Train: 211 [ 100/1251 (  8%)]  Loss: 3.084 (3.01)  Time: 0.290s, 3527.20/s  (0.313s, 3269.15/s)  LR: 2.055e-04  Data: 0.023 (0.051)
Train: 211 [ 150/1251 ( 12%)]  Loss: 3.536 (3.14)  Time: 0.294s, 3482.63/s  (0.307s, 3330.72/s)  LR: 2.054e-04  Data: 0.022 (0.042)
Train: 211 [ 200/1251 ( 16%)]  Loss: 3.030 (3.12)  Time: 0.291s, 3518.26/s  (0.305s, 3358.59/s)  LR: 2.052e-04  Data: 0.023 (0.037)
Train: 211 [ 250/1251 ( 20%)]  Loss: 2.777 (3.06)  Time: 0.297s, 3444.03/s  (0.304s, 3372.90/s)  LR: 2.050e-04  Data: 0.026 (0.034)
Train: 211 [ 300/1251 ( 24%)]  Loss: 2.953 (3.05)  Time: 0.301s, 3402.11/s  (0.303s, 3379.53/s)  LR: 2.049e-04  Data: 0.022 (0.033)
Train: 211 [ 350/1251 ( 28%)]  Loss: 3.261 (3.07)  Time: 0.301s, 3401.89/s  (0.303s, 3384.83/s)  LR: 2.047e-04  Data: 0.027 (0.031)
Train: 211 [ 400/1251 ( 32%)]  Loss: 3.111 (3.08)  Time: 0.298s, 3433.83/s  (0.302s, 3389.28/s)  LR: 2.045e-04  Data: 0.024 (0.030)
Train: 211 [ 450/1251 ( 36%)]  Loss: 3.350 (3.10)  Time: 0.305s, 3359.31/s  (0.302s, 3392.28/s)  LR: 2.044e-04  Data: 0.026 (0.029)
Train: 211 [ 500/1251 ( 40%)]  Loss: 2.962 (3.09)  Time: 0.302s, 3394.45/s  (0.302s, 3395.24/s)  LR: 2.042e-04  Data: 0.016 (0.029)
Train: 211 [ 550/1251 ( 44%)]  Loss: 3.348 (3.11)  Time: 0.295s, 3469.53/s  (0.301s, 3398.34/s)  LR: 2.040e-04  Data: 0.021 (0.028)
Train: 211 [ 600/1251 ( 48%)]  Loss: 3.023 (3.11)  Time: 0.295s, 3467.36/s  (0.301s, 3400.57/s)  LR: 2.039e-04  Data: 0.024 (0.028)
Train: 211 [ 650/1251 ( 52%)]  Loss: 3.144 (3.11)  Time: 0.303s, 3381.37/s  (0.301s, 3401.72/s)  LR: 2.037e-04  Data: 0.022 (0.028)
Train: 211 [ 700/1251 ( 56%)]  Loss: 2.785 (3.09)  Time: 0.301s, 3400.97/s  (0.301s, 3402.71/s)  LR: 2.035e-04  Data: 0.026 (0.027)
Train: 211 [ 750/1251 ( 60%)]  Loss: 2.996 (3.08)  Time: 0.305s, 3357.20/s  (0.301s, 3403.24/s)  LR: 2.034e-04  Data: 0.025 (0.027)
Train: 211 [ 800/1251 ( 64%)]  Loss: 3.337 (3.10)  Time: 0.301s, 3398.93/s  (0.301s, 3404.30/s)  LR: 2.032e-04  Data: 0.021 (0.027)
Train: 211 [ 850/1251 ( 68%)]  Loss: 2.840 (3.08)  Time: 0.303s, 3375.53/s  (0.301s, 3404.99/s)  LR: 2.030e-04  Data: 0.023 (0.027)
Train: 211 [ 900/1251 ( 72%)]  Loss: 2.915 (3.07)  Time: 0.301s, 3406.63/s  (0.301s, 3405.86/s)  LR: 2.029e-04  Data: 0.027 (0.026)
Train: 211 [ 950/1251 ( 76%)]  Loss: 3.000 (3.07)  Time: 0.300s, 3410.47/s  (0.301s, 3406.76/s)  LR: 2.027e-04  Data: 0.025 (0.026)
Train: 211 [1000/1251 ( 80%)]  Loss: 3.274 (3.08)  Time: 0.300s, 3414.56/s  (0.300s, 3408.09/s)  LR: 2.025e-04  Data: 0.025 (0.026)
Train: 211 [1050/1251 ( 84%)]  Loss: 3.378 (3.09)  Time: 0.292s, 3503.66/s  (0.300s, 3409.17/s)  LR: 2.024e-04  Data: 0.022 (0.026)
Train: 211 [1100/1251 ( 88%)]  Loss: 3.294 (3.10)  Time: 0.303s, 3381.53/s  (0.300s, 3409.68/s)  LR: 2.022e-04  Data: 0.023 (0.026)
Train: 211 [1150/1251 ( 92%)]  Loss: 2.943 (3.09)  Time: 0.304s, 3371.75/s  (0.300s, 3410.32/s)  LR: 2.020e-04  Data: 0.025 (0.026)
Train: 211 [1200/1251 ( 96%)]  Loss: 3.104 (3.10)  Time: 0.298s, 3435.59/s  (0.300s, 3410.46/s)  LR: 2.019e-04  Data: 0.023 (0.026)
Train: 211 [1250/1251 (100%)]  Loss: 3.307 (3.10)  Time: 0.269s, 3810.69/s  (0.300s, 3413.14/s)  LR: 2.017e-04  Data: 0.000 (0.026)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.064 (2.064)  Loss:  0.5146 (0.5146)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.052 (0.233)  Loss:  0.6270 (0.9989)  Acc@1: 85.7311 (77.7560)  Acc@5: 96.9340 (93.7700)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-211.pth.tar', 77.75600000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-210.pth.tar', 77.67800013916016)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-207.pth.tar', 77.56600000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-209.pth.tar', 77.53999998535156)

Train: 212 [   0/1251 (  0%)]  Loss: 2.982 (2.98)  Time: 2.400s,  426.58/s  (2.400s,  426.58/s)  LR: 2.017e-04  Data: 2.175 (2.175)
Train: 212 [  50/1251 (  4%)]  Loss: 2.674 (2.83)  Time: 0.294s, 3478.60/s  (0.324s, 3164.17/s)  LR: 2.015e-04  Data: 0.025 (0.067)
Train: 212 [ 100/1251 (  8%)]  Loss: 3.223 (2.96)  Time: 0.301s, 3404.05/s  (0.308s, 3322.60/s)  LR: 2.014e-04  Data: 0.026 (0.045)
Train: 212 [ 150/1251 ( 12%)]  Loss: 3.231 (3.03)  Time: 0.299s, 3421.48/s  (0.304s, 3366.94/s)  LR: 2.012e-04  Data: 0.023 (0.038)
Train: 212 [ 200/1251 ( 16%)]  Loss: 3.401 (3.10)  Time: 0.298s, 3431.63/s  (0.302s, 3385.93/s)  LR: 2.010e-04  Data: 0.025 (0.035)
Train: 212 [ 250/1251 ( 20%)]  Loss: 2.905 (3.07)  Time: 0.302s, 3385.29/s  (0.302s, 3395.65/s)  LR: 2.009e-04  Data: 0.021 (0.032)
Train: 212 [ 300/1251 ( 24%)]  Loss: 3.071 (3.07)  Time: 0.301s, 3404.68/s  (0.301s, 3400.81/s)  LR: 2.007e-04  Data: 0.023 (0.031)
Train: 212 [ 350/1251 ( 28%)]  Loss: 3.390 (3.11)  Time: 0.297s, 3448.23/s  (0.301s, 3407.23/s)  LR: 2.005e-04  Data: 0.022 (0.030)
Train: 212 [ 400/1251 ( 32%)]  Loss: 2.820 (3.08)  Time: 0.297s, 3451.63/s  (0.300s, 3410.48/s)  LR: 2.004e-04  Data: 0.023 (0.029)
Train: 212 [ 450/1251 ( 36%)]  Loss: 3.331 (3.10)  Time: 0.294s, 3481.06/s  (0.300s, 3413.37/s)  LR: 2.002e-04  Data: 0.023 (0.028)
Train: 212 [ 500/1251 ( 40%)]  Loss: 3.320 (3.12)  Time: 0.295s, 3466.98/s  (0.300s, 3415.67/s)  LR: 2.001e-04  Data: 0.025 (0.028)
Train: 212 [ 550/1251 ( 44%)]  Loss: 3.234 (3.13)  Time: 0.297s, 3450.36/s  (0.300s, 3416.32/s)  LR: 1.999e-04  Data: 0.024 (0.028)
Train: 212 [ 600/1251 ( 48%)]  Loss: 2.838 (3.11)  Time: 0.297s, 3450.96/s  (0.300s, 3417.42/s)  LR: 1.997e-04  Data: 0.022 (0.027)
Train: 212 [ 650/1251 ( 52%)]  Loss: 3.124 (3.11)  Time: 0.298s, 3432.81/s  (0.300s, 3417.36/s)  LR: 1.996e-04  Data: 0.022 (0.027)
Train: 212 [ 700/1251 ( 56%)]  Loss: 3.203 (3.12)  Time: 0.303s, 3375.53/s  (0.300s, 3417.59/s)  LR: 1.994e-04  Data: 0.024 (0.027)
Train: 212 [ 750/1251 ( 60%)]  Loss: 2.977 (3.11)  Time: 0.302s, 3385.57/s  (0.300s, 3418.27/s)  LR: 1.992e-04  Data: 0.026 (0.026)
Train: 212 [ 800/1251 ( 64%)]  Loss: 3.142 (3.11)  Time: 0.297s, 3448.33/s  (0.299s, 3419.23/s)  LR: 1.991e-04  Data: 0.023 (0.026)
Train: 212 [ 850/1251 ( 68%)]  Loss: 3.284 (3.12)  Time: 0.300s, 3408.32/s  (0.299s, 3420.01/s)  LR: 1.989e-04  Data: 0.023 (0.026)
Train: 212 [ 900/1251 ( 72%)]  Loss: 3.203 (3.12)  Time: 0.297s, 3445.51/s  (0.299s, 3420.18/s)  LR: 1.987e-04  Data: 0.022 (0.026)
Train: 212 [ 950/1251 ( 76%)]  Loss: 3.111 (3.12)  Time: 0.296s, 3459.19/s  (0.299s, 3420.76/s)  LR: 1.986e-04  Data: 0.022 (0.026)
Train: 212 [1000/1251 ( 80%)]  Loss: 2.680 (3.10)  Time: 0.298s, 3438.60/s  (0.299s, 3420.94/s)  LR: 1.984e-04  Data: 0.025 (0.026)
Train: 212 [1050/1251 ( 84%)]  Loss: 2.658 (3.08)  Time: 0.298s, 3432.10/s  (0.299s, 3421.36/s)  LR: 1.982e-04  Data: 0.022 (0.026)
Train: 212 [1100/1251 ( 88%)]  Loss: 3.103 (3.08)  Time: 0.297s, 3446.82/s  (0.299s, 3421.58/s)  LR: 1.981e-04  Data: 0.021 (0.026)
Train: 212 [1150/1251 ( 92%)]  Loss: 3.019 (3.08)  Time: 0.301s, 3404.26/s  (0.299s, 3421.05/s)  LR: 1.979e-04  Data: 0.026 (0.026)
Train: 212 [1200/1251 ( 96%)]  Loss: 3.271 (3.09)  Time: 0.300s, 3411.65/s  (0.299s, 3421.29/s)  LR: 1.977e-04  Data: 0.024 (0.025)
Train: 212 [1250/1251 (100%)]  Loss: 3.050 (3.09)  Time: 0.276s, 3715.31/s  (0.299s, 3423.30/s)  LR: 1.976e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.986 (1.986)  Loss:  0.5420 (0.5420)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.058 (0.236)  Loss:  0.6221 (1.0050)  Acc@1: 85.9670 (77.7340)  Acc@5: 96.9340 (93.9140)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-211.pth.tar', 77.75600000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-212.pth.tar', 77.73399998046875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-210.pth.tar', 77.67800013916016)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-207.pth.tar', 77.56600000732422)

Train: 213 [   0/1251 (  0%)]  Loss: 2.951 (2.95)  Time: 2.099s,  487.94/s  (2.099s,  487.94/s)  LR: 1.976e-04  Data: 1.861 (1.861)
Train: 213 [  50/1251 (  4%)]  Loss: 2.952 (2.95)  Time: 0.296s, 3462.69/s  (0.323s, 3173.91/s)  LR: 1.974e-04  Data: 0.023 (0.061)
Train: 213 [ 100/1251 (  8%)]  Loss: 3.018 (2.97)  Time: 0.292s, 3502.13/s  (0.308s, 3320.82/s)  LR: 1.972e-04  Data: 0.022 (0.043)
Train: 213 [ 150/1251 ( 12%)]  Loss: 2.977 (2.97)  Time: 0.301s, 3404.20/s  (0.304s, 3371.39/s)  LR: 1.971e-04  Data: 0.023 (0.036)
Train: 213 [ 200/1251 ( 16%)]  Loss: 3.344 (3.05)  Time: 0.297s, 3446.50/s  (0.302s, 3391.33/s)  LR: 1.969e-04  Data: 0.019 (0.033)
Train: 213 [ 250/1251 ( 20%)]  Loss: 3.208 (3.07)  Time: 0.297s, 3452.85/s  (0.301s, 3401.87/s)  LR: 1.968e-04  Data: 0.021 (0.031)
Train: 213 [ 300/1251 ( 24%)]  Loss: 3.041 (3.07)  Time: 0.301s, 3406.33/s  (0.300s, 3409.57/s)  LR: 1.966e-04  Data: 0.023 (0.030)
Train: 213 [ 350/1251 ( 28%)]  Loss: 2.988 (3.06)  Time: 0.295s, 3469.47/s  (0.300s, 3411.88/s)  LR: 1.964e-04  Data: 0.019 (0.029)
Train: 213 [ 400/1251 ( 32%)]  Loss: 3.281 (3.08)  Time: 0.297s, 3451.16/s  (0.300s, 3414.30/s)  LR: 1.963e-04  Data: 0.026 (0.028)
Train: 213 [ 450/1251 ( 36%)]  Loss: 3.172 (3.09)  Time: 0.296s, 3456.04/s  (0.300s, 3415.90/s)  LR: 1.961e-04  Data: 0.028 (0.028)
Train: 213 [ 500/1251 ( 40%)]  Loss: 2.957 (3.08)  Time: 0.298s, 3436.04/s  (0.300s, 3417.59/s)  LR: 1.959e-04  Data: 0.025 (0.027)
Train: 213 [ 550/1251 ( 44%)]  Loss: 3.093 (3.08)  Time: 0.300s, 3415.05/s  (0.300s, 3417.73/s)  LR: 1.958e-04  Data: 0.021 (0.027)
Train: 213 [ 600/1251 ( 48%)]  Loss: 3.293 (3.10)  Time: 0.300s, 3408.27/s  (0.300s, 3417.35/s)  LR: 1.956e-04  Data: 0.025 (0.027)
Train: 213 [ 650/1251 ( 52%)]  Loss: 3.029 (3.09)  Time: 0.307s, 3336.21/s  (0.300s, 3417.16/s)  LR: 1.954e-04  Data: 0.025 (0.026)
Train: 213 [ 700/1251 ( 56%)]  Loss: 2.919 (3.08)  Time: 0.297s, 3451.56/s  (0.300s, 3417.86/s)  LR: 1.953e-04  Data: 0.027 (0.026)
Train: 213 [ 750/1251 ( 60%)]  Loss: 2.949 (3.07)  Time: 0.302s, 3390.09/s  (0.300s, 3417.85/s)  LR: 1.951e-04  Data: 0.021 (0.026)
Train: 213 [ 800/1251 ( 64%)]  Loss: 3.491 (3.10)  Time: 0.300s, 3414.66/s  (0.300s, 3418.00/s)  LR: 1.949e-04  Data: 0.024 (0.026)
Train: 213 [ 850/1251 ( 68%)]  Loss: 3.149 (3.10)  Time: 0.298s, 3441.51/s  (0.300s, 3418.15/s)  LR: 1.948e-04  Data: 0.021 (0.026)
Train: 213 [ 900/1251 ( 72%)]  Loss: 2.998 (3.10)  Time: 0.297s, 3445.46/s  (0.300s, 3417.97/s)  LR: 1.946e-04  Data: 0.024 (0.026)
Train: 213 [ 950/1251 ( 76%)]  Loss: 3.211 (3.10)  Time: 0.303s, 3384.99/s  (0.300s, 3417.14/s)  LR: 1.945e-04  Data: 0.028 (0.026)
Train: 213 [1000/1251 ( 80%)]  Loss: 3.391 (3.11)  Time: 0.303s, 3377.18/s  (0.300s, 3416.42/s)  LR: 1.943e-04  Data: 0.024 (0.025)
Train: 213 [1050/1251 ( 84%)]  Loss: 2.755 (3.10)  Time: 0.300s, 3410.22/s  (0.300s, 3415.80/s)  LR: 1.941e-04  Data: 0.024 (0.025)
Train: 213 [1100/1251 ( 88%)]  Loss: 2.884 (3.09)  Time: 0.303s, 3379.15/s  (0.300s, 3415.27/s)  LR: 1.940e-04  Data: 0.025 (0.025)
Train: 213 [1150/1251 ( 92%)]  Loss: 3.082 (3.09)  Time: 0.297s, 3453.30/s  (0.300s, 3414.53/s)  LR: 1.938e-04  Data: 0.024 (0.025)
Train: 213 [1200/1251 ( 96%)]  Loss: 3.182 (3.09)  Time: 0.299s, 3428.28/s  (0.300s, 3413.79/s)  LR: 1.936e-04  Data: 0.020 (0.025)
Train: 213 [1250/1251 (100%)]  Loss: 3.021 (3.09)  Time: 0.276s, 3713.35/s  (0.300s, 3415.17/s)  LR: 1.935e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.147 (2.147)  Loss:  0.5166 (0.5166)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.041 (0.233)  Loss:  0.6113 (1.0011)  Acc@1: 86.4387 (77.7720)  Acc@5: 97.0519 (93.8460)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-213.pth.tar', 77.77200005615235)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-211.pth.tar', 77.75600000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-212.pth.tar', 77.73399998046875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-210.pth.tar', 77.67800013916016)

Train: 214 [   0/1251 (  0%)]  Loss: 3.246 (3.25)  Time: 2.160s,  474.11/s  (2.160s,  474.11/s)  LR: 1.935e-04  Data: 1.933 (1.933)
Train: 214 [  50/1251 (  4%)]  Loss: 3.088 (3.17)  Time: 0.289s, 3544.96/s  (0.322s, 3177.68/s)  LR: 1.933e-04  Data: 0.021 (0.063)
Train: 214 [ 100/1251 (  8%)]  Loss: 3.395 (3.24)  Time: 0.295s, 3465.77/s  (0.308s, 3326.15/s)  LR: 1.931e-04  Data: 0.022 (0.043)
Train: 214 [ 150/1251 ( 12%)]  Loss: 3.392 (3.28)  Time: 0.298s, 3430.58/s  (0.304s, 3370.63/s)  LR: 1.930e-04  Data: 0.021 (0.037)
Train: 214 [ 200/1251 ( 16%)]  Loss: 3.060 (3.24)  Time: 0.298s, 3435.25/s  (0.302s, 3390.36/s)  LR: 1.928e-04  Data: 0.025 (0.033)
Train: 214 [ 250/1251 ( 20%)]  Loss: 2.948 (3.19)  Time: 0.301s, 3398.10/s  (0.301s, 3396.54/s)  LR: 1.927e-04  Data: 0.020 (0.031)
Train: 214 [ 300/1251 ( 24%)]  Loss: 3.099 (3.18)  Time: 0.301s, 3404.31/s  (0.301s, 3400.24/s)  LR: 1.925e-04  Data: 0.022 (0.030)
Train: 214 [ 350/1251 ( 28%)]  Loss: 3.246 (3.18)  Time: 0.300s, 3412.19/s  (0.301s, 3402.29/s)  LR: 1.923e-04  Data: 0.023 (0.029)
Train: 214 [ 400/1251 ( 32%)]  Loss: 3.176 (3.18)  Time: 0.306s, 3348.34/s  (0.301s, 3404.09/s)  LR: 1.922e-04  Data: 0.023 (0.029)
Train: 214 [ 450/1251 ( 36%)]  Loss: 3.192 (3.18)  Time: 0.297s, 3452.28/s  (0.301s, 3405.69/s)  LR: 1.920e-04  Data: 0.023 (0.028)
Train: 214 [ 500/1251 ( 40%)]  Loss: 3.106 (3.18)  Time: 0.302s, 3396.21/s  (0.301s, 3406.98/s)  LR: 1.918e-04  Data: 0.026 (0.028)
Train: 214 [ 550/1251 ( 44%)]  Loss: 3.180 (3.18)  Time: 0.299s, 3423.28/s  (0.300s, 3407.70/s)  LR: 1.917e-04  Data: 0.017 (0.027)
Train: 214 [ 600/1251 ( 48%)]  Loss: 3.176 (3.18)  Time: 0.305s, 3354.99/s  (0.300s, 3408.03/s)  LR: 1.915e-04  Data: 0.023 (0.027)
Train: 214 [ 650/1251 ( 52%)]  Loss: 3.099 (3.17)  Time: 0.296s, 3457.59/s  (0.300s, 3408.11/s)  LR: 1.914e-04  Data: 0.027 (0.027)
Train: 214 [ 700/1251 ( 56%)]  Loss: 3.401 (3.19)  Time: 0.298s, 3433.66/s  (0.300s, 3408.34/s)  LR: 1.912e-04  Data: 0.022 (0.026)
Train: 214 [ 750/1251 ( 60%)]  Loss: 2.949 (3.17)  Time: 0.301s, 3403.69/s  (0.300s, 3409.15/s)  LR: 1.910e-04  Data: 0.024 (0.026)
Train: 214 [ 800/1251 ( 64%)]  Loss: 3.258 (3.18)  Time: 0.299s, 3429.76/s  (0.300s, 3409.04/s)  LR: 1.909e-04  Data: 0.026 (0.026)
Train: 214 [ 850/1251 ( 68%)]  Loss: 3.098 (3.17)  Time: 0.301s, 3399.38/s  (0.300s, 3408.67/s)  LR: 1.907e-04  Data: 0.023 (0.026)
Train: 214 [ 900/1251 ( 72%)]  Loss: 2.979 (3.16)  Time: 0.299s, 3430.41/s  (0.300s, 3408.84/s)  LR: 1.905e-04  Data: 0.022 (0.026)
Train: 214 [ 950/1251 ( 76%)]  Loss: 3.016 (3.16)  Time: 0.304s, 3368.55/s  (0.300s, 3408.60/s)  LR: 1.904e-04  Data: 0.018 (0.026)
Train: 214 [1000/1251 ( 80%)]  Loss: 3.031 (3.15)  Time: 0.298s, 3431.27/s  (0.300s, 3408.67/s)  LR: 1.902e-04  Data: 0.018 (0.026)
Train: 214 [1050/1251 ( 84%)]  Loss: 3.151 (3.15)  Time: 0.301s, 3400.35/s  (0.300s, 3408.83/s)  LR: 1.901e-04  Data: 0.022 (0.026)
Train: 214 [1100/1251 ( 88%)]  Loss: 3.331 (3.16)  Time: 0.303s, 3383.09/s  (0.300s, 3408.80/s)  LR: 1.899e-04  Data: 0.023 (0.025)
Train: 214 [1150/1251 ( 92%)]  Loss: 2.750 (3.14)  Time: 0.299s, 3426.13/s  (0.300s, 3408.66/s)  LR: 1.897e-04  Data: 0.021 (0.025)
Train: 214 [1200/1251 ( 96%)]  Loss: 2.890 (3.13)  Time: 0.302s, 3393.73/s  (0.300s, 3408.27/s)  LR: 1.896e-04  Data: 0.024 (0.025)
Train: 214 [1250/1251 (100%)]  Loss: 2.783 (3.12)  Time: 0.275s, 3722.84/s  (0.300s, 3409.74/s)  LR: 1.894e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.089 (2.089)  Loss:  0.5190 (0.5190)  Acc@1: 90.1367 (90.1367)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.052 (0.237)  Loss:  0.6455 (0.9958)  Acc@1: 85.7311 (77.6140)  Acc@5: 96.5802 (94.0060)
Train: 215 [   0/1251 (  0%)]  Loss: 3.115 (3.11)  Time: 1.945s,  526.52/s  (1.945s,  526.52/s)  LR: 1.894e-04  Data: 1.719 (1.719)
Train: 215 [  50/1251 (  4%)]  Loss: 3.327 (3.22)  Time: 0.297s, 3451.66/s  (0.321s, 3192.13/s)  LR: 1.892e-04  Data: 0.025 (0.058)
Train: 215 [ 100/1251 (  8%)]  Loss: 3.235 (3.23)  Time: 0.299s, 3424.61/s  (0.308s, 3329.16/s)  LR: 1.891e-04  Data: 0.022 (0.041)
Train: 215 [ 150/1251 ( 12%)]  Loss: 3.439 (3.28)  Time: 0.297s, 3443.37/s  (0.304s, 3363.81/s)  LR: 1.889e-04  Data: 0.021 (0.035)
Train: 215 [ 200/1251 ( 16%)]  Loss: 2.912 (3.21)  Time: 0.296s, 3456.68/s  (0.303s, 3377.84/s)  LR: 1.888e-04  Data: 0.023 (0.032)
Train: 215 [ 250/1251 ( 20%)]  Loss: 3.201 (3.20)  Time: 0.301s, 3396.61/s  (0.303s, 3384.53/s)  LR: 1.886e-04  Data: 0.022 (0.030)
Train: 215 [ 300/1251 ( 24%)]  Loss: 3.052 (3.18)  Time: 0.298s, 3440.02/s  (0.302s, 3388.57/s)  LR: 1.884e-04  Data: 0.021 (0.029)
Train: 215 [ 350/1251 ( 28%)]  Loss: 3.147 (3.18)  Time: 0.303s, 3378.72/s  (0.302s, 3389.73/s)  LR: 1.883e-04  Data: 0.022 (0.028)
Train: 215 [ 400/1251 ( 32%)]  Loss: 3.216 (3.18)  Time: 0.303s, 3377.10/s  (0.302s, 3391.28/s)  LR: 1.881e-04  Data: 0.026 (0.028)
Train: 215 [ 450/1251 ( 36%)]  Loss: 3.292 (3.19)  Time: 0.298s, 3431.36/s  (0.302s, 3392.55/s)  LR: 1.880e-04  Data: 0.025 (0.027)
Train: 215 [ 500/1251 ( 40%)]  Loss: 3.277 (3.20)  Time: 0.303s, 3375.87/s  (0.302s, 3393.27/s)  LR: 1.878e-04  Data: 0.022 (0.027)
Train: 215 [ 550/1251 ( 44%)]  Loss: 2.879 (3.17)  Time: 0.305s, 3354.80/s  (0.302s, 3393.55/s)  LR: 1.876e-04  Data: 0.027 (0.027)
Train: 215 [ 600/1251 ( 48%)]  Loss: 3.178 (3.17)  Time: 0.300s, 3408.50/s  (0.302s, 3393.45/s)  LR: 1.875e-04  Data: 0.025 (0.026)
Train: 215 [ 650/1251 ( 52%)]  Loss: 3.147 (3.17)  Time: 0.307s, 3334.44/s  (0.302s, 3392.83/s)  LR: 1.873e-04  Data: 0.026 (0.026)
Train: 215 [ 700/1251 ( 56%)]  Loss: 2.679 (3.14)  Time: 0.301s, 3402.69/s  (0.302s, 3392.36/s)  LR: 1.871e-04  Data: 0.021 (0.026)
Train: 215 [ 750/1251 ( 60%)]  Loss: 2.900 (3.12)  Time: 0.307s, 3331.60/s  (0.302s, 3391.77/s)  LR: 1.870e-04  Data: 0.024 (0.026)
Train: 215 [ 800/1251 ( 64%)]  Loss: 3.126 (3.12)  Time: 0.306s, 3346.54/s  (0.302s, 3391.43/s)  LR: 1.868e-04  Data: 0.024 (0.026)
Train: 215 [ 850/1251 ( 68%)]  Loss: 3.270 (3.13)  Time: 0.305s, 3356.36/s  (0.302s, 3390.72/s)  LR: 1.867e-04  Data: 0.021 (0.025)
Train: 215 [ 900/1251 ( 72%)]  Loss: 3.306 (3.14)  Time: 0.309s, 3316.38/s  (0.302s, 3389.73/s)  LR: 1.865e-04  Data: 0.028 (0.025)
Train: 215 [ 950/1251 ( 76%)]  Loss: 2.970 (3.13)  Time: 0.304s, 3364.34/s  (0.302s, 3388.45/s)  LR: 1.863e-04  Data: 0.023 (0.025)
Train: 215 [1000/1251 ( 80%)]  Loss: 2.813 (3.12)  Time: 0.302s, 3392.75/s  (0.302s, 3387.78/s)  LR: 1.862e-04  Data: 0.028 (0.025)
Train: 215 [1050/1251 ( 84%)]  Loss: 3.225 (3.12)  Time: 0.307s, 3330.40/s  (0.302s, 3387.18/s)  LR: 1.860e-04  Data: 0.022 (0.025)
Train: 215 [1100/1251 ( 88%)]  Loss: 3.271 (3.13)  Time: 0.302s, 3395.67/s  (0.302s, 3386.34/s)  LR: 1.859e-04  Data: 0.021 (0.025)
Train: 215 [1150/1251 ( 92%)]  Loss: 3.364 (3.14)  Time: 0.305s, 3352.06/s  (0.302s, 3385.55/s)  LR: 1.857e-04  Data: 0.023 (0.025)
Train: 215 [1200/1251 ( 96%)]  Loss: 3.065 (3.14)  Time: 0.306s, 3341.72/s  (0.303s, 3384.80/s)  LR: 1.855e-04  Data: 0.023 (0.025)
Train: 215 [1250/1251 (100%)]  Loss: 3.493 (3.15)  Time: 0.277s, 3701.90/s  (0.302s, 3385.81/s)  LR: 1.854e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.046 (2.046)  Loss:  0.5010 (0.5010)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.040 (0.234)  Loss:  0.6294 (0.9737)  Acc@1: 85.1415 (77.8760)  Acc@5: 97.1698 (94.0420)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-215.pth.tar', 77.87600013916015)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-213.pth.tar', 77.77200005615235)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-211.pth.tar', 77.75600000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-212.pth.tar', 77.73399998046875)

Train: 216 [   0/1251 (  0%)]  Loss: 3.190 (3.19)  Time: 2.314s,  442.55/s  (2.314s,  442.55/s)  LR: 1.854e-04  Data: 2.094 (2.094)
Train: 216 [  50/1251 (  4%)]  Loss: 3.244 (3.22)  Time: 0.291s, 3520.04/s  (0.324s, 3160.44/s)  LR: 1.852e-04  Data: 0.023 (0.065)
Train: 216 [ 100/1251 (  8%)]  Loss: 3.308 (3.25)  Time: 0.294s, 3486.58/s  (0.310s, 3298.37/s)  LR: 1.851e-04  Data: 0.021 (0.045)
Train: 216 [ 150/1251 ( 12%)]  Loss: 3.352 (3.27)  Time: 0.304s, 3367.18/s  (0.307s, 3336.33/s)  LR: 1.849e-04  Data: 0.024 (0.037)
Train: 216 [ 200/1251 ( 16%)]  Loss: 3.031 (3.23)  Time: 0.292s, 3510.95/s  (0.306s, 3351.53/s)  LR: 1.847e-04  Data: 0.021 (0.034)
Train: 216 [ 250/1251 ( 20%)]  Loss: 3.287 (3.24)  Time: 0.303s, 3375.35/s  (0.305s, 3360.26/s)  LR: 1.846e-04  Data: 0.022 (0.032)
Train: 216 [ 300/1251 ( 24%)]  Loss: 3.253 (3.24)  Time: 0.303s, 3383.37/s  (0.305s, 3362.25/s)  LR: 1.844e-04  Data: 0.025 (0.030)
Train: 216 [ 350/1251 ( 28%)]  Loss: 3.199 (3.23)  Time: 0.302s, 3387.08/s  (0.304s, 3366.92/s)  LR: 1.843e-04  Data: 0.021 (0.030)
Train: 216 [ 400/1251 ( 32%)]  Loss: 3.184 (3.23)  Time: 0.299s, 3422.38/s  (0.304s, 3370.24/s)  LR: 1.841e-04  Data: 0.027 (0.029)
Train: 216 [ 450/1251 ( 36%)]  Loss: 3.269 (3.23)  Time: 0.299s, 3420.86/s  (0.304s, 3371.00/s)  LR: 1.839e-04  Data: 0.021 (0.028)
Train: 216 [ 500/1251 ( 40%)]  Loss: 3.137 (3.22)  Time: 0.299s, 3424.80/s  (0.304s, 3371.62/s)  LR: 1.838e-04  Data: 0.025 (0.028)
Train: 216 [ 550/1251 ( 44%)]  Loss: 3.162 (3.22)  Time: 0.296s, 3459.79/s  (0.304s, 3373.24/s)  LR: 1.836e-04  Data: 0.024 (0.027)
Train: 216 [ 600/1251 ( 48%)]  Loss: 3.212 (3.22)  Time: 0.303s, 3378.41/s  (0.303s, 3374.10/s)  LR: 1.835e-04  Data: 0.022 (0.027)
Train: 216 [ 650/1251 ( 52%)]  Loss: 3.283 (3.22)  Time: 0.304s, 3363.90/s  (0.303s, 3375.51/s)  LR: 1.833e-04  Data: 0.023 (0.027)
Train: 216 [ 700/1251 ( 56%)]  Loss: 2.914 (3.20)  Time: 0.304s, 3368.78/s  (0.303s, 3375.37/s)  LR: 1.831e-04  Data: 0.027 (0.027)
Train: 216 [ 750/1251 ( 60%)]  Loss: 3.169 (3.20)  Time: 0.304s, 3370.81/s  (0.303s, 3375.62/s)  LR: 1.830e-04  Data: 0.021 (0.026)
Train: 216 [ 800/1251 ( 64%)]  Loss: 3.055 (3.19)  Time: 0.299s, 3429.36/s  (0.303s, 3376.11/s)  LR: 1.828e-04  Data: 0.025 (0.026)
Train: 216 [ 850/1251 ( 68%)]  Loss: 3.113 (3.19)  Time: 0.307s, 3337.10/s  (0.303s, 3375.87/s)  LR: 1.827e-04  Data: 0.023 (0.026)
Train: 216 [ 900/1251 ( 72%)]  Loss: 2.974 (3.18)  Time: 0.306s, 3351.07/s  (0.303s, 3375.29/s)  LR: 1.825e-04  Data: 0.024 (0.026)
Train: 216 [ 950/1251 ( 76%)]  Loss: 3.200 (3.18)  Time: 0.304s, 3368.06/s  (0.303s, 3375.27/s)  LR: 1.823e-04  Data: 0.023 (0.026)
Train: 216 [1000/1251 ( 80%)]  Loss: 3.019 (3.17)  Time: 0.304s, 3370.84/s  (0.303s, 3374.61/s)  LR: 1.822e-04  Data: 0.024 (0.026)
Train: 216 [1050/1251 ( 84%)]  Loss: 2.612 (3.14)  Time: 0.310s, 3298.48/s  (0.304s, 3373.94/s)  LR: 1.820e-04  Data: 0.021 (0.026)
Train: 216 [1100/1251 ( 88%)]  Loss: 3.268 (3.15)  Time: 0.306s, 3345.78/s  (0.304s, 3373.25/s)  LR: 1.819e-04  Data: 0.022 (0.025)
Train: 216 [1150/1251 ( 92%)]  Loss: 3.366 (3.16)  Time: 0.306s, 3351.40/s  (0.304s, 3373.03/s)  LR: 1.817e-04  Data: 0.025 (0.025)
Train: 216 [1200/1251 ( 96%)]  Loss: 3.086 (3.16)  Time: 0.304s, 3368.28/s  (0.304s, 3372.49/s)  LR: 1.815e-04  Data: 0.021 (0.025)
Train: 216 [1250/1251 (100%)]  Loss: 3.138 (3.15)  Time: 0.277s, 3693.12/s  (0.303s, 3374.13/s)  LR: 1.814e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.035 (2.035)  Loss:  0.5254 (0.5254)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.059 (0.234)  Loss:  0.6030 (0.9840)  Acc@1: 85.3774 (77.9560)  Acc@5: 96.8160 (93.9520)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-216.pth.tar', 77.9560001123047)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-215.pth.tar', 77.87600013916015)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-213.pth.tar', 77.77200005615235)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-211.pth.tar', 77.75600000732422)

Train: 217 [   0/1251 (  0%)]  Loss: 3.173 (3.17)  Time: 2.204s,  464.61/s  (2.204s,  464.61/s)  LR: 1.814e-04  Data: 1.965 (1.965)
Train: 217 [  50/1251 (  4%)]  Loss: 3.074 (3.12)  Time: 0.296s, 3457.07/s  (0.323s, 3173.22/s)  LR: 1.812e-04  Data: 0.022 (0.065)
Train: 217 [ 100/1251 (  8%)]  Loss: 3.091 (3.11)  Time: 0.301s, 3400.82/s  (0.310s, 3300.09/s)  LR: 1.811e-04  Data: 0.023 (0.044)
Train: 217 [ 150/1251 ( 12%)]  Loss: 2.903 (3.06)  Time: 0.303s, 3381.16/s  (0.307s, 3332.21/s)  LR: 1.809e-04  Data: 0.027 (0.037)
Train: 217 [ 200/1251 ( 16%)]  Loss: 3.047 (3.06)  Time: 0.298s, 3440.73/s  (0.306s, 3346.36/s)  LR: 1.807e-04  Data: 0.023 (0.034)
Train: 217 [ 250/1251 ( 20%)]  Loss: 2.926 (3.04)  Time: 0.303s, 3378.24/s  (0.305s, 3352.29/s)  LR: 1.806e-04  Data: 0.026 (0.032)
Train: 217 [ 300/1251 ( 24%)]  Loss: 2.898 (3.02)  Time: 0.310s, 3303.01/s  (0.305s, 3354.10/s)  LR: 1.804e-04  Data: 0.024 (0.031)
Train: 217 [ 350/1251 ( 28%)]  Loss: 3.108 (3.03)  Time: 0.306s, 3347.38/s  (0.305s, 3356.13/s)  LR: 1.803e-04  Data: 0.023 (0.030)
Train: 217 [ 400/1251 ( 32%)]  Loss: 3.229 (3.05)  Time: 0.307s, 3340.05/s  (0.305s, 3355.84/s)  LR: 1.801e-04  Data: 0.024 (0.029)
Train: 217 [ 450/1251 ( 36%)]  Loss: 3.041 (3.05)  Time: 0.309s, 3316.11/s  (0.305s, 3355.95/s)  LR: 1.800e-04  Data: 0.024 (0.028)
Train: 217 [ 500/1251 ( 40%)]  Loss: 3.171 (3.06)  Time: 0.303s, 3383.92/s  (0.305s, 3356.34/s)  LR: 1.798e-04  Data: 0.025 (0.028)
Train: 217 [ 550/1251 ( 44%)]  Loss: 3.203 (3.07)  Time: 0.305s, 3356.15/s  (0.305s, 3355.70/s)  LR: 1.796e-04  Data: 0.023 (0.027)
Train: 217 [ 600/1251 ( 48%)]  Loss: 3.542 (3.11)  Time: 0.306s, 3346.03/s  (0.305s, 3355.95/s)  LR: 1.795e-04  Data: 0.024 (0.027)
Train: 217 [ 650/1251 ( 52%)]  Loss: 3.047 (3.10)  Time: 0.308s, 3326.27/s  (0.305s, 3355.43/s)  LR: 1.793e-04  Data: 0.021 (0.027)
Train: 217 [ 700/1251 ( 56%)]  Loss: 3.034 (3.10)  Time: 0.312s, 3281.43/s  (0.305s, 3355.30/s)  LR: 1.792e-04  Data: 0.026 (0.026)
Train: 217 [ 750/1251 ( 60%)]  Loss: 3.123 (3.10)  Time: 0.306s, 3348.75/s  (0.305s, 3354.76/s)  LR: 1.790e-04  Data: 0.024 (0.026)
Train: 217 [ 800/1251 ( 64%)]  Loss: 3.105 (3.10)  Time: 0.307s, 3334.41/s  (0.305s, 3354.14/s)  LR: 1.788e-04  Data: 0.022 (0.026)
Train: 217 [ 850/1251 ( 68%)]  Loss: 3.364 (3.12)  Time: 0.305s, 3362.87/s  (0.305s, 3354.02/s)  LR: 1.787e-04  Data: 0.026 (0.026)
Train: 217 [ 900/1251 ( 72%)]  Loss: 3.015 (3.11)  Time: 0.305s, 3361.17/s  (0.305s, 3352.77/s)  LR: 1.785e-04  Data: 0.023 (0.026)
Train: 217 [ 950/1251 ( 76%)]  Loss: 2.795 (3.09)  Time: 0.306s, 3349.47/s  (0.305s, 3352.47/s)  LR: 1.784e-04  Data: 0.022 (0.026)
Train: 217 [1000/1251 ( 80%)]  Loss: 3.198 (3.10)  Time: 0.305s, 3361.29/s  (0.305s, 3352.23/s)  LR: 1.782e-04  Data: 0.022 (0.026)
Train: 217 [1050/1251 ( 84%)]  Loss: 3.051 (3.10)  Time: 0.307s, 3333.37/s  (0.305s, 3352.05/s)  LR: 1.781e-04  Data: 0.023 (0.026)
Train: 217 [1100/1251 ( 88%)]  Loss: 3.323 (3.11)  Time: 0.311s, 3296.76/s  (0.306s, 3351.59/s)  LR: 1.779e-04  Data: 0.022 (0.025)
Train: 217 [1150/1251 ( 92%)]  Loss: 3.223 (3.11)  Time: 0.303s, 3380.65/s  (0.306s, 3350.85/s)  LR: 1.777e-04  Data: 0.024 (0.025)
Train: 217 [1200/1251 ( 96%)]  Loss: 3.001 (3.11)  Time: 0.305s, 3355.84/s  (0.306s, 3350.46/s)  LR: 1.776e-04  Data: 0.023 (0.025)
Train: 217 [1250/1251 (100%)]  Loss: 3.089 (3.11)  Time: 0.276s, 3705.17/s  (0.305s, 3352.07/s)  LR: 1.774e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.062 (2.062)  Loss:  0.4966 (0.4966)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.046 (0.234)  Loss:  0.6113 (0.9724)  Acc@1: 85.6132 (77.9800)  Acc@5: 96.6981 (93.9940)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-217.pth.tar', 77.98000008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-216.pth.tar', 77.9560001123047)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-215.pth.tar', 77.87600013916015)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-213.pth.tar', 77.77200005615235)

Train: 218 [   0/1251 (  0%)]  Loss: 3.039 (3.04)  Time: 2.283s,  448.62/s  (2.283s,  448.62/s)  LR: 1.774e-04  Data: 2.059 (2.059)
Train: 218 [  50/1251 (  4%)]  Loss: 2.985 (3.01)  Time: 0.298s, 3437.92/s  (0.327s, 3128.31/s)  LR: 1.773e-04  Data: 0.022 (0.065)
Train: 218 [ 100/1251 (  8%)]  Loss: 2.942 (2.99)  Time: 0.302s, 3390.10/s  (0.314s, 3264.85/s)  LR: 1.771e-04  Data: 0.022 (0.044)
Train: 218 [ 150/1251 ( 12%)]  Loss: 2.814 (2.95)  Time: 0.303s, 3376.46/s  (0.310s, 3302.59/s)  LR: 1.769e-04  Data: 0.024 (0.037)
Train: 218 [ 200/1251 ( 16%)]  Loss: 2.936 (2.94)  Time: 0.307s, 3339.48/s  (0.308s, 3320.10/s)  LR: 1.768e-04  Data: 0.024 (0.034)
Train: 218 [ 250/1251 ( 20%)]  Loss: 3.352 (3.01)  Time: 0.305s, 3357.04/s  (0.308s, 3324.98/s)  LR: 1.766e-04  Data: 0.023 (0.032)
Train: 218 [ 300/1251 ( 24%)]  Loss: 3.062 (3.02)  Time: 0.304s, 3363.73/s  (0.308s, 3327.92/s)  LR: 1.765e-04  Data: 0.022 (0.031)
Train: 218 [ 350/1251 ( 28%)]  Loss: 3.271 (3.05)  Time: 0.309s, 3317.90/s  (0.308s, 3330.00/s)  LR: 1.763e-04  Data: 0.026 (0.030)
Train: 218 [ 400/1251 ( 32%)]  Loss: 3.176 (3.06)  Time: 0.306s, 3346.49/s  (0.307s, 3331.42/s)  LR: 1.762e-04  Data: 0.025 (0.029)
Train: 218 [ 450/1251 ( 36%)]  Loss: 3.077 (3.07)  Time: 0.307s, 3335.25/s  (0.307s, 3332.03/s)  LR: 1.760e-04  Data: 0.025 (0.028)
Train: 218 [ 500/1251 ( 40%)]  Loss: 2.907 (3.05)  Time: 0.303s, 3381.47/s  (0.307s, 3332.15/s)  LR: 1.758e-04  Data: 0.022 (0.028)
Train: 218 [ 550/1251 ( 44%)]  Loss: 3.388 (3.08)  Time: 0.306s, 3344.20/s  (0.307s, 3332.69/s)  LR: 1.757e-04  Data: 0.025 (0.027)
Train: 218 [ 600/1251 ( 48%)]  Loss: 3.080 (3.08)  Time: 0.311s, 3287.92/s  (0.307s, 3333.14/s)  LR: 1.755e-04  Data: 0.024 (0.027)
Train: 218 [ 650/1251 ( 52%)]  Loss: 3.069 (3.08)  Time: 0.310s, 3306.79/s  (0.307s, 3334.23/s)  LR: 1.754e-04  Data: 0.023 (0.027)
Train: 218 [ 700/1251 ( 56%)]  Loss: 2.874 (3.06)  Time: 0.310s, 3303.40/s  (0.307s, 3334.99/s)  LR: 1.752e-04  Data: 0.023 (0.027)
Train: 218 [ 750/1251 ( 60%)]  Loss: 3.168 (3.07)  Time: 0.309s, 3314.42/s  (0.307s, 3335.79/s)  LR: 1.751e-04  Data: 0.022 (0.026)
Train: 218 [ 800/1251 ( 64%)]  Loss: 3.134 (3.07)  Time: 0.303s, 3374.27/s  (0.307s, 3336.15/s)  LR: 1.749e-04  Data: 0.023 (0.026)
Train: 218 [ 850/1251 ( 68%)]  Loss: 2.938 (3.07)  Time: 0.306s, 3350.63/s  (0.307s, 3336.27/s)  LR: 1.747e-04  Data: 0.027 (0.026)
Train: 218 [ 900/1251 ( 72%)]  Loss: 3.177 (3.07)  Time: 0.310s, 3301.02/s  (0.307s, 3336.21/s)  LR: 1.746e-04  Data: 0.025 (0.026)
Train: 218 [ 950/1251 ( 76%)]  Loss: 2.592 (3.05)  Time: 0.310s, 3302.71/s  (0.307s, 3336.20/s)  LR: 1.744e-04  Data: 0.026 (0.026)
Train: 218 [1000/1251 ( 80%)]  Loss: 3.060 (3.05)  Time: 0.306s, 3347.15/s  (0.307s, 3336.27/s)  LR: 1.743e-04  Data: 0.020 (0.026)
Train: 218 [1050/1251 ( 84%)]  Loss: 3.187 (3.06)  Time: 0.307s, 3330.71/s  (0.307s, 3336.38/s)  LR: 1.741e-04  Data: 0.023 (0.026)
Train: 218 [1100/1251 ( 88%)]  Loss: 2.924 (3.05)  Time: 0.307s, 3339.05/s  (0.307s, 3336.83/s)  LR: 1.740e-04  Data: 0.023 (0.025)
Train: 218 [1150/1251 ( 92%)]  Loss: 2.917 (3.04)  Time: 0.309s, 3312.69/s  (0.307s, 3336.86/s)  LR: 1.738e-04  Data: 0.020 (0.025)
Train: 218 [1200/1251 ( 96%)]  Loss: 2.981 (3.04)  Time: 0.307s, 3337.44/s  (0.307s, 3336.98/s)  LR: 1.737e-04  Data: 0.027 (0.025)
Train: 218 [1250/1251 (100%)]  Loss: 3.113 (3.04)  Time: 0.276s, 3709.45/s  (0.307s, 3338.74/s)  LR: 1.735e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.985 (1.985)  Loss:  0.5039 (0.5039)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.059 (0.237)  Loss:  0.6538 (0.9938)  Acc@1: 85.1415 (77.9020)  Acc@5: 97.0519 (93.9660)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-217.pth.tar', 77.98000008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-216.pth.tar', 77.9560001123047)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-218.pth.tar', 77.90200000976563)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-215.pth.tar', 77.87600013916015)

Train: 219 [   0/1251 (  0%)]  Loss: 3.085 (3.08)  Time: 2.306s,  444.15/s  (2.306s,  444.15/s)  LR: 1.735e-04  Data: 2.084 (2.084)
Train: 219 [  50/1251 (  4%)]  Loss: 3.440 (3.26)  Time: 0.302s, 3393.66/s  (0.327s, 3132.66/s)  LR: 1.733e-04  Data: 0.021 (0.065)
Train: 219 [ 100/1251 (  8%)]  Loss: 3.317 (3.28)  Time: 0.299s, 3430.13/s  (0.313s, 3268.12/s)  LR: 1.732e-04  Data: 0.026 (0.045)
Train: 219 [ 150/1251 ( 12%)]  Loss: 3.239 (3.27)  Time: 0.303s, 3382.11/s  (0.310s, 3303.20/s)  LR: 1.730e-04  Data: 0.022 (0.037)
Train: 219 [ 200/1251 ( 16%)]  Loss: 3.189 (3.25)  Time: 0.308s, 3324.81/s  (0.309s, 3318.11/s)  LR: 1.729e-04  Data: 0.024 (0.034)
Train: 219 [ 250/1251 ( 20%)]  Loss: 3.322 (3.27)  Time: 0.310s, 3303.13/s  (0.308s, 3325.22/s)  LR: 1.727e-04  Data: 0.021 (0.032)
Train: 219 [ 300/1251 ( 24%)]  Loss: 2.983 (3.23)  Time: 0.304s, 3366.86/s  (0.307s, 3330.88/s)  LR: 1.726e-04  Data: 0.023 (0.030)
Train: 219 [ 350/1251 ( 28%)]  Loss: 2.938 (3.19)  Time: 0.312s, 3283.28/s  (0.307s, 3334.72/s)  LR: 1.724e-04  Data: 0.028 (0.029)
Train: 219 [ 400/1251 ( 32%)]  Loss: 3.188 (3.19)  Time: 0.307s, 3331.37/s  (0.307s, 3336.23/s)  LR: 1.722e-04  Data: 0.023 (0.029)
Train: 219 [ 450/1251 ( 36%)]  Loss: 3.028 (3.17)  Time: 0.308s, 3325.98/s  (0.307s, 3337.24/s)  LR: 1.721e-04  Data: 0.026 (0.028)
Train: 219 [ 500/1251 ( 40%)]  Loss: 3.077 (3.16)  Time: 0.301s, 3406.79/s  (0.307s, 3338.88/s)  LR: 1.719e-04  Data: 0.029 (0.028)
Train: 219 [ 550/1251 ( 44%)]  Loss: 3.272 (3.17)  Time: 0.307s, 3333.43/s  (0.307s, 3339.75/s)  LR: 1.718e-04  Data: 0.023 (0.027)
Train: 219 [ 600/1251 ( 48%)]  Loss: 3.200 (3.18)  Time: 0.306s, 3350.10/s  (0.307s, 3340.61/s)  LR: 1.716e-04  Data: 0.023 (0.027)
Train: 219 [ 650/1251 ( 52%)]  Loss: 3.180 (3.18)  Time: 0.313s, 3269.97/s  (0.307s, 3340.04/s)  LR: 1.715e-04  Data: 0.026 (0.027)
Train: 219 [ 700/1251 ( 56%)]  Loss: 2.907 (3.16)  Time: 0.307s, 3334.52/s  (0.307s, 3340.19/s)  LR: 1.713e-04  Data: 0.023 (0.026)
Train: 219 [ 750/1251 ( 60%)]  Loss: 3.072 (3.15)  Time: 0.308s, 3327.20/s  (0.307s, 3339.88/s)  LR: 1.712e-04  Data: 0.023 (0.026)
Train: 219 [ 800/1251 ( 64%)]  Loss: 2.885 (3.14)  Time: 0.306s, 3343.47/s  (0.307s, 3339.23/s)  LR: 1.710e-04  Data: 0.025 (0.026)
Train: 219 [ 850/1251 ( 68%)]  Loss: 3.171 (3.14)  Time: 0.307s, 3331.03/s  (0.307s, 3338.65/s)  LR: 1.708e-04  Data: 0.020 (0.026)
Train: 219 [ 900/1251 ( 72%)]  Loss: 3.425 (3.15)  Time: 0.300s, 3408.58/s  (0.307s, 3338.56/s)  LR: 1.707e-04  Data: 0.024 (0.026)
Train: 219 [ 950/1251 ( 76%)]  Loss: 2.938 (3.14)  Time: 0.310s, 3307.26/s  (0.307s, 3338.57/s)  LR: 1.705e-04  Data: 0.024 (0.026)
Train: 219 [1000/1251 ( 80%)]  Loss: 3.351 (3.15)  Time: 0.307s, 3339.44/s  (0.307s, 3338.35/s)  LR: 1.704e-04  Data: 0.023 (0.026)
Train: 219 [1050/1251 ( 84%)]  Loss: 2.846 (3.14)  Time: 0.310s, 3303.53/s  (0.307s, 3338.36/s)  LR: 1.702e-04  Data: 0.023 (0.025)
Train: 219 [1100/1251 ( 88%)]  Loss: 3.377 (3.15)  Time: 0.311s, 3294.51/s  (0.307s, 3338.45/s)  LR: 1.701e-04  Data: 0.025 (0.025)
Train: 219 [1150/1251 ( 92%)]  Loss: 3.290 (3.16)  Time: 0.302s, 3385.65/s  (0.307s, 3338.19/s)  LR: 1.699e-04  Data: 0.019 (0.025)
Train: 219 [1200/1251 ( 96%)]  Loss: 3.174 (3.16)  Time: 0.316s, 3240.82/s  (0.307s, 3337.76/s)  LR: 1.698e-04  Data: 0.021 (0.025)
Train: 219 [1250/1251 (100%)]  Loss: 3.555 (3.17)  Time: 0.276s, 3708.38/s  (0.307s, 3339.47/s)  LR: 1.696e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.022 (2.022)  Loss:  0.5205 (0.5205)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.060 (0.235)  Loss:  0.6230 (0.9984)  Acc@1: 85.6132 (77.9580)  Acc@5: 97.4057 (93.9000)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-217.pth.tar', 77.98000008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-219.pth.tar', 77.95799995605469)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-216.pth.tar', 77.9560001123047)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-218.pth.tar', 77.90200000976563)

Train: 220 [   0/1251 (  0%)]  Loss: 3.154 (3.15)  Time: 2.320s,  441.43/s  (2.320s,  441.43/s)  LR: 1.696e-04  Data: 2.094 (2.094)
Train: 220 [  50/1251 (  4%)]  Loss: 3.184 (3.17)  Time: 0.300s, 3411.73/s  (0.330s, 3107.45/s)  LR: 1.694e-04  Data: 0.020 (0.065)
Train: 220 [ 100/1251 (  8%)]  Loss: 3.089 (3.14)  Time: 0.301s, 3400.27/s  (0.315s, 3247.60/s)  LR: 1.693e-04  Data: 0.026 (0.045)
Train: 220 [ 150/1251 ( 12%)]  Loss: 3.050 (3.12)  Time: 0.303s, 3382.22/s  (0.311s, 3292.01/s)  LR: 1.691e-04  Data: 0.022 (0.038)
Train: 220 [ 200/1251 ( 16%)]  Loss: 3.096 (3.11)  Time: 0.303s, 3382.04/s  (0.309s, 3311.15/s)  LR: 1.690e-04  Data: 0.023 (0.034)
Train: 220 [ 250/1251 ( 20%)]  Loss: 3.196 (3.13)  Time: 0.309s, 3311.01/s  (0.308s, 3320.03/s)  LR: 1.688e-04  Data: 0.024 (0.032)
Train: 220 [ 300/1251 ( 24%)]  Loss: 3.078 (3.12)  Time: 0.306s, 3341.59/s  (0.308s, 3323.50/s)  LR: 1.687e-04  Data: 0.026 (0.031)
Train: 220 [ 350/1251 ( 28%)]  Loss: 2.788 (3.08)  Time: 0.308s, 3329.02/s  (0.308s, 3326.25/s)  LR: 1.685e-04  Data: 0.022 (0.029)
Train: 220 [ 400/1251 ( 32%)]  Loss: 2.985 (3.07)  Time: 0.309s, 3310.83/s  (0.308s, 3327.30/s)  LR: 1.684e-04  Data: 0.024 (0.029)
Train: 220 [ 450/1251 ( 36%)]  Loss: 2.957 (3.06)  Time: 0.303s, 3381.39/s  (0.308s, 3327.88/s)  LR: 1.682e-04  Data: 0.022 (0.028)
Train: 220 [ 500/1251 ( 40%)]  Loss: 3.013 (3.05)  Time: 0.309s, 3313.03/s  (0.308s, 3328.72/s)  LR: 1.681e-04  Data: 0.023 (0.028)
Train: 220 [ 550/1251 ( 44%)]  Loss: 3.338 (3.08)  Time: 0.312s, 3279.19/s  (0.308s, 3329.35/s)  LR: 1.679e-04  Data: 0.023 (0.027)
Train: 220 [ 600/1251 ( 48%)]  Loss: 3.174 (3.08)  Time: 0.304s, 3366.94/s  (0.308s, 3329.78/s)  LR: 1.678e-04  Data: 0.027 (0.027)
Train: 220 [ 650/1251 ( 52%)]  Loss: 3.312 (3.10)  Time: 0.310s, 3305.84/s  (0.307s, 3330.52/s)  LR: 1.676e-04  Data: 0.024 (0.027)
Train: 220 [ 700/1251 ( 56%)]  Loss: 2.972 (3.09)  Time: 0.299s, 3424.13/s  (0.307s, 3330.48/s)  LR: 1.674e-04  Data: 0.025 (0.027)
Train: 220 [ 750/1251 ( 60%)]  Loss: 3.343 (3.11)  Time: 0.309s, 3318.80/s  (0.307s, 3330.95/s)  LR: 1.673e-04  Data: 0.027 (0.026)
Train: 220 [ 800/1251 ( 64%)]  Loss: 3.254 (3.12)  Time: 0.306s, 3347.86/s  (0.307s, 3330.61/s)  LR: 1.671e-04  Data: 0.023 (0.026)
Train: 220 [ 850/1251 ( 68%)]  Loss: 3.074 (3.11)  Time: 0.305s, 3359.87/s  (0.307s, 3330.44/s)  LR: 1.670e-04  Data: 0.022 (0.026)
Train: 220 [ 900/1251 ( 72%)]  Loss: 3.220 (3.12)  Time: 0.308s, 3321.85/s  (0.308s, 3329.97/s)  LR: 1.668e-04  Data: 0.021 (0.026)
Train: 220 [ 950/1251 ( 76%)]  Loss: 3.261 (3.13)  Time: 0.312s, 3279.67/s  (0.308s, 3329.66/s)  LR: 1.667e-04  Data: 0.024 (0.026)
Train: 220 [1000/1251 ( 80%)]  Loss: 3.340 (3.14)  Time: 0.307s, 3339.29/s  (0.308s, 3328.90/s)  LR: 1.665e-04  Data: 0.020 (0.025)
Train: 220 [1050/1251 ( 84%)]  Loss: 3.153 (3.14)  Time: 0.310s, 3298.93/s  (0.308s, 3328.15/s)  LR: 1.664e-04  Data: 0.024 (0.025)
Train: 220 [1100/1251 ( 88%)]  Loss: 3.089 (3.14)  Time: 0.307s, 3330.80/s  (0.308s, 3327.76/s)  LR: 1.662e-04  Data: 0.025 (0.025)
Train: 220 [1150/1251 ( 92%)]  Loss: 3.015 (3.13)  Time: 0.307s, 3339.71/s  (0.308s, 3327.80/s)  LR: 1.661e-04  Data: 0.025 (0.025)
Train: 220 [1200/1251 ( 96%)]  Loss: 2.864 (3.12)  Time: 0.312s, 3283.18/s  (0.308s, 3327.32/s)  LR: 1.659e-04  Data: 0.025 (0.025)
Train: 220 [1250/1251 (100%)]  Loss: 3.041 (3.12)  Time: 0.277s, 3698.38/s  (0.308s, 3328.80/s)  LR: 1.658e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.348 (2.348)  Loss:  0.5190 (0.5190)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.046 (0.234)  Loss:  0.6362 (0.9836)  Acc@1: 85.4953 (78.1820)  Acc@5: 96.8160 (94.0180)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-220.pth.tar', 78.1820000341797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-217.pth.tar', 77.98000008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-219.pth.tar', 77.95799995605469)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-216.pth.tar', 77.9560001123047)

Train: 221 [   0/1251 (  0%)]  Loss: 2.947 (2.95)  Time: 2.362s,  433.49/s  (2.362s,  433.49/s)  LR: 1.658e-04  Data: 2.142 (2.142)
Train: 221 [  50/1251 (  4%)]  Loss: 3.017 (2.98)  Time: 0.301s, 3401.01/s  (0.338s, 3028.04/s)  LR: 1.656e-04  Data: 0.023 (0.076)
Train: 221 [ 100/1251 (  8%)]  Loss: 3.259 (3.07)  Time: 0.298s, 3434.49/s  (0.319s, 3206.00/s)  LR: 1.654e-04  Data: 0.024 (0.050)
Train: 221 [ 150/1251 ( 12%)]  Loss: 3.148 (3.09)  Time: 0.307s, 3334.14/s  (0.314s, 3256.94/s)  LR: 1.653e-04  Data: 0.023 (0.041)
Train: 221 [ 200/1251 ( 16%)]  Loss: 3.211 (3.12)  Time: 0.313s, 3272.41/s  (0.312s, 3279.07/s)  LR: 1.651e-04  Data: 0.023 (0.037)
Train: 221 [ 250/1251 ( 20%)]  Loss: 2.924 (3.08)  Time: 0.312s, 3282.33/s  (0.311s, 3290.21/s)  LR: 1.650e-04  Data: 0.025 (0.034)
Train: 221 [ 300/1251 ( 24%)]  Loss: 3.331 (3.12)  Time: 0.311s, 3290.85/s  (0.311s, 3296.40/s)  LR: 1.648e-04  Data: 0.027 (0.032)
Train: 221 [ 350/1251 ( 28%)]  Loss: 3.108 (3.12)  Time: 0.306s, 3346.54/s  (0.310s, 3301.82/s)  LR: 1.647e-04  Data: 0.021 (0.031)
Train: 221 [ 400/1251 ( 32%)]  Loss: 3.109 (3.12)  Time: 0.309s, 3310.52/s  (0.310s, 3305.52/s)  LR: 1.645e-04  Data: 0.021 (0.030)
Train: 221 [ 450/1251 ( 36%)]  Loss: 3.187 (3.12)  Time: 0.308s, 3323.72/s  (0.310s, 3307.54/s)  LR: 1.644e-04  Data: 0.022 (0.029)
Train: 221 [ 500/1251 ( 40%)]  Loss: 3.019 (3.11)  Time: 0.306s, 3349.45/s  (0.309s, 3308.78/s)  LR: 1.642e-04  Data: 0.027 (0.029)
Train: 221 [ 550/1251 ( 44%)]  Loss: 2.918 (3.10)  Time: 0.312s, 3283.26/s  (0.309s, 3310.01/s)  LR: 1.641e-04  Data: 0.023 (0.028)
Train: 221 [ 600/1251 ( 48%)]  Loss: 2.944 (3.09)  Time: 0.308s, 3325.99/s  (0.309s, 3311.38/s)  LR: 1.639e-04  Data: 0.023 (0.028)
Train: 221 [ 650/1251 ( 52%)]  Loss: 3.396 (3.11)  Time: 0.312s, 3279.74/s  (0.309s, 3311.68/s)  LR: 1.638e-04  Data: 0.023 (0.028)
Train: 221 [ 700/1251 ( 56%)]  Loss: 3.263 (3.12)  Time: 0.311s, 3293.47/s  (0.309s, 3312.58/s)  LR: 1.636e-04  Data: 0.024 (0.027)
Train: 221 [ 750/1251 ( 60%)]  Loss: 3.110 (3.12)  Time: 0.307s, 3336.79/s  (0.309s, 3313.47/s)  LR: 1.635e-04  Data: 0.022 (0.027)
Train: 221 [ 800/1251 ( 64%)]  Loss: 2.673 (3.09)  Time: 0.307s, 3335.14/s  (0.309s, 3313.75/s)  LR: 1.633e-04  Data: 0.023 (0.027)
Train: 221 [ 850/1251 ( 68%)]  Loss: 3.284 (3.10)  Time: 0.305s, 3356.72/s  (0.309s, 3314.47/s)  LR: 1.632e-04  Data: 0.026 (0.026)
Train: 221 [ 900/1251 ( 72%)]  Loss: 3.287 (3.11)  Time: 0.308s, 3319.56/s  (0.309s, 3314.50/s)  LR: 1.630e-04  Data: 0.022 (0.026)
Train: 221 [ 950/1251 ( 76%)]  Loss: 3.052 (3.11)  Time: 0.308s, 3326.33/s  (0.309s, 3314.33/s)  LR: 1.628e-04  Data: 0.023 (0.026)
Train: 221 [1000/1251 ( 80%)]  Loss: 3.122 (3.11)  Time: 0.309s, 3316.14/s  (0.309s, 3314.08/s)  LR: 1.627e-04  Data: 0.025 (0.026)
Train: 221 [1050/1251 ( 84%)]  Loss: 3.021 (3.11)  Time: 0.307s, 3338.94/s  (0.309s, 3314.57/s)  LR: 1.625e-04  Data: 0.022 (0.026)
Train: 221 [1100/1251 ( 88%)]  Loss: 3.298 (3.11)  Time: 0.313s, 3275.72/s  (0.309s, 3314.97/s)  LR: 1.624e-04  Data: 0.026 (0.026)
Train: 221 [1150/1251 ( 92%)]  Loss: 3.380 (3.13)  Time: 0.307s, 3334.15/s  (0.309s, 3315.17/s)  LR: 1.622e-04  Data: 0.022 (0.026)
Train: 221 [1200/1251 ( 96%)]  Loss: 3.208 (3.13)  Time: 0.311s, 3288.51/s  (0.309s, 3315.33/s)  LR: 1.621e-04  Data: 0.022 (0.025)
Train: 221 [1250/1251 (100%)]  Loss: 2.766 (3.11)  Time: 0.277s, 3703.43/s  (0.309s, 3317.45/s)  LR: 1.619e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.141 (2.141)  Loss:  0.5273 (0.5273)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.062 (0.237)  Loss:  0.6250 (0.9870)  Acc@1: 85.9670 (77.8560)  Acc@5: 96.8160 (94.1040)
Train: 222 [   0/1251 (  0%)]  Loss: 2.818 (2.82)  Time: 2.243s,  456.51/s  (2.243s,  456.51/s)  LR: 1.619e-04  Data: 2.017 (2.017)
Train: 222 [  50/1251 (  4%)]  Loss: 2.971 (2.89)  Time: 0.302s, 3394.28/s  (0.330s, 3100.12/s)  LR: 1.618e-04  Data: 0.019 (0.063)
Train: 222 [ 100/1251 (  8%)]  Loss: 3.142 (2.98)  Time: 0.305s, 3362.16/s  (0.317s, 3234.45/s)  LR: 1.616e-04  Data: 0.022 (0.044)
Train: 222 [ 150/1251 ( 12%)]  Loss: 3.095 (3.01)  Time: 0.304s, 3373.39/s  (0.312s, 3279.14/s)  LR: 1.615e-04  Data: 0.023 (0.037)
Train: 222 [ 200/1251 ( 16%)]  Loss: 2.943 (2.99)  Time: 0.307s, 3331.42/s  (0.311s, 3295.93/s)  LR: 1.613e-04  Data: 0.027 (0.034)
Train: 222 [ 250/1251 ( 20%)]  Loss: 3.131 (3.02)  Time: 0.313s, 3273.29/s  (0.310s, 3308.07/s)  LR: 1.612e-04  Data: 0.022 (0.031)
Train: 222 [ 300/1251 ( 24%)]  Loss: 3.050 (3.02)  Time: 0.304s, 3365.60/s  (0.309s, 3314.23/s)  LR: 1.610e-04  Data: 0.020 (0.030)
Train: 222 [ 350/1251 ( 28%)]  Loss: 3.203 (3.04)  Time: 0.309s, 3315.06/s  (0.309s, 3317.65/s)  LR: 1.609e-04  Data: 0.026 (0.029)
Train: 222 [ 400/1251 ( 32%)]  Loss: 3.003 (3.04)  Time: 0.302s, 3387.25/s  (0.308s, 3320.73/s)  LR: 1.607e-04  Data: 0.026 (0.028)
Train: 222 [ 450/1251 ( 36%)]  Loss: 2.703 (3.01)  Time: 0.302s, 3395.40/s  (0.308s, 3324.30/s)  LR: 1.606e-04  Data: 0.021 (0.028)
Train: 222 [ 500/1251 ( 40%)]  Loss: 3.181 (3.02)  Time: 0.304s, 3369.69/s  (0.308s, 3326.13/s)  LR: 1.604e-04  Data: 0.026 (0.027)
Train: 222 [ 550/1251 ( 44%)]  Loss: 2.979 (3.02)  Time: 0.310s, 3308.22/s  (0.308s, 3327.80/s)  LR: 1.603e-04  Data: 0.022 (0.027)
Train: 222 [ 600/1251 ( 48%)]  Loss: 3.001 (3.02)  Time: 0.306s, 3342.77/s  (0.308s, 3328.76/s)  LR: 1.601e-04  Data: 0.022 (0.027)
Train: 222 [ 650/1251 ( 52%)]  Loss: 3.332 (3.04)  Time: 0.310s, 3307.72/s  (0.308s, 3329.89/s)  LR: 1.600e-04  Data: 0.022 (0.026)
Train: 222 [ 700/1251 ( 56%)]  Loss: 3.234 (3.05)  Time: 0.309s, 3309.60/s  (0.307s, 3330.20/s)  LR: 1.598e-04  Data: 0.026 (0.026)
Train: 222 [ 750/1251 ( 60%)]  Loss: 2.982 (3.05)  Time: 0.308s, 3321.12/s  (0.307s, 3330.38/s)  LR: 1.597e-04  Data: 0.021 (0.026)
Train: 222 [ 800/1251 ( 64%)]  Loss: 3.424 (3.07)  Time: 0.310s, 3307.16/s  (0.307s, 3330.59/s)  LR: 1.595e-04  Data: 0.023 (0.026)
Train: 222 [ 850/1251 ( 68%)]  Loss: 3.062 (3.07)  Time: 0.307s, 3336.97/s  (0.307s, 3331.68/s)  LR: 1.594e-04  Data: 0.023 (0.026)
Train: 222 [ 900/1251 ( 72%)]  Loss: 3.370 (3.09)  Time: 0.304s, 3365.91/s  (0.307s, 3331.79/s)  LR: 1.592e-04  Data: 0.022 (0.026)
Train: 222 [ 950/1251 ( 76%)]  Loss: 3.250 (3.09)  Time: 0.308s, 3326.38/s  (0.307s, 3331.88/s)  LR: 1.591e-04  Data: 0.023 (0.025)
Train: 222 [1000/1251 ( 80%)]  Loss: 2.966 (3.09)  Time: 0.317s, 3234.41/s  (0.307s, 3331.31/s)  LR: 1.589e-04  Data: 0.023 (0.025)
Train: 222 [1050/1251 ( 84%)]  Loss: 3.212 (3.09)  Time: 0.307s, 3336.59/s  (0.307s, 3330.89/s)  LR: 1.588e-04  Data: 0.024 (0.025)
Train: 222 [1100/1251 ( 88%)]  Loss: 2.980 (3.09)  Time: 0.305s, 3355.46/s  (0.307s, 3330.97/s)  LR: 1.586e-04  Data: 0.022 (0.025)
Train: 222 [1150/1251 ( 92%)]  Loss: 3.360 (3.10)  Time: 0.311s, 3288.70/s  (0.307s, 3330.87/s)  LR: 1.585e-04  Data: 0.030 (0.025)
Train: 222 [1200/1251 ( 96%)]  Loss: 3.319 (3.11)  Time: 0.308s, 3322.66/s  (0.307s, 3330.65/s)  LR: 1.583e-04  Data: 0.021 (0.025)
Train: 222 [1250/1251 (100%)]  Loss: 3.244 (3.11)  Time: 0.276s, 3706.93/s  (0.307s, 3332.39/s)  LR: 1.582e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.029 (2.029)  Loss:  0.5171 (0.5171)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.058 (0.244)  Loss:  0.6436 (0.9882)  Acc@1: 85.8491 (78.1320)  Acc@5: 97.2877 (94.0540)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-220.pth.tar', 78.1820000341797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-222.pth.tar', 78.13200005859375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-217.pth.tar', 77.98000008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-219.pth.tar', 77.95799995605469)

Train: 223 [   0/1251 (  0%)]  Loss: 2.721 (2.72)  Time: 2.348s,  436.21/s  (2.348s,  436.21/s)  LR: 1.582e-04  Data: 2.126 (2.126)
Train: 223 [  50/1251 (  4%)]  Loss: 2.931 (2.83)  Time: 0.300s, 3414.23/s  (0.329s, 3113.88/s)  LR: 1.580e-04  Data: 0.022 (0.066)
Train: 223 [ 100/1251 (  8%)]  Loss: 3.244 (2.97)  Time: 0.300s, 3413.58/s  (0.315s, 3255.22/s)  LR: 1.579e-04  Data: 0.022 (0.045)
Train: 223 [ 150/1251 ( 12%)]  Loss: 2.941 (2.96)  Time: 0.307s, 3331.83/s  (0.311s, 3292.54/s)  LR: 1.577e-04  Data: 0.021 (0.038)
Train: 223 [ 200/1251 ( 16%)]  Loss: 2.603 (2.89)  Time: 0.305s, 3352.98/s  (0.309s, 3309.47/s)  LR: 1.576e-04  Data: 0.023 (0.034)
Train: 223 [ 250/1251 ( 20%)]  Loss: 3.119 (2.93)  Time: 0.309s, 3311.74/s  (0.309s, 3318.50/s)  LR: 1.574e-04  Data: 0.022 (0.032)
Train: 223 [ 300/1251 ( 24%)]  Loss: 3.083 (2.95)  Time: 0.309s, 3319.13/s  (0.308s, 3321.88/s)  LR: 1.573e-04  Data: 0.021 (0.031)
Train: 223 [ 350/1251 ( 28%)]  Loss: 3.082 (2.97)  Time: 0.305s, 3361.06/s  (0.308s, 3325.74/s)  LR: 1.571e-04  Data: 0.023 (0.030)
Train: 223 [ 400/1251 ( 32%)]  Loss: 3.166 (2.99)  Time: 0.310s, 3298.62/s  (0.308s, 3326.28/s)  LR: 1.570e-04  Data: 0.022 (0.029)
Train: 223 [ 450/1251 ( 36%)]  Loss: 3.151 (3.00)  Time: 0.303s, 3375.54/s  (0.308s, 3327.02/s)  LR: 1.568e-04  Data: 0.025 (0.028)
Train: 223 [ 500/1251 ( 40%)]  Loss: 3.204 (3.02)  Time: 0.309s, 3312.49/s  (0.308s, 3327.75/s)  LR: 1.567e-04  Data: 0.022 (0.028)
Train: 223 [ 550/1251 ( 44%)]  Loss: 3.236 (3.04)  Time: 0.310s, 3298.26/s  (0.308s, 3328.71/s)  LR: 1.565e-04  Data: 0.026 (0.027)
Train: 223 [ 600/1251 ( 48%)]  Loss: 2.949 (3.03)  Time: 0.307s, 3337.82/s  (0.308s, 3328.29/s)  LR: 1.564e-04  Data: 0.024 (0.027)
Train: 223 [ 650/1251 ( 52%)]  Loss: 3.234 (3.05)  Time: 0.313s, 3271.98/s  (0.308s, 3327.58/s)  LR: 1.562e-04  Data: 0.022 (0.027)
Train: 223 [ 700/1251 ( 56%)]  Loss: 3.286 (3.06)  Time: 0.311s, 3297.62/s  (0.308s, 3327.83/s)  LR: 1.561e-04  Data: 0.025 (0.027)
Train: 223 [ 750/1251 ( 60%)]  Loss: 3.315 (3.08)  Time: 0.309s, 3310.79/s  (0.308s, 3327.24/s)  LR: 1.559e-04  Data: 0.022 (0.026)
Train: 223 [ 800/1251 ( 64%)]  Loss: 3.238 (3.09)  Time: 0.306s, 3340.98/s  (0.308s, 3327.06/s)  LR: 1.558e-04  Data: 0.024 (0.026)
Train: 223 [ 850/1251 ( 68%)]  Loss: 3.036 (3.09)  Time: 0.306s, 3346.44/s  (0.308s, 3326.55/s)  LR: 1.556e-04  Data: 0.025 (0.026)
Train: 223 [ 900/1251 ( 72%)]  Loss: 3.213 (3.09)  Time: 0.304s, 3365.66/s  (0.308s, 3326.33/s)  LR: 1.555e-04  Data: 0.018 (0.026)
Train: 223 [ 950/1251 ( 76%)]  Loss: 2.862 (3.08)  Time: 0.313s, 3267.42/s  (0.308s, 3325.61/s)  LR: 1.553e-04  Data: 0.022 (0.026)
Train: 223 [1000/1251 ( 80%)]  Loss: 3.293 (3.09)  Time: 0.307s, 3332.19/s  (0.308s, 3325.19/s)  LR: 1.552e-04  Data: 0.026 (0.026)
Train: 223 [1050/1251 ( 84%)]  Loss: 3.293 (3.10)  Time: 0.309s, 3314.29/s  (0.308s, 3324.79/s)  LR: 1.550e-04  Data: 0.023 (0.025)
Train: 223 [1100/1251 ( 88%)]  Loss: 2.937 (3.09)  Time: 0.310s, 3299.86/s  (0.308s, 3324.23/s)  LR: 1.549e-04  Data: 0.024 (0.025)
Train: 223 [1150/1251 ( 92%)]  Loss: 2.941 (3.09)  Time: 0.306s, 3347.68/s  (0.308s, 3324.36/s)  LR: 1.547e-04  Data: 0.023 (0.025)
Train: 223 [1200/1251 ( 96%)]  Loss: 2.843 (3.08)  Time: 0.308s, 3325.01/s  (0.308s, 3324.07/s)  LR: 1.546e-04  Data: 0.021 (0.025)
Train: 223 [1250/1251 (100%)]  Loss: 3.299 (3.09)  Time: 0.277s, 3696.92/s  (0.308s, 3325.56/s)  LR: 1.544e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.048 (2.048)  Loss:  0.4968 (0.4968)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.060 (0.235)  Loss:  0.6328 (0.9881)  Acc@1: 85.6132 (78.1280)  Acc@5: 96.6981 (93.9640)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-220.pth.tar', 78.1820000341797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-222.pth.tar', 78.13200005859375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-223.pth.tar', 78.1279999560547)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-217.pth.tar', 77.98000008544922)

Train: 224 [   0/1251 (  0%)]  Loss: 3.082 (3.08)  Time: 2.272s,  450.71/s  (2.272s,  450.71/s)  LR: 1.544e-04  Data: 2.041 (2.041)
Train: 224 [  50/1251 (  4%)]  Loss: 3.045 (3.06)  Time: 0.302s, 3395.57/s  (0.331s, 3089.58/s)  LR: 1.543e-04  Data: 0.023 (0.065)
Train: 224 [ 100/1251 (  8%)]  Loss: 3.307 (3.14)  Time: 0.302s, 3393.88/s  (0.317s, 3232.22/s)  LR: 1.541e-04  Data: 0.022 (0.044)
Train: 224 [ 150/1251 ( 12%)]  Loss: 3.065 (3.12)  Time: 0.313s, 3274.74/s  (0.313s, 3275.12/s)  LR: 1.540e-04  Data: 0.022 (0.037)
Train: 224 [ 200/1251 ( 16%)]  Loss: 3.053 (3.11)  Time: 0.304s, 3370.51/s  (0.311s, 3295.63/s)  LR: 1.538e-04  Data: 0.023 (0.034)
Train: 224 [ 250/1251 ( 20%)]  Loss: 3.197 (3.13)  Time: 0.310s, 3303.08/s  (0.310s, 3306.34/s)  LR: 1.537e-04  Data: 0.024 (0.032)
Train: 224 [ 300/1251 ( 24%)]  Loss: 3.095 (3.12)  Time: 0.308s, 3326.47/s  (0.309s, 3312.71/s)  LR: 1.535e-04  Data: 0.023 (0.030)
Train: 224 [ 350/1251 ( 28%)]  Loss: 3.151 (3.12)  Time: 0.311s, 3288.39/s  (0.309s, 3316.75/s)  LR: 1.534e-04  Data: 0.022 (0.029)
Train: 224 [ 400/1251 ( 32%)]  Loss: 3.434 (3.16)  Time: 0.311s, 3288.13/s  (0.308s, 3320.23/s)  LR: 1.532e-04  Data: 0.028 (0.029)
Train: 224 [ 450/1251 ( 36%)]  Loss: 2.887 (3.13)  Time: 0.308s, 3328.02/s  (0.308s, 3322.10/s)  LR: 1.531e-04  Data: 0.021 (0.028)
Train: 224 [ 500/1251 ( 40%)]  Loss: 3.115 (3.13)  Time: 0.307s, 3333.96/s  (0.308s, 3323.29/s)  LR: 1.529e-04  Data: 0.027 (0.028)
Train: 224 [ 550/1251 ( 44%)]  Loss: 3.164 (3.13)  Time: 0.309s, 3309.91/s  (0.308s, 3323.06/s)  LR: 1.528e-04  Data: 0.025 (0.027)
Train: 224 [ 600/1251 ( 48%)]  Loss: 3.288 (3.14)  Time: 0.305s, 3354.75/s  (0.308s, 3323.57/s)  LR: 1.526e-04  Data: 0.023 (0.027)
Train: 224 [ 650/1251 ( 52%)]  Loss: 3.359 (3.16)  Time: 0.306s, 3348.47/s  (0.308s, 3323.57/s)  LR: 1.525e-04  Data: 0.022 (0.027)
Train: 224 [ 700/1251 ( 56%)]  Loss: 3.392 (3.18)  Time: 0.303s, 3376.46/s  (0.308s, 3323.93/s)  LR: 1.523e-04  Data: 0.023 (0.026)
Train: 224 [ 750/1251 ( 60%)]  Loss: 2.804 (3.15)  Time: 0.309s, 3314.35/s  (0.308s, 3324.62/s)  LR: 1.522e-04  Data: 0.023 (0.026)
Train: 224 [ 800/1251 ( 64%)]  Loss: 3.051 (3.15)  Time: 0.305s, 3353.16/s  (0.308s, 3325.85/s)  LR: 1.520e-04  Data: 0.024 (0.026)
Train: 224 [ 850/1251 ( 68%)]  Loss: 2.984 (3.14)  Time: 0.304s, 3363.79/s  (0.308s, 3326.74/s)  LR: 1.519e-04  Data: 0.023 (0.026)
Train: 224 [ 900/1251 ( 72%)]  Loss: 3.318 (3.15)  Time: 0.303s, 3376.02/s  (0.308s, 3327.69/s)  LR: 1.517e-04  Data: 0.020 (0.026)
Train: 224 [ 950/1251 ( 76%)]  Loss: 3.033 (3.14)  Time: 0.309s, 3314.90/s  (0.308s, 3328.21/s)  LR: 1.516e-04  Data: 0.026 (0.026)
Train: 224 [1000/1251 ( 80%)]  Loss: 3.180 (3.14)  Time: 0.306s, 3344.33/s  (0.308s, 3328.78/s)  LR: 1.515e-04  Data: 0.022 (0.025)
Train: 224 [1050/1251 ( 84%)]  Loss: 2.768 (3.13)  Time: 0.307s, 3333.51/s  (0.308s, 3329.35/s)  LR: 1.513e-04  Data: 0.023 (0.025)
Train: 224 [1100/1251 ( 88%)]  Loss: 2.876 (3.12)  Time: 0.309s, 3315.36/s  (0.308s, 3329.73/s)  LR: 1.512e-04  Data: 0.024 (0.025)
Train: 224 [1150/1251 ( 92%)]  Loss: 3.120 (3.12)  Time: 0.302s, 3394.97/s  (0.307s, 3330.09/s)  LR: 1.510e-04  Data: 0.026 (0.025)
Train: 224 [1200/1251 ( 96%)]  Loss: 3.142 (3.12)  Time: 0.306s, 3346.86/s  (0.307s, 3330.28/s)  LR: 1.509e-04  Data: 0.023 (0.025)
Train: 224 [1250/1251 (100%)]  Loss: 3.311 (3.12)  Time: 0.276s, 3706.73/s  (0.307s, 3332.33/s)  LR: 1.507e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.028 (2.028)  Loss:  0.5298 (0.5298)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.060 (0.235)  Loss:  0.6025 (0.9770)  Acc@1: 86.3208 (78.3460)  Acc@5: 97.5236 (94.0580)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-224.pth.tar', 78.34600013427735)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-220.pth.tar', 78.1820000341797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-222.pth.tar', 78.13200005859375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-223.pth.tar', 78.1279999560547)

Train: 225 [   0/1251 (  0%)]  Loss: 3.209 (3.21)  Time: 2.578s,  397.14/s  (2.578s,  397.14/s)  LR: 1.507e-04  Data: 2.346 (2.346)
Train: 225 [  50/1251 (  4%)]  Loss: 3.210 (3.21)  Time: 0.299s, 3426.00/s  (0.333s, 3078.18/s)  LR: 1.506e-04  Data: 0.022 (0.069)
Train: 225 [ 100/1251 (  8%)]  Loss: 3.047 (3.16)  Time: 0.307s, 3334.24/s  (0.317s, 3228.17/s)  LR: 1.504e-04  Data: 0.025 (0.047)
Train: 225 [ 150/1251 ( 12%)]  Loss: 3.434 (3.23)  Time: 0.307s, 3338.32/s  (0.313s, 3272.58/s)  LR: 1.503e-04  Data: 0.023 (0.039)
Train: 225 [ 200/1251 ( 16%)]  Loss: 3.314 (3.24)  Time: 0.303s, 3384.92/s  (0.311s, 3294.88/s)  LR: 1.501e-04  Data: 0.023 (0.035)
Train: 225 [ 250/1251 ( 20%)]  Loss: 3.122 (3.22)  Time: 0.302s, 3386.96/s  (0.310s, 3305.36/s)  LR: 1.500e-04  Data: 0.024 (0.033)
Train: 225 [ 300/1251 ( 24%)]  Loss: 2.896 (3.18)  Time: 0.308s, 3325.82/s  (0.309s, 3312.43/s)  LR: 1.498e-04  Data: 0.021 (0.031)
Train: 225 [ 350/1251 ( 28%)]  Loss: 2.828 (3.13)  Time: 0.306s, 3342.71/s  (0.309s, 3316.29/s)  LR: 1.497e-04  Data: 0.025 (0.030)
Train: 225 [ 400/1251 ( 32%)]  Loss: 2.617 (3.08)  Time: 0.304s, 3368.69/s  (0.308s, 3319.37/s)  LR: 1.495e-04  Data: 0.025 (0.029)
Train: 225 [ 450/1251 ( 36%)]  Loss: 3.342 (3.10)  Time: 0.306s, 3342.60/s  (0.308s, 3321.85/s)  LR: 1.494e-04  Data: 0.030 (0.029)
Train: 225 [ 500/1251 ( 40%)]  Loss: 3.339 (3.12)  Time: 0.305s, 3359.83/s  (0.308s, 3323.02/s)  LR: 1.492e-04  Data: 0.022 (0.028)
Train: 225 [ 550/1251 ( 44%)]  Loss: 2.972 (3.11)  Time: 0.305s, 3357.16/s  (0.308s, 3324.51/s)  LR: 1.491e-04  Data: 0.025 (0.028)
Train: 225 [ 600/1251 ( 48%)]  Loss: 3.018 (3.10)  Time: 0.302s, 3392.65/s  (0.308s, 3326.18/s)  LR: 1.489e-04  Data: 0.024 (0.027)
Train: 225 [ 650/1251 ( 52%)]  Loss: 2.919 (3.09)  Time: 0.312s, 3282.35/s  (0.308s, 3326.05/s)  LR: 1.488e-04  Data: 0.027 (0.027)
Train: 225 [ 700/1251 ( 56%)]  Loss: 3.135 (3.09)  Time: 0.312s, 3279.80/s  (0.308s, 3326.78/s)  LR: 1.487e-04  Data: 0.022 (0.027)
Train: 225 [ 750/1251 ( 60%)]  Loss: 3.088 (3.09)  Time: 0.312s, 3283.21/s  (0.308s, 3326.91/s)  LR: 1.485e-04  Data: 0.020 (0.027)
Train: 225 [ 800/1251 ( 64%)]  Loss: 3.114 (3.09)  Time: 0.305s, 3352.69/s  (0.308s, 3327.08/s)  LR: 1.484e-04  Data: 0.020 (0.026)
Train: 225 [ 850/1251 ( 68%)]  Loss: 3.361 (3.11)  Time: 0.303s, 3377.86/s  (0.308s, 3327.27/s)  LR: 1.482e-04  Data: 0.020 (0.026)
Train: 225 [ 900/1251 ( 72%)]  Loss: 3.280 (3.12)  Time: 0.310s, 3301.67/s  (0.308s, 3327.59/s)  LR: 1.481e-04  Data: 0.025 (0.026)
Train: 225 [ 950/1251 ( 76%)]  Loss: 2.742 (3.10)  Time: 0.307s, 3336.91/s  (0.308s, 3327.40/s)  LR: 1.479e-04  Data: 0.023 (0.026)
Train: 225 [1000/1251 ( 80%)]  Loss: 3.343 (3.11)  Time: 0.302s, 3387.14/s  (0.308s, 3327.57/s)  LR: 1.478e-04  Data: 0.025 (0.026)
Train: 225 [1050/1251 ( 84%)]  Loss: 3.274 (3.12)  Time: 0.307s, 3330.30/s  (0.308s, 3327.76/s)  LR: 1.476e-04  Data: 0.026 (0.026)
Train: 225 [1100/1251 ( 88%)]  Loss: 3.309 (3.13)  Time: 0.301s, 3404.79/s  (0.308s, 3327.95/s)  LR: 1.475e-04  Data: 0.022 (0.026)
Train: 225 [1150/1251 ( 92%)]  Loss: 3.217 (3.13)  Time: 0.309s, 3312.20/s  (0.308s, 3328.24/s)  LR: 1.473e-04  Data: 0.025 (0.025)
Train: 225 [1200/1251 ( 96%)]  Loss: 3.108 (3.13)  Time: 0.307s, 3335.10/s  (0.308s, 3328.40/s)  LR: 1.472e-04  Data: 0.023 (0.025)
Train: 225 [1250/1251 (100%)]  Loss: 3.140 (3.13)  Time: 0.277s, 3701.36/s  (0.307s, 3330.85/s)  LR: 1.470e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.056 (2.056)  Loss:  0.4963 (0.4963)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.054 (0.236)  Loss:  0.6196 (0.9693)  Acc@1: 86.0849 (78.3180)  Acc@5: 96.9340 (94.1400)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-224.pth.tar', 78.34600013427735)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-225.pth.tar', 78.31800016113282)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-220.pth.tar', 78.1820000341797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-222.pth.tar', 78.13200005859375)

Train: 226 [   0/1251 (  0%)]  Loss: 3.081 (3.08)  Time: 2.310s,  443.29/s  (2.310s,  443.29/s)  LR: 1.470e-04  Data: 2.082 (2.082)
Train: 226 [  50/1251 (  4%)]  Loss: 3.147 (3.11)  Time: 0.298s, 3432.13/s  (0.331s, 3092.54/s)  LR: 1.469e-04  Data: 0.024 (0.065)
Train: 226 [ 100/1251 (  8%)]  Loss: 3.247 (3.16)  Time: 0.306s, 3346.10/s  (0.316s, 3243.78/s)  LR: 1.468e-04  Data: 0.022 (0.045)
Train: 226 [ 150/1251 ( 12%)]  Loss: 2.989 (3.12)  Time: 0.308s, 3327.85/s  (0.311s, 3287.78/s)  LR: 1.466e-04  Data: 0.025 (0.038)
Train: 226 [ 200/1251 ( 16%)]  Loss: 2.863 (3.07)  Time: 0.300s, 3411.80/s  (0.309s, 3308.65/s)  LR: 1.465e-04  Data: 0.022 (0.034)
Train: 226 [ 250/1251 ( 20%)]  Loss: 3.128 (3.08)  Time: 0.306s, 3342.76/s  (0.308s, 3319.84/s)  LR: 1.463e-04  Data: 0.026 (0.032)
Train: 226 [ 300/1251 ( 24%)]  Loss: 2.931 (3.06)  Time: 0.310s, 3307.40/s  (0.308s, 3324.62/s)  LR: 1.462e-04  Data: 0.024 (0.030)
Train: 226 [ 350/1251 ( 28%)]  Loss: 2.954 (3.04)  Time: 0.307s, 3332.38/s  (0.308s, 3326.29/s)  LR: 1.460e-04  Data: 0.020 (0.029)
Train: 226 [ 400/1251 ( 32%)]  Loss: 2.811 (3.02)  Time: 0.306s, 3345.30/s  (0.308s, 3327.58/s)  LR: 1.459e-04  Data: 0.026 (0.029)
Train: 226 [ 450/1251 ( 36%)]  Loss: 3.146 (3.03)  Time: 0.305s, 3360.25/s  (0.308s, 3328.07/s)  LR: 1.457e-04  Data: 0.026 (0.028)
Train: 226 [ 500/1251 ( 40%)]  Loss: 3.186 (3.04)  Time: 0.312s, 3283.90/s  (0.308s, 3329.35/s)  LR: 1.456e-04  Data: 0.022 (0.028)
Train: 226 [ 550/1251 ( 44%)]  Loss: 2.995 (3.04)  Time: 0.302s, 3389.63/s  (0.308s, 3329.39/s)  LR: 1.454e-04  Data: 0.023 (0.027)
Train: 226 [ 600/1251 ( 48%)]  Loss: 3.205 (3.05)  Time: 0.305s, 3355.08/s  (0.308s, 3329.98/s)  LR: 1.453e-04  Data: 0.023 (0.027)
Train: 226 [ 650/1251 ( 52%)]  Loss: 3.269 (3.07)  Time: 0.304s, 3372.54/s  (0.307s, 3330.70/s)  LR: 1.452e-04  Data: 0.022 (0.027)
Train: 226 [ 700/1251 ( 56%)]  Loss: 3.208 (3.08)  Time: 0.306s, 3351.08/s  (0.307s, 3331.81/s)  LR: 1.450e-04  Data: 0.022 (0.026)
Train: 226 [ 750/1251 ( 60%)]  Loss: 3.109 (3.08)  Time: 0.306s, 3345.44/s  (0.307s, 3332.51/s)  LR: 1.449e-04  Data: 0.022 (0.026)
Train: 226 [ 800/1251 ( 64%)]  Loss: 3.146 (3.08)  Time: 0.305s, 3361.65/s  (0.307s, 3333.81/s)  LR: 1.447e-04  Data: 0.021 (0.026)
Train: 226 [ 850/1251 ( 68%)]  Loss: 2.877 (3.07)  Time: 0.313s, 3271.12/s  (0.307s, 3333.77/s)  LR: 1.446e-04  Data: 0.024 (0.026)
Train: 226 [ 900/1251 ( 72%)]  Loss: 3.257 (3.08)  Time: 0.310s, 3300.97/s  (0.307s, 3333.89/s)  LR: 1.444e-04  Data: 0.027 (0.026)
Train: 226 [ 950/1251 ( 76%)]  Loss: 3.242 (3.09)  Time: 0.312s, 3282.37/s  (0.307s, 3333.64/s)  LR: 1.443e-04  Data: 0.023 (0.026)
Train: 226 [1000/1251 ( 80%)]  Loss: 2.964 (3.08)  Time: 0.304s, 3367.57/s  (0.307s, 3333.69/s)  LR: 1.441e-04  Data: 0.025 (0.026)
Train: 226 [1050/1251 ( 84%)]  Loss: 3.310 (3.09)  Time: 0.304s, 3367.52/s  (0.307s, 3334.02/s)  LR: 1.440e-04  Data: 0.021 (0.025)
Train: 226 [1100/1251 ( 88%)]  Loss: 2.942 (3.09)  Time: 0.308s, 3321.17/s  (0.307s, 3334.46/s)  LR: 1.439e-04  Data: 0.027 (0.025)
Train: 226 [1150/1251 ( 92%)]  Loss: 2.930 (3.08)  Time: 0.304s, 3373.06/s  (0.307s, 3334.63/s)  LR: 1.437e-04  Data: 0.021 (0.025)
Train: 226 [1200/1251 ( 96%)]  Loss: 3.131 (3.08)  Time: 0.309s, 3317.76/s  (0.307s, 3334.41/s)  LR: 1.436e-04  Data: 0.024 (0.025)
Train: 226 [1250/1251 (100%)]  Loss: 3.049 (3.08)  Time: 0.278s, 3690.04/s  (0.307s, 3336.31/s)  LR: 1.434e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.092 (2.092)  Loss:  0.5005 (0.5005)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.055 (0.241)  Loss:  0.6387 (0.9746)  Acc@1: 84.1981 (78.3580)  Acc@5: 97.2877 (94.2280)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-226.pth.tar', 78.35799998779297)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-224.pth.tar', 78.34600013427735)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-225.pth.tar', 78.31800016113282)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-220.pth.tar', 78.1820000341797)

Train: 227 [   0/1251 (  0%)]  Loss: 2.816 (2.82)  Time: 2.572s,  398.19/s  (2.572s,  398.19/s)  LR: 1.434e-04  Data: 2.349 (2.349)
Train: 227 [  50/1251 (  4%)]  Loss: 3.184 (3.00)  Time: 0.295s, 3473.26/s  (0.331s, 3090.25/s)  LR: 1.433e-04  Data: 0.020 (0.069)
Train: 227 [ 100/1251 (  8%)]  Loss: 3.288 (3.10)  Time: 0.304s, 3373.56/s  (0.316s, 3240.44/s)  LR: 1.431e-04  Data: 0.023 (0.047)
Train: 227 [ 150/1251 ( 12%)]  Loss: 2.755 (3.01)  Time: 0.304s, 3372.84/s  (0.311s, 3288.87/s)  LR: 1.430e-04  Data: 0.022 (0.039)
Train: 227 [ 200/1251 ( 16%)]  Loss: 3.082 (3.03)  Time: 0.304s, 3364.92/s  (0.310s, 3308.50/s)  LR: 1.428e-04  Data: 0.025 (0.035)
Train: 227 [ 250/1251 ( 20%)]  Loss: 3.283 (3.07)  Time: 0.311s, 3296.63/s  (0.309s, 3318.96/s)  LR: 1.427e-04  Data: 0.021 (0.033)
Train: 227 [ 300/1251 ( 24%)]  Loss: 3.022 (3.06)  Time: 0.307s, 3330.43/s  (0.308s, 3324.98/s)  LR: 1.426e-04  Data: 0.023 (0.031)
Train: 227 [ 350/1251 ( 28%)]  Loss: 3.222 (3.08)  Time: 0.310s, 3298.78/s  (0.308s, 3328.70/s)  LR: 1.424e-04  Data: 0.023 (0.030)
Train: 227 [ 400/1251 ( 32%)]  Loss: 3.184 (3.09)  Time: 0.306s, 3345.26/s  (0.307s, 3331.73/s)  LR: 1.423e-04  Data: 0.025 (0.029)
Train: 227 [ 450/1251 ( 36%)]  Loss: 3.127 (3.10)  Time: 0.307s, 3330.60/s  (0.307s, 3333.64/s)  LR: 1.421e-04  Data: 0.022 (0.029)
Train: 227 [ 500/1251 ( 40%)]  Loss: 3.090 (3.10)  Time: 0.309s, 3310.06/s  (0.307s, 3335.38/s)  LR: 1.420e-04  Data: 0.025 (0.028)
Train: 227 [ 550/1251 ( 44%)]  Loss: 2.901 (3.08)  Time: 0.310s, 3302.53/s  (0.307s, 3336.62/s)  LR: 1.418e-04  Data: 0.024 (0.028)
Train: 227 [ 600/1251 ( 48%)]  Loss: 3.194 (3.09)  Time: 0.307s, 3339.42/s  (0.307s, 3338.02/s)  LR: 1.417e-04  Data: 0.024 (0.027)
Train: 227 [ 650/1251 ( 52%)]  Loss: 3.221 (3.10)  Time: 0.303s, 3382.80/s  (0.307s, 3338.19/s)  LR: 1.416e-04  Data: 0.025 (0.027)
Train: 227 [ 700/1251 ( 56%)]  Loss: 2.876 (3.08)  Time: 0.308s, 3324.32/s  (0.307s, 3338.52/s)  LR: 1.414e-04  Data: 0.021 (0.027)
Train: 227 [ 750/1251 ( 60%)]  Loss: 2.673 (3.06)  Time: 0.307s, 3335.28/s  (0.307s, 3339.04/s)  LR: 1.413e-04  Data: 0.024 (0.027)
Train: 227 [ 800/1251 ( 64%)]  Loss: 3.038 (3.06)  Time: 0.308s, 3325.87/s  (0.307s, 3339.07/s)  LR: 1.411e-04  Data: 0.023 (0.026)
Train: 227 [ 850/1251 ( 68%)]  Loss: 3.246 (3.07)  Time: 0.304s, 3370.10/s  (0.307s, 3339.72/s)  LR: 1.410e-04  Data: 0.022 (0.026)
Train: 227 [ 900/1251 ( 72%)]  Loss: 3.143 (3.07)  Time: 0.307s, 3332.24/s  (0.306s, 3340.95/s)  LR: 1.408e-04  Data: 0.027 (0.026)
Train: 227 [ 950/1251 ( 76%)]  Loss: 2.993 (3.07)  Time: 0.304s, 3367.24/s  (0.306s, 3342.11/s)  LR: 1.407e-04  Data: 0.024 (0.026)
Train: 227 [1000/1251 ( 80%)]  Loss: 2.917 (3.06)  Time: 0.304s, 3365.63/s  (0.306s, 3343.36/s)  LR: 1.406e-04  Data: 0.023 (0.026)
Train: 227 [1050/1251 ( 84%)]  Loss: 3.289 (3.07)  Time: 0.309s, 3315.05/s  (0.306s, 3344.17/s)  LR: 1.404e-04  Data: 0.021 (0.026)
Train: 227 [1100/1251 ( 88%)]  Loss: 3.332 (3.08)  Time: 0.305s, 3360.00/s  (0.306s, 3345.34/s)  LR: 1.403e-04  Data: 0.023 (0.026)
Train: 227 [1150/1251 ( 92%)]  Loss: 3.055 (3.08)  Time: 0.309s, 3316.63/s  (0.306s, 3346.69/s)  LR: 1.401e-04  Data: 0.025 (0.026)
Train: 227 [1200/1251 ( 96%)]  Loss: 3.141 (3.08)  Time: 0.309s, 3312.14/s  (0.306s, 3347.54/s)  LR: 1.400e-04  Data: 0.023 (0.025)
Train: 227 [1250/1251 (100%)]  Loss: 3.076 (3.08)  Time: 0.278s, 3689.41/s  (0.306s, 3350.26/s)  LR: 1.398e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.043 (2.043)  Loss:  0.4993 (0.4993)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.060 (0.236)  Loss:  0.6528 (0.9668)  Acc@1: 84.9057 (78.5460)  Acc@5: 96.4623 (94.1740)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-227.pth.tar', 78.5460000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-226.pth.tar', 78.35799998779297)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-224.pth.tar', 78.34600013427735)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-225.pth.tar', 78.31800016113282)

Train: 228 [   0/1251 (  0%)]  Loss: 3.104 (3.10)  Time: 2.159s,  474.26/s  (2.159s,  474.26/s)  LR: 1.398e-04  Data: 1.939 (1.939)
Train: 228 [  50/1251 (  4%)]  Loss: 3.029 (3.07)  Time: 0.296s, 3455.88/s  (0.328s, 3122.90/s)  LR: 1.397e-04  Data: 0.024 (0.062)
Train: 228 [ 100/1251 (  8%)]  Loss: 3.004 (3.05)  Time: 0.297s, 3452.05/s  (0.314s, 3263.88/s)  LR: 1.396e-04  Data: 0.021 (0.043)
Train: 228 [ 150/1251 ( 12%)]  Loss: 2.993 (3.03)  Time: 0.303s, 3376.66/s  (0.310s, 3307.01/s)  LR: 1.394e-04  Data: 0.023 (0.036)
Train: 228 [ 200/1251 ( 16%)]  Loss: 3.135 (3.05)  Time: 0.304s, 3368.00/s  (0.308s, 3328.45/s)  LR: 1.393e-04  Data: 0.023 (0.033)
Train: 228 [ 250/1251 ( 20%)]  Loss: 2.945 (3.03)  Time: 0.300s, 3410.09/s  (0.307s, 3337.67/s)  LR: 1.391e-04  Data: 0.024 (0.031)
Train: 228 [ 300/1251 ( 24%)]  Loss: 2.672 (2.98)  Time: 0.301s, 3397.69/s  (0.306s, 3344.13/s)  LR: 1.390e-04  Data: 0.023 (0.030)
Train: 228 [ 350/1251 ( 28%)]  Loss: 3.194 (3.01)  Time: 0.305s, 3362.01/s  (0.306s, 3348.05/s)  LR: 1.388e-04  Data: 0.024 (0.029)
Train: 228 [ 400/1251 ( 32%)]  Loss: 3.225 (3.03)  Time: 0.303s, 3384.95/s  (0.306s, 3351.32/s)  LR: 1.387e-04  Data: 0.022 (0.028)
Train: 228 [ 450/1251 ( 36%)]  Loss: 3.073 (3.04)  Time: 0.302s, 3389.70/s  (0.305s, 3353.47/s)  LR: 1.386e-04  Data: 0.022 (0.028)
Train: 228 [ 500/1251 ( 40%)]  Loss: 3.143 (3.05)  Time: 0.306s, 3345.08/s  (0.305s, 3356.07/s)  LR: 1.384e-04  Data: 0.027 (0.027)
Train: 228 [ 550/1251 ( 44%)]  Loss: 3.393 (3.08)  Time: 0.307s, 3337.19/s  (0.305s, 3357.82/s)  LR: 1.383e-04  Data: 0.023 (0.027)
Train: 228 [ 600/1251 ( 48%)]  Loss: 3.452 (3.10)  Time: 0.307s, 3332.92/s  (0.305s, 3358.00/s)  LR: 1.381e-04  Data: 0.026 (0.027)
Train: 228 [ 650/1251 ( 52%)]  Loss: 3.167 (3.11)  Time: 0.300s, 3408.73/s  (0.305s, 3359.02/s)  LR: 1.380e-04  Data: 0.023 (0.026)
Train: 228 [ 700/1251 ( 56%)]  Loss: 3.055 (3.11)  Time: 0.303s, 3384.63/s  (0.305s, 3360.92/s)  LR: 1.378e-04  Data: 0.022 (0.026)
Train: 228 [ 750/1251 ( 60%)]  Loss: 3.392 (3.12)  Time: 0.306s, 3347.46/s  (0.305s, 3361.96/s)  LR: 1.377e-04  Data: 0.023 (0.026)
Train: 228 [ 800/1251 ( 64%)]  Loss: 3.344 (3.14)  Time: 0.309s, 3315.78/s  (0.304s, 3362.99/s)  LR: 1.376e-04  Data: 0.023 (0.026)
Train: 228 [ 850/1251 ( 68%)]  Loss: 2.968 (3.13)  Time: 0.304s, 3372.59/s  (0.304s, 3363.24/s)  LR: 1.374e-04  Data: 0.023 (0.026)
Train: 228 [ 900/1251 ( 72%)]  Loss: 2.938 (3.12)  Time: 0.305s, 3353.06/s  (0.304s, 3363.66/s)  LR: 1.373e-04  Data: 0.025 (0.026)
Train: 228 [ 950/1251 ( 76%)]  Loss: 2.885 (3.11)  Time: 0.305s, 3362.24/s  (0.304s, 3363.94/s)  LR: 1.371e-04  Data: 0.023 (0.025)
Train: 228 [1000/1251 ( 80%)]  Loss: 2.955 (3.10)  Time: 0.307s, 3337.47/s  (0.304s, 3364.18/s)  LR: 1.370e-04  Data: 0.020 (0.025)
Train: 228 [1050/1251 ( 84%)]  Loss: 3.195 (3.10)  Time: 0.303s, 3379.37/s  (0.304s, 3364.50/s)  LR: 1.369e-04  Data: 0.025 (0.025)
Train: 228 [1100/1251 ( 88%)]  Loss: 3.265 (3.11)  Time: 0.301s, 3405.08/s  (0.304s, 3365.12/s)  LR: 1.367e-04  Data: 0.023 (0.025)
Train: 228 [1150/1251 ( 92%)]  Loss: 3.323 (3.12)  Time: 0.306s, 3351.37/s  (0.304s, 3366.07/s)  LR: 1.366e-04  Data: 0.022 (0.025)
Train: 228 [1200/1251 ( 96%)]  Loss: 2.842 (3.11)  Time: 0.303s, 3374.44/s  (0.304s, 3366.80/s)  LR: 1.364e-04  Data: 0.027 (0.025)
Train: 228 [1250/1251 (100%)]  Loss: 3.149 (3.11)  Time: 0.277s, 3692.82/s  (0.304s, 3368.87/s)  LR: 1.363e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.067 (2.067)  Loss:  0.4929 (0.4929)  Acc@1: 92.1875 (92.1875)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.049 (0.233)  Loss:  0.6172 (0.9667)  Acc@1: 85.4953 (78.4200)  Acc@5: 97.2877 (94.1860)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-227.pth.tar', 78.5460000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-228.pth.tar', 78.4200000341797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-226.pth.tar', 78.35799998779297)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-224.pth.tar', 78.34600013427735)

Train: 229 [   0/1251 (  0%)]  Loss: 2.983 (2.98)  Time: 2.098s,  488.00/s  (2.098s,  488.00/s)  LR: 1.363e-04  Data: 1.876 (1.876)
Train: 229 [  50/1251 (  4%)]  Loss: 2.685 (2.83)  Time: 0.297s, 3447.44/s  (0.322s, 3179.06/s)  LR: 1.361e-04  Data: 0.021 (0.062)
Train: 229 [ 100/1251 (  8%)]  Loss: 3.263 (2.98)  Time: 0.300s, 3415.83/s  (0.309s, 3312.31/s)  LR: 1.360e-04  Data: 0.023 (0.042)
Train: 229 [ 150/1251 ( 12%)]  Loss: 3.258 (3.05)  Time: 0.301s, 3404.65/s  (0.306s, 3346.74/s)  LR: 1.359e-04  Data: 0.022 (0.036)
Train: 229 [ 200/1251 ( 16%)]  Loss: 3.165 (3.07)  Time: 0.305s, 3358.45/s  (0.305s, 3361.95/s)  LR: 1.357e-04  Data: 0.023 (0.033)
Train: 229 [ 250/1251 ( 20%)]  Loss: 3.162 (3.09)  Time: 0.303s, 3384.81/s  (0.304s, 3370.99/s)  LR: 1.356e-04  Data: 0.026 (0.031)
Train: 229 [ 300/1251 ( 24%)]  Loss: 2.776 (3.04)  Time: 0.298s, 3436.59/s  (0.303s, 3376.85/s)  LR: 1.354e-04  Data: 0.026 (0.030)
Train: 229 [ 350/1251 ( 28%)]  Loss: 3.127 (3.05)  Time: 0.304s, 3364.81/s  (0.303s, 3381.69/s)  LR: 1.353e-04  Data: 0.025 (0.029)
Train: 229 [ 400/1251 ( 32%)]  Loss: 3.405 (3.09)  Time: 0.303s, 3380.37/s  (0.302s, 3385.57/s)  LR: 1.352e-04  Data: 0.026 (0.028)
Train: 229 [ 450/1251 ( 36%)]  Loss: 2.913 (3.07)  Time: 0.295s, 3476.74/s  (0.302s, 3388.01/s)  LR: 1.350e-04  Data: 0.020 (0.028)
Train: 229 [ 500/1251 ( 40%)]  Loss: 3.128 (3.08)  Time: 0.302s, 3390.25/s  (0.302s, 3390.65/s)  LR: 1.349e-04  Data: 0.027 (0.027)
Train: 229 [ 550/1251 ( 44%)]  Loss: 3.213 (3.09)  Time: 0.299s, 3429.90/s  (0.302s, 3392.29/s)  LR: 1.347e-04  Data: 0.023 (0.027)
Train: 229 [ 600/1251 ( 48%)]  Loss: 2.981 (3.08)  Time: 0.300s, 3411.18/s  (0.302s, 3392.25/s)  LR: 1.346e-04  Data: 0.022 (0.027)
Train: 229 [ 650/1251 ( 52%)]  Loss: 3.056 (3.08)  Time: 0.299s, 3423.40/s  (0.302s, 3392.90/s)  LR: 1.345e-04  Data: 0.024 (0.026)
Train: 229 [ 700/1251 ( 56%)]  Loss: 3.097 (3.08)  Time: 0.302s, 3392.47/s  (0.302s, 3392.67/s)  LR: 1.343e-04  Data: 0.022 (0.026)
Train: 229 [ 750/1251 ( 60%)]  Loss: 3.174 (3.09)  Time: 0.302s, 3388.18/s  (0.302s, 3392.54/s)  LR: 1.342e-04  Data: 0.022 (0.026)
Train: 229 [ 800/1251 ( 64%)]  Loss: 2.616 (3.06)  Time: 0.299s, 3423.22/s  (0.302s, 3392.66/s)  LR: 1.340e-04  Data: 0.021 (0.026)
Train: 229 [ 850/1251 ( 68%)]  Loss: 2.646 (3.04)  Time: 0.300s, 3417.65/s  (0.302s, 3391.93/s)  LR: 1.339e-04  Data: 0.023 (0.026)
Train: 229 [ 900/1251 ( 72%)]  Loss: 3.102 (3.04)  Time: 0.302s, 3391.75/s  (0.302s, 3391.62/s)  LR: 1.338e-04  Data: 0.023 (0.026)
Train: 229 [ 950/1251 ( 76%)]  Loss: 2.942 (3.03)  Time: 0.304s, 3364.53/s  (0.302s, 3391.59/s)  LR: 1.336e-04  Data: 0.027 (0.025)
Train: 229 [1000/1251 ( 80%)]  Loss: 3.025 (3.03)  Time: 0.302s, 3386.11/s  (0.302s, 3391.48/s)  LR: 1.335e-04  Data: 0.026 (0.025)
Train: 229 [1050/1251 ( 84%)]  Loss: 2.658 (3.02)  Time: 0.303s, 3383.25/s  (0.302s, 3390.56/s)  LR: 1.333e-04  Data: 0.025 (0.025)
Train: 229 [1100/1251 ( 88%)]  Loss: 3.020 (3.02)  Time: 0.307s, 3339.12/s  (0.302s, 3389.61/s)  LR: 1.332e-04  Data: 0.021 (0.025)
Train: 229 [1150/1251 ( 92%)]  Loss: 3.209 (3.03)  Time: 0.304s, 3363.83/s  (0.302s, 3389.18/s)  LR: 1.331e-04  Data: 0.024 (0.025)
Train: 229 [1200/1251 ( 96%)]  Loss: 3.320 (3.04)  Time: 0.303s, 3383.40/s  (0.302s, 3388.79/s)  LR: 1.329e-04  Data: 0.023 (0.025)
Train: 229 [1250/1251 (100%)]  Loss: 3.050 (3.04)  Time: 0.275s, 3718.63/s  (0.302s, 3390.82/s)  LR: 1.328e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.997 (1.997)  Loss:  0.4912 (0.4912)  Acc@1: 91.2109 (91.2109)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.049 (0.234)  Loss:  0.6123 (0.9528)  Acc@1: 85.6132 (78.4440)  Acc@5: 97.4057 (94.2320)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-227.pth.tar', 78.5460000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-229.pth.tar', 78.44400008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-228.pth.tar', 78.4200000341797)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-226.pth.tar', 78.35799998779297)

Train: 230 [   0/1251 (  0%)]  Loss: 2.983 (2.98)  Time: 2.158s,  474.61/s  (2.158s,  474.61/s)  LR: 1.328e-04  Data: 1.916 (1.916)
Train: 230 [  50/1251 (  4%)]  Loss: 2.953 (2.97)  Time: 0.293s, 3496.33/s  (0.326s, 3144.00/s)  LR: 1.326e-04  Data: 0.022 (0.063)
Train: 230 [ 100/1251 (  8%)]  Loss: 2.859 (2.93)  Time: 0.301s, 3402.07/s  (0.312s, 3283.26/s)  LR: 1.325e-04  Data: 0.025 (0.043)
Train: 230 [ 150/1251 ( 12%)]  Loss: 3.139 (2.98)  Time: 0.303s, 3382.22/s  (0.308s, 3325.97/s)  LR: 1.324e-04  Data: 0.022 (0.037)
Train: 230 [ 200/1251 ( 16%)]  Loss: 3.032 (2.99)  Time: 0.295s, 3469.38/s  (0.306s, 3346.55/s)  LR: 1.322e-04  Data: 0.021 (0.033)
Train: 230 [ 250/1251 ( 20%)]  Loss: 3.166 (3.02)  Time: 0.303s, 3377.89/s  (0.305s, 3356.96/s)  LR: 1.321e-04  Data: 0.023 (0.031)
Train: 230 [ 300/1251 ( 24%)]  Loss: 3.195 (3.05)  Time: 0.304s, 3363.41/s  (0.305s, 3362.84/s)  LR: 1.319e-04  Data: 0.022 (0.030)
Train: 230 [ 350/1251 ( 28%)]  Loss: 3.091 (3.05)  Time: 0.306s, 3342.48/s  (0.304s, 3366.50/s)  LR: 1.318e-04  Data: 0.022 (0.029)
Train: 230 [ 400/1251 ( 32%)]  Loss: 2.667 (3.01)  Time: 0.304s, 3370.50/s  (0.304s, 3368.73/s)  LR: 1.317e-04  Data: 0.022 (0.028)
Train: 230 [ 450/1251 ( 36%)]  Loss: 3.155 (3.02)  Time: 0.302s, 3389.25/s  (0.304s, 3370.22/s)  LR: 1.315e-04  Data: 0.022 (0.028)
Train: 230 [ 500/1251 ( 40%)]  Loss: 2.860 (3.01)  Time: 0.304s, 3366.23/s  (0.304s, 3369.05/s)  LR: 1.314e-04  Data: 0.022 (0.027)
Train: 230 [ 550/1251 ( 44%)]  Loss: 3.320 (3.04)  Time: 0.308s, 3326.07/s  (0.304s, 3368.98/s)  LR: 1.313e-04  Data: 0.023 (0.027)
Train: 230 [ 600/1251 ( 48%)]  Loss: 3.123 (3.04)  Time: 0.305s, 3353.37/s  (0.304s, 3369.68/s)  LR: 1.311e-04  Data: 0.023 (0.027)
Train: 230 [ 650/1251 ( 52%)]  Loss: 3.245 (3.06)  Time: 0.306s, 3341.57/s  (0.304s, 3370.12/s)  LR: 1.310e-04  Data: 0.025 (0.027)
Train: 230 [ 700/1251 ( 56%)]  Loss: 3.081 (3.06)  Time: 0.303s, 3374.69/s  (0.304s, 3370.73/s)  LR: 1.308e-04  Data: 0.022 (0.026)
Train: 230 [ 750/1251 ( 60%)]  Loss: 3.050 (3.06)  Time: 0.305s, 3352.95/s  (0.304s, 3371.77/s)  LR: 1.307e-04  Data: 0.025 (0.026)
Train: 230 [ 800/1251 ( 64%)]  Loss: 3.210 (3.07)  Time: 0.300s, 3415.95/s  (0.304s, 3371.50/s)  LR: 1.306e-04  Data: 0.023 (0.026)
Train: 230 [ 850/1251 ( 68%)]  Loss: 2.957 (3.06)  Time: 0.306s, 3349.36/s  (0.304s, 3371.38/s)  LR: 1.304e-04  Data: 0.027 (0.026)
Train: 230 [ 900/1251 ( 72%)]  Loss: 3.002 (3.06)  Time: 0.306s, 3342.05/s  (0.304s, 3371.30/s)  LR: 1.303e-04  Data: 0.021 (0.026)
Train: 230 [ 950/1251 ( 76%)]  Loss: 3.183 (3.06)  Time: 0.305s, 3362.72/s  (0.304s, 3371.66/s)  LR: 1.301e-04  Data: 0.022 (0.026)
Train: 230 [1000/1251 ( 80%)]  Loss: 3.070 (3.06)  Time: 0.303s, 3384.31/s  (0.304s, 3371.53/s)  LR: 1.300e-04  Data: 0.024 (0.025)
Train: 230 [1050/1251 ( 84%)]  Loss: 3.066 (3.06)  Time: 0.309s, 3309.14/s  (0.304s, 3371.63/s)  LR: 1.299e-04  Data: 0.021 (0.025)
Train: 230 [1100/1251 ( 88%)]  Loss: 2.896 (3.06)  Time: 0.307s, 3336.58/s  (0.304s, 3371.10/s)  LR: 1.297e-04  Data: 0.022 (0.025)
Train: 230 [1150/1251 ( 92%)]  Loss: 3.068 (3.06)  Time: 0.306s, 3347.49/s  (0.304s, 3370.82/s)  LR: 1.296e-04  Data: 0.025 (0.025)
Train: 230 [1200/1251 ( 96%)]  Loss: 2.980 (3.05)  Time: 0.301s, 3399.57/s  (0.304s, 3371.08/s)  LR: 1.295e-04  Data: 0.025 (0.025)
Train: 230 [1250/1251 (100%)]  Loss: 3.096 (3.06)  Time: 0.277s, 3703.02/s  (0.304s, 3373.30/s)  LR: 1.293e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.313 (2.313)  Loss:  0.5029 (0.5029)  Acc@1: 91.6992 (91.6992)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.049 (0.237)  Loss:  0.6089 (0.9657)  Acc@1: 85.1415 (78.4860)  Acc@5: 96.9340 (94.2020)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-227.pth.tar', 78.5460000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-230.pth.tar', 78.48600000976562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-229.pth.tar', 78.44400008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-228.pth.tar', 78.4200000341797)

Train: 231 [   0/1251 (  0%)]  Loss: 2.813 (2.81)  Time: 2.244s,  456.38/s  (2.244s,  456.38/s)  LR: 1.293e-04  Data: 2.018 (2.018)
Train: 231 [  50/1251 (  4%)]  Loss: 2.499 (2.66)  Time: 0.286s, 3585.17/s  (0.331s, 3089.49/s)  LR: 1.292e-04  Data: 0.023 (0.075)
Train: 231 [ 100/1251 (  8%)]  Loss: 3.124 (2.81)  Time: 0.300s, 3418.61/s  (0.314s, 3263.05/s)  LR: 1.290e-04  Data: 0.021 (0.050)
Train: 231 [ 150/1251 ( 12%)]  Loss: 3.045 (2.87)  Time: 0.299s, 3427.63/s  (0.309s, 3314.41/s)  LR: 1.289e-04  Data: 0.024 (0.041)
Train: 231 [ 200/1251 ( 16%)]  Loss: 3.331 (2.96)  Time: 0.300s, 3414.62/s  (0.307s, 3339.78/s)  LR: 1.288e-04  Data: 0.023 (0.037)
Train: 231 [ 250/1251 ( 20%)]  Loss: 2.998 (2.97)  Time: 0.304s, 3373.56/s  (0.305s, 3352.15/s)  LR: 1.286e-04  Data: 0.023 (0.034)
Train: 231 [ 300/1251 ( 24%)]  Loss: 3.153 (2.99)  Time: 0.303s, 3378.41/s  (0.305s, 3360.33/s)  LR: 1.285e-04  Data: 0.028 (0.032)
Train: 231 [ 350/1251 ( 28%)]  Loss: 3.190 (3.02)  Time: 0.305s, 3352.14/s  (0.304s, 3364.02/s)  LR: 1.284e-04  Data: 0.023 (0.031)
Train: 231 [ 400/1251 ( 32%)]  Loss: 3.044 (3.02)  Time: 0.298s, 3440.79/s  (0.304s, 3367.01/s)  LR: 1.282e-04  Data: 0.026 (0.030)
Train: 231 [ 450/1251 ( 36%)]  Loss: 2.621 (2.98)  Time: 0.306s, 3351.28/s  (0.304s, 3369.29/s)  LR: 1.281e-04  Data: 0.020 (0.029)
Train: 231 [ 500/1251 ( 40%)]  Loss: 3.136 (3.00)  Time: 0.300s, 3413.23/s  (0.304s, 3371.69/s)  LR: 1.279e-04  Data: 0.025 (0.029)
Train: 231 [ 550/1251 ( 44%)]  Loss: 3.243 (3.02)  Time: 0.305s, 3360.79/s  (0.304s, 3372.44/s)  LR: 1.278e-04  Data: 0.024 (0.028)
Train: 231 [ 600/1251 ( 48%)]  Loss: 3.163 (3.03)  Time: 0.299s, 3422.12/s  (0.304s, 3373.60/s)  LR: 1.277e-04  Data: 0.023 (0.028)
Train: 231 [ 650/1251 ( 52%)]  Loss: 3.260 (3.04)  Time: 0.303s, 3384.42/s  (0.303s, 3374.69/s)  LR: 1.275e-04  Data: 0.021 (0.028)
Train: 231 [ 700/1251 ( 56%)]  Loss: 2.985 (3.04)  Time: 0.303s, 3376.90/s  (0.303s, 3375.16/s)  LR: 1.274e-04  Data: 0.023 (0.027)
Train: 231 [ 750/1251 ( 60%)]  Loss: 2.788 (3.02)  Time: 0.309s, 3314.42/s  (0.303s, 3375.52/s)  LR: 1.273e-04  Data: 0.023 (0.027)
Train: 231 [ 800/1251 ( 64%)]  Loss: 3.139 (3.03)  Time: 0.304s, 3367.49/s  (0.303s, 3375.72/s)  LR: 1.271e-04  Data: 0.023 (0.027)
Train: 231 [ 850/1251 ( 68%)]  Loss: 2.721 (3.01)  Time: 0.303s, 3384.75/s  (0.303s, 3375.41/s)  LR: 1.270e-04  Data: 0.022 (0.027)
Train: 231 [ 900/1251 ( 72%)]  Loss: 3.337 (3.03)  Time: 0.300s, 3411.40/s  (0.303s, 3374.89/s)  LR: 1.268e-04  Data: 0.022 (0.026)
Train: 231 [ 950/1251 ( 76%)]  Loss: 3.239 (3.04)  Time: 0.307s, 3338.21/s  (0.303s, 3374.78/s)  LR: 1.267e-04  Data: 0.024 (0.026)
Train: 231 [1000/1251 ( 80%)]  Loss: 3.406 (3.06)  Time: 0.301s, 3396.59/s  (0.303s, 3374.64/s)  LR: 1.266e-04  Data: 0.025 (0.026)
Train: 231 [1050/1251 ( 84%)]  Loss: 3.123 (3.06)  Time: 0.304s, 3364.28/s  (0.303s, 3374.95/s)  LR: 1.264e-04  Data: 0.022 (0.026)
Train: 231 [1100/1251 ( 88%)]  Loss: 3.055 (3.06)  Time: 0.300s, 3408.96/s  (0.303s, 3375.32/s)  LR: 1.263e-04  Data: 0.024 (0.026)
Train: 231 [1150/1251 ( 92%)]  Loss: 3.195 (3.07)  Time: 0.305s, 3355.31/s  (0.303s, 3375.48/s)  LR: 1.262e-04  Data: 0.020 (0.026)
Train: 231 [1200/1251 ( 96%)]  Loss: 2.867 (3.06)  Time: 0.308s, 3320.95/s  (0.303s, 3375.24/s)  LR: 1.260e-04  Data: 0.026 (0.026)
Train: 231 [1250/1251 (100%)]  Loss: 3.204 (3.06)  Time: 0.276s, 3711.16/s  (0.303s, 3377.00/s)  LR: 1.259e-04  Data: 0.000 (0.026)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.065 (2.065)  Loss:  0.4712 (0.4712)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.047 (0.235)  Loss:  0.6025 (0.9451)  Acc@1: 85.3774 (78.5680)  Acc@5: 97.5236 (94.2940)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-231.pth.tar', 78.56799998291015)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-227.pth.tar', 78.5460000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-230.pth.tar', 78.48600000976562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-229.pth.tar', 78.44400008544922)

Train: 232 [   0/1251 (  0%)]  Loss: 3.258 (3.26)  Time: 2.356s,  434.70/s  (2.356s,  434.70/s)  LR: 1.259e-04  Data: 2.129 (2.129)
Train: 232 [  50/1251 (  4%)]  Loss: 3.037 (3.15)  Time: 0.299s, 3421.65/s  (0.328s, 3125.02/s)  LR: 1.258e-04  Data: 0.023 (0.065)
Train: 232 [ 100/1251 (  8%)]  Loss: 3.135 (3.14)  Time: 0.301s, 3407.23/s  (0.314s, 3266.17/s)  LR: 1.256e-04  Data: 0.025 (0.044)
Train: 232 [ 150/1251 ( 12%)]  Loss: 3.313 (3.19)  Time: 0.298s, 3431.10/s  (0.309s, 3309.72/s)  LR: 1.255e-04  Data: 0.027 (0.037)
Train: 232 [ 200/1251 ( 16%)]  Loss: 2.997 (3.15)  Time: 0.303s, 3384.49/s  (0.307s, 3331.24/s)  LR: 1.253e-04  Data: 0.026 (0.034)
Train: 232 [ 250/1251 ( 20%)]  Loss: 3.137 (3.15)  Time: 0.301s, 3397.49/s  (0.306s, 3345.53/s)  LR: 1.252e-04  Data: 0.030 (0.032)
Train: 232 [ 300/1251 ( 24%)]  Loss: 2.985 (3.12)  Time: 0.303s, 3380.37/s  (0.305s, 3353.94/s)  LR: 1.251e-04  Data: 0.025 (0.031)
Train: 232 [ 350/1251 ( 28%)]  Loss: 3.041 (3.11)  Time: 0.303s, 3378.70/s  (0.305s, 3358.14/s)  LR: 1.249e-04  Data: 0.022 (0.030)
Train: 232 [ 400/1251 ( 32%)]  Loss: 3.104 (3.11)  Time: 0.304s, 3369.34/s  (0.305s, 3361.66/s)  LR: 1.248e-04  Data: 0.023 (0.029)
Train: 232 [ 450/1251 ( 36%)]  Loss: 3.098 (3.11)  Time: 0.307s, 3338.17/s  (0.304s, 3364.85/s)  LR: 1.247e-04  Data: 0.029 (0.028)
Train: 232 [ 500/1251 ( 40%)]  Loss: 3.245 (3.12)  Time: 0.303s, 3378.39/s  (0.304s, 3365.96/s)  LR: 1.245e-04  Data: 0.022 (0.028)
Train: 232 [ 550/1251 ( 44%)]  Loss: 2.949 (3.11)  Time: 0.301s, 3399.66/s  (0.304s, 3367.47/s)  LR: 1.244e-04  Data: 0.021 (0.027)
Train: 232 [ 600/1251 ( 48%)]  Loss: 2.929 (3.09)  Time: 0.304s, 3365.24/s  (0.304s, 3367.42/s)  LR: 1.243e-04  Data: 0.023 (0.027)
Train: 232 [ 650/1251 ( 52%)]  Loss: 3.261 (3.11)  Time: 0.305s, 3362.84/s  (0.304s, 3368.32/s)  LR: 1.241e-04  Data: 0.021 (0.027)
Train: 232 [ 700/1251 ( 56%)]  Loss: 3.057 (3.10)  Time: 0.305s, 3359.71/s  (0.304s, 3368.31/s)  LR: 1.240e-04  Data: 0.023 (0.027)
Train: 232 [ 750/1251 ( 60%)]  Loss: 3.174 (3.11)  Time: 0.307s, 3330.59/s  (0.304s, 3368.18/s)  LR: 1.239e-04  Data: 0.021 (0.026)
Train: 232 [ 800/1251 ( 64%)]  Loss: 2.784 (3.09)  Time: 0.304s, 3367.48/s  (0.304s, 3368.49/s)  LR: 1.237e-04  Data: 0.024 (0.026)
Train: 232 [ 850/1251 ( 68%)]  Loss: 3.104 (3.09)  Time: 0.304s, 3363.34/s  (0.304s, 3368.79/s)  LR: 1.236e-04  Data: 0.023 (0.026)
Train: 232 [ 900/1251 ( 72%)]  Loss: 2.871 (3.08)  Time: 0.298s, 3433.87/s  (0.304s, 3369.33/s)  LR: 1.235e-04  Data: 0.022 (0.026)
Train: 232 [ 950/1251 ( 76%)]  Loss: 3.133 (3.08)  Time: 0.309s, 3315.10/s  (0.304s, 3369.07/s)  LR: 1.233e-04  Data: 0.023 (0.026)
Train: 232 [1000/1251 ( 80%)]  Loss: 3.171 (3.08)  Time: 0.309s, 3318.68/s  (0.304s, 3368.29/s)  LR: 1.232e-04  Data: 0.025 (0.026)
Train: 232 [1050/1251 ( 84%)]  Loss: 3.071 (3.08)  Time: 0.306s, 3347.88/s  (0.304s, 3367.62/s)  LR: 1.230e-04  Data: 0.022 (0.025)
Train: 232 [1100/1251 ( 88%)]  Loss: 3.221 (3.09)  Time: 0.305s, 3361.12/s  (0.304s, 3366.96/s)  LR: 1.229e-04  Data: 0.024 (0.025)
Train: 232 [1150/1251 ( 92%)]  Loss: 2.911 (3.08)  Time: 0.314s, 3264.05/s  (0.304s, 3366.36/s)  LR: 1.228e-04  Data: 0.025 (0.025)
Train: 232 [1200/1251 ( 96%)]  Loss: 2.982 (3.08)  Time: 0.302s, 3386.28/s  (0.304s, 3365.05/s)  LR: 1.226e-04  Data: 0.024 (0.025)
Train: 232 [1250/1251 (100%)]  Loss: 2.587 (3.06)  Time: 0.275s, 3725.91/s  (0.304s, 3366.51/s)  LR: 1.225e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.038 (2.038)  Loss:  0.4924 (0.4924)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.057 (0.238)  Loss:  0.6187 (0.9503)  Acc@1: 86.6745 (78.6800)  Acc@5: 97.0519 (94.2600)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-232.pth.tar', 78.67999989990234)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-231.pth.tar', 78.56799998291015)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-227.pth.tar', 78.5460000366211)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-230.pth.tar', 78.48600000976562)

Train: 233 [   0/1251 (  0%)]  Loss: 2.999 (3.00)  Time: 2.100s,  487.55/s  (2.100s,  487.55/s)  LR: 1.225e-04  Data: 1.877 (1.877)
Train: 233 [  50/1251 (  4%)]  Loss: 3.099 (3.05)  Time: 0.291s, 3516.72/s  (0.324s, 3157.22/s)  LR: 1.224e-04  Data: 0.017 (0.061)
Train: 233 [ 100/1251 (  8%)]  Loss: 3.117 (3.07)  Time: 0.302s, 3392.99/s  (0.313s, 3276.00/s)  LR: 1.222e-04  Data: 0.023 (0.042)
Train: 233 [ 150/1251 ( 12%)]  Loss: 2.759 (2.99)  Time: 0.303s, 3377.38/s  (0.309s, 3315.14/s)  LR: 1.221e-04  Data: 0.024 (0.036)
Train: 233 [ 200/1251 ( 16%)]  Loss: 3.131 (3.02)  Time: 0.304s, 3366.50/s  (0.307s, 3331.13/s)  LR: 1.220e-04  Data: 0.022 (0.033)
Train: 233 [ 250/1251 ( 20%)]  Loss: 3.158 (3.04)  Time: 0.308s, 3326.68/s  (0.307s, 3339.89/s)  LR: 1.218e-04  Data: 0.022 (0.031)
Train: 233 [ 300/1251 ( 24%)]  Loss: 2.988 (3.04)  Time: 0.305s, 3359.41/s  (0.306s, 3343.22/s)  LR: 1.217e-04  Data: 0.027 (0.030)
Train: 233 [ 350/1251 ( 28%)]  Loss: 2.820 (3.01)  Time: 0.307s, 3336.75/s  (0.306s, 3345.27/s)  LR: 1.216e-04  Data: 0.023 (0.029)
Train: 233 [ 400/1251 ( 32%)]  Loss: 3.235 (3.03)  Time: 0.304s, 3364.73/s  (0.306s, 3346.92/s)  LR: 1.214e-04  Data: 0.026 (0.028)
Train: 233 [ 450/1251 ( 36%)]  Loss: 3.169 (3.05)  Time: 0.305s, 3352.58/s  (0.306s, 3347.92/s)  LR: 1.213e-04  Data: 0.022 (0.028)
Train: 233 [ 500/1251 ( 40%)]  Loss: 3.066 (3.05)  Time: 0.307s, 3339.87/s  (0.306s, 3348.12/s)  LR: 1.212e-04  Data: 0.024 (0.027)
Train: 233 [ 550/1251 ( 44%)]  Loss: 2.950 (3.04)  Time: 0.309s, 3311.78/s  (0.306s, 3348.14/s)  LR: 1.210e-04  Data: 0.020 (0.027)
Train: 233 [ 600/1251 ( 48%)]  Loss: 3.143 (3.05)  Time: 0.302s, 3386.81/s  (0.306s, 3348.20/s)  LR: 1.209e-04  Data: 0.024 (0.027)
Train: 233 [ 650/1251 ( 52%)]  Loss: 3.310 (3.07)  Time: 0.308s, 3328.71/s  (0.306s, 3348.85/s)  LR: 1.208e-04  Data: 0.023 (0.026)
Train: 233 [ 700/1251 ( 56%)]  Loss: 3.076 (3.07)  Time: 0.304s, 3365.47/s  (0.306s, 3349.18/s)  LR: 1.206e-04  Data: 0.026 (0.026)
Train: 233 [ 750/1251 ( 60%)]  Loss: 2.745 (3.05)  Time: 0.308s, 3326.72/s  (0.306s, 3348.82/s)  LR: 1.205e-04  Data: 0.026 (0.026)
Train: 233 [ 800/1251 ( 64%)]  Loss: 3.381 (3.07)  Time: 0.302s, 3386.98/s  (0.306s, 3349.40/s)  LR: 1.204e-04  Data: 0.023 (0.026)
Train: 233 [ 850/1251 ( 68%)]  Loss: 2.840 (3.05)  Time: 0.302s, 3393.96/s  (0.306s, 3349.95/s)  LR: 1.202e-04  Data: 0.021 (0.026)
Train: 233 [ 900/1251 ( 72%)]  Loss: 2.891 (3.05)  Time: 0.302s, 3388.02/s  (0.306s, 3350.55/s)  LR: 1.201e-04  Data: 0.023 (0.026)
Train: 233 [ 950/1251 ( 76%)]  Loss: 3.141 (3.05)  Time: 0.306s, 3349.98/s  (0.306s, 3350.33/s)  LR: 1.200e-04  Data: 0.024 (0.025)
Train: 233 [1000/1251 ( 80%)]  Loss: 2.934 (3.05)  Time: 0.302s, 3394.54/s  (0.306s, 3350.66/s)  LR: 1.198e-04  Data: 0.024 (0.025)
Train: 233 [1050/1251 ( 84%)]  Loss: 3.219 (3.05)  Time: 0.307s, 3330.32/s  (0.306s, 3350.42/s)  LR: 1.197e-04  Data: 0.030 (0.025)
Train: 233 [1100/1251 ( 88%)]  Loss: 3.159 (3.06)  Time: 0.306s, 3346.09/s  (0.306s, 3350.32/s)  LR: 1.196e-04  Data: 0.022 (0.025)
Train: 233 [1150/1251 ( 92%)]  Loss: 3.056 (3.06)  Time: 0.306s, 3350.85/s  (0.306s, 3350.43/s)  LR: 1.194e-04  Data: 0.022 (0.025)
Train: 233 [1200/1251 ( 96%)]  Loss: 3.268 (3.07)  Time: 0.302s, 3389.97/s  (0.306s, 3350.49/s)  LR: 1.193e-04  Data: 0.020 (0.025)
Train: 233 [1250/1251 (100%)]  Loss: 2.738 (3.05)  Time: 0.276s, 3716.33/s  (0.305s, 3352.44/s)  LR: 1.192e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.997 (1.997)  Loss:  0.5044 (0.5044)  Acc@1: 91.2109 (91.2109)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.049 (0.238)  Loss:  0.6216 (0.9699)  Acc@1: 86.5566 (78.7440)  Acc@5: 97.2877 (94.2260)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-233.pth.tar', 78.74399997802735)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-232.pth.tar', 78.67999989990234)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-231.pth.tar', 78.56799998291015)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-227.pth.tar', 78.5460000366211)

Train: 234 [   0/1251 (  0%)]  Loss: 3.205 (3.20)  Time: 2.271s,  450.86/s  (2.271s,  450.86/s)  LR: 1.192e-04  Data: 2.041 (2.041)
Train: 234 [  50/1251 (  4%)]  Loss: 2.891 (3.05)  Time: 0.298s, 3431.03/s  (0.325s, 3152.84/s)  LR: 1.190e-04  Data: 0.025 (0.066)
Train: 234 [ 100/1251 (  8%)]  Loss: 3.002 (3.03)  Time: 0.298s, 3435.36/s  (0.312s, 3282.99/s)  LR: 1.189e-04  Data: 0.020 (0.045)
Train: 234 [ 150/1251 ( 12%)]  Loss: 2.738 (2.96)  Time: 0.308s, 3324.55/s  (0.308s, 3319.39/s)  LR: 1.188e-04  Data: 0.023 (0.038)
Train: 234 [ 200/1251 ( 16%)]  Loss: 3.011 (2.97)  Time: 0.306s, 3342.39/s  (0.307s, 3332.21/s)  LR: 1.186e-04  Data: 0.024 (0.034)
Train: 234 [ 250/1251 ( 20%)]  Loss: 3.159 (3.00)  Time: 0.306s, 3341.60/s  (0.307s, 3337.46/s)  LR: 1.185e-04  Data: 0.021 (0.032)
Train: 234 [ 300/1251 ( 24%)]  Loss: 3.080 (3.01)  Time: 0.308s, 3328.09/s  (0.306s, 3341.04/s)  LR: 1.184e-04  Data: 0.022 (0.030)
Train: 234 [ 350/1251 ( 28%)]  Loss: 2.855 (2.99)  Time: 0.300s, 3413.98/s  (0.306s, 3342.92/s)  LR: 1.182e-04  Data: 0.021 (0.029)
Train: 234 [ 400/1251 ( 32%)]  Loss: 3.009 (2.99)  Time: 0.301s, 3399.90/s  (0.306s, 3345.11/s)  LR: 1.181e-04  Data: 0.023 (0.029)
Train: 234 [ 450/1251 ( 36%)]  Loss: 3.140 (3.01)  Time: 0.304s, 3364.17/s  (0.306s, 3347.10/s)  LR: 1.180e-04  Data: 0.023 (0.028)
Train: 234 [ 500/1251 ( 40%)]  Loss: 3.036 (3.01)  Time: 0.304s, 3370.70/s  (0.306s, 3348.92/s)  LR: 1.178e-04  Data: 0.027 (0.028)
Train: 234 [ 550/1251 ( 44%)]  Loss: 3.061 (3.02)  Time: 0.307s, 3334.00/s  (0.306s, 3349.58/s)  LR: 1.177e-04  Data: 0.023 (0.027)
Train: 234 [ 600/1251 ( 48%)]  Loss: 3.051 (3.02)  Time: 0.310s, 3301.98/s  (0.306s, 3349.52/s)  LR: 1.176e-04  Data: 0.023 (0.027)
Train: 234 [ 650/1251 ( 52%)]  Loss: 3.228 (3.03)  Time: 0.306s, 3350.55/s  (0.306s, 3349.87/s)  LR: 1.174e-04  Data: 0.023 (0.027)
Train: 234 [ 700/1251 ( 56%)]  Loss: 3.124 (3.04)  Time: 0.301s, 3404.43/s  (0.306s, 3349.76/s)  LR: 1.173e-04  Data: 0.025 (0.026)
Train: 234 [ 750/1251 ( 60%)]  Loss: 2.903 (3.03)  Time: 0.307s, 3337.03/s  (0.306s, 3350.12/s)  LR: 1.172e-04  Data: 0.022 (0.026)
Train: 234 [ 800/1251 ( 64%)]  Loss: 2.719 (3.01)  Time: 0.305s, 3360.38/s  (0.306s, 3349.76/s)  LR: 1.171e-04  Data: 0.023 (0.026)
Train: 234 [ 850/1251 ( 68%)]  Loss: 3.158 (3.02)  Time: 0.305s, 3359.83/s  (0.306s, 3348.72/s)  LR: 1.169e-04  Data: 0.022 (0.026)
Train: 234 [ 900/1251 ( 72%)]  Loss: 3.127 (3.03)  Time: 0.304s, 3367.49/s  (0.306s, 3347.57/s)  LR: 1.168e-04  Data: 0.020 (0.026)
Train: 234 [ 950/1251 ( 76%)]  Loss: 3.014 (3.03)  Time: 0.308s, 3327.09/s  (0.306s, 3347.27/s)  LR: 1.167e-04  Data: 0.023 (0.026)
Train: 234 [1000/1251 ( 80%)]  Loss: 3.346 (3.04)  Time: 0.307s, 3335.90/s  (0.306s, 3346.79/s)  LR: 1.165e-04  Data: 0.025 (0.026)
Train: 234 [1050/1251 ( 84%)]  Loss: 3.132 (3.04)  Time: 0.306s, 3349.35/s  (0.306s, 3346.17/s)  LR: 1.164e-04  Data: 0.021 (0.025)
Train: 234 [1100/1251 ( 88%)]  Loss: 3.227 (3.05)  Time: 0.304s, 3370.63/s  (0.306s, 3345.70/s)  LR: 1.163e-04  Data: 0.021 (0.025)
Train: 234 [1150/1251 ( 92%)]  Loss: 3.156 (3.06)  Time: 0.310s, 3306.71/s  (0.306s, 3345.15/s)  LR: 1.161e-04  Data: 0.024 (0.025)
Train: 234 [1200/1251 ( 96%)]  Loss: 3.096 (3.06)  Time: 0.305s, 3355.11/s  (0.306s, 3345.25/s)  LR: 1.160e-04  Data: 0.023 (0.025)
Train: 234 [1250/1251 (100%)]  Loss: 2.772 (3.05)  Time: 0.276s, 3716.84/s  (0.306s, 3347.23/s)  LR: 1.159e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.108 (2.108)  Loss:  0.4924 (0.4924)  Acc@1: 91.9922 (91.9922)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.046 (0.235)  Loss:  0.6387 (0.9632)  Acc@1: 85.1415 (78.7620)  Acc@5: 97.5236 (94.2960)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-234.pth.tar', 78.76200000976563)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-233.pth.tar', 78.74399997802735)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-232.pth.tar', 78.67999989990234)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-231.pth.tar', 78.56799998291015)

Train: 235 [   0/1251 (  0%)]  Loss: 2.652 (2.65)  Time: 2.096s,  488.51/s  (2.096s,  488.51/s)  LR: 1.159e-04  Data: 1.872 (1.872)
Train: 235 [  50/1251 (  4%)]  Loss: 3.325 (2.99)  Time: 0.292s, 3505.10/s  (0.328s, 3123.74/s)  LR: 1.157e-04  Data: 0.024 (0.061)
Train: 235 [ 100/1251 (  8%)]  Loss: 3.260 (3.08)  Time: 0.307s, 3333.21/s  (0.314s, 3258.80/s)  LR: 1.156e-04  Data: 0.018 (0.042)
Train: 235 [ 150/1251 ( 12%)]  Loss: 3.043 (3.07)  Time: 0.304s, 3364.35/s  (0.311s, 3296.59/s)  LR: 1.155e-04  Data: 0.022 (0.036)
Train: 235 [ 200/1251 ( 16%)]  Loss: 3.214 (3.10)  Time: 0.304s, 3365.23/s  (0.309s, 3311.10/s)  LR: 1.153e-04  Data: 0.025 (0.033)
Train: 235 [ 250/1251 ( 20%)]  Loss: 3.353 (3.14)  Time: 0.307s, 3333.49/s  (0.309s, 3319.28/s)  LR: 1.152e-04  Data: 0.022 (0.031)
Train: 235 [ 300/1251 ( 24%)]  Loss: 2.955 (3.11)  Time: 0.301s, 3397.99/s  (0.308s, 3325.27/s)  LR: 1.151e-04  Data: 0.026 (0.030)
Train: 235 [ 350/1251 ( 28%)]  Loss: 3.155 (3.12)  Time: 0.303s, 3379.15/s  (0.308s, 3329.47/s)  LR: 1.150e-04  Data: 0.023 (0.029)
Train: 235 [ 400/1251 ( 32%)]  Loss: 2.607 (3.06)  Time: 0.306s, 3347.38/s  (0.307s, 3331.81/s)  LR: 1.148e-04  Data: 0.024 (0.028)
Train: 235 [ 450/1251 ( 36%)]  Loss: 2.683 (3.02)  Time: 0.302s, 3390.51/s  (0.307s, 3334.21/s)  LR: 1.147e-04  Data: 0.024 (0.028)
Train: 235 [ 500/1251 ( 40%)]  Loss: 3.166 (3.04)  Time: 0.303s, 3374.03/s  (0.307s, 3336.15/s)  LR: 1.146e-04  Data: 0.020 (0.027)
Train: 235 [ 550/1251 ( 44%)]  Loss: 3.170 (3.05)  Time: 0.310s, 3299.37/s  (0.307s, 3338.02/s)  LR: 1.144e-04  Data: 0.025 (0.027)
Train: 235 [ 600/1251 ( 48%)]  Loss: 3.078 (3.05)  Time: 0.306s, 3348.24/s  (0.307s, 3338.89/s)  LR: 1.143e-04  Data: 0.023 (0.027)
Train: 235 [ 650/1251 ( 52%)]  Loss: 3.028 (3.05)  Time: 0.309s, 3312.44/s  (0.307s, 3339.48/s)  LR: 1.142e-04  Data: 0.023 (0.026)
Train: 235 [ 700/1251 ( 56%)]  Loss: 2.980 (3.04)  Time: 0.306s, 3342.66/s  (0.307s, 3340.26/s)  LR: 1.140e-04  Data: 0.021 (0.026)
Train: 235 [ 750/1251 ( 60%)]  Loss: 3.160 (3.05)  Time: 0.309s, 3316.72/s  (0.307s, 3340.08/s)  LR: 1.139e-04  Data: 0.021 (0.026)
Train: 235 [ 800/1251 ( 64%)]  Loss: 2.598 (3.03)  Time: 0.309s, 3315.34/s  (0.307s, 3339.45/s)  LR: 1.138e-04  Data: 0.026 (0.026)
Train: 235 [ 850/1251 ( 68%)]  Loss: 3.366 (3.04)  Time: 0.309s, 3312.41/s  (0.307s, 3339.28/s)  LR: 1.136e-04  Data: 0.026 (0.026)
Train: 235 [ 900/1251 ( 72%)]  Loss: 2.727 (3.03)  Time: 0.306s, 3344.04/s  (0.307s, 3339.50/s)  LR: 1.135e-04  Data: 0.023 (0.026)
Train: 235 [ 950/1251 ( 76%)]  Loss: 2.823 (3.02)  Time: 0.303s, 3374.63/s  (0.307s, 3340.12/s)  LR: 1.134e-04  Data: 0.021 (0.026)
Train: 235 [1000/1251 ( 80%)]  Loss: 2.960 (3.01)  Time: 0.304s, 3373.83/s  (0.307s, 3340.41/s)  LR: 1.133e-04  Data: 0.022 (0.025)
Train: 235 [1050/1251 ( 84%)]  Loss: 3.121 (3.02)  Time: 0.307s, 3335.17/s  (0.307s, 3340.66/s)  LR: 1.131e-04  Data: 0.023 (0.025)
Train: 235 [1100/1251 ( 88%)]  Loss: 2.860 (3.01)  Time: 0.307s, 3337.40/s  (0.307s, 3340.95/s)  LR: 1.130e-04  Data: 0.023 (0.025)
Train: 235 [1150/1251 ( 92%)]  Loss: 2.995 (3.01)  Time: 0.308s, 3326.41/s  (0.307s, 3340.89/s)  LR: 1.129e-04  Data: 0.020 (0.025)
Train: 235 [1200/1251 ( 96%)]  Loss: 3.166 (3.02)  Time: 0.306s, 3344.97/s  (0.307s, 3340.44/s)  LR: 1.127e-04  Data: 0.022 (0.025)
Train: 235 [1250/1251 (100%)]  Loss: 2.948 (3.02)  Time: 0.277s, 3697.72/s  (0.306s, 3342.11/s)  LR: 1.126e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.057 (2.057)  Loss:  0.4907 (0.4907)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.052 (0.235)  Loss:  0.6045 (0.9545)  Acc@1: 86.6745 (78.7580)  Acc@5: 98.2311 (94.2980)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-234.pth.tar', 78.76200000976563)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-235.pth.tar', 78.75800002929688)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-233.pth.tar', 78.74399997802735)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-232.pth.tar', 78.67999989990234)

Train: 236 [   0/1251 (  0%)]  Loss: 2.699 (2.70)  Time: 1.865s,  548.93/s  (1.865s,  548.93/s)  LR: 1.126e-04  Data: 1.641 (1.641)
Train: 236 [  50/1251 (  4%)]  Loss: 2.993 (2.85)  Time: 0.300s, 3415.97/s  (0.328s, 3123.42/s)  LR: 1.125e-04  Data: 0.022 (0.062)
Train: 236 [ 100/1251 (  8%)]  Loss: 2.997 (2.90)  Time: 0.303s, 3380.57/s  (0.315s, 3255.74/s)  LR: 1.124e-04  Data: 0.025 (0.043)
Train: 236 [ 150/1251 ( 12%)]  Loss: 3.316 (3.00)  Time: 0.299s, 3427.45/s  (0.311s, 3296.13/s)  LR: 1.122e-04  Data: 0.027 (0.036)
Train: 236 [ 200/1251 ( 16%)]  Loss: 3.164 (3.03)  Time: 0.300s, 3408.51/s  (0.309s, 3316.03/s)  LR: 1.121e-04  Data: 0.023 (0.033)
Train: 236 [ 250/1251 ( 20%)]  Loss: 2.769 (2.99)  Time: 0.304s, 3369.00/s  (0.308s, 3325.74/s)  LR: 1.120e-04  Data: 0.022 (0.031)
Train: 236 [ 300/1251 ( 24%)]  Loss: 2.947 (2.98)  Time: 0.305s, 3359.45/s  (0.308s, 3329.62/s)  LR: 1.118e-04  Data: 0.022 (0.030)
Train: 236 [ 350/1251 ( 28%)]  Loss: 2.882 (2.97)  Time: 0.306s, 3343.90/s  (0.307s, 3333.06/s)  LR: 1.117e-04  Data: 0.021 (0.029)
Train: 236 [ 400/1251 ( 32%)]  Loss: 2.952 (2.97)  Time: 0.302s, 3392.63/s  (0.307s, 3335.73/s)  LR: 1.116e-04  Data: 0.022 (0.028)
Train: 236 [ 450/1251 ( 36%)]  Loss: 3.070 (2.98)  Time: 0.308s, 3327.77/s  (0.307s, 3335.87/s)  LR: 1.114e-04  Data: 0.024 (0.028)
Train: 236 [ 500/1251 ( 40%)]  Loss: 3.126 (2.99)  Time: 0.305s, 3359.06/s  (0.307s, 3337.70/s)  LR: 1.113e-04  Data: 0.026 (0.027)
Train: 236 [ 550/1251 ( 44%)]  Loss: 3.340 (3.02)  Time: 0.310s, 3308.46/s  (0.307s, 3338.27/s)  LR: 1.112e-04  Data: 0.022 (0.027)
Train: 236 [ 600/1251 ( 48%)]  Loss: 2.914 (3.01)  Time: 0.305s, 3358.09/s  (0.307s, 3339.36/s)  LR: 1.111e-04  Data: 0.026 (0.027)
Train: 236 [ 650/1251 ( 52%)]  Loss: 2.808 (3.00)  Time: 0.311s, 3290.50/s  (0.307s, 3339.51/s)  LR: 1.109e-04  Data: 0.026 (0.026)
Train: 236 [ 700/1251 ( 56%)]  Loss: 3.048 (3.00)  Time: 0.309s, 3315.87/s  (0.307s, 3338.82/s)  LR: 1.108e-04  Data: 0.023 (0.026)
Train: 236 [ 750/1251 ( 60%)]  Loss: 2.912 (3.00)  Time: 0.306s, 3351.60/s  (0.307s, 3338.95/s)  LR: 1.107e-04  Data: 0.023 (0.026)
Train: 236 [ 800/1251 ( 64%)]  Loss: 3.053 (3.00)  Time: 0.306s, 3344.90/s  (0.307s, 3338.46/s)  LR: 1.105e-04  Data: 0.026 (0.026)
Train: 236 [ 850/1251 ( 68%)]  Loss: 2.962 (3.00)  Time: 0.307s, 3335.30/s  (0.307s, 3338.62/s)  LR: 1.104e-04  Data: 0.022 (0.026)
Train: 236 [ 900/1251 ( 72%)]  Loss: 3.039 (3.00)  Time: 0.314s, 3266.00/s  (0.307s, 3338.19/s)  LR: 1.103e-04  Data: 0.021 (0.026)
Train: 236 [ 950/1251 ( 76%)]  Loss: 3.145 (3.01)  Time: 0.309s, 3319.23/s  (0.307s, 3338.23/s)  LR: 1.102e-04  Data: 0.026 (0.025)
Train: 236 [1000/1251 ( 80%)]  Loss: 2.751 (2.99)  Time: 0.307s, 3339.32/s  (0.307s, 3338.04/s)  LR: 1.100e-04  Data: 0.024 (0.025)
Train: 236 [1050/1251 ( 84%)]  Loss: 2.972 (2.99)  Time: 0.311s, 3290.18/s  (0.307s, 3338.12/s)  LR: 1.099e-04  Data: 0.024 (0.025)
Train: 236 [1100/1251 ( 88%)]  Loss: 3.179 (3.00)  Time: 0.310s, 3300.63/s  (0.307s, 3337.62/s)  LR: 1.098e-04  Data: 0.023 (0.025)
Train: 236 [1150/1251 ( 92%)]  Loss: 3.095 (3.01)  Time: 0.307s, 3334.69/s  (0.307s, 3337.26/s)  LR: 1.097e-04  Data: 0.022 (0.025)
Train: 236 [1200/1251 ( 96%)]  Loss: 3.140 (3.01)  Time: 0.304s, 3373.27/s  (0.307s, 3337.59/s)  LR: 1.095e-04  Data: 0.022 (0.025)
Train: 236 [1250/1251 (100%)]  Loss: 2.839 (3.00)  Time: 0.276s, 3711.93/s  (0.307s, 3339.39/s)  LR: 1.094e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.003 (2.003)  Loss:  0.4675 (0.4675)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.056 (0.234)  Loss:  0.6055 (0.9500)  Acc@1: 85.7311 (78.8180)  Acc@5: 97.6415 (94.3820)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-236.pth.tar', 78.81800000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-234.pth.tar', 78.76200000976563)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-235.pth.tar', 78.75800002929688)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-233.pth.tar', 78.74399997802735)

Train: 237 [   0/1251 (  0%)]  Loss: 2.802 (2.80)  Time: 2.079s,  492.45/s  (2.079s,  492.45/s)  LR: 1.094e-04  Data: 1.844 (1.844)
Train: 237 [  50/1251 (  4%)]  Loss: 2.823 (2.81)  Time: 0.300s, 3411.84/s  (0.328s, 3123.53/s)  LR: 1.093e-04  Data: 0.020 (0.062)
Train: 237 [ 100/1251 (  8%)]  Loss: 2.909 (2.84)  Time: 0.297s, 3449.52/s  (0.314s, 3258.66/s)  LR: 1.091e-04  Data: 0.023 (0.043)
Train: 237 [ 150/1251 ( 12%)]  Loss: 3.443 (2.99)  Time: 0.306s, 3349.57/s  (0.310s, 3297.91/s)  LR: 1.090e-04  Data: 0.023 (0.036)
Train: 237 [ 200/1251 ( 16%)]  Loss: 2.988 (2.99)  Time: 0.300s, 3414.41/s  (0.309s, 3316.00/s)  LR: 1.089e-04  Data: 0.023 (0.033)
Train: 237 [ 250/1251 ( 20%)]  Loss: 3.023 (3.00)  Time: 0.303s, 3384.56/s  (0.308s, 3324.53/s)  LR: 1.088e-04  Data: 0.024 (0.031)
Train: 237 [ 300/1251 ( 24%)]  Loss: 3.139 (3.02)  Time: 0.311s, 3290.72/s  (0.307s, 3330.22/s)  LR: 1.086e-04  Data: 0.027 (0.030)
Train: 237 [ 350/1251 ( 28%)]  Loss: 2.879 (3.00)  Time: 0.307s, 3332.05/s  (0.307s, 3334.05/s)  LR: 1.085e-04  Data: 0.025 (0.029)
Train: 237 [ 400/1251 ( 32%)]  Loss: 3.066 (3.01)  Time: 0.306s, 3345.66/s  (0.307s, 3336.86/s)  LR: 1.084e-04  Data: 0.022 (0.028)
Train: 237 [ 450/1251 ( 36%)]  Loss: 3.091 (3.02)  Time: 0.306s, 3350.00/s  (0.307s, 3338.85/s)  LR: 1.082e-04  Data: 0.025 (0.028)
Train: 237 [ 500/1251 ( 40%)]  Loss: 3.036 (3.02)  Time: 0.299s, 3420.70/s  (0.307s, 3340.41/s)  LR: 1.081e-04  Data: 0.020 (0.027)
Train: 237 [ 550/1251 ( 44%)]  Loss: 3.067 (3.02)  Time: 0.306s, 3342.02/s  (0.306s, 3342.01/s)  LR: 1.080e-04  Data: 0.024 (0.027)
Train: 237 [ 600/1251 ( 48%)]  Loss: 3.129 (3.03)  Time: 0.306s, 3345.84/s  (0.306s, 3343.28/s)  LR: 1.079e-04  Data: 0.024 (0.027)
Train: 237 [ 650/1251 ( 52%)]  Loss: 2.924 (3.02)  Time: 0.309s, 3317.24/s  (0.306s, 3343.71/s)  LR: 1.077e-04  Data: 0.025 (0.026)
Train: 237 [ 700/1251 ( 56%)]  Loss: 3.227 (3.04)  Time: 0.308s, 3329.76/s  (0.306s, 3344.05/s)  LR: 1.076e-04  Data: 0.022 (0.026)
Train: 237 [ 750/1251 ( 60%)]  Loss: 2.917 (3.03)  Time: 0.306s, 3342.03/s  (0.306s, 3344.25/s)  LR: 1.075e-04  Data: 0.023 (0.026)
Train: 237 [ 800/1251 ( 64%)]  Loss: 2.922 (3.02)  Time: 0.311s, 3291.36/s  (0.306s, 3345.09/s)  LR: 1.074e-04  Data: 0.023 (0.026)
Train: 237 [ 850/1251 ( 68%)]  Loss: 2.971 (3.02)  Time: 0.306s, 3350.84/s  (0.306s, 3346.32/s)  LR: 1.072e-04  Data: 0.026 (0.026)
Train: 237 [ 900/1251 ( 72%)]  Loss: 2.908 (3.01)  Time: 0.305s, 3357.87/s  (0.306s, 3346.84/s)  LR: 1.071e-04  Data: 0.021 (0.026)
Train: 237 [ 950/1251 ( 76%)]  Loss: 2.834 (3.00)  Time: 0.298s, 3440.90/s  (0.306s, 3347.47/s)  LR: 1.070e-04  Data: 0.019 (0.025)
Train: 237 [1000/1251 ( 80%)]  Loss: 3.150 (3.01)  Time: 0.306s, 3350.11/s  (0.306s, 3348.15/s)  LR: 1.069e-04  Data: 0.022 (0.025)
Train: 237 [1050/1251 ( 84%)]  Loss: 3.088 (3.02)  Time: 0.307s, 3332.56/s  (0.306s, 3348.67/s)  LR: 1.067e-04  Data: 0.024 (0.025)
Train: 237 [1100/1251 ( 88%)]  Loss: 3.072 (3.02)  Time: 0.305s, 3356.22/s  (0.306s, 3348.80/s)  LR: 1.066e-04  Data: 0.022 (0.025)
Train: 237 [1150/1251 ( 92%)]  Loss: 2.705 (3.00)  Time: 0.304s, 3369.65/s  (0.306s, 3349.22/s)  LR: 1.065e-04  Data: 0.022 (0.025)
Train: 237 [1200/1251 ( 96%)]  Loss: 3.217 (3.01)  Time: 0.302s, 3388.58/s  (0.306s, 3349.66/s)  LR: 1.064e-04  Data: 0.025 (0.025)
Train: 237 [1250/1251 (100%)]  Loss: 3.149 (3.02)  Time: 0.278s, 3687.00/s  (0.305s, 3352.44/s)  LR: 1.062e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.035 (2.035)  Loss:  0.4741 (0.4741)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.058 (0.237)  Loss:  0.5879 (0.9523)  Acc@1: 85.9670 (78.6820)  Acc@5: 97.5236 (94.3560)
Train: 238 [   0/1251 (  0%)]  Loss: 2.768 (2.77)  Time: 2.271s,  450.84/s  (2.271s,  450.84/s)  LR: 1.062e-04  Data: 2.041 (2.041)
Train: 238 [  50/1251 (  4%)]  Loss: 2.937 (2.85)  Time: 0.305s, 3361.95/s  (0.328s, 3122.49/s)  LR: 1.061e-04  Data: 0.025 (0.065)
Train: 238 [ 100/1251 (  8%)]  Loss: 3.309 (3.00)  Time: 0.302s, 3394.81/s  (0.313s, 3267.03/s)  LR: 1.060e-04  Data: 0.027 (0.044)
Train: 238 [ 150/1251 ( 12%)]  Loss: 3.088 (3.03)  Time: 0.305s, 3358.96/s  (0.309s, 3310.94/s)  LR: 1.058e-04  Data: 0.023 (0.037)
Train: 238 [ 200/1251 ( 16%)]  Loss: 2.927 (3.01)  Time: 0.304s, 3373.78/s  (0.307s, 3331.07/s)  LR: 1.057e-04  Data: 0.023 (0.034)
Train: 238 [ 250/1251 ( 20%)]  Loss: 3.079 (3.02)  Time: 0.303s, 3379.42/s  (0.306s, 3342.66/s)  LR: 1.056e-04  Data: 0.022 (0.032)
Train: 238 [ 300/1251 ( 24%)]  Loss: 2.958 (3.01)  Time: 0.303s, 3377.92/s  (0.306s, 3348.64/s)  LR: 1.055e-04  Data: 0.021 (0.030)
Train: 238 [ 350/1251 ( 28%)]  Loss: 3.174 (3.03)  Time: 0.301s, 3401.31/s  (0.305s, 3353.33/s)  LR: 1.053e-04  Data: 0.024 (0.029)
Train: 238 [ 400/1251 ( 32%)]  Loss: 2.874 (3.01)  Time: 0.305s, 3355.14/s  (0.305s, 3356.33/s)  LR: 1.052e-04  Data: 0.023 (0.029)
Train: 238 [ 450/1251 ( 36%)]  Loss: 3.021 (3.01)  Time: 0.306s, 3349.41/s  (0.305s, 3357.35/s)  LR: 1.051e-04  Data: 0.019 (0.028)
Train: 238 [ 500/1251 ( 40%)]  Loss: 2.712 (2.99)  Time: 0.301s, 3398.03/s  (0.305s, 3359.53/s)  LR: 1.050e-04  Data: 0.021 (0.028)
Train: 238 [ 550/1251 ( 44%)]  Loss: 2.770 (2.97)  Time: 0.308s, 3327.29/s  (0.305s, 3360.18/s)  LR: 1.048e-04  Data: 0.026 (0.027)
Train: 238 [ 600/1251 ( 48%)]  Loss: 2.803 (2.96)  Time: 0.301s, 3399.99/s  (0.305s, 3361.44/s)  LR: 1.047e-04  Data: 0.025 (0.027)
Train: 238 [ 650/1251 ( 52%)]  Loss: 3.028 (2.96)  Time: 0.305s, 3360.87/s  (0.305s, 3362.76/s)  LR: 1.046e-04  Data: 0.022 (0.027)
Train: 238 [ 700/1251 ( 56%)]  Loss: 2.970 (2.96)  Time: 0.310s, 3308.53/s  (0.304s, 3364.02/s)  LR: 1.045e-04  Data: 0.026 (0.026)
Train: 238 [ 750/1251 ( 60%)]  Loss: 2.959 (2.96)  Time: 0.306s, 3344.37/s  (0.304s, 3364.37/s)  LR: 1.043e-04  Data: 0.020 (0.026)
Train: 238 [ 800/1251 ( 64%)]  Loss: 3.223 (2.98)  Time: 0.306s, 3351.04/s  (0.304s, 3365.32/s)  LR: 1.042e-04  Data: 0.023 (0.026)
Train: 238 [ 850/1251 ( 68%)]  Loss: 3.157 (2.99)  Time: 0.307s, 3332.48/s  (0.304s, 3365.74/s)  LR: 1.041e-04  Data: 0.023 (0.026)
Train: 238 [ 900/1251 ( 72%)]  Loss: 2.874 (2.98)  Time: 0.303s, 3384.54/s  (0.304s, 3365.95/s)  LR: 1.040e-04  Data: 0.025 (0.026)
Train: 238 [ 950/1251 ( 76%)]  Loss: 3.016 (2.98)  Time: 0.306s, 3350.08/s  (0.304s, 3366.10/s)  LR: 1.038e-04  Data: 0.021 (0.026)
Train: 238 [1000/1251 ( 80%)]  Loss: 3.098 (2.99)  Time: 0.303s, 3377.73/s  (0.304s, 3366.64/s)  LR: 1.037e-04  Data: 0.024 (0.026)
Train: 238 [1050/1251 ( 84%)]  Loss: 3.117 (2.99)  Time: 0.312s, 3279.79/s  (0.304s, 3366.49/s)  LR: 1.036e-04  Data: 0.026 (0.025)
Train: 238 [1100/1251 ( 88%)]  Loss: 2.554 (2.97)  Time: 0.299s, 3421.03/s  (0.304s, 3366.92/s)  LR: 1.035e-04  Data: 0.021 (0.025)
Train: 238 [1150/1251 ( 92%)]  Loss: 2.931 (2.97)  Time: 0.305s, 3356.44/s  (0.304s, 3367.34/s)  LR: 1.033e-04  Data: 0.023 (0.025)
Train: 238 [1200/1251 ( 96%)]  Loss: 2.717 (2.96)  Time: 0.303s, 3384.18/s  (0.304s, 3367.38/s)  LR: 1.032e-04  Data: 0.022 (0.025)
Train: 238 [1250/1251 (100%)]  Loss: 3.173 (2.97)  Time: 0.276s, 3709.83/s  (0.304s, 3369.40/s)  LR: 1.031e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.110 (2.110)  Loss:  0.4751 (0.4751)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.054 (0.233)  Loss:  0.6123 (0.9476)  Acc@1: 85.9670 (78.7120)  Acc@5: 97.6415 (94.4240)
Train: 239 [   0/1251 (  0%)]  Loss: 2.750 (2.75)  Time: 2.169s,  472.09/s  (2.169s,  472.09/s)  LR: 1.031e-04  Data: 1.938 (1.938)
Train: 239 [  50/1251 (  4%)]  Loss: 2.837 (2.79)  Time: 0.296s, 3464.49/s  (0.326s, 3142.38/s)  LR: 1.030e-04  Data: 0.023 (0.063)
Train: 239 [ 100/1251 (  8%)]  Loss: 3.005 (2.86)  Time: 0.308s, 3323.66/s  (0.311s, 3288.25/s)  LR: 1.028e-04  Data: 0.024 (0.043)
Train: 239 [ 150/1251 ( 12%)]  Loss: 3.339 (2.98)  Time: 0.300s, 3418.49/s  (0.307s, 3330.42/s)  LR: 1.027e-04  Data: 0.020 (0.037)
Train: 239 [ 200/1251 ( 16%)]  Loss: 2.949 (2.98)  Time: 0.304s, 3369.13/s  (0.306s, 3349.70/s)  LR: 1.026e-04  Data: 0.024 (0.033)
Train: 239 [ 250/1251 ( 20%)]  Loss: 3.196 (3.01)  Time: 0.302s, 3393.96/s  (0.305s, 3360.24/s)  LR: 1.025e-04  Data: 0.024 (0.031)
Train: 239 [ 300/1251 ( 24%)]  Loss: 2.657 (2.96)  Time: 0.299s, 3420.71/s  (0.304s, 3365.51/s)  LR: 1.024e-04  Data: 0.026 (0.030)
Train: 239 [ 350/1251 ( 28%)]  Loss: 2.745 (2.93)  Time: 0.303s, 3384.05/s  (0.304s, 3369.00/s)  LR: 1.022e-04  Data: 0.026 (0.029)
Train: 239 [ 400/1251 ( 32%)]  Loss: 2.963 (2.94)  Time: 0.299s, 3424.31/s  (0.304s, 3372.47/s)  LR: 1.021e-04  Data: 0.022 (0.028)
Train: 239 [ 450/1251 ( 36%)]  Loss: 2.933 (2.94)  Time: 0.301s, 3399.12/s  (0.303s, 3374.98/s)  LR: 1.020e-04  Data: 0.022 (0.028)
Train: 239 [ 500/1251 ( 40%)]  Loss: 3.109 (2.95)  Time: 0.306s, 3350.34/s  (0.303s, 3376.14/s)  LR: 1.019e-04  Data: 0.025 (0.027)
Train: 239 [ 550/1251 ( 44%)]  Loss: 3.108 (2.97)  Time: 0.305s, 3356.96/s  (0.303s, 3376.26/s)  LR: 1.017e-04  Data: 0.027 (0.027)
Train: 239 [ 600/1251 ( 48%)]  Loss: 2.885 (2.96)  Time: 0.300s, 3417.51/s  (0.303s, 3377.17/s)  LR: 1.016e-04  Data: 0.027 (0.027)
Train: 239 [ 650/1251 ( 52%)]  Loss: 3.076 (2.97)  Time: 0.302s, 3390.92/s  (0.303s, 3377.49/s)  LR: 1.015e-04  Data: 0.020 (0.027)
Train: 239 [ 700/1251 ( 56%)]  Loss: 3.041 (2.97)  Time: 0.304s, 3364.70/s  (0.303s, 3377.82/s)  LR: 1.014e-04  Data: 0.024 (0.026)
Train: 239 [ 750/1251 ( 60%)]  Loss: 2.975 (2.97)  Time: 0.306s, 3343.12/s  (0.303s, 3377.94/s)  LR: 1.012e-04  Data: 0.021 (0.026)
Train: 239 [ 800/1251 ( 64%)]  Loss: 3.119 (2.98)  Time: 0.302s, 3392.23/s  (0.303s, 3377.67/s)  LR: 1.011e-04  Data: 0.025 (0.026)
Train: 239 [ 850/1251 ( 68%)]  Loss: 2.733 (2.97)  Time: 0.307s, 3334.67/s  (0.303s, 3378.00/s)  LR: 1.010e-04  Data: 0.024 (0.026)
Train: 239 [ 900/1251 ( 72%)]  Loss: 3.048 (2.97)  Time: 0.310s, 3307.50/s  (0.303s, 3378.04/s)  LR: 1.009e-04  Data: 0.026 (0.026)
Train: 239 [ 950/1251 ( 76%)]  Loss: 3.193 (2.98)  Time: 0.306s, 3351.53/s  (0.303s, 3378.32/s)  LR: 1.007e-04  Data: 0.022 (0.025)
Train: 239 [1000/1251 ( 80%)]  Loss: 3.092 (2.99)  Time: 0.305s, 3355.13/s  (0.303s, 3378.33/s)  LR: 1.006e-04  Data: 0.024 (0.025)
Train: 239 [1050/1251 ( 84%)]  Loss: 3.196 (3.00)  Time: 0.304s, 3366.97/s  (0.303s, 3377.91/s)  LR: 1.005e-04  Data: 0.023 (0.025)
Train: 239 [1100/1251 ( 88%)]  Loss: 2.956 (3.00)  Time: 0.308s, 3329.50/s  (0.303s, 3377.89/s)  LR: 1.004e-04  Data: 0.028 (0.025)
Train: 239 [1150/1251 ( 92%)]  Loss: 2.733 (2.98)  Time: 0.300s, 3418.20/s  (0.303s, 3378.29/s)  LR: 1.003e-04  Data: 0.023 (0.025)
Train: 239 [1200/1251 ( 96%)]  Loss: 2.971 (2.98)  Time: 0.303s, 3378.98/s  (0.303s, 3378.59/s)  LR: 1.001e-04  Data: 0.020 (0.025)
Train: 239 [1250/1251 (100%)]  Loss: 3.141 (2.99)  Time: 0.276s, 3706.71/s  (0.303s, 3381.08/s)  LR: 1.000e-04  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.055 (2.055)  Loss:  0.4702 (0.4702)  Acc@1: 92.7734 (92.7734)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.049 (0.239)  Loss:  0.6152 (0.9468)  Acc@1: 85.3774 (78.6440)  Acc@5: 96.9340 (94.3040)
Train: 240 [   0/1251 (  0%)]  Loss: 2.964 (2.96)  Time: 2.261s,  452.90/s  (2.261s,  452.90/s)  LR: 1.000e-04  Data: 2.030 (2.030)
Train: 240 [  50/1251 (  4%)]  Loss: 3.082 (3.02)  Time: 0.289s, 3538.30/s  (0.321s, 3185.69/s)  LR: 9.989e-05  Data: 0.021 (0.064)
Train: 240 [ 100/1251 (  8%)]  Loss: 2.982 (3.01)  Time: 0.302s, 3396.26/s  (0.308s, 3323.07/s)  LR: 9.977e-05  Data: 0.027 (0.044)
Train: 240 [ 150/1251 ( 12%)]  Loss: 3.147 (3.04)  Time: 0.290s, 3533.39/s  (0.304s, 3364.79/s)  LR: 9.964e-05  Data: 0.025 (0.037)
Train: 240 [ 200/1251 ( 16%)]  Loss: 3.018 (3.04)  Time: 0.300s, 3418.06/s  (0.303s, 3383.60/s)  LR: 9.952e-05  Data: 0.022 (0.034)
Train: 240 [ 250/1251 ( 20%)]  Loss: 2.844 (3.01)  Time: 0.299s, 3422.13/s  (0.302s, 3390.33/s)  LR: 9.940e-05  Data: 0.021 (0.032)
Train: 240 [ 300/1251 ( 24%)]  Loss: 3.257 (3.04)  Time: 0.297s, 3450.60/s  (0.301s, 3397.19/s)  LR: 9.928e-05  Data: 0.024 (0.030)
Train: 240 [ 350/1251 ( 28%)]  Loss: 2.926 (3.03)  Time: 0.301s, 3406.46/s  (0.301s, 3400.90/s)  LR: 9.916e-05  Data: 0.023 (0.029)
Train: 240 [ 400/1251 ( 32%)]  Loss: 2.985 (3.02)  Time: 0.301s, 3404.34/s  (0.301s, 3403.07/s)  LR: 9.903e-05  Data: 0.023 (0.029)
Train: 240 [ 450/1251 ( 36%)]  Loss: 2.899 (3.01)  Time: 0.302s, 3386.34/s  (0.301s, 3404.14/s)  LR: 9.891e-05  Data: 0.023 (0.028)
Train: 240 [ 500/1251 ( 40%)]  Loss: 3.182 (3.03)  Time: 0.302s, 3385.39/s  (0.301s, 3405.72/s)  LR: 9.879e-05  Data: 0.024 (0.028)
Train: 240 [ 550/1251 ( 44%)]  Loss: 3.241 (3.04)  Time: 0.301s, 3400.28/s  (0.301s, 3406.30/s)  LR: 9.867e-05  Data: 0.024 (0.027)
Train: 240 [ 600/1251 ( 48%)]  Loss: 3.033 (3.04)  Time: 0.299s, 3424.26/s  (0.301s, 3406.65/s)  LR: 9.855e-05  Data: 0.028 (0.027)
Train: 240 [ 650/1251 ( 52%)]  Loss: 3.083 (3.05)  Time: 0.303s, 3383.96/s  (0.301s, 3406.91/s)  LR: 9.843e-05  Data: 0.018 (0.027)
Train: 240 [ 700/1251 ( 56%)]  Loss: 3.094 (3.05)  Time: 0.299s, 3422.40/s  (0.301s, 3407.60/s)  LR: 9.831e-05  Data: 0.023 (0.026)
Train: 240 [ 750/1251 ( 60%)]  Loss: 2.829 (3.04)  Time: 0.301s, 3406.77/s  (0.300s, 3407.73/s)  LR: 9.818e-05  Data: 0.024 (0.026)
Train: 240 [ 800/1251 ( 64%)]  Loss: 3.374 (3.06)  Time: 0.298s, 3438.68/s  (0.300s, 3408.24/s)  LR: 9.806e-05  Data: 0.023 (0.026)
Train: 240 [ 850/1251 ( 68%)]  Loss: 3.025 (3.05)  Time: 0.298s, 3438.04/s  (0.300s, 3409.46/s)  LR: 9.794e-05  Data: 0.025 (0.026)
Train: 240 [ 900/1251 ( 72%)]  Loss: 2.833 (3.04)  Time: 0.295s, 3469.20/s  (0.300s, 3409.66/s)  LR: 9.782e-05  Data: 0.020 (0.026)
Train: 240 [ 950/1251 ( 76%)]  Loss: 2.946 (3.04)  Time: 0.301s, 3402.54/s  (0.300s, 3410.40/s)  LR: 9.770e-05  Data: 0.026 (0.026)
Train: 240 [1000/1251 ( 80%)]  Loss: 2.805 (3.03)  Time: 0.305s, 3359.96/s  (0.300s, 3410.21/s)  LR: 9.758e-05  Data: 0.019 (0.026)
Train: 240 [1050/1251 ( 84%)]  Loss: 3.130 (3.03)  Time: 0.305s, 3353.31/s  (0.300s, 3410.15/s)  LR: 9.746e-05  Data: 0.024 (0.026)
Train: 240 [1100/1251 ( 88%)]  Loss: 3.117 (3.03)  Time: 0.290s, 3527.16/s  (0.300s, 3409.99/s)  LR: 9.734e-05  Data: 0.019 (0.025)
Train: 240 [1150/1251 ( 92%)]  Loss: 2.937 (3.03)  Time: 0.297s, 3450.46/s  (0.300s, 3410.03/s)  LR: 9.722e-05  Data: 0.021 (0.025)
Train: 240 [1200/1251 ( 96%)]  Loss: 2.968 (3.03)  Time: 0.297s, 3444.14/s  (0.300s, 3410.05/s)  LR: 9.709e-05  Data: 0.023 (0.025)
Train: 240 [1250/1251 (100%)]  Loss: 2.901 (3.02)  Time: 0.276s, 3714.24/s  (0.300s, 3412.13/s)  LR: 9.697e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.334 (2.334)  Loss:  0.4883 (0.4883)  Acc@1: 91.6992 (91.6992)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.048 (0.235)  Loss:  0.6025 (0.9437)  Acc@1: 85.9670 (79.0360)  Acc@5: 97.6415 (94.4180)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-240.pth.tar', 79.03600010986328)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-236.pth.tar', 78.81800000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-234.pth.tar', 78.76200000976563)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-235.pth.tar', 78.75800002929688)

Train: 241 [   0/1251 (  0%)]  Loss: 2.815 (2.81)  Time: 2.253s,  454.41/s  (2.253s,  454.41/s)  LR: 9.697e-05  Data: 2.027 (2.027)
Train: 241 [  50/1251 (  4%)]  Loss: 2.768 (2.79)  Time: 0.292s, 3512.73/s  (0.328s, 3120.60/s)  LR: 9.685e-05  Data: 0.020 (0.073)
Train: 241 [ 100/1251 (  8%)]  Loss: 2.927 (2.84)  Time: 0.304s, 3370.93/s  (0.311s, 3295.02/s)  LR: 9.673e-05  Data: 0.022 (0.049)
Train: 241 [ 150/1251 ( 12%)]  Loss: 3.118 (2.91)  Time: 0.302s, 3393.81/s  (0.306s, 3351.22/s)  LR: 9.661e-05  Data: 0.024 (0.041)
Train: 241 [ 200/1251 ( 16%)]  Loss: 2.782 (2.88)  Time: 0.293s, 3490.79/s  (0.303s, 3378.46/s)  LR: 9.649e-05  Data: 0.026 (0.036)
Train: 241 [ 250/1251 ( 20%)]  Loss: 2.775 (2.86)  Time: 0.294s, 3486.42/s  (0.302s, 3393.79/s)  LR: 9.637e-05  Data: 0.022 (0.034)
Train: 241 [ 300/1251 ( 24%)]  Loss: 3.048 (2.89)  Time: 0.296s, 3461.42/s  (0.301s, 3402.23/s)  LR: 9.625e-05  Data: 0.023 (0.032)
Train: 241 [ 350/1251 ( 28%)]  Loss: 3.127 (2.92)  Time: 0.299s, 3424.57/s  (0.300s, 3407.85/s)  LR: 9.613e-05  Data: 0.022 (0.031)
Train: 241 [ 400/1251 ( 32%)]  Loss: 3.140 (2.94)  Time: 0.300s, 3410.00/s  (0.300s, 3411.63/s)  LR: 9.601e-05  Data: 0.024 (0.030)
Train: 241 [ 450/1251 ( 36%)]  Loss: 3.091 (2.96)  Time: 0.303s, 3376.61/s  (0.300s, 3415.15/s)  LR: 9.589e-05  Data: 0.028 (0.029)
Train: 241 [ 500/1251 ( 40%)]  Loss: 3.113 (2.97)  Time: 0.301s, 3398.27/s  (0.300s, 3417.07/s)  LR: 9.577e-05  Data: 0.023 (0.029)
Train: 241 [ 550/1251 ( 44%)]  Loss: 2.807 (2.96)  Time: 0.294s, 3486.84/s  (0.299s, 3419.21/s)  LR: 9.565e-05  Data: 0.023 (0.028)
Train: 241 [ 600/1251 ( 48%)]  Loss: 2.925 (2.96)  Time: 0.298s, 3431.92/s  (0.299s, 3419.52/s)  LR: 9.553e-05  Data: 0.022 (0.028)
Train: 241 [ 650/1251 ( 52%)]  Loss: 3.023 (2.96)  Time: 0.298s, 3430.93/s  (0.299s, 3420.57/s)  LR: 9.541e-05  Data: 0.027 (0.027)
Train: 241 [ 700/1251 ( 56%)]  Loss: 2.879 (2.96)  Time: 0.300s, 3409.00/s  (0.299s, 3420.88/s)  LR: 9.529e-05  Data: 0.026 (0.027)
Train: 241 [ 750/1251 ( 60%)]  Loss: 2.782 (2.94)  Time: 0.300s, 3418.37/s  (0.299s, 3421.64/s)  LR: 9.517e-05  Data: 0.023 (0.027)
Train: 241 [ 800/1251 ( 64%)]  Loss: 3.284 (2.96)  Time: 0.298s, 3439.49/s  (0.299s, 3421.89/s)  LR: 9.505e-05  Data: 0.023 (0.027)
Train: 241 [ 850/1251 ( 68%)]  Loss: 3.058 (2.97)  Time: 0.304s, 3365.24/s  (0.299s, 3421.50/s)  LR: 9.493e-05  Data: 0.023 (0.026)
Train: 241 [ 900/1251 ( 72%)]  Loss: 3.147 (2.98)  Time: 0.300s, 3412.81/s  (0.299s, 3421.51/s)  LR: 9.481e-05  Data: 0.024 (0.026)
Train: 241 [ 950/1251 ( 76%)]  Loss: 2.913 (2.98)  Time: 0.298s, 3441.45/s  (0.299s, 3421.04/s)  LR: 9.469e-05  Data: 0.024 (0.026)
Train: 241 [1000/1251 ( 80%)]  Loss: 2.896 (2.97)  Time: 0.299s, 3422.99/s  (0.299s, 3420.61/s)  LR: 9.457e-05  Data: 0.020 (0.026)
Train: 241 [1050/1251 ( 84%)]  Loss: 3.038 (2.98)  Time: 0.302s, 3394.69/s  (0.299s, 3420.53/s)  LR: 9.445e-05  Data: 0.025 (0.026)
Train: 241 [1100/1251 ( 88%)]  Loss: 2.968 (2.97)  Time: 0.301s, 3397.69/s  (0.299s, 3420.30/s)  LR: 9.434e-05  Data: 0.027 (0.026)
Train: 241 [1150/1251 ( 92%)]  Loss: 3.041 (2.98)  Time: 0.300s, 3414.75/s  (0.299s, 3420.59/s)  LR: 9.422e-05  Data: 0.021 (0.026)
Train: 241 [1200/1251 ( 96%)]  Loss: 3.106 (2.98)  Time: 0.302s, 3393.28/s  (0.299s, 3420.65/s)  LR: 9.410e-05  Data: 0.024 (0.026)
Train: 241 [1250/1251 (100%)]  Loss: 2.789 (2.98)  Time: 0.273s, 3746.75/s  (0.299s, 3423.01/s)  LR: 9.398e-05  Data: 0.000 (0.026)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.001 (2.001)  Loss:  0.4915 (0.4915)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.056 (0.237)  Loss:  0.6108 (0.9447)  Acc@1: 85.1415 (79.1000)  Acc@5: 97.1698 (94.3780)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-241.pth.tar', 79.10000000976562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-240.pth.tar', 79.03600010986328)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-236.pth.tar', 78.81800000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-234.pth.tar', 78.76200000976563)

Train: 242 [   0/1251 (  0%)]  Loss: 2.803 (2.80)  Time: 2.664s,  384.41/s  (2.664s,  384.41/s)  LR: 9.398e-05  Data: 2.447 (2.447)
Train: 242 [  50/1251 (  4%)]  Loss: 3.020 (2.91)  Time: 0.296s, 3464.31/s  (0.332s, 3086.18/s)  LR: 9.386e-05  Data: 0.023 (0.071)
Train: 242 [ 100/1251 (  8%)]  Loss: 3.056 (2.96)  Time: 0.299s, 3425.17/s  (0.314s, 3263.82/s)  LR: 9.374e-05  Data: 0.022 (0.048)
Train: 242 [ 150/1251 ( 12%)]  Loss: 3.187 (3.02)  Time: 0.296s, 3455.95/s  (0.308s, 3321.28/s)  LR: 9.362e-05  Data: 0.022 (0.040)
Train: 242 [ 200/1251 ( 16%)]  Loss: 2.924 (3.00)  Time: 0.294s, 3484.54/s  (0.306s, 3351.78/s)  LR: 9.350e-05  Data: 0.021 (0.036)
Train: 242 [ 250/1251 ( 20%)]  Loss: 2.803 (2.97)  Time: 0.300s, 3411.16/s  (0.304s, 3371.65/s)  LR: 9.338e-05  Data: 0.025 (0.033)
Train: 242 [ 300/1251 ( 24%)]  Loss: 2.824 (2.95)  Time: 0.299s, 3419.50/s  (0.303s, 3384.08/s)  LR: 9.326e-05  Data: 0.024 (0.032)
Train: 242 [ 350/1251 ( 28%)]  Loss: 2.971 (2.95)  Time: 0.292s, 3510.93/s  (0.302s, 3393.54/s)  LR: 9.315e-05  Data: 0.026 (0.030)
Train: 242 [ 400/1251 ( 32%)]  Loss: 3.079 (2.96)  Time: 0.293s, 3497.39/s  (0.301s, 3399.47/s)  LR: 9.303e-05  Data: 0.023 (0.030)
Train: 242 [ 450/1251 ( 36%)]  Loss: 2.656 (2.93)  Time: 0.302s, 3388.34/s  (0.301s, 3404.18/s)  LR: 9.291e-05  Data: 0.028 (0.029)
Train: 242 [ 500/1251 ( 40%)]  Loss: 2.642 (2.91)  Time: 0.301s, 3396.96/s  (0.300s, 3407.88/s)  LR: 9.279e-05  Data: 0.024 (0.028)
Train: 242 [ 550/1251 ( 44%)]  Loss: 3.098 (2.92)  Time: 0.295s, 3466.48/s  (0.300s, 3410.37/s)  LR: 9.267e-05  Data: 0.024 (0.028)
Train: 242 [ 600/1251 ( 48%)]  Loss: 3.292 (2.95)  Time: 0.300s, 3410.84/s  (0.300s, 3413.08/s)  LR: 9.256e-05  Data: 0.023 (0.028)
Train: 242 [ 650/1251 ( 52%)]  Loss: 3.243 (2.97)  Time: 0.296s, 3465.06/s  (0.300s, 3414.75/s)  LR: 9.244e-05  Data: 0.020 (0.027)
Train: 242 [ 700/1251 ( 56%)]  Loss: 2.895 (2.97)  Time: 0.297s, 3448.56/s  (0.300s, 3416.56/s)  LR: 9.232e-05  Data: 0.022 (0.027)
Train: 242 [ 750/1251 ( 60%)]  Loss: 3.041 (2.97)  Time: 0.304s, 3368.66/s  (0.300s, 3417.59/s)  LR: 9.220e-05  Data: 0.024 (0.027)
Train: 242 [ 800/1251 ( 64%)]  Loss: 2.924 (2.97)  Time: 0.294s, 3482.83/s  (0.300s, 3418.29/s)  LR: 9.208e-05  Data: 0.027 (0.027)
Train: 242 [ 850/1251 ( 68%)]  Loss: 3.074 (2.97)  Time: 0.301s, 3403.71/s  (0.299s, 3419.04/s)  LR: 9.197e-05  Data: 0.021 (0.026)
Train: 242 [ 900/1251 ( 72%)]  Loss: 3.066 (2.98)  Time: 0.306s, 3349.32/s  (0.299s, 3419.74/s)  LR: 9.185e-05  Data: 0.027 (0.026)
Train: 242 [ 950/1251 ( 76%)]  Loss: 2.913 (2.98)  Time: 0.303s, 3379.94/s  (0.299s, 3420.38/s)  LR: 9.173e-05  Data: 0.020 (0.026)
Train: 242 [1000/1251 ( 80%)]  Loss: 3.053 (2.98)  Time: 0.300s, 3417.51/s  (0.299s, 3421.06/s)  LR: 9.161e-05  Data: 0.023 (0.026)
Train: 242 [1050/1251 ( 84%)]  Loss: 3.043 (2.98)  Time: 0.303s, 3383.94/s  (0.299s, 3422.27/s)  LR: 9.150e-05  Data: 0.024 (0.026)
Train: 242 [1100/1251 ( 88%)]  Loss: 2.700 (2.97)  Time: 0.301s, 3396.98/s  (0.299s, 3422.63/s)  LR: 9.138e-05  Data: 0.023 (0.026)
Train: 242 [1150/1251 ( 92%)]  Loss: 3.056 (2.97)  Time: 0.297s, 3450.84/s  (0.299s, 3423.59/s)  LR: 9.126e-05  Data: 0.021 (0.026)
Train: 242 [1200/1251 ( 96%)]  Loss: 3.141 (2.98)  Time: 0.297s, 3451.89/s  (0.299s, 3423.80/s)  LR: 9.114e-05  Data: 0.025 (0.026)
Train: 242 [1250/1251 (100%)]  Loss: 2.739 (2.97)  Time: 0.275s, 3721.77/s  (0.299s, 3425.91/s)  LR: 9.103e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.997 (1.997)  Loss:  0.4717 (0.4717)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.053 (0.236)  Loss:  0.6040 (0.9398)  Acc@1: 86.3208 (78.9860)  Acc@5: 97.4057 (94.3940)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-241.pth.tar', 79.10000000976562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-240.pth.tar', 79.03600010986328)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-242.pth.tar', 78.98600000488281)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-236.pth.tar', 78.81800000732422)

Train: 243 [   0/1251 (  0%)]  Loss: 3.047 (3.05)  Time: 2.172s,  471.49/s  (2.172s,  471.49/s)  LR: 9.103e-05  Data: 1.952 (1.952)
Train: 243 [  50/1251 (  4%)]  Loss: 2.923 (2.98)  Time: 0.292s, 3511.18/s  (0.318s, 3221.72/s)  LR: 9.091e-05  Data: 0.022 (0.063)
Train: 243 [ 100/1251 (  8%)]  Loss: 3.192 (3.05)  Time: 0.299s, 3429.45/s  (0.304s, 3366.05/s)  LR: 9.079e-05  Data: 0.022 (0.044)
Train: 243 [ 150/1251 ( 12%)]  Loss: 2.992 (3.04)  Time: 0.294s, 3480.72/s  (0.300s, 3408.67/s)  LR: 9.067e-05  Data: 0.022 (0.037)
Train: 243 [ 200/1251 ( 16%)]  Loss: 3.117 (3.05)  Time: 0.293s, 3494.12/s  (0.299s, 3428.07/s)  LR: 9.056e-05  Data: 0.028 (0.034)
Train: 243 [ 250/1251 ( 20%)]  Loss: 3.159 (3.07)  Time: 0.297s, 3452.25/s  (0.298s, 3439.05/s)  LR: 9.044e-05  Data: 0.023 (0.032)
Train: 243 [ 300/1251 ( 24%)]  Loss: 2.994 (3.06)  Time: 0.294s, 3477.42/s  (0.297s, 3446.49/s)  LR: 9.032e-05  Data: 0.023 (0.030)
Train: 243 [ 350/1251 ( 28%)]  Loss: 2.735 (3.02)  Time: 0.294s, 3484.12/s  (0.297s, 3450.17/s)  LR: 9.021e-05  Data: 0.022 (0.029)
Train: 243 [ 400/1251 ( 32%)]  Loss: 3.082 (3.03)  Time: 0.299s, 3425.05/s  (0.297s, 3452.34/s)  LR: 9.009e-05  Data: 0.022 (0.029)
Train: 243 [ 450/1251 ( 36%)]  Loss: 3.077 (3.03)  Time: 0.291s, 3523.35/s  (0.296s, 3454.07/s)  LR: 8.997e-05  Data: 0.024 (0.028)
Train: 243 [ 500/1251 ( 40%)]  Loss: 3.054 (3.03)  Time: 0.298s, 3440.73/s  (0.296s, 3456.64/s)  LR: 8.986e-05  Data: 0.027 (0.028)
Train: 243 [ 550/1251 ( 44%)]  Loss: 2.964 (3.03)  Time: 0.292s, 3506.59/s  (0.296s, 3458.42/s)  LR: 8.974e-05  Data: 0.024 (0.027)
Train: 243 [ 600/1251 ( 48%)]  Loss: 3.256 (3.05)  Time: 0.300s, 3409.45/s  (0.296s, 3459.63/s)  LR: 8.963e-05  Data: 0.016 (0.027)
Train: 243 [ 650/1251 ( 52%)]  Loss: 2.914 (3.04)  Time: 0.299s, 3425.70/s  (0.296s, 3460.87/s)  LR: 8.951e-05  Data: 0.022 (0.027)
Train: 243 [ 700/1251 ( 56%)]  Loss: 2.869 (3.02)  Time: 0.297s, 3444.03/s  (0.296s, 3461.46/s)  LR: 8.939e-05  Data: 0.026 (0.026)
Train: 243 [ 750/1251 ( 60%)]  Loss: 3.113 (3.03)  Time: 0.298s, 3431.42/s  (0.296s, 3462.19/s)  LR: 8.928e-05  Data: 0.020 (0.026)
Train: 243 [ 800/1251 ( 64%)]  Loss: 2.992 (3.03)  Time: 0.285s, 3590.05/s  (0.296s, 3462.77/s)  LR: 8.916e-05  Data: 0.025 (0.026)
Train: 243 [ 850/1251 ( 68%)]  Loss: 2.905 (3.02)  Time: 0.293s, 3498.01/s  (0.296s, 3462.60/s)  LR: 8.905e-05  Data: 0.024 (0.026)
Train: 243 [ 900/1251 ( 72%)]  Loss: 3.175 (3.03)  Time: 0.293s, 3497.90/s  (0.296s, 3463.01/s)  LR: 8.893e-05  Data: 0.025 (0.026)
Train: 243 [ 950/1251 ( 76%)]  Loss: 3.040 (3.03)  Time: 0.299s, 3427.20/s  (0.296s, 3463.68/s)  LR: 8.881e-05  Data: 0.026 (0.026)
Train: 243 [1000/1251 ( 80%)]  Loss: 3.002 (3.03)  Time: 0.298s, 3441.51/s  (0.296s, 3463.98/s)  LR: 8.870e-05  Data: 0.022 (0.026)
Train: 243 [1050/1251 ( 84%)]  Loss: 2.547 (3.01)  Time: 0.296s, 3454.01/s  (0.296s, 3463.71/s)  LR: 8.858e-05  Data: 0.025 (0.025)
Train: 243 [1100/1251 ( 88%)]  Loss: 2.863 (3.00)  Time: 0.289s, 3540.65/s  (0.296s, 3464.04/s)  LR: 8.847e-05  Data: 0.023 (0.025)
Train: 243 [1150/1251 ( 92%)]  Loss: 2.862 (2.99)  Time: 0.291s, 3518.68/s  (0.296s, 3465.05/s)  LR: 8.835e-05  Data: 0.020 (0.025)
Train: 243 [1200/1251 ( 96%)]  Loss: 3.113 (3.00)  Time: 0.298s, 3434.53/s  (0.295s, 3465.88/s)  LR: 8.824e-05  Data: 0.023 (0.025)
Train: 243 [1250/1251 (100%)]  Loss: 3.004 (3.00)  Time: 0.270s, 3790.19/s  (0.295s, 3468.67/s)  LR: 8.812e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.082 (2.082)  Loss:  0.4753 (0.4753)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.043 (0.236)  Loss:  0.6152 (0.9394)  Acc@1: 85.4953 (79.0640)  Acc@5: 97.1698 (94.4100)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-241.pth.tar', 79.10000000976562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-243.pth.tar', 79.06400003417968)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-240.pth.tar', 79.03600010986328)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-242.pth.tar', 78.98600000488281)

Train: 244 [   0/1251 (  0%)]  Loss: 2.769 (2.77)  Time: 2.171s,  471.65/s  (2.171s,  471.65/s)  LR: 8.812e-05  Data: 1.945 (1.945)
Train: 244 [  50/1251 (  4%)]  Loss: 3.367 (3.07)  Time: 0.298s, 3441.75/s  (0.316s, 3243.79/s)  LR: 8.800e-05  Data: 0.025 (0.062)
Train: 244 [ 100/1251 (  8%)]  Loss: 2.903 (3.01)  Time: 0.289s, 3538.54/s  (0.302s, 3391.12/s)  LR: 8.789e-05  Data: 0.022 (0.043)
Train: 244 [ 150/1251 ( 12%)]  Loss: 3.073 (3.03)  Time: 0.292s, 3507.60/s  (0.298s, 3433.96/s)  LR: 8.777e-05  Data: 0.022 (0.037)
Train: 244 [ 200/1251 ( 16%)]  Loss: 3.011 (3.02)  Time: 0.292s, 3501.07/s  (0.297s, 3451.38/s)  LR: 8.766e-05  Data: 0.027 (0.033)
Train: 244 [ 250/1251 ( 20%)]  Loss: 3.018 (3.02)  Time: 0.291s, 3519.03/s  (0.296s, 3461.93/s)  LR: 8.754e-05  Data: 0.023 (0.031)
Train: 244 [ 300/1251 ( 24%)]  Loss: 3.285 (3.06)  Time: 0.291s, 3524.34/s  (0.295s, 3467.88/s)  LR: 8.743e-05  Data: 0.024 (0.030)
Train: 244 [ 350/1251 ( 28%)]  Loss: 2.953 (3.05)  Time: 0.294s, 3479.42/s  (0.295s, 3471.73/s)  LR: 8.731e-05  Data: 0.022 (0.029)
Train: 244 [ 400/1251 ( 32%)]  Loss: 3.499 (3.10)  Time: 0.292s, 3501.00/s  (0.295s, 3474.24/s)  LR: 8.720e-05  Data: 0.024 (0.028)
Train: 244 [ 450/1251 ( 36%)]  Loss: 2.985 (3.09)  Time: 0.289s, 3541.68/s  (0.295s, 3474.85/s)  LR: 8.709e-05  Data: 0.021 (0.028)
Train: 244 [ 500/1251 ( 40%)]  Loss: 2.807 (3.06)  Time: 0.293s, 3498.41/s  (0.295s, 3473.96/s)  LR: 8.697e-05  Data: 0.022 (0.028)
Train: 244 [ 550/1251 ( 44%)]  Loss: 3.135 (3.07)  Time: 0.303s, 3381.66/s  (0.295s, 3473.27/s)  LR: 8.686e-05  Data: 0.026 (0.027)
Train: 244 [ 600/1251 ( 48%)]  Loss: 2.919 (3.06)  Time: 0.296s, 3454.67/s  (0.295s, 3472.64/s)  LR: 8.674e-05  Data: 0.024 (0.027)
Train: 244 [ 650/1251 ( 52%)]  Loss: 2.916 (3.05)  Time: 0.303s, 3384.17/s  (0.295s, 3470.38/s)  LR: 8.663e-05  Data: 0.021 (0.027)
Train: 244 [ 700/1251 ( 56%)]  Loss: 3.023 (3.04)  Time: 0.301s, 3399.71/s  (0.295s, 3469.14/s)  LR: 8.651e-05  Data: 0.025 (0.026)
Train: 244 [ 750/1251 ( 60%)]  Loss: 2.784 (3.03)  Time: 0.292s, 3512.65/s  (0.295s, 3468.32/s)  LR: 8.640e-05  Data: 0.023 (0.026)
Train: 244 [ 800/1251 ( 64%)]  Loss: 2.728 (3.01)  Time: 0.290s, 3532.61/s  (0.295s, 3466.98/s)  LR: 8.629e-05  Data: 0.026 (0.026)
Train: 244 [ 850/1251 ( 68%)]  Loss: 3.236 (3.02)  Time: 0.301s, 3402.85/s  (0.295s, 3465.95/s)  LR: 8.617e-05  Data: 0.026 (0.026)
Train: 244 [ 900/1251 ( 72%)]  Loss: 2.894 (3.02)  Time: 0.296s, 3459.55/s  (0.296s, 3464.67/s)  LR: 8.606e-05  Data: 0.026 (0.026)
Train: 244 [ 950/1251 ( 76%)]  Loss: 3.221 (3.03)  Time: 0.298s, 3435.22/s  (0.296s, 3463.19/s)  LR: 8.594e-05  Data: 0.023 (0.026)
Train: 244 [1000/1251 ( 80%)]  Loss: 3.076 (3.03)  Time: 0.298s, 3435.02/s  (0.296s, 3461.89/s)  LR: 8.583e-05  Data: 0.022 (0.026)
Train: 244 [1050/1251 ( 84%)]  Loss: 2.983 (3.03)  Time: 0.299s, 3424.71/s  (0.296s, 3460.24/s)  LR: 8.572e-05  Data: 0.027 (0.026)
Train: 244 [1100/1251 ( 88%)]  Loss: 2.876 (3.02)  Time: 0.297s, 3445.09/s  (0.296s, 3459.49/s)  LR: 8.560e-05  Data: 0.025 (0.025)
Train: 244 [1150/1251 ( 92%)]  Loss: 3.054 (3.02)  Time: 0.295s, 3469.67/s  (0.296s, 3458.64/s)  LR: 8.549e-05  Data: 0.023 (0.025)
Train: 244 [1200/1251 ( 96%)]  Loss: 3.028 (3.02)  Time: 0.290s, 3532.28/s  (0.296s, 3457.92/s)  LR: 8.537e-05  Data: 0.028 (0.025)
Train: 244 [1250/1251 (100%)]  Loss: 2.972 (3.02)  Time: 0.275s, 3720.54/s  (0.296s, 3459.04/s)  LR: 8.526e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.026 (2.026)  Loss:  0.4756 (0.4756)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.055 (0.239)  Loss:  0.6299 (0.9392)  Acc@1: 84.6698 (79.1460)  Acc@5: 97.1698 (94.5140)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-244.pth.tar', 79.14600006347656)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-241.pth.tar', 79.10000000976562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-243.pth.tar', 79.06400003417968)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-240.pth.tar', 79.03600010986328)

Train: 245 [   0/1251 (  0%)]  Loss: 3.029 (3.03)  Time: 2.198s,  465.89/s  (2.198s,  465.89/s)  LR: 8.526e-05  Data: 1.969 (1.969)
Train: 245 [  50/1251 (  4%)]  Loss: 2.989 (3.01)  Time: 0.292s, 3510.70/s  (0.319s, 3210.88/s)  LR: 8.515e-05  Data: 0.023 (0.064)
Train: 245 [ 100/1251 (  8%)]  Loss: 2.877 (2.96)  Time: 0.290s, 3525.48/s  (0.305s, 3358.99/s)  LR: 8.503e-05  Data: 0.023 (0.044)
Train: 245 [ 150/1251 ( 12%)]  Loss: 3.128 (3.01)  Time: 0.296s, 3456.32/s  (0.301s, 3404.31/s)  LR: 8.492e-05  Data: 0.026 (0.037)
Train: 245 [ 200/1251 ( 16%)]  Loss: 2.814 (2.97)  Time: 0.295s, 3474.01/s  (0.299s, 3426.62/s)  LR: 8.481e-05  Data: 0.030 (0.034)
Train: 245 [ 250/1251 ( 20%)]  Loss: 3.054 (2.98)  Time: 0.295s, 3475.86/s  (0.298s, 3437.46/s)  LR: 8.469e-05  Data: 0.023 (0.032)
Train: 245 [ 300/1251 ( 24%)]  Loss: 3.120 (3.00)  Time: 0.302s, 3388.44/s  (0.298s, 3441.54/s)  LR: 8.458e-05  Data: 0.026 (0.030)
Train: 245 [ 350/1251 ( 28%)]  Loss: 2.804 (2.98)  Time: 0.292s, 3504.37/s  (0.297s, 3445.56/s)  LR: 8.447e-05  Data: 0.021 (0.029)
Train: 245 [ 400/1251 ( 32%)]  Loss: 2.947 (2.97)  Time: 0.293s, 3494.00/s  (0.297s, 3448.44/s)  LR: 8.435e-05  Data: 0.024 (0.029)
Train: 245 [ 450/1251 ( 36%)]  Loss: 3.017 (2.98)  Time: 0.297s, 3445.24/s  (0.297s, 3451.10/s)  LR: 8.424e-05  Data: 0.024 (0.028)
Train: 245 [ 500/1251 ( 40%)]  Loss: 3.118 (2.99)  Time: 0.297s, 3444.83/s  (0.297s, 3452.68/s)  LR: 8.413e-05  Data: 0.028 (0.028)
Train: 245 [ 550/1251 ( 44%)]  Loss: 3.336 (3.02)  Time: 0.297s, 3445.34/s  (0.296s, 3453.98/s)  LR: 8.402e-05  Data: 0.021 (0.027)
Train: 245 [ 600/1251 ( 48%)]  Loss: 3.225 (3.04)  Time: 0.292s, 3502.10/s  (0.296s, 3454.62/s)  LR: 8.390e-05  Data: 0.021 (0.027)
Train: 245 [ 650/1251 ( 52%)]  Loss: 3.268 (3.05)  Time: 0.295s, 3465.80/s  (0.296s, 3455.23/s)  LR: 8.379e-05  Data: 0.023 (0.027)
Train: 245 [ 700/1251 ( 56%)]  Loss: 3.000 (3.05)  Time: 0.300s, 3411.35/s  (0.296s, 3456.37/s)  LR: 8.368e-05  Data: 0.022 (0.026)
Train: 245 [ 750/1251 ( 60%)]  Loss: 3.005 (3.05)  Time: 0.297s, 3446.76/s  (0.296s, 3457.63/s)  LR: 8.357e-05  Data: 0.021 (0.026)
Train: 245 [ 800/1251 ( 64%)]  Loss: 3.076 (3.05)  Time: 0.295s, 3472.66/s  (0.296s, 3458.39/s)  LR: 8.345e-05  Data: 0.022 (0.026)
Train: 245 [ 850/1251 ( 68%)]  Loss: 2.749 (3.03)  Time: 0.298s, 3430.52/s  (0.296s, 3458.27/s)  LR: 8.334e-05  Data: 0.023 (0.026)
Train: 245 [ 900/1251 ( 72%)]  Loss: 2.968 (3.03)  Time: 0.298s, 3439.01/s  (0.296s, 3458.11/s)  LR: 8.323e-05  Data: 0.027 (0.026)
Train: 245 [ 950/1251 ( 76%)]  Loss: 2.698 (3.01)  Time: 0.299s, 3420.40/s  (0.296s, 3457.10/s)  LR: 8.312e-05  Data: 0.026 (0.026)
Train: 245 [1000/1251 ( 80%)]  Loss: 2.990 (3.01)  Time: 0.296s, 3456.00/s  (0.296s, 3456.10/s)  LR: 8.301e-05  Data: 0.023 (0.025)
Train: 245 [1050/1251 ( 84%)]  Loss: 3.091 (3.01)  Time: 0.295s, 3475.89/s  (0.296s, 3455.00/s)  LR: 8.289e-05  Data: 0.024 (0.025)
Train: 245 [1100/1251 ( 88%)]  Loss: 3.138 (3.02)  Time: 0.299s, 3422.41/s  (0.296s, 3454.06/s)  LR: 8.278e-05  Data: 0.024 (0.025)
Train: 245 [1150/1251 ( 92%)]  Loss: 3.235 (3.03)  Time: 0.299s, 3423.02/s  (0.297s, 3453.01/s)  LR: 8.267e-05  Data: 0.023 (0.025)
Train: 245 [1200/1251 ( 96%)]  Loss: 3.072 (3.03)  Time: 0.288s, 3551.74/s  (0.297s, 3452.39/s)  LR: 8.256e-05  Data: 0.025 (0.025)
Train: 245 [1250/1251 (100%)]  Loss: 2.797 (3.02)  Time: 0.271s, 3781.88/s  (0.296s, 3453.63/s)  LR: 8.245e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.104 (2.104)  Loss:  0.4731 (0.4731)  Acc@1: 91.6992 (91.6992)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.053 (0.233)  Loss:  0.6201 (0.9420)  Acc@1: 85.7311 (79.1080)  Acc@5: 97.0519 (94.5080)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-244.pth.tar', 79.14600006347656)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-245.pth.tar', 79.10800000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-241.pth.tar', 79.10000000976562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-243.pth.tar', 79.06400003417968)

Train: 246 [   0/1251 (  0%)]  Loss: 2.699 (2.70)  Time: 2.156s,  475.01/s  (2.156s,  475.01/s)  LR: 8.244e-05  Data: 1.919 (1.919)
Train: 246 [  50/1251 (  4%)]  Loss: 2.817 (2.76)  Time: 0.293s, 3497.91/s  (0.317s, 3228.18/s)  LR: 8.233e-05  Data: 0.022 (0.062)
Train: 246 [ 100/1251 (  8%)]  Loss: 3.139 (2.89)  Time: 0.296s, 3461.18/s  (0.304s, 3367.37/s)  LR: 8.222e-05  Data: 0.029 (0.043)
Train: 246 [ 150/1251 ( 12%)]  Loss: 2.904 (2.89)  Time: 0.294s, 3481.64/s  (0.300s, 3410.86/s)  LR: 8.211e-05  Data: 0.023 (0.037)
Train: 246 [ 200/1251 ( 16%)]  Loss: 3.050 (2.92)  Time: 0.295s, 3468.60/s  (0.299s, 3430.12/s)  LR: 8.200e-05  Data: 0.022 (0.034)
Train: 246 [ 250/1251 ( 20%)]  Loss: 2.987 (2.93)  Time: 0.294s, 3488.69/s  (0.298s, 3438.93/s)  LR: 8.189e-05  Data: 0.022 (0.032)
Train: 246 [ 300/1251 ( 24%)]  Loss: 3.183 (2.97)  Time: 0.295s, 3475.29/s  (0.297s, 3447.38/s)  LR: 8.178e-05  Data: 0.024 (0.030)
Train: 246 [ 350/1251 ( 28%)]  Loss: 2.998 (2.97)  Time: 0.293s, 3500.33/s  (0.297s, 3451.64/s)  LR: 8.167e-05  Data: 0.021 (0.029)
Train: 246 [ 400/1251 ( 32%)]  Loss: 2.784 (2.95)  Time: 0.303s, 3377.99/s  (0.297s, 3453.56/s)  LR: 8.155e-05  Data: 0.027 (0.029)
Train: 246 [ 450/1251 ( 36%)]  Loss: 2.689 (2.93)  Time: 0.297s, 3442.97/s  (0.296s, 3455.03/s)  LR: 8.144e-05  Data: 0.022 (0.028)
Train: 246 [ 500/1251 ( 40%)]  Loss: 3.096 (2.94)  Time: 0.286s, 3575.09/s  (0.296s, 3455.29/s)  LR: 8.133e-05  Data: 0.022 (0.028)
Train: 246 [ 550/1251 ( 44%)]  Loss: 3.050 (2.95)  Time: 0.301s, 3407.61/s  (0.296s, 3455.64/s)  LR: 8.122e-05  Data: 0.018 (0.027)
Train: 246 [ 600/1251 ( 48%)]  Loss: 2.857 (2.94)  Time: 0.302s, 3394.00/s  (0.296s, 3456.54/s)  LR: 8.111e-05  Data: 0.024 (0.027)
Train: 246 [ 650/1251 ( 52%)]  Loss: 2.897 (2.94)  Time: 0.303s, 3383.68/s  (0.296s, 3458.20/s)  LR: 8.100e-05  Data: 0.021 (0.027)
Train: 246 [ 700/1251 ( 56%)]  Loss: 2.764 (2.93)  Time: 0.292s, 3511.23/s  (0.296s, 3458.76/s)  LR: 8.089e-05  Data: 0.022 (0.026)
Train: 246 [ 750/1251 ( 60%)]  Loss: 3.183 (2.94)  Time: 0.291s, 3524.31/s  (0.296s, 3458.95/s)  LR: 8.078e-05  Data: 0.024 (0.026)
Train: 246 [ 800/1251 ( 64%)]  Loss: 2.825 (2.94)  Time: 0.296s, 3459.55/s  (0.296s, 3459.06/s)  LR: 8.067e-05  Data: 0.021 (0.026)
Train: 246 [ 850/1251 ( 68%)]  Loss: 3.032 (2.94)  Time: 0.291s, 3521.74/s  (0.296s, 3459.21/s)  LR: 8.056e-05  Data: 0.019 (0.026)
Train: 246 [ 900/1251 ( 72%)]  Loss: 2.988 (2.94)  Time: 0.299s, 3423.10/s  (0.296s, 3459.58/s)  LR: 8.045e-05  Data: 0.025 (0.026)
Train: 246 [ 950/1251 ( 76%)]  Loss: 2.603 (2.93)  Time: 0.299s, 3423.07/s  (0.296s, 3459.84/s)  LR: 8.034e-05  Data: 0.015 (0.026)
Train: 246 [1000/1251 ( 80%)]  Loss: 2.785 (2.92)  Time: 0.303s, 3378.34/s  (0.296s, 3460.10/s)  LR: 8.023e-05  Data: 0.022 (0.026)
Train: 246 [1050/1251 ( 84%)]  Loss: 2.614 (2.91)  Time: 0.303s, 3382.72/s  (0.296s, 3459.85/s)  LR: 8.012e-05  Data: 0.022 (0.025)
Train: 246 [1100/1251 ( 88%)]  Loss: 3.237 (2.92)  Time: 0.294s, 3477.61/s  (0.296s, 3459.68/s)  LR: 8.001e-05  Data: 0.024 (0.025)
Train: 246 [1150/1251 ( 92%)]  Loss: 2.758 (2.91)  Time: 0.294s, 3484.32/s  (0.296s, 3459.05/s)  LR: 7.990e-05  Data: 0.025 (0.025)
Train: 246 [1200/1251 ( 96%)]  Loss: 3.098 (2.92)  Time: 0.299s, 3427.82/s  (0.296s, 3458.75/s)  LR: 7.979e-05  Data: 0.026 (0.025)
Train: 246 [1250/1251 (100%)]  Loss: 2.826 (2.92)  Time: 0.276s, 3713.16/s  (0.296s, 3460.38/s)  LR: 7.968e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.034 (2.034)  Loss:  0.4792 (0.4792)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.041 (0.236)  Loss:  0.6099 (0.9400)  Acc@1: 85.9670 (78.9900)  Acc@5: 97.4057 (94.4880)
Train: 247 [   0/1251 (  0%)]  Loss: 2.642 (2.64)  Time: 2.317s,  441.94/s  (2.317s,  441.94/s)  LR: 7.968e-05  Data: 2.093 (2.093)
Train: 247 [  50/1251 (  4%)]  Loss: 2.826 (2.73)  Time: 0.283s, 3615.62/s  (0.317s, 3231.68/s)  LR: 7.957e-05  Data: 0.022 (0.066)
Train: 247 [ 100/1251 (  8%)]  Loss: 2.680 (2.72)  Time: 0.290s, 3526.35/s  (0.303s, 3383.74/s)  LR: 7.946e-05  Data: 0.020 (0.045)
Train: 247 [ 150/1251 ( 12%)]  Loss: 2.809 (2.74)  Time: 0.291s, 3519.32/s  (0.299s, 3426.15/s)  LR: 7.935e-05  Data: 0.021 (0.037)
Train: 247 [ 200/1251 ( 16%)]  Loss: 3.168 (2.83)  Time: 0.297s, 3448.72/s  (0.297s, 3445.79/s)  LR: 7.924e-05  Data: 0.023 (0.034)
Train: 247 [ 250/1251 ( 20%)]  Loss: 3.379 (2.92)  Time: 0.300s, 3410.56/s  (0.296s, 3454.11/s)  LR: 7.913e-05  Data: 0.028 (0.032)
Train: 247 [ 300/1251 ( 24%)]  Loss: 2.998 (2.93)  Time: 0.296s, 3462.40/s  (0.296s, 3457.56/s)  LR: 7.902e-05  Data: 0.021 (0.030)
Train: 247 [ 350/1251 ( 28%)]  Loss: 2.706 (2.90)  Time: 0.304s, 3365.40/s  (0.296s, 3459.81/s)  LR: 7.891e-05  Data: 0.027 (0.029)
Train: 247 [ 400/1251 ( 32%)]  Loss: 2.839 (2.89)  Time: 0.291s, 3515.81/s  (0.296s, 3459.89/s)  LR: 7.880e-05  Data: 0.020 (0.029)
Train: 247 [ 450/1251 ( 36%)]  Loss: 2.885 (2.89)  Time: 0.293s, 3498.84/s  (0.296s, 3458.27/s)  LR: 7.869e-05  Data: 0.023 (0.028)
Train: 247 [ 500/1251 ( 40%)]  Loss: 2.790 (2.88)  Time: 0.295s, 3474.53/s  (0.296s, 3458.00/s)  LR: 7.858e-05  Data: 0.026 (0.028)
Train: 247 [ 550/1251 ( 44%)]  Loss: 3.202 (2.91)  Time: 0.288s, 3552.84/s  (0.296s, 3456.66/s)  LR: 7.847e-05  Data: 0.022 (0.027)
Train: 247 [ 600/1251 ( 48%)]  Loss: 3.075 (2.92)  Time: 0.289s, 3540.89/s  (0.296s, 3455.67/s)  LR: 7.836e-05  Data: 0.024 (0.027)
Train: 247 [ 650/1251 ( 52%)]  Loss: 2.943 (2.92)  Time: 0.299s, 3427.24/s  (0.297s, 3453.39/s)  LR: 7.826e-05  Data: 0.027 (0.027)
Train: 247 [ 700/1251 ( 56%)]  Loss: 2.830 (2.92)  Time: 0.303s, 3377.98/s  (0.297s, 3451.92/s)  LR: 7.815e-05  Data: 0.021 (0.026)
Train: 247 [ 750/1251 ( 60%)]  Loss: 2.953 (2.92)  Time: 0.299s, 3427.97/s  (0.297s, 3450.82/s)  LR: 7.804e-05  Data: 0.025 (0.026)
Train: 247 [ 800/1251 ( 64%)]  Loss: 2.619 (2.90)  Time: 0.299s, 3429.98/s  (0.297s, 3449.29/s)  LR: 7.793e-05  Data: 0.025 (0.026)
Train: 247 [ 850/1251 ( 68%)]  Loss: 2.853 (2.90)  Time: 0.294s, 3484.74/s  (0.297s, 3448.27/s)  LR: 7.782e-05  Data: 0.022 (0.026)
Train: 247 [ 900/1251 ( 72%)]  Loss: 2.713 (2.89)  Time: 0.300s, 3411.96/s  (0.297s, 3447.49/s)  LR: 7.771e-05  Data: 0.026 (0.026)
Train: 247 [ 950/1251 ( 76%)]  Loss: 2.956 (2.89)  Time: 0.298s, 3433.80/s  (0.297s, 3445.80/s)  LR: 7.760e-05  Data: 0.022 (0.026)
Train: 247 [1000/1251 ( 80%)]  Loss: 2.914 (2.89)  Time: 0.296s, 3460.51/s  (0.297s, 3444.30/s)  LR: 7.750e-05  Data: 0.021 (0.026)
Train: 247 [1050/1251 ( 84%)]  Loss: 2.966 (2.90)  Time: 0.299s, 3426.71/s  (0.297s, 3443.20/s)  LR: 7.739e-05  Data: 0.024 (0.026)
Train: 247 [1100/1251 ( 88%)]  Loss: 2.965 (2.90)  Time: 0.301s, 3404.14/s  (0.298s, 3441.84/s)  LR: 7.728e-05  Data: 0.022 (0.025)
Train: 247 [1150/1251 ( 92%)]  Loss: 2.761 (2.89)  Time: 0.306s, 3349.25/s  (0.298s, 3440.75/s)  LR: 7.717e-05  Data: 0.024 (0.025)
Train: 247 [1200/1251 ( 96%)]  Loss: 3.079 (2.90)  Time: 0.304s, 3373.95/s  (0.298s, 3439.85/s)  LR: 7.706e-05  Data: 0.026 (0.025)
Train: 247 [1250/1251 (100%)]  Loss: 3.142 (2.91)  Time: 0.276s, 3712.96/s  (0.298s, 3440.72/s)  LR: 7.696e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.140 (2.140)  Loss:  0.4729 (0.4729)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.058 (0.238)  Loss:  0.6396 (0.9365)  Acc@1: 85.3774 (79.1620)  Acc@5: 97.0519 (94.5100)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-247.pth.tar', 79.16200011230468)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-244.pth.tar', 79.14600006347656)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-245.pth.tar', 79.10800000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-241.pth.tar', 79.10000000976562)

Train: 248 [   0/1251 (  0%)]  Loss: 2.989 (2.99)  Time: 2.090s,  489.84/s  (2.090s,  489.84/s)  LR: 7.695e-05  Data: 1.869 (1.869)
Train: 248 [  50/1251 (  4%)]  Loss: 2.740 (2.86)  Time: 0.304s, 3369.67/s  (0.325s, 3147.26/s)  LR: 7.685e-05  Data: 0.027 (0.063)
Train: 248 [ 100/1251 (  8%)]  Loss: 2.940 (2.89)  Time: 0.288s, 3554.90/s  (0.310s, 3298.81/s)  LR: 7.674e-05  Data: 0.022 (0.043)
Train: 248 [ 150/1251 ( 12%)]  Loss: 3.147 (2.95)  Time: 0.293s, 3494.94/s  (0.306s, 3346.71/s)  LR: 7.663e-05  Data: 0.026 (0.037)
Train: 248 [ 200/1251 ( 16%)]  Loss: 2.710 (2.91)  Time: 0.296s, 3456.53/s  (0.304s, 3372.28/s)  LR: 7.652e-05  Data: 0.024 (0.033)
Train: 248 [ 250/1251 ( 20%)]  Loss: 2.877 (2.90)  Time: 0.297s, 3452.51/s  (0.302s, 3386.56/s)  LR: 7.642e-05  Data: 0.025 (0.032)
Train: 248 [ 300/1251 ( 24%)]  Loss: 2.907 (2.90)  Time: 0.298s, 3440.33/s  (0.302s, 3393.27/s)  LR: 7.631e-05  Data: 0.021 (0.030)
Train: 248 [ 350/1251 ( 28%)]  Loss: 2.800 (2.89)  Time: 0.302s, 3392.36/s  (0.301s, 3396.72/s)  LR: 7.620e-05  Data: 0.024 (0.029)
Train: 248 [ 400/1251 ( 32%)]  Loss: 3.081 (2.91)  Time: 0.295s, 3473.20/s  (0.301s, 3399.02/s)  LR: 7.609e-05  Data: 0.023 (0.029)
Train: 248 [ 450/1251 ( 36%)]  Loss: 2.970 (2.92)  Time: 0.297s, 3453.25/s  (0.301s, 3399.92/s)  LR: 7.599e-05  Data: 0.023 (0.028)
Train: 248 [ 500/1251 ( 40%)]  Loss: 3.274 (2.95)  Time: 0.300s, 3414.57/s  (0.301s, 3401.15/s)  LR: 7.588e-05  Data: 0.022 (0.028)
Train: 248 [ 550/1251 ( 44%)]  Loss: 3.087 (2.96)  Time: 0.302s, 3385.44/s  (0.301s, 3401.32/s)  LR: 7.577e-05  Data: 0.022 (0.027)
Train: 248 [ 600/1251 ( 48%)]  Loss: 3.221 (2.98)  Time: 0.301s, 3401.92/s  (0.301s, 3400.90/s)  LR: 7.567e-05  Data: 0.023 (0.027)
Train: 248 [ 650/1251 ( 52%)]  Loss: 2.863 (2.97)  Time: 0.307s, 3330.33/s  (0.301s, 3400.47/s)  LR: 7.556e-05  Data: 0.028 (0.027)
Train: 248 [ 700/1251 ( 56%)]  Loss: 2.933 (2.97)  Time: 0.300s, 3409.93/s  (0.301s, 3400.41/s)  LR: 7.545e-05  Data: 0.022 (0.027)
Train: 248 [ 750/1251 ( 60%)]  Loss: 3.126 (2.98)  Time: 0.304s, 3368.84/s  (0.301s, 3400.37/s)  LR: 7.534e-05  Data: 0.022 (0.026)
Train: 248 [ 800/1251 ( 64%)]  Loss: 2.718 (2.96)  Time: 0.302s, 3390.18/s  (0.301s, 3400.31/s)  LR: 7.524e-05  Data: 0.025 (0.026)
Train: 248 [ 850/1251 ( 68%)]  Loss: 3.163 (2.97)  Time: 0.311s, 3294.81/s  (0.301s, 3400.01/s)  LR: 7.513e-05  Data: 0.022 (0.026)
Train: 248 [ 900/1251 ( 72%)]  Loss: 3.094 (2.98)  Time: 0.298s, 3432.97/s  (0.301s, 3399.76/s)  LR: 7.502e-05  Data: 0.022 (0.026)
Train: 248 [ 950/1251 ( 76%)]  Loss: 3.102 (2.99)  Time: 0.306s, 3344.42/s  (0.301s, 3399.86/s)  LR: 7.492e-05  Data: 0.020 (0.026)
Train: 248 [1000/1251 ( 80%)]  Loss: 2.865 (2.98)  Time: 0.301s, 3396.56/s  (0.301s, 3399.80/s)  LR: 7.481e-05  Data: 0.022 (0.026)
Train: 248 [1050/1251 ( 84%)]  Loss: 3.378 (3.00)  Time: 0.302s, 3394.17/s  (0.301s, 3399.83/s)  LR: 7.471e-05  Data: 0.022 (0.026)
Train: 248 [1100/1251 ( 88%)]  Loss: 2.869 (2.99)  Time: 0.306s, 3346.92/s  (0.301s, 3399.70/s)  LR: 7.460e-05  Data: 0.022 (0.025)
Train: 248 [1150/1251 ( 92%)]  Loss: 2.841 (2.99)  Time: 0.304s, 3370.07/s  (0.301s, 3399.58/s)  LR: 7.449e-05  Data: 0.025 (0.025)
Train: 248 [1200/1251 ( 96%)]  Loss: 3.102 (2.99)  Time: 0.301s, 3396.97/s  (0.301s, 3399.29/s)  LR: 7.439e-05  Data: 0.023 (0.025)
Train: 248 [1250/1251 (100%)]  Loss: 3.154 (3.00)  Time: 0.275s, 3719.90/s  (0.301s, 3400.50/s)  LR: 7.428e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.050 (2.050)  Loss:  0.4922 (0.4922)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.055 (0.240)  Loss:  0.6050 (0.9412)  Acc@1: 86.5566 (79.1760)  Acc@5: 97.2877 (94.4060)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-248.pth.tar', 79.17599997802735)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-247.pth.tar', 79.16200011230468)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-244.pth.tar', 79.14600006347656)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-245.pth.tar', 79.10800000732422)

Train: 249 [   0/1251 (  0%)]  Loss: 2.909 (2.91)  Time: 2.143s,  477.87/s  (2.143s,  477.87/s)  LR: 7.428e-05  Data: 1.916 (1.916)
Train: 249 [  50/1251 (  4%)]  Loss: 3.211 (3.06)  Time: 0.293s, 3491.85/s  (0.324s, 3159.62/s)  LR: 7.417e-05  Data: 0.027 (0.067)
Train: 249 [ 100/1251 (  8%)]  Loss: 3.002 (3.04)  Time: 0.301s, 3397.52/s  (0.310s, 3303.61/s)  LR: 7.407e-05  Data: 0.023 (0.045)
Train: 249 [ 150/1251 ( 12%)]  Loss: 2.856 (2.99)  Time: 0.299s, 3425.67/s  (0.306s, 3346.84/s)  LR: 7.396e-05  Data: 0.023 (0.038)
Train: 249 [ 200/1251 ( 16%)]  Loss: 2.692 (2.93)  Time: 0.304s, 3373.93/s  (0.305s, 3362.85/s)  LR: 7.386e-05  Data: 0.025 (0.034)
Train: 249 [ 250/1251 ( 20%)]  Loss: 3.012 (2.95)  Time: 0.302s, 3396.09/s  (0.303s, 3374.99/s)  LR: 7.375e-05  Data: 0.022 (0.032)
Train: 249 [ 300/1251 ( 24%)]  Loss: 3.228 (2.99)  Time: 0.299s, 3424.47/s  (0.303s, 3381.49/s)  LR: 7.364e-05  Data: 0.022 (0.031)
Train: 249 [ 350/1251 ( 28%)]  Loss: 2.966 (2.98)  Time: 0.304s, 3368.53/s  (0.303s, 3383.78/s)  LR: 7.354e-05  Data: 0.023 (0.030)
Train: 249 [ 400/1251 ( 32%)]  Loss: 3.033 (2.99)  Time: 0.300s, 3418.55/s  (0.302s, 3385.45/s)  LR: 7.343e-05  Data: 0.026 (0.029)
Train: 249 [ 450/1251 ( 36%)]  Loss: 2.924 (2.98)  Time: 0.302s, 3387.44/s  (0.302s, 3386.73/s)  LR: 7.333e-05  Data: 0.023 (0.028)
Train: 249 [ 500/1251 ( 40%)]  Loss: 2.741 (2.96)  Time: 0.301s, 3397.73/s  (0.302s, 3387.63/s)  LR: 7.322e-05  Data: 0.023 (0.028)
Train: 249 [ 550/1251 ( 44%)]  Loss: 2.942 (2.96)  Time: 0.307s, 3333.71/s  (0.302s, 3388.63/s)  LR: 7.312e-05  Data: 0.022 (0.027)
Train: 249 [ 600/1251 ( 48%)]  Loss: 3.153 (2.97)  Time: 0.297s, 3447.57/s  (0.302s, 3390.03/s)  LR: 7.301e-05  Data: 0.021 (0.027)
Train: 249 [ 650/1251 ( 52%)]  Loss: 2.792 (2.96)  Time: 0.304s, 3369.58/s  (0.302s, 3390.16/s)  LR: 7.291e-05  Data: 0.023 (0.027)
Train: 249 [ 700/1251 ( 56%)]  Loss: 2.838 (2.95)  Time: 0.300s, 3414.70/s  (0.302s, 3390.62/s)  LR: 7.280e-05  Data: 0.026 (0.027)
Train: 249 [ 750/1251 ( 60%)]  Loss: 2.827 (2.95)  Time: 0.299s, 3426.10/s  (0.302s, 3390.96/s)  LR: 7.270e-05  Data: 0.024 (0.026)
Train: 249 [ 800/1251 ( 64%)]  Loss: 3.043 (2.95)  Time: 0.302s, 3394.86/s  (0.302s, 3390.85/s)  LR: 7.259e-05  Data: 0.023 (0.026)
Train: 249 [ 850/1251 ( 68%)]  Loss: 3.078 (2.96)  Time: 0.302s, 3396.10/s  (0.302s, 3390.43/s)  LR: 7.249e-05  Data: 0.022 (0.026)
Train: 249 [ 900/1251 ( 72%)]  Loss: 3.188 (2.97)  Time: 0.300s, 3410.44/s  (0.302s, 3389.81/s)  LR: 7.238e-05  Data: 0.022 (0.026)
Train: 249 [ 950/1251 ( 76%)]  Loss: 3.142 (2.98)  Time: 0.305s, 3355.94/s  (0.302s, 3389.29/s)  LR: 7.228e-05  Data: 0.022 (0.026)
Train: 249 [1000/1251 ( 80%)]  Loss: 2.908 (2.98)  Time: 0.304s, 3373.88/s  (0.302s, 3388.85/s)  LR: 7.217e-05  Data: 0.020 (0.026)
Train: 249 [1050/1251 ( 84%)]  Loss: 2.859 (2.97)  Time: 0.302s, 3395.67/s  (0.302s, 3388.28/s)  LR: 7.207e-05  Data: 0.023 (0.026)
Train: 249 [1100/1251 ( 88%)]  Loss: 2.903 (2.97)  Time: 0.300s, 3412.24/s  (0.302s, 3387.66/s)  LR: 7.197e-05  Data: 0.023 (0.025)
Train: 249 [1150/1251 ( 92%)]  Loss: 3.010 (2.97)  Time: 0.305s, 3353.61/s  (0.302s, 3386.99/s)  LR: 7.186e-05  Data: 0.023 (0.025)
Train: 249 [1200/1251 ( 96%)]  Loss: 3.013 (2.97)  Time: 0.303s, 3378.26/s  (0.302s, 3386.50/s)  LR: 7.176e-05  Data: 0.024 (0.025)
Train: 249 [1250/1251 (100%)]  Loss: 2.934 (2.97)  Time: 0.275s, 3716.98/s  (0.302s, 3387.53/s)  LR: 7.165e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.080 (2.080)  Loss:  0.4629 (0.4629)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.056 (0.238)  Loss:  0.5991 (0.9298)  Acc@1: 86.6745 (79.3140)  Acc@5: 97.4057 (94.4840)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-249.pth.tar', 79.31400002929688)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-248.pth.tar', 79.17599997802735)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-247.pth.tar', 79.16200011230468)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-244.pth.tar', 79.14600006347656)

Train: 250 [   0/1251 (  0%)]  Loss: 3.001 (3.00)  Time: 2.196s,  466.33/s  (2.196s,  466.33/s)  LR: 7.165e-05  Data: 1.970 (1.970)
Train: 250 [  50/1251 (  4%)]  Loss: 2.989 (2.99)  Time: 0.292s, 3507.29/s  (0.324s, 3161.77/s)  LR: 7.155e-05  Data: 0.019 (0.063)
Train: 250 [ 100/1251 (  8%)]  Loss: 3.102 (3.03)  Time: 0.300s, 3418.96/s  (0.310s, 3300.39/s)  LR: 7.144e-05  Data: 0.023 (0.043)
Train: 250 [ 150/1251 ( 12%)]  Loss: 2.872 (2.99)  Time: 0.301s, 3403.03/s  (0.307s, 3339.23/s)  LR: 7.134e-05  Data: 0.025 (0.037)
Train: 250 [ 200/1251 ( 16%)]  Loss: 2.846 (2.96)  Time: 0.304s, 3372.47/s  (0.305s, 3359.87/s)  LR: 7.123e-05  Data: 0.026 (0.033)
Train: 250 [ 250/1251 ( 20%)]  Loss: 3.132 (2.99)  Time: 0.303s, 3383.93/s  (0.304s, 3369.74/s)  LR: 7.113e-05  Data: 0.028 (0.031)
Train: 250 [ 300/1251 ( 24%)]  Loss: 3.191 (3.02)  Time: 0.300s, 3411.59/s  (0.303s, 3376.03/s)  LR: 7.103e-05  Data: 0.023 (0.030)
Train: 250 [ 350/1251 ( 28%)]  Loss: 2.925 (3.01)  Time: 0.301s, 3399.73/s  (0.303s, 3379.17/s)  LR: 7.092e-05  Data: 0.024 (0.029)
Train: 250 [ 400/1251 ( 32%)]  Loss: 2.910 (3.00)  Time: 0.302s, 3395.55/s  (0.303s, 3381.98/s)  LR: 7.082e-05  Data: 0.023 (0.029)
Train: 250 [ 450/1251 ( 36%)]  Loss: 3.063 (3.00)  Time: 0.302s, 3391.11/s  (0.303s, 3382.10/s)  LR: 7.072e-05  Data: 0.023 (0.028)
Train: 250 [ 500/1251 ( 40%)]  Loss: 3.081 (3.01)  Time: 0.304s, 3370.17/s  (0.303s, 3382.11/s)  LR: 7.061e-05  Data: 0.025 (0.027)
Train: 250 [ 550/1251 ( 44%)]  Loss: 3.050 (3.01)  Time: 0.303s, 3377.43/s  (0.303s, 3381.86/s)  LR: 7.051e-05  Data: 0.022 (0.027)
Train: 250 [ 600/1251 ( 48%)]  Loss: 2.904 (3.01)  Time: 0.301s, 3407.48/s  (0.303s, 3382.04/s)  LR: 7.041e-05  Data: 0.022 (0.027)
Train: 250 [ 650/1251 ( 52%)]  Loss: 2.652 (2.98)  Time: 0.307s, 3335.01/s  (0.303s, 3381.95/s)  LR: 7.030e-05  Data: 0.019 (0.027)
Train: 250 [ 700/1251 ( 56%)]  Loss: 3.087 (2.99)  Time: 0.299s, 3420.32/s  (0.303s, 3381.95/s)  LR: 7.020e-05  Data: 0.024 (0.026)
Train: 250 [ 750/1251 ( 60%)]  Loss: 2.991 (2.99)  Time: 0.302s, 3386.90/s  (0.303s, 3381.61/s)  LR: 7.010e-05  Data: 0.026 (0.026)
Train: 250 [ 800/1251 ( 64%)]  Loss: 3.053 (2.99)  Time: 0.301s, 3396.75/s  (0.303s, 3381.49/s)  LR: 6.999e-05  Data: 0.023 (0.026)
Train: 250 [ 850/1251 ( 68%)]  Loss: 3.048 (2.99)  Time: 0.306s, 3347.57/s  (0.303s, 3380.83/s)  LR: 6.989e-05  Data: 0.023 (0.026)
Train: 250 [ 900/1251 ( 72%)]  Loss: 2.963 (2.99)  Time: 0.308s, 3322.43/s  (0.303s, 3380.14/s)  LR: 6.979e-05  Data: 0.028 (0.026)
Train: 250 [ 950/1251 ( 76%)]  Loss: 2.732 (2.98)  Time: 0.306s, 3341.46/s  (0.303s, 3379.83/s)  LR: 6.969e-05  Data: 0.024 (0.026)
Train: 250 [1000/1251 ( 80%)]  Loss: 3.315 (3.00)  Time: 0.301s, 3399.11/s  (0.303s, 3379.19/s)  LR: 6.958e-05  Data: 0.022 (0.026)
Train: 250 [1050/1251 ( 84%)]  Loss: 2.948 (2.99)  Time: 0.304s, 3363.20/s  (0.303s, 3378.76/s)  LR: 6.948e-05  Data: 0.023 (0.025)
Train: 250 [1100/1251 ( 88%)]  Loss: 2.913 (2.99)  Time: 0.302s, 3391.16/s  (0.303s, 3377.98/s)  LR: 6.938e-05  Data: 0.022 (0.025)
Train: 250 [1150/1251 ( 92%)]  Loss: 2.850 (2.98)  Time: 0.303s, 3375.84/s  (0.303s, 3376.92/s)  LR: 6.928e-05  Data: 0.021 (0.025)
Train: 250 [1200/1251 ( 96%)]  Loss: 2.843 (2.98)  Time: 0.304s, 3368.29/s  (0.303s, 3375.98/s)  LR: 6.917e-05  Data: 0.025 (0.025)
Train: 250 [1250/1251 (100%)]  Loss: 3.280 (2.99)  Time: 0.277s, 3700.31/s  (0.303s, 3377.07/s)  LR: 6.907e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.359 (2.359)  Loss:  0.4827 (0.4827)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.055 (0.236)  Loss:  0.6299 (0.9391)  Acc@1: 86.2028 (79.2100)  Acc@5: 97.1698 (94.4880)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-249.pth.tar', 79.31400002929688)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-250.pth.tar', 79.20999995361328)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-248.pth.tar', 79.17599997802735)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-247.pth.tar', 79.16200011230468)

Train: 251 [   0/1251 (  0%)]  Loss: 2.951 (2.95)  Time: 2.079s,  492.51/s  (2.079s,  492.51/s)  LR: 6.907e-05  Data: 1.851 (1.851)
Train: 251 [  50/1251 (  4%)]  Loss: 2.709 (2.83)  Time: 0.301s, 3405.23/s  (0.331s, 3096.51/s)  LR: 6.897e-05  Data: 0.022 (0.072)
Train: 251 [ 100/1251 (  8%)]  Loss: 2.998 (2.89)  Time: 0.300s, 3409.46/s  (0.315s, 3252.35/s)  LR: 6.886e-05  Data: 0.026 (0.048)
Train: 251 [ 150/1251 ( 12%)]  Loss: 2.615 (2.82)  Time: 0.304s, 3364.25/s  (0.311s, 3297.30/s)  LR: 6.876e-05  Data: 0.025 (0.040)
Train: 251 [ 200/1251 ( 16%)]  Loss: 3.270 (2.91)  Time: 0.304s, 3363.77/s  (0.309s, 3316.61/s)  LR: 6.866e-05  Data: 0.024 (0.036)
Train: 251 [ 250/1251 ( 20%)]  Loss: 3.044 (2.93)  Time: 0.303s, 3381.91/s  (0.308s, 3326.85/s)  LR: 6.856e-05  Data: 0.026 (0.033)
Train: 251 [ 300/1251 ( 24%)]  Loss: 2.669 (2.89)  Time: 0.303s, 3378.02/s  (0.307s, 3333.91/s)  LR: 6.846e-05  Data: 0.023 (0.032)
Train: 251 [ 350/1251 ( 28%)]  Loss: 2.848 (2.89)  Time: 0.309s, 3316.60/s  (0.307s, 3337.26/s)  LR: 6.836e-05  Data: 0.020 (0.031)
Train: 251 [ 400/1251 ( 32%)]  Loss: 3.008 (2.90)  Time: 0.305s, 3355.02/s  (0.307s, 3340.09/s)  LR: 6.825e-05  Data: 0.027 (0.030)
Train: 251 [ 450/1251 ( 36%)]  Loss: 2.972 (2.91)  Time: 0.309s, 3318.83/s  (0.306s, 3341.65/s)  LR: 6.815e-05  Data: 0.021 (0.029)
Train: 251 [ 500/1251 ( 40%)]  Loss: 3.008 (2.92)  Time: 0.307s, 3333.97/s  (0.306s, 3343.46/s)  LR: 6.805e-05  Data: 0.021 (0.028)
Train: 251 [ 550/1251 ( 44%)]  Loss: 2.968 (2.92)  Time: 0.307s, 3338.91/s  (0.306s, 3344.52/s)  LR: 6.795e-05  Data: 0.026 (0.028)
Train: 251 [ 600/1251 ( 48%)]  Loss: 3.058 (2.93)  Time: 0.307s, 3338.80/s  (0.306s, 3346.14/s)  LR: 6.785e-05  Data: 0.024 (0.028)
Train: 251 [ 650/1251 ( 52%)]  Loss: 2.898 (2.93)  Time: 0.310s, 3304.87/s  (0.306s, 3346.51/s)  LR: 6.775e-05  Data: 0.026 (0.027)
Train: 251 [ 700/1251 ( 56%)]  Loss: 3.212 (2.95)  Time: 0.308s, 3323.09/s  (0.306s, 3346.19/s)  LR: 6.765e-05  Data: 0.021 (0.027)
Train: 251 [ 750/1251 ( 60%)]  Loss: 2.763 (2.94)  Time: 0.307s, 3340.89/s  (0.306s, 3346.79/s)  LR: 6.754e-05  Data: 0.024 (0.027)
Train: 251 [ 800/1251 ( 64%)]  Loss: 2.608 (2.92)  Time: 0.307s, 3336.52/s  (0.306s, 3346.39/s)  LR: 6.744e-05  Data: 0.023 (0.027)
Train: 251 [ 850/1251 ( 68%)]  Loss: 2.976 (2.92)  Time: 0.307s, 3334.64/s  (0.306s, 3346.49/s)  LR: 6.734e-05  Data: 0.022 (0.026)
Train: 251 [ 900/1251 ( 72%)]  Loss: 2.957 (2.92)  Time: 0.309s, 3317.14/s  (0.306s, 3346.11/s)  LR: 6.724e-05  Data: 0.024 (0.026)
Train: 251 [ 950/1251 ( 76%)]  Loss: 2.902 (2.92)  Time: 0.304s, 3363.83/s  (0.306s, 3346.35/s)  LR: 6.714e-05  Data: 0.025 (0.026)
Train: 251 [1000/1251 ( 80%)]  Loss: 3.076 (2.93)  Time: 0.307s, 3337.92/s  (0.306s, 3346.77/s)  LR: 6.704e-05  Data: 0.024 (0.026)
Train: 251 [1050/1251 ( 84%)]  Loss: 2.690 (2.92)  Time: 0.305s, 3358.56/s  (0.306s, 3346.54/s)  LR: 6.694e-05  Data: 0.024 (0.026)
Train: 251 [1100/1251 ( 88%)]  Loss: 2.719 (2.91)  Time: 0.313s, 3276.23/s  (0.306s, 3346.57/s)  LR: 6.684e-05  Data: 0.024 (0.026)
Train: 251 [1150/1251 ( 92%)]  Loss: 2.902 (2.91)  Time: 0.305s, 3361.80/s  (0.306s, 3346.53/s)  LR: 6.674e-05  Data: 0.028 (0.026)
Train: 251 [1200/1251 ( 96%)]  Loss: 2.996 (2.91)  Time: 0.307s, 3331.40/s  (0.306s, 3346.34/s)  LR: 6.664e-05  Data: 0.027 (0.026)
Train: 251 [1250/1251 (100%)]  Loss: 2.701 (2.90)  Time: 0.276s, 3710.48/s  (0.306s, 3348.16/s)  LR: 6.654e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.085 (2.085)  Loss:  0.4761 (0.4761)  Acc@1: 92.3828 (92.3828)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.055 (0.242)  Loss:  0.6284 (0.9419)  Acc@1: 86.0849 (79.1860)  Acc@5: 97.1698 (94.4520)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-249.pth.tar', 79.31400002929688)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-250.pth.tar', 79.20999995361328)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-251.pth.tar', 79.18600003173827)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-248.pth.tar', 79.17599997802735)

Train: 252 [   0/1251 (  0%)]  Loss: 2.898 (2.90)  Time: 2.020s,  507.01/s  (2.020s,  507.01/s)  LR: 6.654e-05  Data: 1.793 (1.793)
Train: 252 [  50/1251 (  4%)]  Loss: 2.918 (2.91)  Time: 0.299s, 3425.78/s  (0.326s, 3140.21/s)  LR: 6.644e-05  Data: 0.023 (0.061)
Train: 252 [ 100/1251 (  8%)]  Loss: 3.203 (3.01)  Time: 0.305s, 3359.59/s  (0.313s, 3269.32/s)  LR: 6.633e-05  Data: 0.024 (0.043)
Train: 252 [ 150/1251 ( 12%)]  Loss: 3.024 (3.01)  Time: 0.305s, 3358.70/s  (0.310s, 3306.39/s)  LR: 6.623e-05  Data: 0.024 (0.036)
Train: 252 [ 200/1251 ( 16%)]  Loss: 2.917 (2.99)  Time: 0.303s, 3377.95/s  (0.308s, 3320.17/s)  LR: 6.613e-05  Data: 0.023 (0.033)
Train: 252 [ 250/1251 ( 20%)]  Loss: 2.900 (2.98)  Time: 0.311s, 3291.57/s  (0.308s, 3323.24/s)  LR: 6.603e-05  Data: 0.021 (0.031)
Train: 252 [ 300/1251 ( 24%)]  Loss: 3.066 (2.99)  Time: 0.304s, 3366.67/s  (0.308s, 3327.45/s)  LR: 6.593e-05  Data: 0.027 (0.030)
Train: 252 [ 350/1251 ( 28%)]  Loss: 3.143 (3.01)  Time: 0.305s, 3355.25/s  (0.308s, 3329.06/s)  LR: 6.584e-05  Data: 0.022 (0.029)
Train: 252 [ 400/1251 ( 32%)]  Loss: 2.877 (2.99)  Time: 0.306s, 3351.17/s  (0.308s, 3329.46/s)  LR: 6.574e-05  Data: 0.028 (0.028)
Train: 252 [ 450/1251 ( 36%)]  Loss: 2.955 (2.99)  Time: 0.311s, 3292.82/s  (0.307s, 3330.28/s)  LR: 6.564e-05  Data: 0.021 (0.028)
Train: 252 [ 500/1251 ( 40%)]  Loss: 3.002 (2.99)  Time: 0.310s, 3307.13/s  (0.307s, 3330.32/s)  LR: 6.554e-05  Data: 0.023 (0.027)
Train: 252 [ 550/1251 ( 44%)]  Loss: 3.104 (3.00)  Time: 0.314s, 3264.88/s  (0.307s, 3330.61/s)  LR: 6.544e-05  Data: 0.023 (0.027)
Train: 252 [ 600/1251 ( 48%)]  Loss: 3.142 (3.01)  Time: 0.306s, 3343.21/s  (0.307s, 3330.57/s)  LR: 6.534e-05  Data: 0.025 (0.027)
Train: 252 [ 650/1251 ( 52%)]  Loss: 3.174 (3.02)  Time: 0.305s, 3354.28/s  (0.307s, 3330.46/s)  LR: 6.524e-05  Data: 0.021 (0.026)
Train: 252 [ 700/1251 ( 56%)]  Loss: 2.959 (3.02)  Time: 0.307s, 3330.33/s  (0.308s, 3329.76/s)  LR: 6.514e-05  Data: 0.022 (0.026)
Train: 252 [ 750/1251 ( 60%)]  Loss: 3.044 (3.02)  Time: 0.307s, 3340.24/s  (0.308s, 3329.39/s)  LR: 6.504e-05  Data: 0.024 (0.026)
Train: 252 [ 800/1251 ( 64%)]  Loss: 2.594 (3.00)  Time: 0.308s, 3328.36/s  (0.308s, 3329.50/s)  LR: 6.494e-05  Data: 0.027 (0.026)
Train: 252 [ 850/1251 ( 68%)]  Loss: 2.893 (2.99)  Time: 0.305s, 3360.60/s  (0.308s, 3329.17/s)  LR: 6.484e-05  Data: 0.022 (0.026)
Train: 252 [ 900/1251 ( 72%)]  Loss: 3.046 (2.99)  Time: 0.310s, 3307.99/s  (0.308s, 3328.81/s)  LR: 6.474e-05  Data: 0.021 (0.026)
Train: 252 [ 950/1251 ( 76%)]  Loss: 2.751 (2.98)  Time: 0.310s, 3305.55/s  (0.308s, 3328.65/s)  LR: 6.464e-05  Data: 0.022 (0.025)
Train: 252 [1000/1251 ( 80%)]  Loss: 3.104 (2.99)  Time: 0.310s, 3298.35/s  (0.308s, 3328.17/s)  LR: 6.454e-05  Data: 0.024 (0.025)
Train: 252 [1050/1251 ( 84%)]  Loss: 3.036 (2.99)  Time: 0.306s, 3345.92/s  (0.308s, 3328.18/s)  LR: 6.445e-05  Data: 0.023 (0.025)
Train: 252 [1100/1251 ( 88%)]  Loss: 3.102 (2.99)  Time: 0.306s, 3351.18/s  (0.308s, 3327.88/s)  LR: 6.435e-05  Data: 0.021 (0.025)
Train: 252 [1150/1251 ( 92%)]  Loss: 2.856 (2.99)  Time: 0.312s, 3286.95/s  (0.308s, 3328.08/s)  LR: 6.425e-05  Data: 0.024 (0.025)
Train: 252 [1200/1251 ( 96%)]  Loss: 2.954 (2.99)  Time: 0.301s, 3403.05/s  (0.308s, 3327.76/s)  LR: 6.415e-05  Data: 0.023 (0.025)
Train: 252 [1250/1251 (100%)]  Loss: 2.702 (2.98)  Time: 0.277s, 3701.57/s  (0.308s, 3329.71/s)  LR: 6.405e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.066 (2.066)  Loss:  0.4817 (0.4817)  Acc@1: 92.0898 (92.0898)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.059 (0.239)  Loss:  0.6187 (0.9354)  Acc@1: 85.4953 (79.3720)  Acc@5: 97.0519 (94.5060)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-252.pth.tar', 79.37200003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-249.pth.tar', 79.31400002929688)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-250.pth.tar', 79.20999995361328)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-251.pth.tar', 79.18600003173827)

Train: 253 [   0/1251 (  0%)]  Loss: 2.851 (2.85)  Time: 2.321s,  441.11/s  (2.321s,  441.11/s)  LR: 6.405e-05  Data: 2.080 (2.080)
Train: 253 [  50/1251 (  4%)]  Loss: 2.917 (2.88)  Time: 0.298s, 3441.56/s  (0.327s, 3133.01/s)  LR: 6.395e-05  Data: 0.022 (0.065)
Train: 253 [ 100/1251 (  8%)]  Loss: 2.760 (2.84)  Time: 0.305s, 3352.00/s  (0.314s, 3266.10/s)  LR: 6.385e-05  Data: 0.026 (0.044)
Train: 253 [ 150/1251 ( 12%)]  Loss: 2.662 (2.80)  Time: 0.307s, 3333.77/s  (0.310s, 3302.85/s)  LR: 6.375e-05  Data: 0.024 (0.038)
Train: 253 [ 200/1251 ( 16%)]  Loss: 2.958 (2.83)  Time: 0.303s, 3375.68/s  (0.308s, 3319.36/s)  LR: 6.366e-05  Data: 0.022 (0.034)
Train: 253 [ 250/1251 ( 20%)]  Loss: 2.665 (2.80)  Time: 0.306s, 3346.59/s  (0.308s, 3325.12/s)  LR: 6.356e-05  Data: 0.023 (0.032)
Train: 253 [ 300/1251 ( 24%)]  Loss: 2.502 (2.76)  Time: 0.305s, 3358.10/s  (0.308s, 3327.49/s)  LR: 6.346e-05  Data: 0.020 (0.030)
Train: 253 [ 350/1251 ( 28%)]  Loss: 2.895 (2.78)  Time: 0.307s, 3338.85/s  (0.308s, 3328.30/s)  LR: 6.336e-05  Data: 0.025 (0.029)
Train: 253 [ 400/1251 ( 32%)]  Loss: 3.192 (2.82)  Time: 0.305s, 3352.16/s  (0.308s, 3329.18/s)  LR: 6.326e-05  Data: 0.028 (0.029)
Train: 253 [ 450/1251 ( 36%)]  Loss: 2.881 (2.83)  Time: 0.308s, 3324.14/s  (0.307s, 3330.48/s)  LR: 6.317e-05  Data: 0.021 (0.028)
Train: 253 [ 500/1251 ( 40%)]  Loss: 2.760 (2.82)  Time: 0.312s, 3286.61/s  (0.307s, 3330.61/s)  LR: 6.307e-05  Data: 0.022 (0.028)
Train: 253 [ 550/1251 ( 44%)]  Loss: 3.066 (2.84)  Time: 0.307s, 3337.11/s  (0.307s, 3330.66/s)  LR: 6.297e-05  Data: 0.016 (0.027)
Train: 253 [ 600/1251 ( 48%)]  Loss: 2.735 (2.83)  Time: 0.308s, 3327.73/s  (0.307s, 3331.68/s)  LR: 6.287e-05  Data: 0.023 (0.027)
Train: 253 [ 650/1251 ( 52%)]  Loss: 3.063 (2.85)  Time: 0.310s, 3305.25/s  (0.307s, 3331.51/s)  LR: 6.278e-05  Data: 0.020 (0.027)
Train: 253 [ 700/1251 ( 56%)]  Loss: 3.044 (2.86)  Time: 0.309s, 3308.89/s  (0.307s, 3331.17/s)  LR: 6.268e-05  Data: 0.022 (0.027)
Train: 253 [ 750/1251 ( 60%)]  Loss: 2.411 (2.83)  Time: 0.310s, 3301.01/s  (0.307s, 3331.10/s)  LR: 6.258e-05  Data: 0.025 (0.026)
Train: 253 [ 800/1251 ( 64%)]  Loss: 2.721 (2.83)  Time: 0.309s, 3311.54/s  (0.307s, 3330.56/s)  LR: 6.249e-05  Data: 0.019 (0.026)
Train: 253 [ 850/1251 ( 68%)]  Loss: 2.843 (2.83)  Time: 0.310s, 3301.63/s  (0.308s, 3329.26/s)  LR: 6.239e-05  Data: 0.023 (0.026)
Train: 253 [ 900/1251 ( 72%)]  Loss: 3.005 (2.84)  Time: 0.309s, 3318.44/s  (0.308s, 3328.80/s)  LR: 6.229e-05  Data: 0.026 (0.026)
Train: 253 [ 950/1251 ( 76%)]  Loss: 2.754 (2.83)  Time: 0.309s, 3312.62/s  (0.308s, 3328.67/s)  LR: 6.219e-05  Data: 0.024 (0.026)
Train: 253 [1000/1251 ( 80%)]  Loss: 2.813 (2.83)  Time: 0.308s, 3328.15/s  (0.308s, 3327.82/s)  LR: 6.210e-05  Data: 0.022 (0.026)
Train: 253 [1050/1251 ( 84%)]  Loss: 2.984 (2.84)  Time: 0.310s, 3298.20/s  (0.308s, 3326.71/s)  LR: 6.200e-05  Data: 0.022 (0.025)
Train: 253 [1100/1251 ( 88%)]  Loss: 3.122 (2.85)  Time: 0.309s, 3316.35/s  (0.308s, 3326.31/s)  LR: 6.190e-05  Data: 0.026 (0.025)
Train: 253 [1150/1251 ( 92%)]  Loss: 2.865 (2.85)  Time: 0.311s, 3293.12/s  (0.308s, 3325.76/s)  LR: 6.181e-05  Data: 0.023 (0.025)
Train: 253 [1200/1251 ( 96%)]  Loss: 3.084 (2.86)  Time: 0.307s, 3340.42/s  (0.308s, 3325.09/s)  LR: 6.171e-05  Data: 0.022 (0.025)
Train: 253 [1250/1251 (100%)]  Loss: 3.087 (2.87)  Time: 0.277s, 3702.65/s  (0.308s, 3326.38/s)  LR: 6.161e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.101 (2.101)  Loss:  0.4714 (0.4714)  Acc@1: 92.2852 (92.2852)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.064 (0.236)  Loss:  0.6221 (0.9338)  Acc@1: 85.0236 (79.3260)  Acc@5: 97.4057 (94.5240)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-252.pth.tar', 79.37200003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-253.pth.tar', 79.32600008789062)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-249.pth.tar', 79.31400002929688)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-250.pth.tar', 79.20999995361328)

Train: 254 [   0/1251 (  0%)]  Loss: 2.376 (2.38)  Time: 2.249s,  455.28/s  (2.249s,  455.28/s)  LR: 6.161e-05  Data: 2.008 (2.008)
Train: 254 [  50/1251 (  4%)]  Loss: 2.887 (2.63)  Time: 0.297s, 3449.46/s  (0.333s, 3078.56/s)  LR: 6.152e-05  Data: 0.022 (0.064)
Train: 254 [ 100/1251 (  8%)]  Loss: 2.656 (2.64)  Time: 0.300s, 3410.74/s  (0.318s, 3224.64/s)  LR: 6.142e-05  Data: 0.023 (0.044)
Train: 254 [ 150/1251 ( 12%)]  Loss: 3.049 (2.74)  Time: 0.305s, 3358.13/s  (0.314s, 3266.23/s)  LR: 6.132e-05  Data: 0.021 (0.037)
Train: 254 [ 200/1251 ( 16%)]  Loss: 3.119 (2.82)  Time: 0.306s, 3348.42/s  (0.312s, 3286.61/s)  LR: 6.123e-05  Data: 0.026 (0.034)
Train: 254 [ 250/1251 ( 20%)]  Loss: 2.788 (2.81)  Time: 0.303s, 3379.93/s  (0.311s, 3296.59/s)  LR: 6.113e-05  Data: 0.026 (0.032)
Train: 254 [ 300/1251 ( 24%)]  Loss: 2.707 (2.80)  Time: 0.307s, 3334.08/s  (0.310s, 3303.49/s)  LR: 6.103e-05  Data: 0.024 (0.030)
Train: 254 [ 350/1251 ( 28%)]  Loss: 2.879 (2.81)  Time: 0.307s, 3340.26/s  (0.310s, 3307.63/s)  LR: 6.094e-05  Data: 0.023 (0.030)
Train: 254 [ 400/1251 ( 32%)]  Loss: 2.579 (2.78)  Time: 0.307s, 3340.65/s  (0.309s, 3311.14/s)  LR: 6.084e-05  Data: 0.024 (0.029)
Train: 254 [ 450/1251 ( 36%)]  Loss: 3.246 (2.83)  Time: 0.313s, 3270.07/s  (0.309s, 3313.75/s)  LR: 6.075e-05  Data: 0.021 (0.028)
Train: 254 [ 500/1251 ( 40%)]  Loss: 3.189 (2.86)  Time: 0.310s, 3308.42/s  (0.309s, 3315.41/s)  LR: 6.065e-05  Data: 0.020 (0.028)
Train: 254 [ 550/1251 ( 44%)]  Loss: 2.988 (2.87)  Time: 0.306s, 3348.93/s  (0.309s, 3317.26/s)  LR: 6.056e-05  Data: 0.022 (0.027)
Train: 254 [ 600/1251 ( 48%)]  Loss: 3.119 (2.89)  Time: 0.305s, 3353.60/s  (0.309s, 3319.12/s)  LR: 6.046e-05  Data: 0.022 (0.027)
Train: 254 [ 650/1251 ( 52%)]  Loss: 3.219 (2.91)  Time: 0.307s, 3340.81/s  (0.308s, 3321.11/s)  LR: 6.036e-05  Data: 0.024 (0.027)
Train: 254 [ 700/1251 ( 56%)]  Loss: 2.928 (2.92)  Time: 0.308s, 3328.22/s  (0.308s, 3322.30/s)  LR: 6.027e-05  Data: 0.023 (0.026)
Train: 254 [ 750/1251 ( 60%)]  Loss: 2.785 (2.91)  Time: 0.312s, 3284.90/s  (0.308s, 3322.87/s)  LR: 6.017e-05  Data: 0.026 (0.026)
Train: 254 [ 800/1251 ( 64%)]  Loss: 2.976 (2.91)  Time: 0.308s, 3321.79/s  (0.308s, 3322.91/s)  LR: 6.008e-05  Data: 0.022 (0.026)
Train: 254 [ 850/1251 ( 68%)]  Loss: 2.978 (2.91)  Time: 0.308s, 3328.94/s  (0.308s, 3323.16/s)  LR: 5.998e-05  Data: 0.023 (0.026)
Train: 254 [ 900/1251 ( 72%)]  Loss: 2.977 (2.92)  Time: 0.310s, 3305.74/s  (0.308s, 3323.31/s)  LR: 5.989e-05  Data: 0.024 (0.026)
Train: 254 [ 950/1251 ( 76%)]  Loss: 2.928 (2.92)  Time: 0.311s, 3290.28/s  (0.308s, 3323.93/s)  LR: 5.979e-05  Data: 0.023 (0.026)
Train: 254 [1000/1251 ( 80%)]  Loss: 2.890 (2.92)  Time: 0.306s, 3341.78/s  (0.308s, 3324.28/s)  LR: 5.970e-05  Data: 0.019 (0.026)
Train: 254 [1050/1251 ( 84%)]  Loss: 2.775 (2.91)  Time: 0.306s, 3345.43/s  (0.308s, 3324.56/s)  LR: 5.960e-05  Data: 0.023 (0.025)
Train: 254 [1100/1251 ( 88%)]  Loss: 2.681 (2.90)  Time: 0.307s, 3338.24/s  (0.308s, 3324.77/s)  LR: 5.951e-05  Data: 0.023 (0.025)
Train: 254 [1150/1251 ( 92%)]  Loss: 2.686 (2.89)  Time: 0.313s, 3269.80/s  (0.308s, 3325.09/s)  LR: 5.941e-05  Data: 0.027 (0.025)
Train: 254 [1200/1251 ( 96%)]  Loss: 3.115 (2.90)  Time: 0.304s, 3370.11/s  (0.308s, 3325.47/s)  LR: 5.932e-05  Data: 0.024 (0.025)
Train: 254 [1250/1251 (100%)]  Loss: 3.045 (2.91)  Time: 0.276s, 3715.23/s  (0.308s, 3327.70/s)  LR: 5.922e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.045 (2.045)  Loss:  0.4795 (0.4795)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.041 (0.236)  Loss:  0.6084 (0.9349)  Acc@1: 86.3208 (79.2280)  Acc@5: 97.6415 (94.5880)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-252.pth.tar', 79.37200003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-253.pth.tar', 79.32600008789062)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-249.pth.tar', 79.31400002929688)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-254.pth.tar', 79.22800013427734)

Train: 255 [   0/1251 (  0%)]  Loss: 3.131 (3.13)  Time: 2.133s,  480.07/s  (2.133s,  480.07/s)  LR: 5.922e-05  Data: 1.905 (1.905)
Train: 255 [  50/1251 (  4%)]  Loss: 2.866 (3.00)  Time: 0.296s, 3462.39/s  (0.327s, 3127.33/s)  LR: 5.913e-05  Data: 0.024 (0.062)
Train: 255 [ 100/1251 (  8%)]  Loss: 3.130 (3.04)  Time: 0.302s, 3394.35/s  (0.314s, 3258.18/s)  LR: 5.903e-05  Data: 0.024 (0.043)
Train: 255 [ 150/1251 ( 12%)]  Loss: 2.854 (3.00)  Time: 0.302s, 3386.62/s  (0.311s, 3296.13/s)  LR: 5.894e-05  Data: 0.022 (0.036)
Train: 255 [ 200/1251 ( 16%)]  Loss: 2.751 (2.95)  Time: 0.305s, 3355.02/s  (0.309s, 3312.01/s)  LR: 5.884e-05  Data: 0.023 (0.033)
Train: 255 [ 250/1251 ( 20%)]  Loss: 3.090 (2.97)  Time: 0.308s, 3320.47/s  (0.309s, 3318.95/s)  LR: 5.875e-05  Data: 0.024 (0.031)
Train: 255 [ 300/1251 ( 24%)]  Loss: 3.191 (3.00)  Time: 0.312s, 3285.94/s  (0.308s, 3325.67/s)  LR: 5.866e-05  Data: 0.025 (0.030)
Train: 255 [ 350/1251 ( 28%)]  Loss: 2.879 (2.99)  Time: 0.307s, 3336.64/s  (0.307s, 3330.37/s)  LR: 5.856e-05  Data: 0.022 (0.029)
Train: 255 [ 400/1251 ( 32%)]  Loss: 2.860 (2.97)  Time: 0.307s, 3339.28/s  (0.307s, 3332.47/s)  LR: 5.847e-05  Data: 0.022 (0.028)
Train: 255 [ 450/1251 ( 36%)]  Loss: 2.751 (2.95)  Time: 0.303s, 3385.03/s  (0.307s, 3334.37/s)  LR: 5.837e-05  Data: 0.023 (0.028)
Train: 255 [ 500/1251 ( 40%)]  Loss: 2.795 (2.94)  Time: 0.308s, 3326.68/s  (0.307s, 3336.21/s)  LR: 5.828e-05  Data: 0.024 (0.027)
Train: 255 [ 550/1251 ( 44%)]  Loss: 3.080 (2.95)  Time: 0.309s, 3314.00/s  (0.307s, 3337.01/s)  LR: 5.819e-05  Data: 0.025 (0.027)
Train: 255 [ 600/1251 ( 48%)]  Loss: 3.035 (2.95)  Time: 0.307s, 3338.59/s  (0.307s, 3337.60/s)  LR: 5.809e-05  Data: 0.024 (0.027)
Train: 255 [ 650/1251 ( 52%)]  Loss: 3.007 (2.96)  Time: 0.306s, 3341.74/s  (0.307s, 3337.75/s)  LR: 5.800e-05  Data: 0.025 (0.026)
Train: 255 [ 700/1251 ( 56%)]  Loss: 2.755 (2.95)  Time: 0.307s, 3337.96/s  (0.307s, 3337.82/s)  LR: 5.791e-05  Data: 0.022 (0.026)
Train: 255 [ 750/1251 ( 60%)]  Loss: 2.947 (2.95)  Time: 0.306s, 3341.26/s  (0.307s, 3338.92/s)  LR: 5.781e-05  Data: 0.023 (0.026)
Train: 255 [ 800/1251 ( 64%)]  Loss: 2.967 (2.95)  Time: 0.304s, 3370.69/s  (0.307s, 3339.86/s)  LR: 5.772e-05  Data: 0.023 (0.026)
Train: 255 [ 850/1251 ( 68%)]  Loss: 3.001 (2.95)  Time: 0.306s, 3344.76/s  (0.307s, 3340.87/s)  LR: 5.763e-05  Data: 0.022 (0.026)
Train: 255 [ 900/1251 ( 72%)]  Loss: 3.156 (2.96)  Time: 0.304s, 3370.90/s  (0.306s, 3341.68/s)  LR: 5.753e-05  Data: 0.021 (0.026)
Train: 255 [ 950/1251 ( 76%)]  Loss: 3.042 (2.96)  Time: 0.306s, 3342.76/s  (0.306s, 3342.25/s)  LR: 5.744e-05  Data: 0.022 (0.025)
Train: 255 [1000/1251 ( 80%)]  Loss: 2.891 (2.96)  Time: 0.305s, 3355.85/s  (0.306s, 3342.88/s)  LR: 5.735e-05  Data: 0.025 (0.025)
Train: 255 [1050/1251 ( 84%)]  Loss: 2.686 (2.95)  Time: 0.305s, 3357.76/s  (0.306s, 3343.35/s)  LR: 5.725e-05  Data: 0.023 (0.025)
Train: 255 [1100/1251 ( 88%)]  Loss: 2.982 (2.95)  Time: 0.301s, 3402.12/s  (0.306s, 3343.96/s)  LR: 5.716e-05  Data: 0.022 (0.025)
Train: 255 [1150/1251 ( 92%)]  Loss: 2.762 (2.94)  Time: 0.308s, 3326.16/s  (0.306s, 3344.61/s)  LR: 5.707e-05  Data: 0.028 (0.025)
Train: 255 [1200/1251 ( 96%)]  Loss: 3.094 (2.95)  Time: 0.305s, 3352.75/s  (0.306s, 3344.95/s)  LR: 5.698e-05  Data: 0.025 (0.025)
Train: 255 [1250/1251 (100%)]  Loss: 2.928 (2.95)  Time: 0.275s, 3718.91/s  (0.306s, 3346.69/s)  LR: 5.688e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.090 (2.090)  Loss:  0.4810 (0.4810)  Acc@1: 92.0898 (92.0898)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.051 (0.236)  Loss:  0.6216 (0.9383)  Acc@1: 85.9670 (79.2760)  Acc@5: 97.4057 (94.4940)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-252.pth.tar', 79.37200003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-253.pth.tar', 79.32600008789062)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-249.pth.tar', 79.31400002929688)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-255.pth.tar', 79.27600010986328)

Train: 256 [   0/1251 (  0%)]  Loss: 2.866 (2.87)  Time: 2.207s,  463.99/s  (2.207s,  463.99/s)  LR: 5.688e-05  Data: 1.985 (1.985)
Train: 256 [  50/1251 (  4%)]  Loss: 2.502 (2.68)  Time: 0.296s, 3456.01/s  (0.328s, 3117.65/s)  LR: 5.679e-05  Data: 0.022 (0.065)
Train: 256 [ 100/1251 (  8%)]  Loss: 2.670 (2.68)  Time: 0.296s, 3461.25/s  (0.314s, 3265.42/s)  LR: 5.670e-05  Data: 0.025 (0.045)
Train: 256 [ 150/1251 ( 12%)]  Loss: 2.975 (2.75)  Time: 0.305s, 3362.30/s  (0.309s, 3308.99/s)  LR: 5.660e-05  Data: 0.025 (0.038)
Train: 256 [ 200/1251 ( 16%)]  Loss: 2.975 (2.80)  Time: 0.302s, 3394.26/s  (0.308s, 3327.46/s)  LR: 5.651e-05  Data: 0.023 (0.034)
Train: 256 [ 250/1251 ( 20%)]  Loss: 3.107 (2.85)  Time: 0.303s, 3383.13/s  (0.307s, 3339.44/s)  LR: 5.642e-05  Data: 0.022 (0.032)
Train: 256 [ 300/1251 ( 24%)]  Loss: 2.911 (2.86)  Time: 0.307s, 3337.80/s  (0.306s, 3346.89/s)  LR: 5.633e-05  Data: 0.021 (0.031)
Train: 256 [ 350/1251 ( 28%)]  Loss: 2.707 (2.84)  Time: 0.304s, 3371.98/s  (0.306s, 3349.30/s)  LR: 5.624e-05  Data: 0.023 (0.030)
Train: 256 [ 400/1251 ( 32%)]  Loss: 3.080 (2.87)  Time: 0.301s, 3402.05/s  (0.305s, 3352.54/s)  LR: 5.614e-05  Data: 0.025 (0.029)
Train: 256 [ 450/1251 ( 36%)]  Loss: 2.933 (2.87)  Time: 0.304s, 3365.55/s  (0.305s, 3355.34/s)  LR: 5.605e-05  Data: 0.023 (0.028)
Train: 256 [ 500/1251 ( 40%)]  Loss: 2.906 (2.88)  Time: 0.307s, 3337.94/s  (0.305s, 3357.78/s)  LR: 5.596e-05  Data: 0.023 (0.028)
Train: 256 [ 550/1251 ( 44%)]  Loss: 2.962 (2.88)  Time: 0.301s, 3407.16/s  (0.305s, 3359.20/s)  LR: 5.587e-05  Data: 0.021 (0.027)
Train: 256 [ 600/1251 ( 48%)]  Loss: 2.725 (2.87)  Time: 0.307s, 3335.51/s  (0.305s, 3360.88/s)  LR: 5.578e-05  Data: 0.022 (0.027)
Train: 256 [ 650/1251 ( 52%)]  Loss: 3.290 (2.90)  Time: 0.302s, 3387.14/s  (0.305s, 3362.45/s)  LR: 5.568e-05  Data: 0.024 (0.027)
Train: 256 [ 700/1251 ( 56%)]  Loss: 2.986 (2.91)  Time: 0.304s, 3369.64/s  (0.304s, 3363.95/s)  LR: 5.559e-05  Data: 0.024 (0.027)
Train: 256 [ 750/1251 ( 60%)]  Loss: 2.684 (2.89)  Time: 0.302s, 3394.69/s  (0.304s, 3364.92/s)  LR: 5.550e-05  Data: 0.025 (0.026)
Train: 256 [ 800/1251 ( 64%)]  Loss: 2.918 (2.89)  Time: 0.307s, 3334.41/s  (0.304s, 3365.90/s)  LR: 5.541e-05  Data: 0.026 (0.026)
Train: 256 [ 850/1251 ( 68%)]  Loss: 2.670 (2.88)  Time: 0.304s, 3373.32/s  (0.304s, 3366.07/s)  LR: 5.532e-05  Data: 0.022 (0.026)
Train: 256 [ 900/1251 ( 72%)]  Loss: 2.785 (2.88)  Time: 0.302s, 3388.17/s  (0.304s, 3366.89/s)  LR: 5.523e-05  Data: 0.023 (0.026)
Train: 256 [ 950/1251 ( 76%)]  Loss: 2.980 (2.88)  Time: 0.303s, 3377.78/s  (0.304s, 3367.71/s)  LR: 5.514e-05  Data: 0.027 (0.026)
Train: 256 [1000/1251 ( 80%)]  Loss: 2.939 (2.88)  Time: 0.302s, 3386.71/s  (0.304s, 3368.35/s)  LR: 5.505e-05  Data: 0.027 (0.026)
Train: 256 [1050/1251 ( 84%)]  Loss: 2.699 (2.88)  Time: 0.303s, 3382.47/s  (0.304s, 3369.20/s)  LR: 5.495e-05  Data: 0.024 (0.026)
Train: 256 [1100/1251 ( 88%)]  Loss: 2.947 (2.88)  Time: 0.307s, 3340.37/s  (0.304s, 3370.34/s)  LR: 5.486e-05  Data: 0.022 (0.025)
Train: 256 [1150/1251 ( 92%)]  Loss: 2.463 (2.86)  Time: 0.312s, 3283.05/s  (0.304s, 3370.98/s)  LR: 5.477e-05  Data: 0.024 (0.025)
Train: 256 [1200/1251 ( 96%)]  Loss: 2.891 (2.86)  Time: 0.301s, 3402.27/s  (0.304s, 3372.15/s)  LR: 5.468e-05  Data: 0.023 (0.025)
Train: 256 [1250/1251 (100%)]  Loss: 3.099 (2.87)  Time: 0.276s, 3710.88/s  (0.303s, 3374.60/s)  LR: 5.459e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.032 (2.032)  Loss:  0.4690 (0.4690)  Acc@1: 92.2852 (92.2852)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.041 (0.236)  Loss:  0.6147 (0.9345)  Acc@1: 86.6745 (79.2880)  Acc@5: 97.5236 (94.5940)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-252.pth.tar', 79.37200003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-253.pth.tar', 79.32600008789062)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-249.pth.tar', 79.31400002929688)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-256.pth.tar', 79.28799989990235)

Train: 257 [   0/1251 (  0%)]  Loss: 3.048 (3.05)  Time: 2.480s,  412.97/s  (2.480s,  412.97/s)  LR: 5.459e-05  Data: 2.264 (2.264)
Train: 257 [  50/1251 (  4%)]  Loss: 3.090 (3.07)  Time: 0.294s, 3487.03/s  (0.320s, 3200.32/s)  LR: 5.450e-05  Data: 0.022 (0.068)
Train: 257 [ 100/1251 (  8%)]  Loss: 3.006 (3.05)  Time: 0.289s, 3538.24/s  (0.307s, 3333.03/s)  LR: 5.441e-05  Data: 0.022 (0.046)
Train: 257 [ 150/1251 ( 12%)]  Loss: 2.924 (3.02)  Time: 0.301s, 3400.73/s  (0.304s, 3366.30/s)  LR: 5.432e-05  Data: 0.027 (0.039)
Train: 257 [ 200/1251 ( 16%)]  Loss: 2.930 (3.00)  Time: 0.300s, 3414.01/s  (0.303s, 3378.54/s)  LR: 5.423e-05  Data: 0.026 (0.035)
Train: 257 [ 250/1251 ( 20%)]  Loss: 3.195 (3.03)  Time: 0.307s, 3339.33/s  (0.302s, 3385.25/s)  LR: 5.414e-05  Data: 0.024 (0.033)
Train: 257 [ 300/1251 ( 24%)]  Loss: 2.974 (3.02)  Time: 0.300s, 3410.96/s  (0.302s, 3389.67/s)  LR: 5.405e-05  Data: 0.020 (0.031)
Train: 257 [ 350/1251 ( 28%)]  Loss: 2.990 (3.02)  Time: 0.297s, 3445.93/s  (0.302s, 3391.72/s)  LR: 5.396e-05  Data: 0.021 (0.030)
Train: 257 [ 400/1251 ( 32%)]  Loss: 2.622 (2.98)  Time: 0.303s, 3380.32/s  (0.302s, 3392.35/s)  LR: 5.387e-05  Data: 0.023 (0.029)
Train: 257 [ 450/1251 ( 36%)]  Loss: 2.946 (2.97)  Time: 0.298s, 3440.56/s  (0.302s, 3394.90/s)  LR: 5.378e-05  Data: 0.022 (0.029)
Train: 257 [ 500/1251 ( 40%)]  Loss: 2.608 (2.94)  Time: 0.304s, 3369.59/s  (0.302s, 3395.39/s)  LR: 5.369e-05  Data: 0.021 (0.028)
Train: 257 [ 550/1251 ( 44%)]  Loss: 2.937 (2.94)  Time: 0.306s, 3350.55/s  (0.302s, 3395.65/s)  LR: 5.360e-05  Data: 0.025 (0.028)
Train: 257 [ 600/1251 ( 48%)]  Loss: 3.004 (2.94)  Time: 0.299s, 3421.11/s  (0.302s, 3396.06/s)  LR: 5.351e-05  Data: 0.024 (0.027)
Train: 257 [ 650/1251 ( 52%)]  Loss: 2.832 (2.94)  Time: 0.301s, 3401.20/s  (0.301s, 3396.66/s)  LR: 5.342e-05  Data: 0.022 (0.027)
Train: 257 [ 700/1251 ( 56%)]  Loss: 2.960 (2.94)  Time: 0.303s, 3378.75/s  (0.301s, 3396.45/s)  LR: 5.333e-05  Data: 0.021 (0.027)
Train: 257 [ 750/1251 ( 60%)]  Loss: 2.973 (2.94)  Time: 0.302s, 3391.53/s  (0.301s, 3396.66/s)  LR: 5.324e-05  Data: 0.025 (0.027)
Train: 257 [ 800/1251 ( 64%)]  Loss: 2.890 (2.94)  Time: 0.301s, 3401.39/s  (0.301s, 3397.16/s)  LR: 5.315e-05  Data: 0.024 (0.026)
Train: 257 [ 850/1251 ( 68%)]  Loss: 2.998 (2.94)  Time: 0.301s, 3404.69/s  (0.301s, 3398.04/s)  LR: 5.306e-05  Data: 0.023 (0.026)
Train: 257 [ 900/1251 ( 72%)]  Loss: 2.969 (2.94)  Time: 0.300s, 3412.67/s  (0.301s, 3398.26/s)  LR: 5.297e-05  Data: 0.023 (0.026)
Train: 257 [ 950/1251 ( 76%)]  Loss: 2.874 (2.94)  Time: 0.297s, 3452.75/s  (0.301s, 3399.37/s)  LR: 5.288e-05  Data: 0.022 (0.026)
Train: 257 [1000/1251 ( 80%)]  Loss: 3.104 (2.95)  Time: 0.297s, 3451.26/s  (0.301s, 3400.17/s)  LR: 5.279e-05  Data: 0.023 (0.026)
Train: 257 [1050/1251 ( 84%)]  Loss: 2.639 (2.93)  Time: 0.300s, 3410.94/s  (0.301s, 3400.69/s)  LR: 5.270e-05  Data: 0.023 (0.026)
Train: 257 [1100/1251 ( 88%)]  Loss: 2.980 (2.93)  Time: 0.298s, 3435.17/s  (0.301s, 3401.80/s)  LR: 5.261e-05  Data: 0.022 (0.026)
Train: 257 [1150/1251 ( 92%)]  Loss: 3.020 (2.94)  Time: 0.305s, 3360.36/s  (0.301s, 3402.14/s)  LR: 5.253e-05  Data: 0.023 (0.025)
Train: 257 [1200/1251 ( 96%)]  Loss: 2.792 (2.93)  Time: 0.299s, 3422.51/s  (0.301s, 3402.92/s)  LR: 5.244e-05  Data: 0.024 (0.025)
Train: 257 [1250/1251 (100%)]  Loss: 2.979 (2.93)  Time: 0.276s, 3714.20/s  (0.301s, 3405.54/s)  LR: 5.235e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.991 (1.991)  Loss:  0.4707 (0.4707)  Acc@1: 92.0898 (92.0898)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.052 (0.233)  Loss:  0.6133 (0.9295)  Acc@1: 85.8491 (79.4220)  Acc@5: 97.4057 (94.5900)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-257.pth.tar', 79.42199992919922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-252.pth.tar', 79.37200003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-253.pth.tar', 79.32600008789062)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-249.pth.tar', 79.31400002929688)

Train: 258 [   0/1251 (  0%)]  Loss: 2.814 (2.81)  Time: 2.222s,  460.81/s  (2.222s,  460.81/s)  LR: 5.235e-05  Data: 1.980 (1.980)
Train: 258 [  50/1251 (  4%)]  Loss: 2.776 (2.80)  Time: 0.299s, 3426.29/s  (0.321s, 3191.39/s)  LR: 5.226e-05  Data: 0.025 (0.063)
Train: 258 [ 100/1251 (  8%)]  Loss: 2.631 (2.74)  Time: 0.297s, 3449.30/s  (0.306s, 3341.52/s)  LR: 5.217e-05  Data: 0.021 (0.044)
Train: 258 [ 150/1251 ( 12%)]  Loss: 2.872 (2.77)  Time: 0.295s, 3476.47/s  (0.303s, 3384.46/s)  LR: 5.208e-05  Data: 0.019 (0.037)
Train: 258 [ 200/1251 ( 16%)]  Loss: 2.732 (2.76)  Time: 0.300s, 3408.13/s  (0.301s, 3406.70/s)  LR: 5.199e-05  Data: 0.026 (0.033)
Train: 258 [ 250/1251 ( 20%)]  Loss: 2.683 (2.75)  Time: 0.293s, 3491.95/s  (0.300s, 3417.70/s)  LR: 5.190e-05  Data: 0.020 (0.031)
Train: 258 [ 300/1251 ( 24%)]  Loss: 2.754 (2.75)  Time: 0.304s, 3370.56/s  (0.299s, 3422.40/s)  LR: 5.182e-05  Data: 0.023 (0.030)
Train: 258 [ 350/1251 ( 28%)]  Loss: 2.846 (2.76)  Time: 0.297s, 3448.89/s  (0.299s, 3424.13/s)  LR: 5.173e-05  Data: 0.021 (0.029)
Train: 258 [ 400/1251 ( 32%)]  Loss: 3.023 (2.79)  Time: 0.298s, 3439.56/s  (0.299s, 3425.99/s)  LR: 5.164e-05  Data: 0.023 (0.028)
Train: 258 [ 450/1251 ( 36%)]  Loss: 2.855 (2.80)  Time: 0.300s, 3416.16/s  (0.299s, 3428.34/s)  LR: 5.155e-05  Data: 0.024 (0.028)
Train: 258 [ 500/1251 ( 40%)]  Loss: 3.004 (2.82)  Time: 0.303s, 3375.48/s  (0.299s, 3430.35/s)  LR: 5.146e-05  Data: 0.021 (0.027)
Train: 258 [ 550/1251 ( 44%)]  Loss: 2.872 (2.82)  Time: 0.298s, 3433.02/s  (0.298s, 3430.57/s)  LR: 5.138e-05  Data: 0.025 (0.027)
Train: 258 [ 600/1251 ( 48%)]  Loss: 2.882 (2.83)  Time: 0.297s, 3448.78/s  (0.298s, 3432.49/s)  LR: 5.129e-05  Data: 0.025 (0.027)
Train: 258 [ 650/1251 ( 52%)]  Loss: 2.930 (2.83)  Time: 0.294s, 3480.16/s  (0.298s, 3434.42/s)  LR: 5.120e-05  Data: 0.023 (0.027)
Train: 258 [ 700/1251 ( 56%)]  Loss: 3.074 (2.85)  Time: 0.296s, 3460.40/s  (0.298s, 3436.05/s)  LR: 5.111e-05  Data: 0.025 (0.026)
Train: 258 [ 750/1251 ( 60%)]  Loss: 2.745 (2.84)  Time: 0.293s, 3496.17/s  (0.298s, 3436.87/s)  LR: 5.103e-05  Data: 0.022 (0.026)
Train: 258 [ 800/1251 ( 64%)]  Loss: 2.839 (2.84)  Time: 0.293s, 3489.42/s  (0.298s, 3437.22/s)  LR: 5.094e-05  Data: 0.019 (0.026)
Train: 258 [ 850/1251 ( 68%)]  Loss: 3.252 (2.87)  Time: 0.295s, 3469.45/s  (0.298s, 3438.23/s)  LR: 5.085e-05  Data: 0.026 (0.026)
Train: 258 [ 900/1251 ( 72%)]  Loss: 2.882 (2.87)  Time: 0.292s, 3503.41/s  (0.298s, 3439.08/s)  LR: 5.076e-05  Data: 0.025 (0.026)
Train: 258 [ 950/1251 ( 76%)]  Loss: 2.769 (2.86)  Time: 0.292s, 3509.38/s  (0.298s, 3439.63/s)  LR: 5.068e-05  Data: 0.021 (0.026)
Train: 258 [1000/1251 ( 80%)]  Loss: 2.998 (2.87)  Time: 0.292s, 3504.11/s  (0.298s, 3440.21/s)  LR: 5.059e-05  Data: 0.023 (0.026)
Train: 258 [1050/1251 ( 84%)]  Loss: 2.639 (2.86)  Time: 0.296s, 3454.81/s  (0.298s, 3440.68/s)  LR: 5.050e-05  Data: 0.024 (0.025)
Train: 258 [1100/1251 ( 88%)]  Loss: 3.143 (2.87)  Time: 0.298s, 3435.63/s  (0.298s, 3440.21/s)  LR: 5.042e-05  Data: 0.023 (0.025)
Train: 258 [1150/1251 ( 92%)]  Loss: 2.770 (2.87)  Time: 0.299s, 3424.18/s  (0.298s, 3440.06/s)  LR: 5.033e-05  Data: 0.023 (0.025)
Train: 258 [1200/1251 ( 96%)]  Loss: 3.051 (2.87)  Time: 0.294s, 3484.75/s  (0.298s, 3439.93/s)  LR: 5.024e-05  Data: 0.024 (0.025)
Train: 258 [1250/1251 (100%)]  Loss: 2.740 (2.87)  Time: 0.269s, 3803.40/s  (0.298s, 3441.80/s)  LR: 5.016e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.997 (1.997)  Loss:  0.4668 (0.4668)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.059 (0.237)  Loss:  0.6348 (0.9335)  Acc@1: 85.0236 (79.3680)  Acc@5: 97.8774 (94.6300)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-257.pth.tar', 79.42199992919922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-252.pth.tar', 79.37200003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-258.pth.tar', 79.3679999584961)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-253.pth.tar', 79.32600008789062)

Train: 259 [   0/1251 (  0%)]  Loss: 2.940 (2.94)  Time: 2.095s,  488.83/s  (2.095s,  488.83/s)  LR: 5.015e-05  Data: 1.869 (1.869)
Train: 259 [  50/1251 (  4%)]  Loss: 3.074 (3.01)  Time: 0.296s, 3464.58/s  (0.325s, 3147.09/s)  LR: 5.007e-05  Data: 0.023 (0.062)
Train: 259 [ 100/1251 (  8%)]  Loss: 3.196 (3.07)  Time: 0.300s, 3411.85/s  (0.310s, 3306.45/s)  LR: 4.998e-05  Data: 0.022 (0.043)
Train: 259 [ 150/1251 ( 12%)]  Loss: 2.629 (2.96)  Time: 0.300s, 3417.13/s  (0.305s, 3362.26/s)  LR: 4.989e-05  Data: 0.026 (0.036)
Train: 259 [ 200/1251 ( 16%)]  Loss: 2.877 (2.94)  Time: 0.286s, 3575.67/s  (0.302s, 3388.82/s)  LR: 4.981e-05  Data: 0.023 (0.033)
Train: 259 [ 250/1251 ( 20%)]  Loss: 2.429 (2.86)  Time: 0.294s, 3485.72/s  (0.301s, 3403.02/s)  LR: 4.972e-05  Data: 0.027 (0.031)
Train: 259 [ 300/1251 ( 24%)]  Loss: 2.871 (2.86)  Time: 0.300s, 3409.43/s  (0.300s, 3410.91/s)  LR: 4.963e-05  Data: 0.024 (0.030)
Train: 259 [ 350/1251 ( 28%)]  Loss: 2.921 (2.87)  Time: 0.297s, 3442.64/s  (0.300s, 3417.77/s)  LR: 4.955e-05  Data: 0.022 (0.029)
Train: 259 [ 400/1251 ( 32%)]  Loss: 2.873 (2.87)  Time: 0.290s, 3525.09/s  (0.299s, 3424.98/s)  LR: 4.946e-05  Data: 0.026 (0.028)
Train: 259 [ 450/1251 ( 36%)]  Loss: 3.076 (2.89)  Time: 0.291s, 3516.34/s  (0.299s, 3428.82/s)  LR: 4.938e-05  Data: 0.024 (0.028)
Train: 259 [ 500/1251 ( 40%)]  Loss: 3.072 (2.91)  Time: 0.293s, 3495.45/s  (0.298s, 3433.56/s)  LR: 4.929e-05  Data: 0.023 (0.027)
Train: 259 [ 550/1251 ( 44%)]  Loss: 2.652 (2.88)  Time: 0.296s, 3454.76/s  (0.298s, 3436.91/s)  LR: 4.920e-05  Data: 0.023 (0.027)
Train: 259 [ 600/1251 ( 48%)]  Loss: 3.011 (2.89)  Time: 0.290s, 3529.79/s  (0.298s, 3439.50/s)  LR: 4.912e-05  Data: 0.022 (0.027)
Train: 259 [ 650/1251 ( 52%)]  Loss: 2.487 (2.86)  Time: 0.290s, 3534.43/s  (0.298s, 3441.93/s)  LR: 4.903e-05  Data: 0.021 (0.027)
Train: 259 [ 700/1251 ( 56%)]  Loss: 2.905 (2.87)  Time: 0.301s, 3404.92/s  (0.297s, 3443.73/s)  LR: 4.895e-05  Data: 0.025 (0.026)
Train: 259 [ 750/1251 ( 60%)]  Loss: 2.951 (2.87)  Time: 0.295s, 3468.29/s  (0.297s, 3444.88/s)  LR: 4.886e-05  Data: 0.022 (0.026)
Train: 259 [ 800/1251 ( 64%)]  Loss: 2.889 (2.87)  Time: 0.288s, 3555.66/s  (0.297s, 3446.02/s)  LR: 4.878e-05  Data: 0.024 (0.026)
Train: 259 [ 850/1251 ( 68%)]  Loss: 3.200 (2.89)  Time: 0.305s, 3357.24/s  (0.297s, 3446.75/s)  LR: 4.869e-05  Data: 0.026 (0.026)
Train: 259 [ 900/1251 ( 72%)]  Loss: 2.795 (2.89)  Time: 0.294s, 3480.43/s  (0.297s, 3448.13/s)  LR: 4.861e-05  Data: 0.026 (0.026)
Train: 259 [ 950/1251 ( 76%)]  Loss: 3.060 (2.90)  Time: 0.297s, 3444.89/s  (0.297s, 3449.03/s)  LR: 4.852e-05  Data: 0.025 (0.026)
Train: 259 [1000/1251 ( 80%)]  Loss: 2.848 (2.89)  Time: 0.294s, 3486.71/s  (0.297s, 3449.74/s)  LR: 4.844e-05  Data: 0.023 (0.026)
Train: 259 [1050/1251 ( 84%)]  Loss: 2.989 (2.90)  Time: 0.297s, 3449.96/s  (0.297s, 3450.56/s)  LR: 4.835e-05  Data: 0.023 (0.025)
Train: 259 [1100/1251 ( 88%)]  Loss: 2.652 (2.89)  Time: 0.295s, 3472.44/s  (0.297s, 3451.41/s)  LR: 4.827e-05  Data: 0.021 (0.025)
Train: 259 [1150/1251 ( 92%)]  Loss: 2.635 (2.88)  Time: 0.290s, 3525.45/s  (0.297s, 3452.13/s)  LR: 4.818e-05  Data: 0.023 (0.025)
Train: 259 [1200/1251 ( 96%)]  Loss: 2.921 (2.88)  Time: 0.294s, 3479.90/s  (0.297s, 3452.19/s)  LR: 4.810e-05  Data: 0.022 (0.025)
Train: 259 [1250/1251 (100%)]  Loss: 2.967 (2.88)  Time: 0.274s, 3742.97/s  (0.296s, 3454.81/s)  LR: 4.801e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.076 (2.076)  Loss:  0.4636 (0.4636)  Acc@1: 91.9922 (91.9922)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.055 (0.240)  Loss:  0.6182 (0.9243)  Acc@1: 85.3774 (79.4380)  Acc@5: 97.6415 (94.5920)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-259.pth.tar', 79.43800011230469)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-257.pth.tar', 79.42199992919922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-252.pth.tar', 79.37200003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-258.pth.tar', 79.3679999584961)

Train: 260 [   0/1251 (  0%)]  Loss: 2.919 (2.92)  Time: 2.139s,  478.82/s  (2.139s,  478.82/s)  LR: 4.801e-05  Data: 1.917 (1.917)
Train: 260 [  50/1251 (  4%)]  Loss: 3.006 (2.96)  Time: 0.293s, 3489.49/s  (0.323s, 3168.78/s)  LR: 4.792e-05  Data: 0.024 (0.062)
Train: 260 [ 100/1251 (  8%)]  Loss: 2.830 (2.92)  Time: 0.296s, 3462.11/s  (0.307s, 3333.30/s)  LR: 4.784e-05  Data: 0.023 (0.043)
Train: 260 [ 150/1251 ( 12%)]  Loss: 2.816 (2.89)  Time: 0.285s, 3598.94/s  (0.302s, 3391.84/s)  LR: 4.776e-05  Data: 0.023 (0.036)
Train: 260 [ 200/1251 ( 16%)]  Loss: 3.191 (2.95)  Time: 0.300s, 3415.59/s  (0.299s, 3421.16/s)  LR: 4.767e-05  Data: 0.024 (0.033)
Train: 260 [ 250/1251 ( 20%)]  Loss: 2.855 (2.94)  Time: 0.291s, 3513.08/s  (0.298s, 3435.46/s)  LR: 4.759e-05  Data: 0.024 (0.031)
Train: 260 [ 300/1251 ( 24%)]  Loss: 2.921 (2.93)  Time: 0.299s, 3429.79/s  (0.297s, 3445.88/s)  LR: 4.750e-05  Data: 0.023 (0.030)
Train: 260 [ 350/1251 ( 28%)]  Loss: 2.837 (2.92)  Time: 0.299s, 3424.48/s  (0.297s, 3453.01/s)  LR: 4.742e-05  Data: 0.025 (0.029)
Train: 260 [ 400/1251 ( 32%)]  Loss: 2.867 (2.92)  Time: 0.287s, 3570.94/s  (0.296s, 3457.90/s)  LR: 4.733e-05  Data: 0.018 (0.028)
Train: 260 [ 450/1251 ( 36%)]  Loss: 2.854 (2.91)  Time: 0.298s, 3435.20/s  (0.296s, 3461.77/s)  LR: 4.725e-05  Data: 0.028 (0.028)
Train: 260 [ 500/1251 ( 40%)]  Loss: 2.958 (2.91)  Time: 0.294s, 3484.21/s  (0.296s, 3464.18/s)  LR: 4.717e-05  Data: 0.027 (0.027)
Train: 260 [ 550/1251 ( 44%)]  Loss: 2.974 (2.92)  Time: 0.295s, 3470.81/s  (0.295s, 3465.97/s)  LR: 4.708e-05  Data: 0.024 (0.027)
Train: 260 [ 600/1251 ( 48%)]  Loss: 2.897 (2.92)  Time: 0.294s, 3484.55/s  (0.295s, 3467.76/s)  LR: 4.700e-05  Data: 0.025 (0.027)
Train: 260 [ 650/1251 ( 52%)]  Loss: 2.806 (2.91)  Time: 0.296s, 3465.27/s  (0.295s, 3469.02/s)  LR: 4.692e-05  Data: 0.026 (0.026)
Train: 260 [ 700/1251 ( 56%)]  Loss: 2.753 (2.90)  Time: 0.289s, 3548.40/s  (0.295s, 3468.55/s)  LR: 4.683e-05  Data: 0.019 (0.026)
Train: 260 [ 750/1251 ( 60%)]  Loss: 2.800 (2.89)  Time: 0.302s, 3388.06/s  (0.295s, 3467.73/s)  LR: 4.675e-05  Data: 0.026 (0.026)
Train: 260 [ 800/1251 ( 64%)]  Loss: 2.814 (2.89)  Time: 0.293s, 3491.80/s  (0.295s, 3467.45/s)  LR: 4.666e-05  Data: 0.023 (0.026)
Train: 260 [ 850/1251 ( 68%)]  Loss: 2.816 (2.88)  Time: 0.299s, 3422.89/s  (0.295s, 3466.12/s)  LR: 4.658e-05  Data: 0.029 (0.026)
Train: 260 [ 900/1251 ( 72%)]  Loss: 2.932 (2.89)  Time: 0.294s, 3481.83/s  (0.295s, 3466.45/s)  LR: 4.650e-05  Data: 0.023 (0.026)
Train: 260 [ 950/1251 ( 76%)]  Loss: 3.101 (2.90)  Time: 0.301s, 3407.00/s  (0.295s, 3466.26/s)  LR: 4.641e-05  Data: 0.022 (0.026)
Train: 260 [1000/1251 ( 80%)]  Loss: 2.836 (2.89)  Time: 0.300s, 3414.50/s  (0.295s, 3466.40/s)  LR: 4.633e-05  Data: 0.023 (0.025)
Train: 260 [1050/1251 ( 84%)]  Loss: 2.746 (2.89)  Time: 0.292s, 3507.65/s  (0.295s, 3466.96/s)  LR: 4.625e-05  Data: 0.022 (0.025)
Train: 260 [1100/1251 ( 88%)]  Loss: 3.125 (2.90)  Time: 0.294s, 3485.28/s  (0.295s, 3466.92/s)  LR: 4.617e-05  Data: 0.022 (0.025)
Train: 260 [1150/1251 ( 92%)]  Loss: 2.993 (2.90)  Time: 0.300s, 3413.30/s  (0.295s, 3466.76/s)  LR: 4.608e-05  Data: 0.026 (0.025)
Train: 260 [1200/1251 ( 96%)]  Loss: 3.054 (2.91)  Time: 0.298s, 3434.15/s  (0.295s, 3466.42/s)  LR: 4.600e-05  Data: 0.022 (0.025)
Train: 260 [1250/1251 (100%)]  Loss: 3.048 (2.91)  Time: 0.275s, 3717.30/s  (0.295s, 3467.49/s)  LR: 4.592e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.224 (2.224)  Loss:  0.4658 (0.4658)  Acc@1: 92.2852 (92.2852)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.048 (0.240)  Loss:  0.5967 (0.9245)  Acc@1: 86.6745 (79.3000)  Acc@5: 97.7594 (94.5840)
Train: 261 [   0/1251 (  0%)]  Loss: 2.495 (2.50)  Time: 2.487s,  411.76/s  (2.487s,  411.76/s)  LR: 4.592e-05  Data: 2.259 (2.259)
Train: 261 [  50/1251 (  4%)]  Loss: 2.900 (2.70)  Time: 0.289s, 3547.35/s  (0.330s, 3103.59/s)  LR: 4.583e-05  Data: 0.021 (0.078)
Train: 261 [ 100/1251 (  8%)]  Loss: 2.864 (2.75)  Time: 0.294s, 3480.24/s  (0.310s, 3298.43/s)  LR: 4.575e-05  Data: 0.022 (0.051)
Train: 261 [ 150/1251 ( 12%)]  Loss: 2.883 (2.79)  Time: 0.294s, 3478.00/s  (0.305s, 3359.85/s)  LR: 4.567e-05  Data: 0.027 (0.042)
Train: 261 [ 200/1251 ( 16%)]  Loss: 2.896 (2.81)  Time: 0.296s, 3464.38/s  (0.302s, 3389.16/s)  LR: 4.559e-05  Data: 0.022 (0.037)
Train: 261 [ 250/1251 ( 20%)]  Loss: 3.301 (2.89)  Time: 0.293s, 3495.83/s  (0.301s, 3405.83/s)  LR: 4.550e-05  Data: 0.023 (0.035)
Train: 261 [ 300/1251 ( 24%)]  Loss: 2.651 (2.86)  Time: 0.300s, 3415.94/s  (0.300s, 3418.28/s)  LR: 4.542e-05  Data: 0.026 (0.033)
Train: 261 [ 350/1251 ( 28%)]  Loss: 2.968 (2.87)  Time: 0.295s, 3466.21/s  (0.299s, 3424.70/s)  LR: 4.534e-05  Data: 0.027 (0.031)
Train: 261 [ 400/1251 ( 32%)]  Loss: 2.936 (2.88)  Time: 0.293s, 3499.69/s  (0.299s, 3429.64/s)  LR: 4.526e-05  Data: 0.023 (0.030)
Train: 261 [ 450/1251 ( 36%)]  Loss: 3.138 (2.90)  Time: 0.301s, 3403.66/s  (0.298s, 3433.81/s)  LR: 4.517e-05  Data: 0.025 (0.030)
Train: 261 [ 500/1251 ( 40%)]  Loss: 2.906 (2.90)  Time: 0.297s, 3445.90/s  (0.298s, 3437.03/s)  LR: 4.509e-05  Data: 0.026 (0.029)
Train: 261 [ 550/1251 ( 44%)]  Loss: 2.590 (2.88)  Time: 0.292s, 3505.30/s  (0.298s, 3438.95/s)  LR: 4.501e-05  Data: 0.023 (0.028)
Train: 261 [ 600/1251 ( 48%)]  Loss: 2.845 (2.87)  Time: 0.298s, 3441.34/s  (0.298s, 3441.34/s)  LR: 4.493e-05  Data: 0.023 (0.028)
Train: 261 [ 650/1251 ( 52%)]  Loss: 2.765 (2.87)  Time: 0.295s, 3471.06/s  (0.297s, 3443.45/s)  LR: 4.485e-05  Data: 0.022 (0.028)
Train: 261 [ 700/1251 ( 56%)]  Loss: 2.759 (2.86)  Time: 0.293s, 3496.61/s  (0.297s, 3444.15/s)  LR: 4.477e-05  Data: 0.023 (0.027)
Train: 261 [ 750/1251 ( 60%)]  Loss: 2.881 (2.86)  Time: 0.304s, 3373.31/s  (0.297s, 3445.07/s)  LR: 4.468e-05  Data: 0.024 (0.027)
Train: 261 [ 800/1251 ( 64%)]  Loss: 2.785 (2.86)  Time: 0.293s, 3497.31/s  (0.297s, 3445.49/s)  LR: 4.460e-05  Data: 0.024 (0.027)
Train: 261 [ 850/1251 ( 68%)]  Loss: 2.948 (2.86)  Time: 0.297s, 3451.96/s  (0.297s, 3445.50/s)  LR: 4.452e-05  Data: 0.024 (0.027)
Train: 261 [ 900/1251 ( 72%)]  Loss: 2.891 (2.86)  Time: 0.300s, 3413.17/s  (0.297s, 3445.77/s)  LR: 4.444e-05  Data: 0.021 (0.027)
Train: 261 [ 950/1251 ( 76%)]  Loss: 2.792 (2.86)  Time: 0.297s, 3442.87/s  (0.297s, 3445.59/s)  LR: 4.436e-05  Data: 0.027 (0.026)
Train: 261 [1000/1251 ( 80%)]  Loss: 3.113 (2.87)  Time: 0.302s, 3392.31/s  (0.297s, 3445.60/s)  LR: 4.428e-05  Data: 0.024 (0.026)
Train: 261 [1050/1251 ( 84%)]  Loss: 2.948 (2.88)  Time: 0.301s, 3400.72/s  (0.297s, 3445.25/s)  LR: 4.420e-05  Data: 0.022 (0.026)
Train: 261 [1100/1251 ( 88%)]  Loss: 2.685 (2.87)  Time: 0.297s, 3451.73/s  (0.297s, 3445.05/s)  LR: 4.412e-05  Data: 0.021 (0.026)
Train: 261 [1150/1251 ( 92%)]  Loss: 2.679 (2.86)  Time: 0.294s, 3478.40/s  (0.297s, 3444.96/s)  LR: 4.403e-05  Data: 0.021 (0.026)
Train: 261 [1200/1251 ( 96%)]  Loss: 2.934 (2.86)  Time: 0.292s, 3503.84/s  (0.297s, 3444.98/s)  LR: 4.395e-05  Data: 0.023 (0.026)
Train: 261 [1250/1251 (100%)]  Loss: 2.827 (2.86)  Time: 0.270s, 3798.87/s  (0.297s, 3446.67/s)  LR: 4.387e-05  Data: 0.000 (0.026)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.105 (2.105)  Loss:  0.4556 (0.4556)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.055 (0.233)  Loss:  0.6021 (0.9206)  Acc@1: 86.6745 (79.4280)  Acc@5: 97.5236 (94.5960)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-259.pth.tar', 79.43800011230469)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-261.pth.tar', 79.42800002929687)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-257.pth.tar', 79.42199992919922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-252.pth.tar', 79.37200003417969)

Train: 262 [   0/1251 (  0%)]  Loss: 3.117 (3.12)  Time: 2.180s,  469.68/s  (2.180s,  469.68/s)  LR: 4.387e-05  Data: 1.961 (1.961)
Train: 262 [  50/1251 (  4%)]  Loss: 2.753 (2.94)  Time: 0.296s, 3455.57/s  (0.322s, 3180.53/s)  LR: 4.379e-05  Data: 0.024 (0.062)
Train: 262 [ 100/1251 (  8%)]  Loss: 2.928 (2.93)  Time: 0.290s, 3525.84/s  (0.307s, 3337.11/s)  LR: 4.371e-05  Data: 0.025 (0.043)
Train: 262 [ 150/1251 ( 12%)]  Loss: 3.043 (2.96)  Time: 0.295s, 3469.10/s  (0.302s, 3387.64/s)  LR: 4.363e-05  Data: 0.023 (0.036)
Train: 262 [ 200/1251 ( 16%)]  Loss: 3.029 (2.97)  Time: 0.291s, 3518.47/s  (0.300s, 3414.42/s)  LR: 4.355e-05  Data: 0.024 (0.033)
Train: 262 [ 250/1251 ( 20%)]  Loss: 2.785 (2.94)  Time: 0.294s, 3487.43/s  (0.299s, 3426.39/s)  LR: 4.347e-05  Data: 0.022 (0.031)
Train: 262 [ 300/1251 ( 24%)]  Loss: 2.943 (2.94)  Time: 0.301s, 3406.93/s  (0.298s, 3432.38/s)  LR: 4.339e-05  Data: 0.026 (0.030)
Train: 262 [ 350/1251 ( 28%)]  Loss: 2.900 (2.94)  Time: 0.296s, 3453.88/s  (0.298s, 3437.17/s)  LR: 4.331e-05  Data: 0.024 (0.029)
Train: 262 [ 400/1251 ( 32%)]  Loss: 2.886 (2.93)  Time: 0.297s, 3445.23/s  (0.298s, 3440.76/s)  LR: 4.323e-05  Data: 0.023 (0.028)
Train: 262 [ 450/1251 ( 36%)]  Loss: 2.828 (2.92)  Time: 0.295s, 3469.16/s  (0.297s, 3443.38/s)  LR: 4.315e-05  Data: 0.023 (0.028)
Train: 262 [ 500/1251 ( 40%)]  Loss: 2.916 (2.92)  Time: 0.298s, 3435.55/s  (0.297s, 3444.74/s)  LR: 4.307e-05  Data: 0.025 (0.027)
Train: 262 [ 550/1251 ( 44%)]  Loss: 2.933 (2.92)  Time: 0.292s, 3503.88/s  (0.297s, 3445.69/s)  LR: 4.299e-05  Data: 0.020 (0.027)
Train: 262 [ 600/1251 ( 48%)]  Loss: 2.835 (2.92)  Time: 0.290s, 3526.87/s  (0.297s, 3447.45/s)  LR: 4.291e-05  Data: 0.022 (0.027)
Train: 262 [ 650/1251 ( 52%)]  Loss: 2.516 (2.89)  Time: 0.296s, 3457.67/s  (0.297s, 3448.71/s)  LR: 4.283e-05  Data: 0.024 (0.027)
Train: 262 [ 700/1251 ( 56%)]  Loss: 2.736 (2.88)  Time: 0.299s, 3424.13/s  (0.297s, 3448.98/s)  LR: 4.275e-05  Data: 0.022 (0.026)
Train: 262 [ 750/1251 ( 60%)]  Loss: 2.956 (2.88)  Time: 0.294s, 3482.19/s  (0.297s, 3449.61/s)  LR: 4.267e-05  Data: 0.023 (0.026)
Train: 262 [ 800/1251 ( 64%)]  Loss: 2.916 (2.88)  Time: 0.294s, 3481.05/s  (0.297s, 3449.80/s)  LR: 4.259e-05  Data: 0.022 (0.026)
Train: 262 [ 850/1251 ( 68%)]  Loss: 2.785 (2.88)  Time: 0.300s, 3416.07/s  (0.297s, 3449.85/s)  LR: 4.251e-05  Data: 0.023 (0.026)
Train: 262 [ 900/1251 ( 72%)]  Loss: 3.147 (2.89)  Time: 0.300s, 3416.72/s  (0.297s, 3449.28/s)  LR: 4.243e-05  Data: 0.022 (0.026)
Train: 262 [ 950/1251 ( 76%)]  Loss: 2.841 (2.89)  Time: 0.302s, 3386.68/s  (0.297s, 3449.42/s)  LR: 4.235e-05  Data: 0.022 (0.026)
Train: 262 [1000/1251 ( 80%)]  Loss: 2.914 (2.89)  Time: 0.297s, 3446.41/s  (0.297s, 3449.28/s)  LR: 4.227e-05  Data: 0.023 (0.026)
Train: 262 [1050/1251 ( 84%)]  Loss: 2.946 (2.89)  Time: 0.295s, 3476.65/s  (0.297s, 3448.42/s)  LR: 4.219e-05  Data: 0.024 (0.025)
Train: 262 [1100/1251 ( 88%)]  Loss: 2.897 (2.89)  Time: 0.300s, 3413.66/s  (0.297s, 3448.31/s)  LR: 4.212e-05  Data: 0.023 (0.025)
Train: 262 [1150/1251 ( 92%)]  Loss: 2.818 (2.89)  Time: 0.299s, 3429.78/s  (0.297s, 3447.77/s)  LR: 4.204e-05  Data: 0.021 (0.025)
Train: 262 [1200/1251 ( 96%)]  Loss: 2.833 (2.89)  Time: 0.293s, 3492.66/s  (0.297s, 3447.17/s)  LR: 4.196e-05  Data: 0.023 (0.025)
Train: 262 [1250/1251 (100%)]  Loss: 3.059 (2.89)  Time: 0.276s, 3712.90/s  (0.297s, 3448.25/s)  LR: 4.188e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.008 (2.008)  Loss:  0.4604 (0.4604)  Acc@1: 92.1875 (92.1875)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.052 (0.237)  Loss:  0.6211 (0.9182)  Acc@1: 86.0849 (79.5560)  Acc@5: 97.4057 (94.6820)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-262.pth.tar', 79.55600003173828)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-259.pth.tar', 79.43800011230469)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-261.pth.tar', 79.42800002929687)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-257.pth.tar', 79.42199992919922)

Train: 263 [   0/1251 (  0%)]  Loss: 2.940 (2.94)  Time: 2.508s,  408.27/s  (2.508s,  408.27/s)  LR: 4.188e-05  Data: 2.292 (2.292)
Train: 263 [  50/1251 (  4%)]  Loss: 2.896 (2.92)  Time: 0.284s, 3611.82/s  (0.322s, 3182.62/s)  LR: 4.180e-05  Data: 0.018 (0.069)
Train: 263 [ 100/1251 (  8%)]  Loss: 2.919 (2.92)  Time: 0.294s, 3487.80/s  (0.307s, 3338.69/s)  LR: 4.172e-05  Data: 0.021 (0.046)
Train: 263 [ 150/1251 ( 12%)]  Loss: 2.715 (2.87)  Time: 0.290s, 3527.00/s  (0.303s, 3382.66/s)  LR: 4.164e-05  Data: 0.023 (0.039)
Train: 263 [ 200/1251 ( 16%)]  Loss: 2.731 (2.84)  Time: 0.300s, 3409.00/s  (0.301s, 3403.50/s)  LR: 4.156e-05  Data: 0.022 (0.035)
Train: 263 [ 250/1251 ( 20%)]  Loss: 3.024 (2.87)  Time: 0.293s, 3490.61/s  (0.300s, 3417.43/s)  LR: 4.149e-05  Data: 0.021 (0.032)
Train: 263 [ 300/1251 ( 24%)]  Loss: 2.792 (2.86)  Time: 0.294s, 3479.26/s  (0.299s, 3423.87/s)  LR: 4.141e-05  Data: 0.025 (0.031)
Train: 263 [ 350/1251 ( 28%)]  Loss: 2.817 (2.85)  Time: 0.299s, 3421.93/s  (0.299s, 3427.89/s)  LR: 4.133e-05  Data: 0.026 (0.030)
Train: 263 [ 400/1251 ( 32%)]  Loss: 3.190 (2.89)  Time: 0.293s, 3494.17/s  (0.299s, 3430.40/s)  LR: 4.125e-05  Data: 0.028 (0.029)
Train: 263 [ 450/1251 ( 36%)]  Loss: 2.945 (2.90)  Time: 0.299s, 3422.81/s  (0.298s, 3432.15/s)  LR: 4.117e-05  Data: 0.027 (0.029)
Train: 263 [ 500/1251 ( 40%)]  Loss: 2.952 (2.90)  Time: 0.293s, 3492.46/s  (0.298s, 3432.90/s)  LR: 4.110e-05  Data: 0.024 (0.028)
Train: 263 [ 550/1251 ( 44%)]  Loss: 3.049 (2.91)  Time: 0.293s, 3491.73/s  (0.298s, 3433.53/s)  LR: 4.102e-05  Data: 0.021 (0.028)
Train: 263 [ 600/1251 ( 48%)]  Loss: 2.827 (2.91)  Time: 0.296s, 3453.90/s  (0.298s, 3434.80/s)  LR: 4.094e-05  Data: 0.026 (0.027)
Train: 263 [ 650/1251 ( 52%)]  Loss: 3.016 (2.92)  Time: 0.294s, 3485.13/s  (0.298s, 3435.26/s)  LR: 4.086e-05  Data: 0.027 (0.027)
Train: 263 [ 700/1251 ( 56%)]  Loss: 2.822 (2.91)  Time: 0.301s, 3398.39/s  (0.298s, 3435.89/s)  LR: 4.078e-05  Data: 0.022 (0.027)
Train: 263 [ 750/1251 ( 60%)]  Loss: 2.801 (2.90)  Time: 0.300s, 3416.13/s  (0.298s, 3436.56/s)  LR: 4.071e-05  Data: 0.023 (0.027)
Train: 263 [ 800/1251 ( 64%)]  Loss: 2.935 (2.90)  Time: 0.299s, 3421.39/s  (0.298s, 3436.35/s)  LR: 4.063e-05  Data: 0.027 (0.026)
Train: 263 [ 850/1251 ( 68%)]  Loss: 3.086 (2.91)  Time: 0.295s, 3465.41/s  (0.298s, 3435.91/s)  LR: 4.055e-05  Data: 0.019 (0.026)
Train: 263 [ 900/1251 ( 72%)]  Loss: 2.870 (2.91)  Time: 0.296s, 3458.43/s  (0.298s, 3435.94/s)  LR: 4.047e-05  Data: 0.021 (0.026)
Train: 263 [ 950/1251 ( 76%)]  Loss: 2.972 (2.91)  Time: 0.301s, 3406.02/s  (0.298s, 3434.78/s)  LR: 4.040e-05  Data: 0.022 (0.026)
Train: 263 [1000/1251 ( 80%)]  Loss: 2.951 (2.92)  Time: 0.292s, 3500.98/s  (0.298s, 3434.57/s)  LR: 4.032e-05  Data: 0.023 (0.026)
Train: 263 [1050/1251 ( 84%)]  Loss: 3.057 (2.92)  Time: 0.299s, 3423.65/s  (0.298s, 3433.79/s)  LR: 4.024e-05  Data: 0.025 (0.026)
Train: 263 [1100/1251 ( 88%)]  Loss: 2.953 (2.92)  Time: 0.296s, 3460.73/s  (0.298s, 3433.14/s)  LR: 4.017e-05  Data: 0.023 (0.026)
Train: 263 [1150/1251 ( 92%)]  Loss: 3.020 (2.93)  Time: 0.301s, 3406.18/s  (0.298s, 3432.83/s)  LR: 4.009e-05  Data: 0.022 (0.025)
Train: 263 [1200/1251 ( 96%)]  Loss: 3.032 (2.93)  Time: 0.295s, 3466.11/s  (0.298s, 3432.24/s)  LR: 4.001e-05  Data: 0.024 (0.025)
Train: 263 [1250/1251 (100%)]  Loss: 2.905 (2.93)  Time: 0.273s, 3753.08/s  (0.298s, 3434.10/s)  LR: 3.994e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.011 (2.011)  Loss:  0.4795 (0.4795)  Acc@1: 92.0898 (92.0898)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.056 (0.233)  Loss:  0.6045 (0.9190)  Acc@1: 86.7924 (79.5980)  Acc@5: 97.6415 (94.6060)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-263.pth.tar', 79.59799995117187)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-262.pth.tar', 79.55600003173828)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-259.pth.tar', 79.43800011230469)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-261.pth.tar', 79.42800002929687)

Train: 264 [   0/1251 (  0%)]  Loss: 2.917 (2.92)  Time: 2.287s,  447.67/s  (2.287s,  447.67/s)  LR: 3.993e-05  Data: 2.070 (2.070)
Train: 264 [  50/1251 (  4%)]  Loss: 2.521 (2.72)  Time: 0.292s, 3503.90/s  (0.323s, 3170.51/s)  LR: 3.986e-05  Data: 0.022 (0.065)
Train: 264 [ 100/1251 (  8%)]  Loss: 2.989 (2.81)  Time: 0.301s, 3402.67/s  (0.309s, 3311.58/s)  LR: 3.978e-05  Data: 0.023 (0.044)
Train: 264 [ 150/1251 ( 12%)]  Loss: 2.698 (2.78)  Time: 0.300s, 3408.13/s  (0.305s, 3358.79/s)  LR: 3.971e-05  Data: 0.023 (0.037)
Train: 264 [ 200/1251 ( 16%)]  Loss: 2.969 (2.82)  Time: 0.304s, 3373.43/s  (0.303s, 3380.14/s)  LR: 3.963e-05  Data: 0.026 (0.034)
Train: 264 [ 250/1251 ( 20%)]  Loss: 2.978 (2.85)  Time: 0.298s, 3438.62/s  (0.302s, 3388.99/s)  LR: 3.955e-05  Data: 0.025 (0.032)
Train: 264 [ 300/1251 ( 24%)]  Loss: 2.633 (2.81)  Time: 0.300s, 3417.47/s  (0.301s, 3397.18/s)  LR: 3.948e-05  Data: 0.027 (0.031)
Train: 264 [ 350/1251 ( 28%)]  Loss: 2.978 (2.84)  Time: 0.295s, 3467.08/s  (0.301s, 3400.75/s)  LR: 3.940e-05  Data: 0.024 (0.030)
Train: 264 [ 400/1251 ( 32%)]  Loss: 2.983 (2.85)  Time: 0.301s, 3406.02/s  (0.301s, 3404.59/s)  LR: 3.932e-05  Data: 0.029 (0.029)
Train: 264 [ 450/1251 ( 36%)]  Loss: 3.061 (2.87)  Time: 0.301s, 3403.39/s  (0.300s, 3407.82/s)  LR: 3.925e-05  Data: 0.023 (0.028)
Train: 264 [ 500/1251 ( 40%)]  Loss: 2.862 (2.87)  Time: 0.302s, 3387.49/s  (0.300s, 3408.80/s)  LR: 3.917e-05  Data: 0.024 (0.028)
Train: 264 [ 550/1251 ( 44%)]  Loss: 2.823 (2.87)  Time: 0.297s, 3447.41/s  (0.300s, 3410.52/s)  LR: 3.910e-05  Data: 0.020 (0.027)
Train: 264 [ 600/1251 ( 48%)]  Loss: 2.918 (2.87)  Time: 0.299s, 3422.40/s  (0.300s, 3412.42/s)  LR: 3.902e-05  Data: 0.024 (0.027)
Train: 264 [ 650/1251 ( 52%)]  Loss: 3.025 (2.88)  Time: 0.303s, 3376.80/s  (0.300s, 3413.76/s)  LR: 3.895e-05  Data: 0.023 (0.027)
Train: 264 [ 700/1251 ( 56%)]  Loss: 2.794 (2.88)  Time: 0.303s, 3385.05/s  (0.300s, 3413.94/s)  LR: 3.887e-05  Data: 0.026 (0.027)
Train: 264 [ 750/1251 ( 60%)]  Loss: 3.188 (2.90)  Time: 0.303s, 3376.49/s  (0.300s, 3414.68/s)  LR: 3.879e-05  Data: 0.025 (0.026)
Train: 264 [ 800/1251 ( 64%)]  Loss: 2.700 (2.88)  Time: 0.296s, 3462.21/s  (0.300s, 3414.59/s)  LR: 3.872e-05  Data: 0.025 (0.026)
Train: 264 [ 850/1251 ( 68%)]  Loss: 2.760 (2.88)  Time: 0.296s, 3454.01/s  (0.300s, 3414.59/s)  LR: 3.864e-05  Data: 0.026 (0.026)
Train: 264 [ 900/1251 ( 72%)]  Loss: 2.734 (2.87)  Time: 0.299s, 3428.54/s  (0.300s, 3414.11/s)  LR: 3.857e-05  Data: 0.024 (0.026)
Train: 264 [ 950/1251 ( 76%)]  Loss: 2.782 (2.87)  Time: 0.300s, 3413.06/s  (0.300s, 3414.11/s)  LR: 3.849e-05  Data: 0.023 (0.026)
Train: 264 [1000/1251 ( 80%)]  Loss: 2.822 (2.86)  Time: 0.300s, 3416.16/s  (0.300s, 3413.65/s)  LR: 3.842e-05  Data: 0.026 (0.026)
Train: 264 [1050/1251 ( 84%)]  Loss: 2.683 (2.86)  Time: 0.299s, 3419.37/s  (0.300s, 3413.03/s)  LR: 3.834e-05  Data: 0.021 (0.026)
Train: 264 [1100/1251 ( 88%)]  Loss: 2.912 (2.86)  Time: 0.298s, 3440.88/s  (0.300s, 3412.89/s)  LR: 3.827e-05  Data: 0.019 (0.025)
Train: 264 [1150/1251 ( 92%)]  Loss: 3.119 (2.87)  Time: 0.295s, 3468.22/s  (0.300s, 3412.20/s)  LR: 3.819e-05  Data: 0.023 (0.025)
Train: 264 [1200/1251 ( 96%)]  Loss: 3.149 (2.88)  Time: 0.300s, 3411.89/s  (0.300s, 3411.71/s)  LR: 3.812e-05  Data: 0.020 (0.025)
Train: 264 [1250/1251 (100%)]  Loss: 3.047 (2.89)  Time: 0.276s, 3711.33/s  (0.300s, 3413.42/s)  LR: 3.804e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.004 (2.004)  Loss:  0.4612 (0.4612)  Acc@1: 92.2852 (92.2852)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.050 (0.234)  Loss:  0.6514 (0.9282)  Acc@1: 85.9670 (79.5120)  Acc@5: 96.9340 (94.5920)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-263.pth.tar', 79.59799995117187)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-262.pth.tar', 79.55600003173828)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-264.pth.tar', 79.51199998046874)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-259.pth.tar', 79.43800011230469)

Train: 265 [   0/1251 (  0%)]  Loss: 3.041 (3.04)  Time: 2.081s,  492.08/s  (2.081s,  492.08/s)  LR: 3.804e-05  Data: 1.858 (1.858)
Train: 265 [  50/1251 (  4%)]  Loss: 3.034 (3.04)  Time: 0.285s, 3599.13/s  (0.317s, 3226.85/s)  LR: 3.797e-05  Data: 0.021 (0.061)
Train: 265 [ 100/1251 (  8%)]  Loss: 3.064 (3.05)  Time: 0.290s, 3530.68/s  (0.305s, 3360.53/s)  LR: 3.789e-05  Data: 0.025 (0.043)
Train: 265 [ 150/1251 ( 12%)]  Loss: 3.041 (3.05)  Time: 0.297s, 3450.58/s  (0.302s, 3390.69/s)  LR: 3.782e-05  Data: 0.021 (0.036)
Train: 265 [ 200/1251 ( 16%)]  Loss: 2.523 (2.94)  Time: 0.301s, 3402.98/s  (0.301s, 3400.40/s)  LR: 3.774e-05  Data: 0.023 (0.033)
Train: 265 [ 250/1251 ( 20%)]  Loss: 3.089 (2.97)  Time: 0.298s, 3432.38/s  (0.301s, 3407.03/s)  LR: 3.767e-05  Data: 0.024 (0.031)
Train: 265 [ 300/1251 ( 24%)]  Loss: 3.012 (2.97)  Time: 0.301s, 3403.22/s  (0.300s, 3409.44/s)  LR: 3.760e-05  Data: 0.026 (0.030)
Train: 265 [ 350/1251 ( 28%)]  Loss: 2.700 (2.94)  Time: 0.304s, 3373.32/s  (0.300s, 3411.83/s)  LR: 3.752e-05  Data: 0.022 (0.029)
Train: 265 [ 400/1251 ( 32%)]  Loss: 3.254 (2.97)  Time: 0.299s, 3422.57/s  (0.300s, 3411.55/s)  LR: 3.745e-05  Data: 0.026 (0.028)
Train: 265 [ 450/1251 ( 36%)]  Loss: 2.506 (2.93)  Time: 0.303s, 3378.44/s  (0.300s, 3410.03/s)  LR: 3.737e-05  Data: 0.022 (0.028)
Train: 265 [ 500/1251 ( 40%)]  Loss: 2.975 (2.93)  Time: 0.306s, 3347.53/s  (0.300s, 3407.84/s)  LR: 3.730e-05  Data: 0.023 (0.027)
Train: 265 [ 550/1251 ( 44%)]  Loss: 2.778 (2.92)  Time: 0.308s, 3326.05/s  (0.301s, 3406.16/s)  LR: 3.723e-05  Data: 0.029 (0.027)
Train: 265 [ 600/1251 ( 48%)]  Loss: 2.610 (2.89)  Time: 0.303s, 3377.44/s  (0.301s, 3404.90/s)  LR: 3.715e-05  Data: 0.029 (0.027)
Train: 265 [ 650/1251 ( 52%)]  Loss: 2.744 (2.88)  Time: 0.306s, 3344.04/s  (0.301s, 3403.54/s)  LR: 3.708e-05  Data: 0.025 (0.027)
Train: 265 [ 700/1251 ( 56%)]  Loss: 2.792 (2.88)  Time: 0.304s, 3367.81/s  (0.301s, 3402.26/s)  LR: 3.701e-05  Data: 0.026 (0.026)
Train: 265 [ 750/1251 ( 60%)]  Loss: 2.740 (2.87)  Time: 0.301s, 3399.11/s  (0.301s, 3401.93/s)  LR: 3.693e-05  Data: 0.023 (0.026)
Train: 265 [ 800/1251 ( 64%)]  Loss: 2.870 (2.87)  Time: 0.299s, 3428.29/s  (0.301s, 3401.00/s)  LR: 3.686e-05  Data: 0.024 (0.026)
Train: 265 [ 850/1251 ( 68%)]  Loss: 2.754 (2.86)  Time: 0.304s, 3370.28/s  (0.301s, 3400.65/s)  LR: 3.679e-05  Data: 0.025 (0.026)
Train: 265 [ 900/1251 ( 72%)]  Loss: 2.886 (2.86)  Time: 0.306s, 3341.91/s  (0.301s, 3399.43/s)  LR: 3.671e-05  Data: 0.022 (0.026)
Train: 265 [ 950/1251 ( 76%)]  Loss: 2.992 (2.87)  Time: 0.303s, 3377.77/s  (0.301s, 3397.99/s)  LR: 3.664e-05  Data: 0.026 (0.026)
Train: 265 [1000/1251 ( 80%)]  Loss: 2.992 (2.88)  Time: 0.301s, 3398.78/s  (0.301s, 3396.83/s)  LR: 3.657e-05  Data: 0.023 (0.026)
Train: 265 [1050/1251 ( 84%)]  Loss: 2.880 (2.88)  Time: 0.305s, 3352.43/s  (0.302s, 3396.03/s)  LR: 3.649e-05  Data: 0.022 (0.026)
Train: 265 [1100/1251 ( 88%)]  Loss: 2.824 (2.87)  Time: 0.307s, 3333.11/s  (0.302s, 3395.43/s)  LR: 3.642e-05  Data: 0.022 (0.025)
Train: 265 [1150/1251 ( 92%)]  Loss: 3.101 (2.88)  Time: 0.303s, 3376.62/s  (0.302s, 3394.52/s)  LR: 3.635e-05  Data: 0.022 (0.025)
Train: 265 [1200/1251 ( 96%)]  Loss: 2.748 (2.88)  Time: 0.301s, 3399.08/s  (0.302s, 3393.47/s)  LR: 3.627e-05  Data: 0.023 (0.025)
Train: 265 [1250/1251 (100%)]  Loss: 2.808 (2.88)  Time: 0.276s, 3713.41/s  (0.302s, 3394.61/s)  LR: 3.620e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.022 (2.022)  Loss:  0.4773 (0.4773)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.053 (0.239)  Loss:  0.6235 (0.9216)  Acc@1: 85.8491 (79.5900)  Acc@5: 97.6415 (94.6820)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-263.pth.tar', 79.59799995117187)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-265.pth.tar', 79.58999992919922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-262.pth.tar', 79.55600003173828)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-264.pth.tar', 79.51199998046874)

Train: 266 [   0/1251 (  0%)]  Loss: 2.706 (2.71)  Time: 2.223s,  460.70/s  (2.223s,  460.70/s)  LR: 3.620e-05  Data: 1.998 (1.998)
Train: 266 [  50/1251 (  4%)]  Loss: 3.040 (2.87)  Time: 0.296s, 3453.93/s  (0.323s, 3167.00/s)  LR: 3.613e-05  Data: 0.026 (0.063)
Train: 266 [ 100/1251 (  8%)]  Loss: 2.752 (2.83)  Time: 0.300s, 3414.58/s  (0.310s, 3304.82/s)  LR: 3.606e-05  Data: 0.026 (0.043)
Train: 266 [ 150/1251 ( 12%)]  Loss: 2.835 (2.83)  Time: 0.300s, 3411.07/s  (0.307s, 3339.31/s)  LR: 3.598e-05  Data: 0.024 (0.037)
Train: 266 [ 200/1251 ( 16%)]  Loss: 2.832 (2.83)  Time: 0.300s, 3411.79/s  (0.305s, 3357.15/s)  LR: 3.591e-05  Data: 0.022 (0.034)
Train: 266 [ 250/1251 ( 20%)]  Loss: 2.760 (2.82)  Time: 0.305s, 3357.98/s  (0.304s, 3366.26/s)  LR: 3.584e-05  Data: 0.023 (0.032)
Train: 266 [ 300/1251 ( 24%)]  Loss: 2.802 (2.82)  Time: 0.302s, 3389.85/s  (0.304s, 3371.28/s)  LR: 3.577e-05  Data: 0.025 (0.030)
Train: 266 [ 350/1251 ( 28%)]  Loss: 2.915 (2.83)  Time: 0.300s, 3416.22/s  (0.304s, 3373.69/s)  LR: 3.569e-05  Data: 0.023 (0.029)
Train: 266 [ 400/1251 ( 32%)]  Loss: 2.925 (2.84)  Time: 0.298s, 3438.38/s  (0.303s, 3374.69/s)  LR: 3.562e-05  Data: 0.026 (0.029)
Train: 266 [ 450/1251 ( 36%)]  Loss: 2.571 (2.81)  Time: 0.301s, 3396.86/s  (0.303s, 3375.89/s)  LR: 3.555e-05  Data: 0.022 (0.028)
Train: 266 [ 500/1251 ( 40%)]  Loss: 2.982 (2.83)  Time: 0.305s, 3359.54/s  (0.303s, 3376.94/s)  LR: 3.548e-05  Data: 0.023 (0.028)
Train: 266 [ 550/1251 ( 44%)]  Loss: 3.086 (2.85)  Time: 0.308s, 3329.18/s  (0.303s, 3378.38/s)  LR: 3.541e-05  Data: 0.023 (0.027)
Train: 266 [ 600/1251 ( 48%)]  Loss: 2.773 (2.84)  Time: 0.304s, 3365.83/s  (0.303s, 3379.25/s)  LR: 3.534e-05  Data: 0.025 (0.027)
Train: 266 [ 650/1251 ( 52%)]  Loss: 3.057 (2.86)  Time: 0.303s, 3378.61/s  (0.303s, 3379.36/s)  LR: 3.526e-05  Data: 0.022 (0.027)
Train: 266 [ 700/1251 ( 56%)]  Loss: 2.945 (2.87)  Time: 0.302s, 3387.48/s  (0.303s, 3379.60/s)  LR: 3.519e-05  Data: 0.023 (0.026)
Train: 266 [ 750/1251 ( 60%)]  Loss: 3.047 (2.88)  Time: 0.304s, 3366.88/s  (0.303s, 3379.35/s)  LR: 3.512e-05  Data: 0.025 (0.026)
Train: 266 [ 800/1251 ( 64%)]  Loss: 2.813 (2.87)  Time: 0.305s, 3361.91/s  (0.303s, 3379.42/s)  LR: 3.505e-05  Data: 0.021 (0.026)
Train: 266 [ 850/1251 ( 68%)]  Loss: 2.859 (2.87)  Time: 0.307s, 3340.15/s  (0.303s, 3379.15/s)  LR: 3.498e-05  Data: 0.021 (0.026)
Train: 266 [ 900/1251 ( 72%)]  Loss: 3.065 (2.88)  Time: 0.302s, 3388.25/s  (0.303s, 3378.94/s)  LR: 3.491e-05  Data: 0.021 (0.026)
Train: 266 [ 950/1251 ( 76%)]  Loss: 2.723 (2.87)  Time: 0.301s, 3405.32/s  (0.303s, 3378.61/s)  LR: 3.484e-05  Data: 0.022 (0.026)
Train: 266 [1000/1251 ( 80%)]  Loss: 2.713 (2.87)  Time: 0.304s, 3365.81/s  (0.303s, 3377.92/s)  LR: 3.477e-05  Data: 0.023 (0.026)
Train: 266 [1050/1251 ( 84%)]  Loss: 3.081 (2.88)  Time: 0.304s, 3370.63/s  (0.303s, 3377.68/s)  LR: 3.469e-05  Data: 0.024 (0.025)
Train: 266 [1100/1251 ( 88%)]  Loss: 3.166 (2.89)  Time: 0.302s, 3393.62/s  (0.303s, 3377.36/s)  LR: 3.462e-05  Data: 0.023 (0.025)
Train: 266 [1150/1251 ( 92%)]  Loss: 2.825 (2.89)  Time: 0.302s, 3394.78/s  (0.303s, 3376.90/s)  LR: 3.455e-05  Data: 0.020 (0.025)
Train: 266 [1200/1251 ( 96%)]  Loss: 2.692 (2.88)  Time: 0.301s, 3398.14/s  (0.303s, 3375.88/s)  LR: 3.448e-05  Data: 0.028 (0.025)
Train: 266 [1250/1251 (100%)]  Loss: 2.992 (2.88)  Time: 0.275s, 3718.97/s  (0.303s, 3376.85/s)  LR: 3.441e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.026 (2.026)  Loss:  0.4851 (0.4851)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.053 (0.237)  Loss:  0.6372 (0.9325)  Acc@1: 86.4387 (79.4920)  Acc@5: 97.4057 (94.6200)
Train: 267 [   0/1251 (  0%)]  Loss: 2.893 (2.89)  Time: 2.371s,  431.94/s  (2.371s,  431.94/s)  LR: 3.441e-05  Data: 2.147 (2.147)
Train: 267 [  50/1251 (  4%)]  Loss: 3.065 (2.98)  Time: 0.296s, 3458.06/s  (0.323s, 3165.44/s)  LR: 3.434e-05  Data: 0.026 (0.066)
Train: 267 [ 100/1251 (  8%)]  Loss: 2.576 (2.84)  Time: 0.301s, 3403.45/s  (0.311s, 3294.80/s)  LR: 3.427e-05  Data: 0.023 (0.045)
Train: 267 [ 150/1251 ( 12%)]  Loss: 3.093 (2.91)  Time: 0.294s, 3481.42/s  (0.308s, 3327.59/s)  LR: 3.420e-05  Data: 0.019 (0.038)
Train: 267 [ 200/1251 ( 16%)]  Loss: 2.656 (2.86)  Time: 0.302s, 3385.94/s  (0.307s, 3339.35/s)  LR: 3.413e-05  Data: 0.021 (0.034)
Train: 267 [ 250/1251 ( 20%)]  Loss: 2.778 (2.84)  Time: 0.304s, 3370.78/s  (0.306s, 3345.29/s)  LR: 3.406e-05  Data: 0.022 (0.032)
Train: 267 [ 300/1251 ( 24%)]  Loss: 2.905 (2.85)  Time: 0.305s, 3362.69/s  (0.306s, 3347.31/s)  LR: 3.399e-05  Data: 0.022 (0.031)
Train: 267 [ 350/1251 ( 28%)]  Loss: 3.019 (2.87)  Time: 0.303s, 3379.37/s  (0.306s, 3348.50/s)  LR: 3.392e-05  Data: 0.023 (0.030)
Train: 267 [ 400/1251 ( 32%)]  Loss: 2.898 (2.88)  Time: 0.310s, 3298.32/s  (0.306s, 3348.89/s)  LR: 3.385e-05  Data: 0.022 (0.029)
Train: 267 [ 450/1251 ( 36%)]  Loss: 2.797 (2.87)  Time: 0.307s, 3339.72/s  (0.306s, 3346.52/s)  LR: 3.378e-05  Data: 0.023 (0.028)
Train: 267 [ 500/1251 ( 40%)]  Loss: 3.093 (2.89)  Time: 0.304s, 3369.20/s  (0.306s, 3345.63/s)  LR: 3.371e-05  Data: 0.022 (0.028)
Train: 267 [ 550/1251 ( 44%)]  Loss: 2.913 (2.89)  Time: 0.311s, 3296.75/s  (0.306s, 3344.43/s)  LR: 3.364e-05  Data: 0.022 (0.027)
Train: 267 [ 600/1251 ( 48%)]  Loss: 2.998 (2.90)  Time: 0.309s, 3311.97/s  (0.306s, 3343.20/s)  LR: 3.357e-05  Data: 0.026 (0.027)
Train: 267 [ 650/1251 ( 52%)]  Loss: 2.931 (2.90)  Time: 0.309s, 3314.17/s  (0.306s, 3341.51/s)  LR: 3.350e-05  Data: 0.020 (0.027)
Train: 267 [ 700/1251 ( 56%)]  Loss: 2.794 (2.89)  Time: 0.309s, 3317.10/s  (0.307s, 3340.05/s)  LR: 3.343e-05  Data: 0.023 (0.027)
Train: 267 [ 750/1251 ( 60%)]  Loss: 2.821 (2.89)  Time: 0.310s, 3307.18/s  (0.307s, 3338.35/s)  LR: 3.336e-05  Data: 0.021 (0.026)
Train: 267 [ 800/1251 ( 64%)]  Loss: 3.005 (2.90)  Time: 0.310s, 3300.08/s  (0.307s, 3337.57/s)  LR: 3.329e-05  Data: 0.021 (0.026)
Train: 267 [ 850/1251 ( 68%)]  Loss: 2.977 (2.90)  Time: 0.311s, 3297.44/s  (0.307s, 3336.96/s)  LR: 3.322e-05  Data: 0.028 (0.026)
Train: 267 [ 900/1251 ( 72%)]  Loss: 2.939 (2.90)  Time: 0.310s, 3300.36/s  (0.307s, 3336.45/s)  LR: 3.315e-05  Data: 0.021 (0.026)
Train: 267 [ 950/1251 ( 76%)]  Loss: 2.610 (2.89)  Time: 0.308s, 3322.76/s  (0.307s, 3335.75/s)  LR: 3.309e-05  Data: 0.023 (0.026)
Train: 267 [1000/1251 ( 80%)]  Loss: 2.786 (2.88)  Time: 0.307s, 3340.37/s  (0.307s, 3335.21/s)  LR: 3.302e-05  Data: 0.026 (0.026)
Train: 267 [1050/1251 ( 84%)]  Loss: 2.456 (2.86)  Time: 0.309s, 3315.22/s  (0.307s, 3334.76/s)  LR: 3.295e-05  Data: 0.023 (0.026)
Train: 267 [1100/1251 ( 88%)]  Loss: 3.030 (2.87)  Time: 0.308s, 3324.77/s  (0.307s, 3334.23/s)  LR: 3.288e-05  Data: 0.024 (0.025)
Train: 267 [1150/1251 ( 92%)]  Loss: 2.909 (2.87)  Time: 0.310s, 3302.14/s  (0.307s, 3333.49/s)  LR: 3.281e-05  Data: 0.019 (0.025)
Train: 267 [1200/1251 ( 96%)]  Loss: 3.156 (2.88)  Time: 0.310s, 3299.04/s  (0.307s, 3333.08/s)  LR: 3.274e-05  Data: 0.027 (0.025)
Train: 267 [1250/1251 (100%)]  Loss: 2.815 (2.88)  Time: 0.276s, 3716.09/s  (0.307s, 3334.80/s)  LR: 3.267e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.005 (2.005)  Loss:  0.4543 (0.4543)  Acc@1: 92.5781 (92.5781)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.068 (0.235)  Loss:  0.6250 (0.9141)  Acc@1: 85.7311 (79.5580)  Acc@5: 97.9953 (94.6900)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-263.pth.tar', 79.59799995117187)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-265.pth.tar', 79.58999992919922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-267.pth.tar', 79.55800000732422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-262.pth.tar', 79.55600003173828)

Train: 268 [   0/1251 (  0%)]  Loss: 3.003 (3.00)  Time: 2.143s,  477.85/s  (2.143s,  477.85/s)  LR: 3.267e-05  Data: 1.894 (1.894)
Train: 268 [  50/1251 (  4%)]  Loss: 2.545 (2.77)  Time: 0.296s, 3461.23/s  (0.327s, 3134.43/s)  LR: 3.260e-05  Data: 0.025 (0.063)
Train: 268 [ 100/1251 (  8%)]  Loss: 2.819 (2.79)  Time: 0.308s, 3319.95/s  (0.314s, 3259.23/s)  LR: 3.253e-05  Data: 0.022 (0.044)
Train: 268 [ 150/1251 ( 12%)]  Loss: 2.773 (2.79)  Time: 0.304s, 3365.06/s  (0.311s, 3292.54/s)  LR: 3.247e-05  Data: 0.022 (0.037)
Train: 268 [ 200/1251 ( 16%)]  Loss: 2.843 (2.80)  Time: 0.304s, 3371.93/s  (0.310s, 3308.15/s)  LR: 3.240e-05  Data: 0.022 (0.034)
Train: 268 [ 250/1251 ( 20%)]  Loss: 2.958 (2.82)  Time: 0.310s, 3307.74/s  (0.309s, 3316.23/s)  LR: 3.233e-05  Data: 0.023 (0.032)
Train: 268 [ 300/1251 ( 24%)]  Loss: 2.880 (2.83)  Time: 0.312s, 3280.92/s  (0.309s, 3318.93/s)  LR: 3.226e-05  Data: 0.024 (0.030)
Train: 268 [ 350/1251 ( 28%)]  Loss: 2.761 (2.82)  Time: 0.310s, 3303.25/s  (0.308s, 3321.05/s)  LR: 3.219e-05  Data: 0.024 (0.029)
Train: 268 [ 400/1251 ( 32%)]  Loss: 2.915 (2.83)  Time: 0.308s, 3322.01/s  (0.308s, 3323.41/s)  LR: 3.213e-05  Data: 0.024 (0.029)
Train: 268 [ 450/1251 ( 36%)]  Loss: 2.710 (2.82)  Time: 0.311s, 3295.53/s  (0.308s, 3325.29/s)  LR: 3.206e-05  Data: 0.022 (0.028)
Train: 268 [ 500/1251 ( 40%)]  Loss: 3.047 (2.84)  Time: 0.305s, 3356.81/s  (0.308s, 3325.19/s)  LR: 3.199e-05  Data: 0.023 (0.028)
Train: 268 [ 550/1251 ( 44%)]  Loss: 2.912 (2.85)  Time: 0.307s, 3336.10/s  (0.308s, 3325.65/s)  LR: 3.192e-05  Data: 0.024 (0.027)
Train: 268 [ 600/1251 ( 48%)]  Loss: 3.040 (2.86)  Time: 0.307s, 3330.82/s  (0.308s, 3326.26/s)  LR: 3.186e-05  Data: 0.024 (0.027)
Train: 268 [ 650/1251 ( 52%)]  Loss: 2.999 (2.87)  Time: 0.310s, 3299.43/s  (0.308s, 3326.23/s)  LR: 3.179e-05  Data: 0.023 (0.027)
Train: 268 [ 700/1251 ( 56%)]  Loss: 2.805 (2.87)  Time: 0.308s, 3327.44/s  (0.308s, 3327.09/s)  LR: 3.172e-05  Data: 0.022 (0.026)
Train: 268 [ 750/1251 ( 60%)]  Loss: 2.845 (2.87)  Time: 0.311s, 3294.93/s  (0.308s, 3326.81/s)  LR: 3.165e-05  Data: 0.026 (0.026)
Train: 268 [ 800/1251 ( 64%)]  Loss: 2.901 (2.87)  Time: 0.309s, 3319.22/s  (0.308s, 3327.65/s)  LR: 3.159e-05  Data: 0.022 (0.026)
Train: 268 [ 850/1251 ( 68%)]  Loss: 2.803 (2.86)  Time: 0.312s, 3278.57/s  (0.308s, 3327.92/s)  LR: 3.152e-05  Data: 0.023 (0.026)
Train: 268 [ 900/1251 ( 72%)]  Loss: 3.117 (2.88)  Time: 0.303s, 3377.75/s  (0.308s, 3327.91/s)  LR: 3.145e-05  Data: 0.022 (0.026)
Train: 268 [ 950/1251 ( 76%)]  Loss: 2.835 (2.88)  Time: 0.310s, 3302.83/s  (0.308s, 3327.89/s)  LR: 3.139e-05  Data: 0.024 (0.026)
Train: 268 [1000/1251 ( 80%)]  Loss: 2.882 (2.88)  Time: 0.301s, 3404.49/s  (0.308s, 3327.91/s)  LR: 3.132e-05  Data: 0.021 (0.026)
Train: 268 [1050/1251 ( 84%)]  Loss: 3.034 (2.88)  Time: 0.309s, 3317.47/s  (0.308s, 3327.72/s)  LR: 3.125e-05  Data: 0.025 (0.025)
Train: 268 [1100/1251 ( 88%)]  Loss: 3.027 (2.89)  Time: 0.312s, 3281.00/s  (0.308s, 3327.24/s)  LR: 3.118e-05  Data: 0.026 (0.025)
Train: 268 [1150/1251 ( 92%)]  Loss: 3.063 (2.90)  Time: 0.316s, 3236.66/s  (0.308s, 3326.67/s)  LR: 3.112e-05  Data: 0.024 (0.025)
Train: 268 [1200/1251 ( 96%)]  Loss: 2.932 (2.90)  Time: 0.308s, 3319.96/s  (0.308s, 3326.26/s)  LR: 3.105e-05  Data: 0.023 (0.025)
Train: 268 [1250/1251 (100%)]  Loss: 2.805 (2.89)  Time: 0.276s, 3715.01/s  (0.308s, 3327.97/s)  LR: 3.099e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.031 (2.031)  Loss:  0.4583 (0.4583)  Acc@1: 93.0664 (93.0664)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.062 (0.239)  Loss:  0.6240 (0.9163)  Acc@1: 85.1415 (79.6300)  Acc@5: 97.8774 (94.7020)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-268.pth.tar', 79.63000000976562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-263.pth.tar', 79.59799995117187)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-265.pth.tar', 79.58999992919922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-267.pth.tar', 79.55800000732422)

Train: 269 [   0/1251 (  0%)]  Loss: 2.702 (2.70)  Time: 1.971s,  519.62/s  (1.971s,  519.62/s)  LR: 3.098e-05  Data: 1.740 (1.740)
Train: 269 [  50/1251 (  4%)]  Loss: 2.804 (2.75)  Time: 0.301s, 3403.12/s  (0.333s, 3077.83/s)  LR: 3.092e-05  Data: 0.022 (0.059)
Train: 269 [ 100/1251 (  8%)]  Loss: 2.915 (2.81)  Time: 0.305s, 3359.58/s  (0.319s, 3210.68/s)  LR: 3.085e-05  Data: 0.023 (0.041)
Train: 269 [ 150/1251 ( 12%)]  Loss: 2.797 (2.80)  Time: 0.309s, 3314.54/s  (0.314s, 3257.15/s)  LR: 3.079e-05  Data: 0.023 (0.035)
Train: 269 [ 200/1251 ( 16%)]  Loss: 3.061 (2.86)  Time: 0.301s, 3402.02/s  (0.312s, 3281.08/s)  LR: 3.072e-05  Data: 0.022 (0.032)
Train: 269 [ 250/1251 ( 20%)]  Loss: 2.812 (2.85)  Time: 0.309s, 3309.01/s  (0.311s, 3291.81/s)  LR: 3.065e-05  Data: 0.027 (0.031)
Train: 269 [ 300/1251 ( 24%)]  Loss: 2.727 (2.83)  Time: 0.308s, 3323.87/s  (0.310s, 3300.75/s)  LR: 3.059e-05  Data: 0.020 (0.029)
Train: 269 [ 350/1251 ( 28%)]  Loss: 2.818 (2.83)  Time: 0.315s, 3252.50/s  (0.310s, 3305.58/s)  LR: 3.052e-05  Data: 0.028 (0.029)
Train: 269 [ 400/1251 ( 32%)]  Loss: 2.862 (2.83)  Time: 0.314s, 3259.31/s  (0.309s, 3308.64/s)  LR: 3.046e-05  Data: 0.023 (0.028)
Train: 269 [ 450/1251 ( 36%)]  Loss: 2.576 (2.81)  Time: 0.307s, 3333.03/s  (0.309s, 3309.76/s)  LR: 3.039e-05  Data: 0.021 (0.027)
Train: 269 [ 500/1251 ( 40%)]  Loss: 2.729 (2.80)  Time: 0.307s, 3331.79/s  (0.309s, 3311.35/s)  LR: 3.032e-05  Data: 0.025 (0.027)
Train: 269 [ 550/1251 ( 44%)]  Loss: 3.119 (2.83)  Time: 0.310s, 3298.62/s  (0.309s, 3312.64/s)  LR: 3.026e-05  Data: 0.024 (0.027)
Train: 269 [ 600/1251 ( 48%)]  Loss: 2.800 (2.82)  Time: 0.306s, 3351.00/s  (0.309s, 3314.19/s)  LR: 3.019e-05  Data: 0.022 (0.026)
Train: 269 [ 650/1251 ( 52%)]  Loss: 2.860 (2.83)  Time: 0.310s, 3306.89/s  (0.309s, 3314.82/s)  LR: 3.013e-05  Data: 0.025 (0.026)
Train: 269 [ 700/1251 ( 56%)]  Loss: 2.998 (2.84)  Time: 0.304s, 3370.82/s  (0.309s, 3315.32/s)  LR: 3.006e-05  Data: 0.022 (0.026)
Train: 269 [ 750/1251 ( 60%)]  Loss: 2.911 (2.84)  Time: 0.314s, 3257.85/s  (0.309s, 3316.12/s)  LR: 3.000e-05  Data: 0.028 (0.026)
Train: 269 [ 800/1251 ( 64%)]  Loss: 2.719 (2.84)  Time: 0.304s, 3369.20/s  (0.309s, 3316.13/s)  LR: 2.993e-05  Data: 0.023 (0.026)
Train: 269 [ 850/1251 ( 68%)]  Loss: 3.062 (2.85)  Time: 0.308s, 3322.16/s  (0.309s, 3316.24/s)  LR: 2.987e-05  Data: 0.019 (0.026)
Train: 269 [ 900/1251 ( 72%)]  Loss: 3.023 (2.86)  Time: 0.310s, 3307.38/s  (0.309s, 3316.39/s)  LR: 2.980e-05  Data: 0.024 (0.025)
Train: 269 [ 950/1251 ( 76%)]  Loss: 2.426 (2.84)  Time: 0.310s, 3304.70/s  (0.309s, 3316.63/s)  LR: 2.974e-05  Data: 0.020 (0.025)
Train: 269 [1000/1251 ( 80%)]  Loss: 2.936 (2.84)  Time: 0.307s, 3330.26/s  (0.309s, 3317.20/s)  LR: 2.967e-05  Data: 0.021 (0.025)
Train: 269 [1050/1251 ( 84%)]  Loss: 2.901 (2.84)  Time: 0.307s, 3334.04/s  (0.309s, 3317.26/s)  LR: 2.961e-05  Data: 0.025 (0.025)
Train: 269 [1100/1251 ( 88%)]  Loss: 2.558 (2.83)  Time: 0.308s, 3324.93/s  (0.309s, 3316.99/s)  LR: 2.954e-05  Data: 0.020 (0.025)
Train: 269 [1150/1251 ( 92%)]  Loss: 2.498 (2.82)  Time: 0.307s, 3338.18/s  (0.309s, 3317.22/s)  LR: 2.948e-05  Data: 0.022 (0.025)
Train: 269 [1200/1251 ( 96%)]  Loss: 3.160 (2.83)  Time: 0.306s, 3342.77/s  (0.309s, 3317.59/s)  LR: 2.941e-05  Data: 0.021 (0.025)
Train: 269 [1250/1251 (100%)]  Loss: 2.701 (2.83)  Time: 0.278s, 3686.53/s  (0.308s, 3319.96/s)  LR: 2.935e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.107 (2.107)  Loss:  0.4666 (0.4666)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.057 (0.232)  Loss:  0.6172 (0.9239)  Acc@1: 85.4953 (79.5760)  Acc@5: 97.6415 (94.6860)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-268.pth.tar', 79.63000000976562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-263.pth.tar', 79.59799995117187)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-265.pth.tar', 79.58999992919922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-269.pth.tar', 79.57600003417969)

Train: 270 [   0/1251 (  0%)]  Loss: 2.855 (2.86)  Time: 2.487s,  411.67/s  (2.487s,  411.67/s)  LR: 2.935e-05  Data: 2.260 (2.260)
Train: 270 [  50/1251 (  4%)]  Loss: 2.874 (2.86)  Time: 0.305s, 3355.56/s  (0.331s, 3095.88/s)  LR: 2.928e-05  Data: 0.021 (0.068)
Train: 270 [ 100/1251 (  8%)]  Loss: 2.831 (2.85)  Time: 0.305s, 3354.49/s  (0.316s, 3236.86/s)  LR: 2.922e-05  Data: 0.023 (0.046)
Train: 270 [ 150/1251 ( 12%)]  Loss: 2.747 (2.83)  Time: 0.307s, 3340.54/s  (0.313s, 3276.11/s)  LR: 2.916e-05  Data: 0.022 (0.038)
Train: 270 [ 200/1251 ( 16%)]  Loss: 2.726 (2.81)  Time: 0.307s, 3333.56/s  (0.311s, 3296.38/s)  LR: 2.909e-05  Data: 0.023 (0.035)
Train: 270 [ 250/1251 ( 20%)]  Loss: 2.763 (2.80)  Time: 0.308s, 3320.77/s  (0.310s, 3308.22/s)  LR: 2.903e-05  Data: 0.024 (0.033)
Train: 270 [ 300/1251 ( 24%)]  Loss: 2.921 (2.82)  Time: 0.306s, 3347.59/s  (0.309s, 3313.80/s)  LR: 2.896e-05  Data: 0.023 (0.031)
Train: 270 [ 350/1251 ( 28%)]  Loss: 2.971 (2.84)  Time: 0.305s, 3359.24/s  (0.309s, 3315.61/s)  LR: 2.890e-05  Data: 0.021 (0.030)
Train: 270 [ 400/1251 ( 32%)]  Loss: 3.047 (2.86)  Time: 0.309s, 3317.24/s  (0.309s, 3317.28/s)  LR: 2.884e-05  Data: 0.025 (0.029)
Train: 270 [ 450/1251 ( 36%)]  Loss: 2.994 (2.87)  Time: 0.308s, 3319.71/s  (0.309s, 3318.68/s)  LR: 2.877e-05  Data: 0.022 (0.028)
Train: 270 [ 500/1251 ( 40%)]  Loss: 2.937 (2.88)  Time: 0.305s, 3353.45/s  (0.308s, 3319.91/s)  LR: 2.871e-05  Data: 0.025 (0.028)
Train: 270 [ 550/1251 ( 44%)]  Loss: 2.830 (2.87)  Time: 0.310s, 3298.43/s  (0.308s, 3320.49/s)  LR: 2.865e-05  Data: 0.023 (0.028)
Train: 270 [ 600/1251 ( 48%)]  Loss: 2.939 (2.88)  Time: 0.312s, 3283.40/s  (0.308s, 3321.37/s)  LR: 2.858e-05  Data: 0.027 (0.027)
Train: 270 [ 650/1251 ( 52%)]  Loss: 2.673 (2.86)  Time: 0.308s, 3320.14/s  (0.308s, 3321.50/s)  LR: 2.852e-05  Data: 0.027 (0.027)
Train: 270 [ 700/1251 ( 56%)]  Loss: 2.650 (2.85)  Time: 0.311s, 3294.19/s  (0.308s, 3321.53/s)  LR: 2.846e-05  Data: 0.023 (0.027)
Train: 270 [ 750/1251 ( 60%)]  Loss: 3.073 (2.86)  Time: 0.309s, 3308.80/s  (0.308s, 3321.68/s)  LR: 2.839e-05  Data: 0.021 (0.026)
Train: 270 [ 800/1251 ( 64%)]  Loss: 2.730 (2.86)  Time: 0.309s, 3313.77/s  (0.308s, 3322.01/s)  LR: 2.833e-05  Data: 0.020 (0.026)
Train: 270 [ 850/1251 ( 68%)]  Loss: 2.663 (2.85)  Time: 0.311s, 3291.52/s  (0.308s, 3321.93/s)  LR: 2.827e-05  Data: 0.023 (0.026)
Train: 270 [ 900/1251 ( 72%)]  Loss: 3.133 (2.86)  Time: 0.312s, 3286.46/s  (0.308s, 3322.02/s)  LR: 2.820e-05  Data: 0.026 (0.026)
Train: 270 [ 950/1251 ( 76%)]  Loss: 2.511 (2.84)  Time: 0.312s, 3284.40/s  (0.308s, 3321.74/s)  LR: 2.814e-05  Data: 0.023 (0.026)
Train: 270 [1000/1251 ( 80%)]  Loss: 3.008 (2.85)  Time: 0.313s, 3274.15/s  (0.308s, 3321.88/s)  LR: 2.808e-05  Data: 0.023 (0.026)
Train: 270 [1050/1251 ( 84%)]  Loss: 2.955 (2.86)  Time: 0.309s, 3318.84/s  (0.308s, 3321.64/s)  LR: 2.802e-05  Data: 0.023 (0.026)
Train: 270 [1100/1251 ( 88%)]  Loss: 2.856 (2.86)  Time: 0.307s, 3335.83/s  (0.308s, 3321.99/s)  LR: 2.795e-05  Data: 0.025 (0.025)
Train: 270 [1150/1251 ( 92%)]  Loss: 2.972 (2.86)  Time: 0.303s, 3383.05/s  (0.308s, 3322.30/s)  LR: 2.789e-05  Data: 0.020 (0.025)
Train: 270 [1200/1251 ( 96%)]  Loss: 2.754 (2.86)  Time: 0.307s, 3336.09/s  (0.308s, 3322.59/s)  LR: 2.783e-05  Data: 0.023 (0.025)
Train: 270 [1250/1251 (100%)]  Loss: 2.633 (2.85)  Time: 0.276s, 3716.49/s  (0.308s, 3324.54/s)  LR: 2.777e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.190 (2.190)  Loss:  0.4731 (0.4731)  Acc@1: 92.3828 (92.3828)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.058 (0.234)  Loss:  0.6333 (0.9247)  Acc@1: 85.3774 (79.5880)  Acc@5: 97.7594 (94.7240)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-268.pth.tar', 79.63000000976562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-263.pth.tar', 79.59799995117187)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-265.pth.tar', 79.58999992919922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-270.pth.tar', 79.58800011230468)

Train: 271 [   0/1251 (  0%)]  Loss: 2.645 (2.65)  Time: 2.328s,  439.81/s  (2.328s,  439.81/s)  LR: 2.776e-05  Data: 2.101 (2.101)
Train: 271 [  50/1251 (  4%)]  Loss: 2.942 (2.79)  Time: 0.302s, 3390.15/s  (0.337s, 3034.26/s)  LR: 2.770e-05  Data: 0.023 (0.075)
Train: 271 [ 100/1251 (  8%)]  Loss: 3.107 (2.90)  Time: 0.305s, 3357.84/s  (0.320s, 3203.71/s)  LR: 2.764e-05  Data: 0.022 (0.049)
Train: 271 [ 150/1251 ( 12%)]  Loss: 2.652 (2.84)  Time: 0.308s, 3324.67/s  (0.315s, 3255.71/s)  LR: 2.758e-05  Data: 0.026 (0.041)
Train: 271 [ 200/1251 ( 16%)]  Loss: 2.847 (2.84)  Time: 0.302s, 3393.10/s  (0.312s, 3282.50/s)  LR: 2.752e-05  Data: 0.022 (0.036)
Train: 271 [ 250/1251 ( 20%)]  Loss: 3.095 (2.88)  Time: 0.306s, 3345.65/s  (0.311s, 3296.75/s)  LR: 2.745e-05  Data: 0.023 (0.034)
Train: 271 [ 300/1251 ( 24%)]  Loss: 2.911 (2.89)  Time: 0.306s, 3351.64/s  (0.310s, 3305.54/s)  LR: 2.739e-05  Data: 0.023 (0.032)
Train: 271 [ 350/1251 ( 28%)]  Loss: 2.593 (2.85)  Time: 0.313s, 3273.58/s  (0.309s, 3309.95/s)  LR: 2.733e-05  Data: 0.022 (0.031)
Train: 271 [ 400/1251 ( 32%)]  Loss: 3.041 (2.87)  Time: 0.305s, 3360.23/s  (0.309s, 3313.04/s)  LR: 2.727e-05  Data: 0.025 (0.030)
Train: 271 [ 450/1251 ( 36%)]  Loss: 2.903 (2.87)  Time: 0.311s, 3289.92/s  (0.309s, 3316.80/s)  LR: 2.721e-05  Data: 0.031 (0.029)
Train: 271 [ 500/1251 ( 40%)]  Loss: 2.854 (2.87)  Time: 0.307s, 3335.91/s  (0.308s, 3320.23/s)  LR: 2.715e-05  Data: 0.022 (0.029)
Train: 271 [ 550/1251 ( 44%)]  Loss: 3.054 (2.89)  Time: 0.308s, 3325.36/s  (0.308s, 3322.47/s)  LR: 2.708e-05  Data: 0.024 (0.028)
Train: 271 [ 600/1251 ( 48%)]  Loss: 2.553 (2.86)  Time: 0.306s, 3350.35/s  (0.308s, 3324.27/s)  LR: 2.702e-05  Data: 0.024 (0.028)
Train: 271 [ 650/1251 ( 52%)]  Loss: 2.721 (2.85)  Time: 0.302s, 3385.27/s  (0.308s, 3325.46/s)  LR: 2.696e-05  Data: 0.020 (0.027)
Train: 271 [ 700/1251 ( 56%)]  Loss: 2.734 (2.84)  Time: 0.306s, 3341.41/s  (0.308s, 3327.08/s)  LR: 2.690e-05  Data: 0.025 (0.027)
Train: 271 [ 750/1251 ( 60%)]  Loss: 2.705 (2.83)  Time: 0.303s, 3376.47/s  (0.308s, 3328.43/s)  LR: 2.684e-05  Data: 0.025 (0.027)
Train: 271 [ 800/1251 ( 64%)]  Loss: 2.815 (2.83)  Time: 0.301s, 3403.93/s  (0.308s, 3330.04/s)  LR: 2.678e-05  Data: 0.021 (0.027)
Train: 271 [ 850/1251 ( 68%)]  Loss: 2.611 (2.82)  Time: 0.304s, 3373.46/s  (0.307s, 3330.81/s)  LR: 2.672e-05  Data: 0.023 (0.026)
Train: 271 [ 900/1251 ( 72%)]  Loss: 2.623 (2.81)  Time: 0.305s, 3353.05/s  (0.307s, 3331.85/s)  LR: 2.666e-05  Data: 0.026 (0.026)
Train: 271 [ 950/1251 ( 76%)]  Loss: 2.766 (2.81)  Time: 0.307s, 3332.58/s  (0.307s, 3332.80/s)  LR: 2.660e-05  Data: 0.021 (0.026)
Train: 271 [1000/1251 ( 80%)]  Loss: 2.813 (2.81)  Time: 0.313s, 3271.42/s  (0.307s, 3333.31/s)  LR: 2.654e-05  Data: 0.019 (0.026)
Train: 271 [1050/1251 ( 84%)]  Loss: 3.141 (2.82)  Time: 0.307s, 3334.98/s  (0.307s, 3334.09/s)  LR: 2.647e-05  Data: 0.023 (0.026)
Train: 271 [1100/1251 ( 88%)]  Loss: 3.016 (2.83)  Time: 0.304s, 3365.90/s  (0.307s, 3334.89/s)  LR: 2.641e-05  Data: 0.023 (0.026)
Train: 271 [1150/1251 ( 92%)]  Loss: 2.997 (2.84)  Time: 0.303s, 3380.94/s  (0.307s, 3335.49/s)  LR: 2.635e-05  Data: 0.026 (0.026)
Train: 271 [1200/1251 ( 96%)]  Loss: 3.173 (2.85)  Time: 0.305s, 3362.40/s  (0.307s, 3335.84/s)  LR: 2.629e-05  Data: 0.023 (0.026)
Train: 271 [1250/1251 (100%)]  Loss: 2.677 (2.85)  Time: 0.275s, 3723.83/s  (0.307s, 3338.27/s)  LR: 2.623e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.024 (2.024)  Loss:  0.4624 (0.4624)  Acc@1: 92.5781 (92.5781)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.052 (0.242)  Loss:  0.6235 (0.9217)  Acc@1: 86.2028 (79.6340)  Acc@5: 97.7594 (94.6600)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-271.pth.tar', 79.6340000830078)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-268.pth.tar', 79.63000000976562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-263.pth.tar', 79.59799995117187)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-265.pth.tar', 79.58999992919922)

Train: 272 [   0/1251 (  0%)]  Loss: 2.897 (2.90)  Time: 2.443s,  419.14/s  (2.443s,  419.14/s)  LR: 2.623e-05  Data: 2.221 (2.221)
Train: 272 [  50/1251 (  4%)]  Loss: 2.952 (2.92)  Time: 0.293s, 3496.08/s  (0.326s, 3138.23/s)  LR: 2.617e-05  Data: 0.028 (0.067)
Train: 272 [ 100/1251 (  8%)]  Loss: 2.751 (2.87)  Time: 0.303s, 3379.95/s  (0.312s, 3281.24/s)  LR: 2.611e-05  Data: 0.025 (0.045)
Train: 272 [ 150/1251 ( 12%)]  Loss: 2.887 (2.87)  Time: 0.306s, 3343.93/s  (0.308s, 3319.97/s)  LR: 2.605e-05  Data: 0.024 (0.038)
Train: 272 [ 200/1251 ( 16%)]  Loss: 2.892 (2.88)  Time: 0.301s, 3405.33/s  (0.307s, 3336.46/s)  LR: 2.599e-05  Data: 0.025 (0.034)
Train: 272 [ 250/1251 ( 20%)]  Loss: 2.897 (2.88)  Time: 0.305s, 3358.29/s  (0.306s, 3344.69/s)  LR: 2.593e-05  Data: 0.026 (0.032)
Train: 272 [ 300/1251 ( 24%)]  Loss: 2.536 (2.83)  Time: 0.307s, 3336.52/s  (0.306s, 3349.41/s)  LR: 2.587e-05  Data: 0.026 (0.031)
Train: 272 [ 350/1251 ( 28%)]  Loss: 3.132 (2.87)  Time: 0.309s, 3316.90/s  (0.305s, 3353.56/s)  LR: 2.581e-05  Data: 0.024 (0.030)
Train: 272 [ 400/1251 ( 32%)]  Loss: 2.747 (2.85)  Time: 0.298s, 3432.05/s  (0.305s, 3355.72/s)  LR: 2.575e-05  Data: 0.020 (0.029)
Train: 272 [ 450/1251 ( 36%)]  Loss: 2.679 (2.84)  Time: 0.305s, 3354.09/s  (0.305s, 3355.85/s)  LR: 2.569e-05  Data: 0.021 (0.028)
Train: 272 [ 500/1251 ( 40%)]  Loss: 2.886 (2.84)  Time: 0.307s, 3334.73/s  (0.305s, 3355.83/s)  LR: 2.563e-05  Data: 0.021 (0.028)
Train: 272 [ 550/1251 ( 44%)]  Loss: 2.878 (2.84)  Time: 0.305s, 3352.94/s  (0.305s, 3355.82/s)  LR: 2.558e-05  Data: 0.020 (0.027)
Train: 272 [ 600/1251 ( 48%)]  Loss: 2.885 (2.85)  Time: 0.305s, 3352.62/s  (0.305s, 3356.83/s)  LR: 2.552e-05  Data: 0.024 (0.027)
Train: 272 [ 650/1251 ( 52%)]  Loss: 2.896 (2.85)  Time: 0.309s, 3312.99/s  (0.305s, 3356.98/s)  LR: 2.546e-05  Data: 0.022 (0.027)
Train: 272 [ 700/1251 ( 56%)]  Loss: 2.811 (2.85)  Time: 0.307s, 3337.52/s  (0.305s, 3357.66/s)  LR: 2.540e-05  Data: 0.024 (0.027)
Train: 272 [ 750/1251 ( 60%)]  Loss: 2.816 (2.85)  Time: 0.308s, 3319.57/s  (0.305s, 3358.24/s)  LR: 2.534e-05  Data: 0.024 (0.026)
Train: 272 [ 800/1251 ( 64%)]  Loss: 2.700 (2.84)  Time: 0.301s, 3399.03/s  (0.305s, 3357.94/s)  LR: 2.528e-05  Data: 0.018 (0.026)
Train: 272 [ 850/1251 ( 68%)]  Loss: 2.540 (2.82)  Time: 0.301s, 3399.68/s  (0.305s, 3357.87/s)  LR: 2.522e-05  Data: 0.021 (0.026)
Train: 272 [ 900/1251 ( 72%)]  Loss: 2.942 (2.83)  Time: 0.304s, 3373.18/s  (0.305s, 3357.80/s)  LR: 2.516e-05  Data: 0.023 (0.026)
Train: 272 [ 950/1251 ( 76%)]  Loss: 2.426 (2.81)  Time: 0.306s, 3348.60/s  (0.305s, 3357.87/s)  LR: 2.510e-05  Data: 0.026 (0.026)
Train: 272 [1000/1251 ( 80%)]  Loss: 2.844 (2.81)  Time: 0.308s, 3327.68/s  (0.305s, 3358.31/s)  LR: 2.505e-05  Data: 0.021 (0.026)
Train: 272 [1050/1251 ( 84%)]  Loss: 2.847 (2.81)  Time: 0.309s, 3310.95/s  (0.305s, 3358.68/s)  LR: 2.499e-05  Data: 0.024 (0.026)
Train: 272 [1100/1251 ( 88%)]  Loss: 2.829 (2.81)  Time: 0.304s, 3365.92/s  (0.305s, 3358.53/s)  LR: 2.493e-05  Data: 0.023 (0.026)
Train: 272 [1150/1251 ( 92%)]  Loss: 2.976 (2.82)  Time: 0.305s, 3358.98/s  (0.305s, 3358.45/s)  LR: 2.487e-05  Data: 0.021 (0.025)
Train: 272 [1200/1251 ( 96%)]  Loss: 2.688 (2.81)  Time: 0.308s, 3326.34/s  (0.305s, 3358.18/s)  LR: 2.481e-05  Data: 0.023 (0.025)
Train: 272 [1250/1251 (100%)]  Loss: 2.840 (2.81)  Time: 0.275s, 3720.06/s  (0.305s, 3359.92/s)  LR: 2.475e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.075 (2.075)  Loss:  0.4731 (0.4731)  Acc@1: 92.0898 (92.0898)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.055 (0.238)  Loss:  0.6250 (0.9120)  Acc@1: 84.9057 (79.7520)  Acc@5: 97.6415 (94.7580)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-272.pth.tar', 79.75200016601562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-271.pth.tar', 79.6340000830078)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-268.pth.tar', 79.63000000976562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-263.pth.tar', 79.59799995117187)

Train: 273 [   0/1251 (  0%)]  Loss: 2.310 (2.31)  Time: 2.387s,  429.03/s  (2.387s,  429.03/s)  LR: 2.475e-05  Data: 2.161 (2.161)
Train: 273 [  50/1251 (  4%)]  Loss: 2.742 (2.53)  Time: 0.293s, 3495.31/s  (0.327s, 3136.08/s)  LR: 2.469e-05  Data: 0.026 (0.066)
Train: 273 [ 100/1251 (  8%)]  Loss: 2.832 (2.63)  Time: 0.298s, 3434.34/s  (0.313s, 3274.98/s)  LR: 2.464e-05  Data: 0.024 (0.045)
Train: 273 [ 150/1251 ( 12%)]  Loss: 2.743 (2.66)  Time: 0.299s, 3428.25/s  (0.309s, 3317.65/s)  LR: 2.458e-05  Data: 0.022 (0.038)
Train: 273 [ 200/1251 ( 16%)]  Loss: 2.976 (2.72)  Time: 0.296s, 3462.39/s  (0.307s, 3339.41/s)  LR: 2.452e-05  Data: 0.020 (0.034)
Train: 273 [ 250/1251 ( 20%)]  Loss: 2.852 (2.74)  Time: 0.299s, 3428.59/s  (0.306s, 3350.27/s)  LR: 2.446e-05  Data: 0.021 (0.032)
Train: 273 [ 300/1251 ( 24%)]  Loss: 3.078 (2.79)  Time: 0.306s, 3350.41/s  (0.305s, 3356.62/s)  LR: 2.441e-05  Data: 0.025 (0.031)
Train: 273 [ 350/1251 ( 28%)]  Loss: 2.850 (2.80)  Time: 0.300s, 3417.80/s  (0.305s, 3360.58/s)  LR: 2.435e-05  Data: 0.025 (0.030)
Train: 273 [ 400/1251 ( 32%)]  Loss: 2.688 (2.79)  Time: 0.300s, 3414.91/s  (0.304s, 3363.38/s)  LR: 2.429e-05  Data: 0.023 (0.029)
Train: 273 [ 450/1251 ( 36%)]  Loss: 2.717 (2.78)  Time: 0.306s, 3344.82/s  (0.304s, 3364.63/s)  LR: 2.423e-05  Data: 0.022 (0.028)
Train: 273 [ 500/1251 ( 40%)]  Loss: 2.990 (2.80)  Time: 0.298s, 3432.41/s  (0.304s, 3365.71/s)  LR: 2.418e-05  Data: 0.022 (0.028)
Train: 273 [ 550/1251 ( 44%)]  Loss: 2.830 (2.80)  Time: 0.303s, 3377.06/s  (0.304s, 3366.95/s)  LR: 2.412e-05  Data: 0.023 (0.027)
Train: 273 [ 600/1251 ( 48%)]  Loss: 2.832 (2.80)  Time: 0.300s, 3416.23/s  (0.304s, 3368.24/s)  LR: 2.406e-05  Data: 0.022 (0.027)
Train: 273 [ 650/1251 ( 52%)]  Loss: 2.830 (2.80)  Time: 0.301s, 3402.78/s  (0.304s, 3369.10/s)  LR: 2.400e-05  Data: 0.023 (0.027)
Train: 273 [ 700/1251 ( 56%)]  Loss: 2.827 (2.81)  Time: 0.305s, 3359.01/s  (0.304s, 3370.22/s)  LR: 2.395e-05  Data: 0.024 (0.027)
Train: 273 [ 750/1251 ( 60%)]  Loss: 2.757 (2.80)  Time: 0.299s, 3420.44/s  (0.304s, 3371.04/s)  LR: 2.389e-05  Data: 0.024 (0.026)
Train: 273 [ 800/1251 ( 64%)]  Loss: 2.814 (2.80)  Time: 0.298s, 3436.48/s  (0.304s, 3371.87/s)  LR: 2.383e-05  Data: 0.022 (0.026)
Train: 273 [ 850/1251 ( 68%)]  Loss: 2.965 (2.81)  Time: 0.302s, 3391.36/s  (0.304s, 3371.90/s)  LR: 2.378e-05  Data: 0.022 (0.026)
Train: 273 [ 900/1251 ( 72%)]  Loss: 2.913 (2.82)  Time: 0.311s, 3297.69/s  (0.304s, 3372.03/s)  LR: 2.372e-05  Data: 0.026 (0.026)
Train: 273 [ 950/1251 ( 76%)]  Loss: 2.989 (2.83)  Time: 0.298s, 3436.89/s  (0.304s, 3372.47/s)  LR: 2.366e-05  Data: 0.017 (0.026)
Train: 273 [1000/1251 ( 80%)]  Loss: 3.003 (2.84)  Time: 0.304s, 3370.20/s  (0.304s, 3372.54/s)  LR: 2.361e-05  Data: 0.022 (0.026)
Train: 273 [1050/1251 ( 84%)]  Loss: 3.158 (2.85)  Time: 0.298s, 3435.94/s  (0.304s, 3372.75/s)  LR: 2.355e-05  Data: 0.024 (0.026)
Train: 273 [1100/1251 ( 88%)]  Loss: 2.793 (2.85)  Time: 0.303s, 3374.25/s  (0.304s, 3372.98/s)  LR: 2.349e-05  Data: 0.027 (0.025)
Train: 273 [1150/1251 ( 92%)]  Loss: 2.709 (2.84)  Time: 0.301s, 3400.77/s  (0.304s, 3373.41/s)  LR: 2.344e-05  Data: 0.023 (0.025)
Train: 273 [1200/1251 ( 96%)]  Loss: 2.755 (2.84)  Time: 0.297s, 3444.05/s  (0.304s, 3373.84/s)  LR: 2.338e-05  Data: 0.023 (0.025)
Train: 273 [1250/1251 (100%)]  Loss: 2.651 (2.83)  Time: 0.274s, 3733.42/s  (0.303s, 3375.52/s)  LR: 2.333e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.054 (2.054)  Loss:  0.4814 (0.4814)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.054 (0.237)  Loss:  0.6284 (0.9204)  Acc@1: 86.2028 (79.7000)  Acc@5: 97.8774 (94.6920)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-272.pth.tar', 79.75200016601562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-273.pth.tar', 79.69999995361329)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-271.pth.tar', 79.6340000830078)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-268.pth.tar', 79.63000000976562)

Train: 274 [   0/1251 (  0%)]  Loss: 2.386 (2.39)  Time: 2.156s,  474.86/s  (2.156s,  474.86/s)  LR: 2.333e-05  Data: 1.942 (1.942)
Train: 274 [  50/1251 (  4%)]  Loss: 2.972 (2.68)  Time: 0.300s, 3408.48/s  (0.323s, 3170.94/s)  LR: 2.327e-05  Data: 0.024 (0.062)
Train: 274 [ 100/1251 (  8%)]  Loss: 2.742 (2.70)  Time: 0.297s, 3450.11/s  (0.309s, 3310.72/s)  LR: 2.321e-05  Data: 0.025 (0.043)
Train: 274 [ 150/1251 ( 12%)]  Loss: 2.884 (2.75)  Time: 0.301s, 3401.19/s  (0.306s, 3350.59/s)  LR: 2.316e-05  Data: 0.022 (0.036)
Train: 274 [ 200/1251 ( 16%)]  Loss: 2.749 (2.75)  Time: 0.299s, 3424.93/s  (0.304s, 3365.39/s)  LR: 2.310e-05  Data: 0.027 (0.033)
Train: 274 [ 250/1251 ( 20%)]  Loss: 2.895 (2.77)  Time: 0.303s, 3379.55/s  (0.304s, 3372.60/s)  LR: 2.305e-05  Data: 0.023 (0.031)
Train: 274 [ 300/1251 ( 24%)]  Loss: 2.637 (2.75)  Time: 0.300s, 3416.81/s  (0.303s, 3377.73/s)  LR: 2.299e-05  Data: 0.026 (0.030)
Train: 274 [ 350/1251 ( 28%)]  Loss: 3.118 (2.80)  Time: 0.300s, 3418.71/s  (0.303s, 3381.92/s)  LR: 2.294e-05  Data: 0.022 (0.029)
Train: 274 [ 400/1251 ( 32%)]  Loss: 2.723 (2.79)  Time: 0.300s, 3413.22/s  (0.303s, 3383.35/s)  LR: 2.288e-05  Data: 0.024 (0.028)
Train: 274 [ 450/1251 ( 36%)]  Loss: 2.842 (2.79)  Time: 0.302s, 3386.92/s  (0.302s, 3385.63/s)  LR: 2.282e-05  Data: 0.023 (0.028)
Train: 274 [ 500/1251 ( 40%)]  Loss: 2.987 (2.81)  Time: 0.306s, 3344.60/s  (0.302s, 3386.38/s)  LR: 2.277e-05  Data: 0.023 (0.027)
Train: 274 [ 550/1251 ( 44%)]  Loss: 2.822 (2.81)  Time: 0.302s, 3387.99/s  (0.302s, 3388.19/s)  LR: 2.271e-05  Data: 0.026 (0.027)
Train: 274 [ 600/1251 ( 48%)]  Loss: 3.036 (2.83)  Time: 0.304s, 3372.78/s  (0.302s, 3389.55/s)  LR: 2.266e-05  Data: 0.024 (0.027)
Train: 274 [ 650/1251 ( 52%)]  Loss: 2.796 (2.83)  Time: 0.306s, 3348.99/s  (0.302s, 3390.65/s)  LR: 2.260e-05  Data: 0.023 (0.027)
Train: 274 [ 700/1251 ( 56%)]  Loss: 3.025 (2.84)  Time: 0.302s, 3385.17/s  (0.302s, 3391.26/s)  LR: 2.255e-05  Data: 0.026 (0.026)
Train: 274 [ 750/1251 ( 60%)]  Loss: 2.590 (2.83)  Time: 0.302s, 3391.14/s  (0.302s, 3392.10/s)  LR: 2.250e-05  Data: 0.021 (0.026)
Train: 274 [ 800/1251 ( 64%)]  Loss: 2.963 (2.83)  Time: 0.297s, 3448.61/s  (0.302s, 3392.93/s)  LR: 2.244e-05  Data: 0.023 (0.026)
Train: 274 [ 850/1251 ( 68%)]  Loss: 2.847 (2.83)  Time: 0.305s, 3355.36/s  (0.302s, 3393.23/s)  LR: 2.239e-05  Data: 0.022 (0.026)
Train: 274 [ 900/1251 ( 72%)]  Loss: 2.708 (2.83)  Time: 0.304s, 3371.74/s  (0.302s, 3392.75/s)  LR: 2.233e-05  Data: 0.022 (0.026)
Train: 274 [ 950/1251 ( 76%)]  Loss: 2.861 (2.83)  Time: 0.301s, 3399.02/s  (0.302s, 3392.96/s)  LR: 2.228e-05  Data: 0.026 (0.026)
Train: 274 [1000/1251 ( 80%)]  Loss: 2.540 (2.82)  Time: 0.299s, 3423.08/s  (0.302s, 3393.10/s)  LR: 2.222e-05  Data: 0.022 (0.026)
Train: 274 [1050/1251 ( 84%)]  Loss: 2.804 (2.81)  Time: 0.302s, 3392.14/s  (0.302s, 3393.42/s)  LR: 2.217e-05  Data: 0.022 (0.025)
Train: 274 [1100/1251 ( 88%)]  Loss: 2.762 (2.81)  Time: 0.297s, 3447.31/s  (0.302s, 3393.48/s)  LR: 2.211e-05  Data: 0.024 (0.025)
Train: 274 [1150/1251 ( 92%)]  Loss: 2.864 (2.81)  Time: 0.300s, 3417.38/s  (0.302s, 3393.45/s)  LR: 2.206e-05  Data: 0.026 (0.025)
Train: 274 [1200/1251 ( 96%)]  Loss: 2.866 (2.82)  Time: 0.301s, 3401.76/s  (0.302s, 3393.58/s)  LR: 2.201e-05  Data: 0.025 (0.025)
Train: 274 [1250/1251 (100%)]  Loss: 2.882 (2.82)  Time: 0.276s, 3715.35/s  (0.302s, 3395.63/s)  LR: 2.195e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.084 (2.084)  Loss:  0.4683 (0.4683)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.046 (0.236)  Loss:  0.6484 (0.9201)  Acc@1: 85.1415 (79.7040)  Acc@5: 97.4057 (94.7360)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-272.pth.tar', 79.75200016601562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-274.pth.tar', 79.70400000976562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-273.pth.tar', 79.69999995361329)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-271.pth.tar', 79.6340000830078)

Train: 275 [   0/1251 (  0%)]  Loss: 3.103 (3.10)  Time: 1.990s,  514.57/s  (1.990s,  514.57/s)  LR: 2.195e-05  Data: 1.775 (1.775)
Train: 275 [  50/1251 (  4%)]  Loss: 2.889 (3.00)  Time: 0.290s, 3531.90/s  (0.318s, 3220.48/s)  LR: 2.190e-05  Data: 0.024 (0.060)
Train: 275 [ 100/1251 (  8%)]  Loss: 2.913 (2.97)  Time: 0.301s, 3406.79/s  (0.306s, 3345.46/s)  LR: 2.184e-05  Data: 0.024 (0.042)
Train: 275 [ 150/1251 ( 12%)]  Loss: 2.619 (2.88)  Time: 0.307s, 3332.08/s  (0.303s, 3375.73/s)  LR: 2.179e-05  Data: 0.025 (0.036)
Train: 275 [ 200/1251 ( 16%)]  Loss: 3.023 (2.91)  Time: 0.295s, 3470.96/s  (0.302s, 3387.23/s)  LR: 2.174e-05  Data: 0.023 (0.033)
Train: 275 [ 250/1251 ( 20%)]  Loss: 2.955 (2.92)  Time: 0.296s, 3460.09/s  (0.302s, 3395.64/s)  LR: 2.168e-05  Data: 0.022 (0.031)
Train: 275 [ 300/1251 ( 24%)]  Loss: 2.403 (2.84)  Time: 0.300s, 3409.81/s  (0.301s, 3400.92/s)  LR: 2.163e-05  Data: 0.022 (0.030)
Train: 275 [ 350/1251 ( 28%)]  Loss: 2.933 (2.85)  Time: 0.306s, 3344.12/s  (0.301s, 3404.12/s)  LR: 2.158e-05  Data: 0.024 (0.029)
Train: 275 [ 400/1251 ( 32%)]  Loss: 2.935 (2.86)  Time: 0.301s, 3404.61/s  (0.301s, 3406.14/s)  LR: 2.152e-05  Data: 0.026 (0.028)
Train: 275 [ 450/1251 ( 36%)]  Loss: 2.666 (2.84)  Time: 0.302s, 3395.63/s  (0.301s, 3405.07/s)  LR: 2.147e-05  Data: 0.020 (0.028)
Train: 275 [ 500/1251 ( 40%)]  Loss: 2.821 (2.84)  Time: 0.299s, 3425.83/s  (0.301s, 3405.59/s)  LR: 2.142e-05  Data: 0.024 (0.027)
Train: 275 [ 550/1251 ( 44%)]  Loss: 3.001 (2.85)  Time: 0.304s, 3363.95/s  (0.301s, 3405.25/s)  LR: 2.136e-05  Data: 0.021 (0.027)
Train: 275 [ 600/1251 ( 48%)]  Loss: 2.992 (2.87)  Time: 0.302s, 3395.03/s  (0.301s, 3405.60/s)  LR: 2.131e-05  Data: 0.022 (0.027)
Train: 275 [ 650/1251 ( 52%)]  Loss: 2.943 (2.87)  Time: 0.302s, 3388.73/s  (0.301s, 3406.51/s)  LR: 2.126e-05  Data: 0.026 (0.027)
Train: 275 [ 700/1251 ( 56%)]  Loss: 2.703 (2.86)  Time: 0.300s, 3412.84/s  (0.301s, 3407.23/s)  LR: 2.120e-05  Data: 0.025 (0.026)
Train: 275 [ 750/1251 ( 60%)]  Loss: 2.898 (2.86)  Time: 0.295s, 3471.33/s  (0.300s, 3408.03/s)  LR: 2.115e-05  Data: 0.024 (0.026)
Train: 275 [ 800/1251 ( 64%)]  Loss: 2.825 (2.86)  Time: 0.300s, 3412.78/s  (0.300s, 3409.12/s)  LR: 2.110e-05  Data: 0.022 (0.026)
Train: 275 [ 850/1251 ( 68%)]  Loss: 3.110 (2.87)  Time: 0.304s, 3373.13/s  (0.300s, 3410.23/s)  LR: 2.105e-05  Data: 0.024 (0.026)
Train: 275 [ 900/1251 ( 72%)]  Loss: 2.707 (2.87)  Time: 0.297s, 3445.25/s  (0.300s, 3411.28/s)  LR: 2.099e-05  Data: 0.023 (0.026)
Train: 275 [ 950/1251 ( 76%)]  Loss: 2.699 (2.86)  Time: 0.296s, 3464.49/s  (0.300s, 3412.25/s)  LR: 2.094e-05  Data: 0.022 (0.026)
Train: 275 [1000/1251 ( 80%)]  Loss: 2.828 (2.86)  Time: 0.299s, 3425.05/s  (0.300s, 3412.73/s)  LR: 2.089e-05  Data: 0.025 (0.025)
Train: 275 [1050/1251 ( 84%)]  Loss: 2.903 (2.86)  Time: 0.301s, 3404.79/s  (0.300s, 3413.59/s)  LR: 2.084e-05  Data: 0.022 (0.025)
Train: 275 [1100/1251 ( 88%)]  Loss: 2.944 (2.86)  Time: 0.295s, 3470.80/s  (0.300s, 3414.21/s)  LR: 2.079e-05  Data: 0.025 (0.025)
Train: 275 [1150/1251 ( 92%)]  Loss: 3.017 (2.87)  Time: 0.295s, 3469.33/s  (0.300s, 3414.87/s)  LR: 2.073e-05  Data: 0.023 (0.025)
Train: 275 [1200/1251 ( 96%)]  Loss: 3.026 (2.87)  Time: 0.300s, 3418.65/s  (0.300s, 3415.26/s)  LR: 2.068e-05  Data: 0.024 (0.025)
Train: 275 [1250/1251 (100%)]  Loss: 2.902 (2.88)  Time: 0.272s, 3758.24/s  (0.300s, 3417.53/s)  LR: 2.063e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.089 (2.089)  Loss:  0.4746 (0.4746)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.052 (0.242)  Loss:  0.6323 (0.9160)  Acc@1: 85.3774 (79.7720)  Acc@5: 97.7594 (94.8020)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-275.pth.tar', 79.77199998291016)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-272.pth.tar', 79.75200016601562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-274.pth.tar', 79.70400000976562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-273.pth.tar', 79.69999995361329)

Train: 276 [   0/1251 (  0%)]  Loss: 2.851 (2.85)  Time: 2.136s,  479.32/s  (2.136s,  479.32/s)  LR: 2.063e-05  Data: 1.922 (1.922)
Train: 276 [  50/1251 (  4%)]  Loss: 2.777 (2.81)  Time: 0.294s, 3488.69/s  (0.316s, 3238.86/s)  LR: 2.058e-05  Data: 0.026 (0.062)
Train: 276 [ 100/1251 (  8%)]  Loss: 2.837 (2.82)  Time: 0.290s, 3531.82/s  (0.305s, 3361.01/s)  LR: 2.053e-05  Data: 0.022 (0.043)
Train: 276 [ 150/1251 ( 12%)]  Loss: 2.783 (2.81)  Time: 0.296s, 3463.97/s  (0.302s, 3392.55/s)  LR: 2.047e-05  Data: 0.021 (0.036)
Train: 276 [ 200/1251 ( 16%)]  Loss: 2.955 (2.84)  Time: 0.296s, 3462.45/s  (0.301s, 3403.52/s)  LR: 2.042e-05  Data: 0.023 (0.033)
Train: 276 [ 250/1251 ( 20%)]  Loss: 3.094 (2.88)  Time: 0.296s, 3454.31/s  (0.300s, 3409.78/s)  LR: 2.037e-05  Data: 0.019 (0.031)
Train: 276 [ 300/1251 ( 24%)]  Loss: 3.028 (2.90)  Time: 0.298s, 3432.15/s  (0.300s, 3415.21/s)  LR: 2.032e-05  Data: 0.022 (0.030)
Train: 276 [ 350/1251 ( 28%)]  Loss: 2.886 (2.90)  Time: 0.297s, 3445.21/s  (0.299s, 3419.83/s)  LR: 2.027e-05  Data: 0.027 (0.029)
Train: 276 [ 400/1251 ( 32%)]  Loss: 2.754 (2.89)  Time: 0.303s, 3381.32/s  (0.299s, 3421.44/s)  LR: 2.022e-05  Data: 0.024 (0.028)
Train: 276 [ 450/1251 ( 36%)]  Loss: 2.391 (2.84)  Time: 0.292s, 3505.39/s  (0.299s, 3422.53/s)  LR: 2.017e-05  Data: 0.022 (0.028)
Train: 276 [ 500/1251 ( 40%)]  Loss: 2.973 (2.85)  Time: 0.297s, 3449.65/s  (0.299s, 3423.12/s)  LR: 2.012e-05  Data: 0.023 (0.027)
Train: 276 [ 550/1251 ( 44%)]  Loss: 2.635 (2.83)  Time: 0.302s, 3388.56/s  (0.299s, 3422.76/s)  LR: 2.006e-05  Data: 0.032 (0.027)
Train: 276 [ 600/1251 ( 48%)]  Loss: 2.754 (2.82)  Time: 0.295s, 3477.02/s  (0.299s, 3423.91/s)  LR: 2.001e-05  Data: 0.026 (0.027)
Train: 276 [ 650/1251 ( 52%)]  Loss: 2.823 (2.82)  Time: 0.297s, 3449.95/s  (0.299s, 3423.95/s)  LR: 1.996e-05  Data: 0.022 (0.027)
Train: 276 [ 700/1251 ( 56%)]  Loss: 2.756 (2.82)  Time: 0.300s, 3416.16/s  (0.299s, 3423.72/s)  LR: 1.991e-05  Data: 0.023 (0.026)
Train: 276 [ 750/1251 ( 60%)]  Loss: 3.072 (2.84)  Time: 0.300s, 3413.57/s  (0.299s, 3423.60/s)  LR: 1.986e-05  Data: 0.022 (0.026)
Train: 276 [ 800/1251 ( 64%)]  Loss: 2.965 (2.84)  Time: 0.297s, 3448.34/s  (0.299s, 3423.01/s)  LR: 1.981e-05  Data: 0.021 (0.026)
Train: 276 [ 850/1251 ( 68%)]  Loss: 2.800 (2.84)  Time: 0.304s, 3371.92/s  (0.299s, 3423.15/s)  LR: 1.976e-05  Data: 0.024 (0.026)
Train: 276 [ 900/1251 ( 72%)]  Loss: 2.776 (2.84)  Time: 0.295s, 3472.75/s  (0.299s, 3423.78/s)  LR: 1.971e-05  Data: 0.020 (0.026)
Train: 276 [ 950/1251 ( 76%)]  Loss: 3.165 (2.85)  Time: 0.298s, 3433.63/s  (0.299s, 3423.83/s)  LR: 1.966e-05  Data: 0.023 (0.026)
Train: 276 [1000/1251 ( 80%)]  Loss: 2.845 (2.85)  Time: 0.301s, 3396.97/s  (0.299s, 3423.62/s)  LR: 1.961e-05  Data: 0.018 (0.025)
Train: 276 [1050/1251 ( 84%)]  Loss: 2.685 (2.85)  Time: 0.304s, 3371.50/s  (0.299s, 3423.09/s)  LR: 1.956e-05  Data: 0.023 (0.025)
Train: 276 [1100/1251 ( 88%)]  Loss: 2.896 (2.85)  Time: 0.294s, 3484.31/s  (0.299s, 3423.46/s)  LR: 1.951e-05  Data: 0.021 (0.025)
Train: 276 [1150/1251 ( 92%)]  Loss: 2.847 (2.85)  Time: 0.297s, 3445.83/s  (0.299s, 3423.35/s)  LR: 1.946e-05  Data: 0.021 (0.025)
Train: 276 [1200/1251 ( 96%)]  Loss: 3.087 (2.86)  Time: 0.302s, 3395.99/s  (0.299s, 3422.96/s)  LR: 1.941e-05  Data: 0.025 (0.025)
Train: 276 [1250/1251 (100%)]  Loss: 2.898 (2.86)  Time: 0.275s, 3722.88/s  (0.299s, 3425.09/s)  LR: 1.936e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.064 (2.064)  Loss:  0.4695 (0.4695)  Acc@1: 92.2852 (92.2852)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.046 (0.235)  Loss:  0.6230 (0.9147)  Acc@1: 86.0849 (79.7520)  Acc@5: 97.0519 (94.7040)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-275.pth.tar', 79.77199998291016)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-272.pth.tar', 79.75200016601562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-276.pth.tar', 79.75200003173828)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-274.pth.tar', 79.70400000976562)

Train: 277 [   0/1251 (  0%)]  Loss: 3.067 (3.07)  Time: 2.315s,  442.29/s  (2.315s,  442.29/s)  LR: 1.936e-05  Data: 2.092 (2.092)
Train: 277 [  50/1251 (  4%)]  Loss: 2.783 (2.92)  Time: 0.288s, 3552.10/s  (0.321s, 3187.26/s)  LR: 1.931e-05  Data: 0.024 (0.066)
Train: 277 [ 100/1251 (  8%)]  Loss: 2.947 (2.93)  Time: 0.299s, 3422.08/s  (0.307s, 3332.89/s)  LR: 1.926e-05  Data: 0.026 (0.045)
Train: 277 [ 150/1251 ( 12%)]  Loss: 2.859 (2.91)  Time: 0.294s, 3486.20/s  (0.303s, 3376.33/s)  LR: 1.921e-05  Data: 0.021 (0.038)
Train: 277 [ 200/1251 ( 16%)]  Loss: 2.919 (2.91)  Time: 0.301s, 3402.09/s  (0.301s, 3397.16/s)  LR: 1.916e-05  Data: 0.027 (0.034)
Train: 277 [ 250/1251 ( 20%)]  Loss: 3.020 (2.93)  Time: 0.304s, 3372.54/s  (0.301s, 3406.16/s)  LR: 1.911e-05  Data: 0.023 (0.032)
Train: 277 [ 300/1251 ( 24%)]  Loss: 2.594 (2.88)  Time: 0.301s, 3401.21/s  (0.300s, 3413.64/s)  LR: 1.906e-05  Data: 0.021 (0.031)
Train: 277 [ 350/1251 ( 28%)]  Loss: 2.647 (2.85)  Time: 0.304s, 3373.18/s  (0.299s, 3419.26/s)  LR: 1.901e-05  Data: 0.021 (0.030)
Train: 277 [ 400/1251 ( 32%)]  Loss: 2.870 (2.86)  Time: 0.306s, 3347.45/s  (0.299s, 3422.34/s)  LR: 1.897e-05  Data: 0.026 (0.029)
Train: 277 [ 450/1251 ( 36%)]  Loss: 2.614 (2.83)  Time: 0.298s, 3437.55/s  (0.299s, 3425.63/s)  LR: 1.892e-05  Data: 0.023 (0.028)
Train: 277 [ 500/1251 ( 40%)]  Loss: 2.861 (2.83)  Time: 0.297s, 3445.61/s  (0.299s, 3428.65/s)  LR: 1.887e-05  Data: 0.026 (0.028)
Train: 277 [ 550/1251 ( 44%)]  Loss: 2.946 (2.84)  Time: 0.298s, 3434.70/s  (0.298s, 3430.67/s)  LR: 1.882e-05  Data: 0.023 (0.027)
Train: 277 [ 600/1251 ( 48%)]  Loss: 2.914 (2.85)  Time: 0.296s, 3455.64/s  (0.298s, 3432.50/s)  LR: 1.877e-05  Data: 0.023 (0.027)
Train: 277 [ 650/1251 ( 52%)]  Loss: 2.844 (2.85)  Time: 0.296s, 3458.80/s  (0.298s, 3432.71/s)  LR: 1.872e-05  Data: 0.026 (0.027)
Train: 277 [ 700/1251 ( 56%)]  Loss: 3.060 (2.86)  Time: 0.298s, 3432.96/s  (0.298s, 3434.14/s)  LR: 1.867e-05  Data: 0.022 (0.027)
Train: 277 [ 750/1251 ( 60%)]  Loss: 2.664 (2.85)  Time: 0.298s, 3433.24/s  (0.298s, 3434.55/s)  LR: 1.862e-05  Data: 0.027 (0.026)
Train: 277 [ 800/1251 ( 64%)]  Loss: 2.759 (2.85)  Time: 0.296s, 3455.72/s  (0.298s, 3435.15/s)  LR: 1.858e-05  Data: 0.021 (0.026)
Train: 277 [ 850/1251 ( 68%)]  Loss: 2.734 (2.84)  Time: 0.299s, 3425.80/s  (0.298s, 3435.06/s)  LR: 1.853e-05  Data: 0.022 (0.026)
Train: 277 [ 900/1251 ( 72%)]  Loss: 3.074 (2.85)  Time: 0.299s, 3428.01/s  (0.298s, 3435.81/s)  LR: 1.848e-05  Data: 0.021 (0.026)
Train: 277 [ 950/1251 ( 76%)]  Loss: 2.310 (2.82)  Time: 0.295s, 3467.27/s  (0.298s, 3436.23/s)  LR: 1.843e-05  Data: 0.024 (0.026)
Train: 277 [1000/1251 ( 80%)]  Loss: 2.757 (2.82)  Time: 0.300s, 3411.80/s  (0.298s, 3436.51/s)  LR: 1.838e-05  Data: 0.025 (0.026)
Train: 277 [1050/1251 ( 84%)]  Loss: 2.894 (2.82)  Time: 0.298s, 3432.59/s  (0.298s, 3436.60/s)  LR: 1.834e-05  Data: 0.023 (0.026)
Train: 277 [1100/1251 ( 88%)]  Loss: 2.662 (2.82)  Time: 0.300s, 3409.03/s  (0.298s, 3436.95/s)  LR: 1.829e-05  Data: 0.023 (0.025)
Train: 277 [1150/1251 ( 92%)]  Loss: 3.006 (2.83)  Time: 0.304s, 3371.26/s  (0.298s, 3437.07/s)  LR: 1.824e-05  Data: 0.027 (0.025)
Train: 277 [1200/1251 ( 96%)]  Loss: 2.541 (2.81)  Time: 0.300s, 3413.93/s  (0.298s, 3437.40/s)  LR: 1.819e-05  Data: 0.021 (0.025)
Train: 277 [1250/1251 (100%)]  Loss: 3.079 (2.82)  Time: 0.274s, 3743.10/s  (0.298s, 3439.88/s)  LR: 1.814e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.089 (2.089)  Loss:  0.4668 (0.4668)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.045 (0.234)  Loss:  0.6309 (0.9190)  Acc@1: 85.3774 (79.7080)  Acc@5: 97.4057 (94.6960)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-275.pth.tar', 79.77199998291016)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-272.pth.tar', 79.75200016601562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-276.pth.tar', 79.75200003173828)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-277.pth.tar', 79.70799998291015)

Train: 278 [   0/1251 (  0%)]  Loss: 2.865 (2.86)  Time: 2.283s,  448.49/s  (2.283s,  448.49/s)  LR: 1.814e-05  Data: 2.059 (2.059)
Train: 278 [  50/1251 (  4%)]  Loss: 3.096 (2.98)  Time: 0.289s, 3546.54/s  (0.322s, 3175.86/s)  LR: 1.810e-05  Data: 0.022 (0.065)
Train: 278 [ 100/1251 (  8%)]  Loss: 2.718 (2.89)  Time: 0.295s, 3471.70/s  (0.307s, 3336.01/s)  LR: 1.805e-05  Data: 0.021 (0.044)
Train: 278 [ 150/1251 ( 12%)]  Loss: 2.870 (2.89)  Time: 0.294s, 3480.23/s  (0.303s, 3382.29/s)  LR: 1.800e-05  Data: 0.022 (0.037)
Train: 278 [ 200/1251 ( 16%)]  Loss: 2.883 (2.89)  Time: 0.294s, 3485.26/s  (0.301s, 3405.48/s)  LR: 1.795e-05  Data: 0.026 (0.034)
Train: 278 [ 250/1251 ( 20%)]  Loss: 2.763 (2.87)  Time: 0.293s, 3498.82/s  (0.300s, 3418.80/s)  LR: 1.791e-05  Data: 0.026 (0.032)
Train: 278 [ 300/1251 ( 24%)]  Loss: 2.940 (2.88)  Time: 0.303s, 3379.99/s  (0.299s, 3427.52/s)  LR: 1.786e-05  Data: 0.026 (0.030)
Train: 278 [ 350/1251 ( 28%)]  Loss: 2.747 (2.86)  Time: 0.300s, 3412.87/s  (0.298s, 3430.84/s)  LR: 1.781e-05  Data: 0.021 (0.029)
Train: 278 [ 400/1251 ( 32%)]  Loss: 3.078 (2.88)  Time: 0.294s, 3483.48/s  (0.298s, 3434.97/s)  LR: 1.777e-05  Data: 0.023 (0.029)
Train: 278 [ 450/1251 ( 36%)]  Loss: 3.099 (2.91)  Time: 0.297s, 3449.48/s  (0.298s, 3437.25/s)  LR: 1.772e-05  Data: 0.021 (0.028)
Train: 278 [ 500/1251 ( 40%)]  Loss: 2.950 (2.91)  Time: 0.299s, 3426.01/s  (0.298s, 3439.51/s)  LR: 1.767e-05  Data: 0.023 (0.028)
Train: 278 [ 550/1251 ( 44%)]  Loss: 2.908 (2.91)  Time: 0.299s, 3427.58/s  (0.298s, 3440.58/s)  LR: 1.763e-05  Data: 0.027 (0.027)
Train: 278 [ 600/1251 ( 48%)]  Loss: 2.771 (2.90)  Time: 0.299s, 3428.06/s  (0.298s, 3441.45/s)  LR: 1.758e-05  Data: 0.024 (0.027)
Train: 278 [ 650/1251 ( 52%)]  Loss: 2.782 (2.89)  Time: 0.300s, 3408.40/s  (0.297s, 3442.50/s)  LR: 1.753e-05  Data: 0.022 (0.027)
Train: 278 [ 700/1251 ( 56%)]  Loss: 3.000 (2.90)  Time: 0.294s, 3478.79/s  (0.297s, 3443.50/s)  LR: 1.749e-05  Data: 0.022 (0.027)
Train: 278 [ 750/1251 ( 60%)]  Loss: 2.836 (2.89)  Time: 0.297s, 3445.87/s  (0.297s, 3444.37/s)  LR: 1.744e-05  Data: 0.023 (0.026)
Train: 278 [ 800/1251 ( 64%)]  Loss: 2.930 (2.90)  Time: 0.295s, 3468.10/s  (0.297s, 3444.93/s)  LR: 1.739e-05  Data: 0.022 (0.026)
Train: 278 [ 850/1251 ( 68%)]  Loss: 3.003 (2.90)  Time: 0.294s, 3477.78/s  (0.297s, 3445.35/s)  LR: 1.735e-05  Data: 0.023 (0.026)
Train: 278 [ 900/1251 ( 72%)]  Loss: 3.120 (2.91)  Time: 0.298s, 3441.28/s  (0.297s, 3445.46/s)  LR: 1.730e-05  Data: 0.023 (0.026)
Train: 278 [ 950/1251 ( 76%)]  Loss: 2.814 (2.91)  Time: 0.300s, 3408.12/s  (0.297s, 3445.90/s)  LR: 1.726e-05  Data: 0.021 (0.026)
Train: 278 [1000/1251 ( 80%)]  Loss: 2.917 (2.91)  Time: 0.295s, 3472.34/s  (0.297s, 3445.89/s)  LR: 1.721e-05  Data: 0.026 (0.026)
Train: 278 [1050/1251 ( 84%)]  Loss: 3.038 (2.91)  Time: 0.297s, 3442.49/s  (0.297s, 3445.76/s)  LR: 1.716e-05  Data: 0.026 (0.026)
Train: 278 [1100/1251 ( 88%)]  Loss: 3.091 (2.92)  Time: 0.298s, 3439.72/s  (0.297s, 3444.88/s)  LR: 1.712e-05  Data: 0.024 (0.025)
Train: 278 [1150/1251 ( 92%)]  Loss: 2.998 (2.93)  Time: 0.307s, 3337.38/s  (0.297s, 3443.75/s)  LR: 1.707e-05  Data: 0.019 (0.025)
Train: 278 [1200/1251 ( 96%)]  Loss: 2.471 (2.91)  Time: 0.303s, 3380.16/s  (0.297s, 3443.31/s)  LR: 1.703e-05  Data: 0.026 (0.025)
Train: 278 [1250/1251 (100%)]  Loss: 3.089 (2.91)  Time: 0.275s, 3729.73/s  (0.297s, 3444.92/s)  LR: 1.698e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.126 (2.126)  Loss:  0.4656 (0.4656)  Acc@1: 91.8945 (91.8945)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.041 (0.234)  Loss:  0.6221 (0.9208)  Acc@1: 86.3208 (79.7580)  Acc@5: 97.8774 (94.7580)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-275.pth.tar', 79.77199998291016)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-278.pth.tar', 79.75800000488282)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-272.pth.tar', 79.75200016601562)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-276.pth.tar', 79.75200003173828)

Train: 279 [   0/1251 (  0%)]  Loss: 2.711 (2.71)  Time: 2.209s,  463.62/s  (2.209s,  463.62/s)  LR: 1.698e-05  Data: 1.983 (1.983)
Train: 279 [  50/1251 (  4%)]  Loss: 2.790 (2.75)  Time: 0.286s, 3581.40/s  (0.322s, 3183.03/s)  LR: 1.694e-05  Data: 0.022 (0.063)
Train: 279 [ 100/1251 (  8%)]  Loss: 2.595 (2.70)  Time: 0.295s, 3466.29/s  (0.307s, 3333.75/s)  LR: 1.689e-05  Data: 0.023 (0.044)
Train: 279 [ 150/1251 ( 12%)]  Loss: 3.108 (2.80)  Time: 0.287s, 3563.84/s  (0.302s, 3388.42/s)  LR: 1.684e-05  Data: 0.022 (0.037)
Train: 279 [ 200/1251 ( 16%)]  Loss: 2.746 (2.79)  Time: 0.284s, 3600.01/s  (0.300s, 3415.43/s)  LR: 1.680e-05  Data: 0.024 (0.034)
Train: 279 [ 250/1251 ( 20%)]  Loss: 2.586 (2.76)  Time: 0.296s, 3461.16/s  (0.299s, 3429.79/s)  LR: 1.675e-05  Data: 0.024 (0.032)
Train: 279 [ 300/1251 ( 24%)]  Loss: 2.955 (2.78)  Time: 0.297s, 3444.25/s  (0.298s, 3437.87/s)  LR: 1.671e-05  Data: 0.025 (0.030)
Train: 279 [ 350/1251 ( 28%)]  Loss: 2.631 (2.77)  Time: 0.293s, 3498.67/s  (0.297s, 3443.25/s)  LR: 1.666e-05  Data: 0.022 (0.029)
Train: 279 [ 400/1251 ( 32%)]  Loss: 2.919 (2.78)  Time: 0.291s, 3517.81/s  (0.297s, 3446.69/s)  LR: 1.662e-05  Data: 0.022 (0.029)
Train: 279 [ 450/1251 ( 36%)]  Loss: 3.066 (2.81)  Time: 0.297s, 3442.41/s  (0.297s, 3449.67/s)  LR: 1.658e-05  Data: 0.028 (0.028)
Train: 279 [ 500/1251 ( 40%)]  Loss: 2.771 (2.81)  Time: 0.290s, 3530.93/s  (0.297s, 3452.42/s)  LR: 1.653e-05  Data: 0.023 (0.028)
Train: 279 [ 550/1251 ( 44%)]  Loss: 2.937 (2.82)  Time: 0.298s, 3431.02/s  (0.296s, 3454.40/s)  LR: 1.649e-05  Data: 0.025 (0.027)
Train: 279 [ 600/1251 ( 48%)]  Loss: 2.634 (2.80)  Time: 0.292s, 3506.07/s  (0.296s, 3455.21/s)  LR: 1.644e-05  Data: 0.024 (0.027)
Train: 279 [ 650/1251 ( 52%)]  Loss: 3.018 (2.82)  Time: 0.295s, 3467.83/s  (0.296s, 3456.44/s)  LR: 1.640e-05  Data: 0.022 (0.027)
Train: 279 [ 700/1251 ( 56%)]  Loss: 2.743 (2.81)  Time: 0.300s, 3414.94/s  (0.296s, 3457.39/s)  LR: 1.635e-05  Data: 0.023 (0.027)
Train: 279 [ 750/1251 ( 60%)]  Loss: 2.809 (2.81)  Time: 0.294s, 3485.21/s  (0.296s, 3457.75/s)  LR: 1.631e-05  Data: 0.022 (0.026)
Train: 279 [ 800/1251 ( 64%)]  Loss: 2.797 (2.81)  Time: 0.291s, 3520.33/s  (0.296s, 3457.67/s)  LR: 1.626e-05  Data: 0.023 (0.026)
Train: 279 [ 850/1251 ( 68%)]  Loss: 2.872 (2.82)  Time: 0.300s, 3417.70/s  (0.296s, 3457.65/s)  LR: 1.622e-05  Data: 0.021 (0.026)
Train: 279 [ 900/1251 ( 72%)]  Loss: 3.005 (2.83)  Time: 0.298s, 3441.90/s  (0.296s, 3458.10/s)  LR: 1.618e-05  Data: 0.025 (0.026)
Train: 279 [ 950/1251 ( 76%)]  Loss: 2.772 (2.82)  Time: 0.298s, 3433.96/s  (0.296s, 3458.30/s)  LR: 1.613e-05  Data: 0.022 (0.026)
Train: 279 [1000/1251 ( 80%)]  Loss: 2.697 (2.82)  Time: 0.301s, 3399.34/s  (0.296s, 3458.01/s)  LR: 1.609e-05  Data: 0.025 (0.026)
Train: 279 [1050/1251 ( 84%)]  Loss: 3.014 (2.83)  Time: 0.294s, 3478.05/s  (0.296s, 3457.49/s)  LR: 1.605e-05  Data: 0.022 (0.026)
Train: 279 [1100/1251 ( 88%)]  Loss: 2.506 (2.81)  Time: 0.296s, 3459.30/s  (0.296s, 3456.82/s)  LR: 1.600e-05  Data: 0.022 (0.026)
Train: 279 [1150/1251 ( 92%)]  Loss: 2.791 (2.81)  Time: 0.296s, 3453.86/s  (0.296s, 3457.03/s)  LR: 1.596e-05  Data: 0.024 (0.025)
Train: 279 [1200/1251 ( 96%)]  Loss: 2.812 (2.81)  Time: 0.299s, 3419.18/s  (0.296s, 3456.73/s)  LR: 1.591e-05  Data: 0.026 (0.025)
Train: 279 [1250/1251 (100%)]  Loss: 2.518 (2.80)  Time: 0.275s, 3721.89/s  (0.296s, 3458.45/s)  LR: 1.587e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.042 (2.042)  Loss:  0.4602 (0.4602)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.044 (0.232)  Loss:  0.6133 (0.9122)  Acc@1: 85.8491 (79.7720)  Acc@5: 97.7594 (94.7600)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-279.pth.tar', 79.77200005859375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-275.pth.tar', 79.77199998291016)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-278.pth.tar', 79.75800000488282)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-272.pth.tar', 79.75200016601562)

Train: 280 [   0/1251 (  0%)]  Loss: 2.812 (2.81)  Time: 1.868s,  548.19/s  (1.868s,  548.19/s)  LR: 1.587e-05  Data: 1.629 (1.629)
Train: 280 [  50/1251 (  4%)]  Loss: 2.837 (2.82)  Time: 0.293s, 3495.83/s  (0.315s, 3247.83/s)  LR: 1.583e-05  Data: 0.024 (0.057)
Train: 280 [ 100/1251 (  8%)]  Loss: 2.669 (2.77)  Time: 0.289s, 3544.81/s  (0.302s, 3388.08/s)  LR: 1.578e-05  Data: 0.022 (0.040)
Train: 280 [ 150/1251 ( 12%)]  Loss: 2.588 (2.73)  Time: 0.294s, 3482.49/s  (0.299s, 3421.56/s)  LR: 1.574e-05  Data: 0.020 (0.035)
Train: 280 [ 200/1251 ( 16%)]  Loss: 2.651 (2.71)  Time: 0.298s, 3434.95/s  (0.298s, 3439.71/s)  LR: 1.570e-05  Data: 0.023 (0.032)
Train: 280 [ 250/1251 ( 20%)]  Loss: 2.778 (2.72)  Time: 0.297s, 3446.09/s  (0.297s, 3448.41/s)  LR: 1.566e-05  Data: 0.020 (0.030)
Train: 280 [ 300/1251 ( 24%)]  Loss: 2.882 (2.75)  Time: 0.290s, 3536.23/s  (0.297s, 3453.04/s)  LR: 1.561e-05  Data: 0.021 (0.029)
Train: 280 [ 350/1251 ( 28%)]  Loss: 2.751 (2.75)  Time: 0.292s, 3505.01/s  (0.296s, 3456.09/s)  LR: 1.557e-05  Data: 0.025 (0.028)
Train: 280 [ 400/1251 ( 32%)]  Loss: 3.080 (2.78)  Time: 0.290s, 3530.25/s  (0.296s, 3457.39/s)  LR: 1.553e-05  Data: 0.024 (0.028)
Train: 280 [ 450/1251 ( 36%)]  Loss: 2.867 (2.79)  Time: 0.296s, 3462.33/s  (0.296s, 3458.44/s)  LR: 1.548e-05  Data: 0.022 (0.027)
Train: 280 [ 500/1251 ( 40%)]  Loss: 2.777 (2.79)  Time: 0.301s, 3402.73/s  (0.296s, 3457.31/s)  LR: 1.544e-05  Data: 0.023 (0.027)
Train: 280 [ 550/1251 ( 44%)]  Loss: 2.907 (2.80)  Time: 0.298s, 3436.20/s  (0.296s, 3457.28/s)  LR: 1.540e-05  Data: 0.022 (0.027)
Train: 280 [ 600/1251 ( 48%)]  Loss: 2.466 (2.77)  Time: 0.297s, 3445.44/s  (0.296s, 3457.70/s)  LR: 1.536e-05  Data: 0.021 (0.026)
Train: 280 [ 650/1251 ( 52%)]  Loss: 3.002 (2.79)  Time: 0.302s, 3386.93/s  (0.296s, 3456.67/s)  LR: 1.532e-05  Data: 0.024 (0.026)
Train: 280 [ 700/1251 ( 56%)]  Loss: 2.721 (2.79)  Time: 0.296s, 3463.22/s  (0.296s, 3457.01/s)  LR: 1.527e-05  Data: 0.019 (0.026)
Train: 280 [ 750/1251 ( 60%)]  Loss: 3.008 (2.80)  Time: 0.300s, 3416.70/s  (0.296s, 3456.83/s)  LR: 1.523e-05  Data: 0.025 (0.026)
Train: 280 [ 800/1251 ( 64%)]  Loss: 2.756 (2.80)  Time: 0.293s, 3494.33/s  (0.296s, 3456.67/s)  LR: 1.519e-05  Data: 0.025 (0.026)
Train: 280 [ 850/1251 ( 68%)]  Loss: 3.006 (2.81)  Time: 0.289s, 3542.54/s  (0.296s, 3457.31/s)  LR: 1.515e-05  Data: 0.023 (0.025)
Train: 280 [ 900/1251 ( 72%)]  Loss: 2.760 (2.81)  Time: 0.294s, 3479.79/s  (0.296s, 3456.82/s)  LR: 1.511e-05  Data: 0.025 (0.025)
Train: 280 [ 950/1251 ( 76%)]  Loss: 2.662 (2.80)  Time: 0.300s, 3412.37/s  (0.296s, 3455.64/s)  LR: 1.506e-05  Data: 0.024 (0.025)
Train: 280 [1000/1251 ( 80%)]  Loss: 2.491 (2.78)  Time: 0.295s, 3472.53/s  (0.296s, 3454.77/s)  LR: 1.502e-05  Data: 0.023 (0.025)
Train: 280 [1050/1251 ( 84%)]  Loss: 2.951 (2.79)  Time: 0.299s, 3420.52/s  (0.296s, 3453.83/s)  LR: 1.498e-05  Data: 0.024 (0.025)
Train: 280 [1100/1251 ( 88%)]  Loss: 3.120 (2.81)  Time: 0.295s, 3476.34/s  (0.297s, 3452.98/s)  LR: 1.494e-05  Data: 0.023 (0.025)
Train: 280 [1150/1251 ( 92%)]  Loss: 2.941 (2.81)  Time: 0.298s, 3433.41/s  (0.297s, 3452.23/s)  LR: 1.490e-05  Data: 0.022 (0.025)
Train: 280 [1200/1251 ( 96%)]  Loss: 2.788 (2.81)  Time: 0.299s, 3427.77/s  (0.297s, 3451.88/s)  LR: 1.486e-05  Data: 0.022 (0.025)
Train: 280 [1250/1251 (100%)]  Loss: 2.733 (2.81)  Time: 0.275s, 3720.55/s  (0.297s, 3453.04/s)  LR: 1.482e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.230 (2.230)  Loss:  0.4585 (0.4585)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.052 (0.233)  Loss:  0.6172 (0.9153)  Acc@1: 86.2028 (79.7800)  Acc@5: 97.7594 (94.7580)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-280.pth.tar', 79.7800000830078)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-279.pth.tar', 79.77200005859375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-275.pth.tar', 79.77199998291016)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-278.pth.tar', 79.75800000488282)

Train: 281 [   0/1251 (  0%)]  Loss: 2.790 (2.79)  Time: 2.459s,  416.42/s  (2.459s,  416.42/s)  LR: 1.481e-05  Data: 2.228 (2.228)
Train: 281 [  50/1251 (  4%)]  Loss: 2.942 (2.87)  Time: 0.292s, 3509.20/s  (0.329s, 3110.07/s)  LR: 1.477e-05  Data: 0.023 (0.077)
Train: 281 [ 100/1251 (  8%)]  Loss: 2.857 (2.86)  Time: 0.297s, 3451.74/s  (0.311s, 3292.41/s)  LR: 1.473e-05  Data: 0.022 (0.051)
Train: 281 [ 150/1251 ( 12%)]  Loss: 2.654 (2.81)  Time: 0.298s, 3440.47/s  (0.306s, 3351.40/s)  LR: 1.469e-05  Data: 0.028 (0.042)
Train: 281 [ 200/1251 ( 16%)]  Loss: 2.367 (2.72)  Time: 0.300s, 3415.64/s  (0.303s, 3379.64/s)  LR: 1.465e-05  Data: 0.023 (0.037)
Train: 281 [ 250/1251 ( 20%)]  Loss: 2.482 (2.68)  Time: 0.302s, 3392.81/s  (0.302s, 3395.85/s)  LR: 1.461e-05  Data: 0.023 (0.035)
Train: 281 [ 300/1251 ( 24%)]  Loss: 2.848 (2.71)  Time: 0.301s, 3406.33/s  (0.301s, 3405.43/s)  LR: 1.457e-05  Data: 0.023 (0.033)
Train: 281 [ 350/1251 ( 28%)]  Loss: 2.785 (2.72)  Time: 0.298s, 3432.77/s  (0.300s, 3412.87/s)  LR: 1.453e-05  Data: 0.021 (0.031)
Train: 281 [ 400/1251 ( 32%)]  Loss: 2.722 (2.72)  Time: 0.294s, 3477.80/s  (0.300s, 3417.73/s)  LR: 1.449e-05  Data: 0.025 (0.031)
Train: 281 [ 450/1251 ( 36%)]  Loss: 3.110 (2.76)  Time: 0.300s, 3418.41/s  (0.299s, 3420.49/s)  LR: 1.445e-05  Data: 0.022 (0.030)
Train: 281 [ 500/1251 ( 40%)]  Loss: 2.801 (2.76)  Time: 0.292s, 3509.55/s  (0.299s, 3421.78/s)  LR: 1.441e-05  Data: 0.023 (0.029)
Train: 281 [ 550/1251 ( 44%)]  Loss: 2.666 (2.75)  Time: 0.295s, 3467.37/s  (0.299s, 3423.11/s)  LR: 1.437e-05  Data: 0.021 (0.029)
Train: 281 [ 600/1251 ( 48%)]  Loss: 2.444 (2.73)  Time: 0.302s, 3396.28/s  (0.299s, 3422.69/s)  LR: 1.433e-05  Data: 0.021 (0.028)
Train: 281 [ 650/1251 ( 52%)]  Loss: 2.973 (2.75)  Time: 0.300s, 3408.13/s  (0.299s, 3423.45/s)  LR: 1.429e-05  Data: 0.023 (0.028)
Train: 281 [ 700/1251 ( 56%)]  Loss: 2.728 (2.74)  Time: 0.300s, 3410.63/s  (0.299s, 3423.90/s)  LR: 1.425e-05  Data: 0.023 (0.027)
Train: 281 [ 750/1251 ( 60%)]  Loss: 2.631 (2.74)  Time: 0.302s, 3387.84/s  (0.299s, 3423.83/s)  LR: 1.421e-05  Data: 0.024 (0.027)
Train: 281 [ 800/1251 ( 64%)]  Loss: 2.761 (2.74)  Time: 0.297s, 3443.30/s  (0.299s, 3423.18/s)  LR: 1.417e-05  Data: 0.022 (0.027)
Train: 281 [ 850/1251 ( 68%)]  Loss: 2.685 (2.74)  Time: 0.301s, 3406.94/s  (0.299s, 3422.77/s)  LR: 1.413e-05  Data: 0.023 (0.027)
Train: 281 [ 900/1251 ( 72%)]  Loss: 2.877 (2.74)  Time: 0.303s, 3380.57/s  (0.299s, 3423.13/s)  LR: 1.409e-05  Data: 0.021 (0.027)
Train: 281 [ 950/1251 ( 76%)]  Loss: 2.848 (2.75)  Time: 0.298s, 3436.61/s  (0.299s, 3423.33/s)  LR: 1.405e-05  Data: 0.028 (0.027)
Train: 281 [1000/1251 ( 80%)]  Loss: 2.967 (2.76)  Time: 0.296s, 3458.25/s  (0.299s, 3423.33/s)  LR: 1.401e-05  Data: 0.022 (0.026)
Train: 281 [1050/1251 ( 84%)]  Loss: 2.603 (2.75)  Time: 0.306s, 3349.40/s  (0.299s, 3423.35/s)  LR: 1.397e-05  Data: 0.024 (0.026)
Train: 281 [1100/1251 ( 88%)]  Loss: 2.759 (2.75)  Time: 0.301s, 3403.24/s  (0.299s, 3423.62/s)  LR: 1.393e-05  Data: 0.023 (0.026)
Train: 281 [1150/1251 ( 92%)]  Loss: 2.755 (2.75)  Time: 0.300s, 3410.72/s  (0.299s, 3423.76/s)  LR: 1.389e-05  Data: 0.020 (0.026)
Train: 281 [1200/1251 ( 96%)]  Loss: 2.915 (2.76)  Time: 0.303s, 3381.23/s  (0.299s, 3423.51/s)  LR: 1.385e-05  Data: 0.026 (0.026)
Train: 281 [1250/1251 (100%)]  Loss: 2.780 (2.76)  Time: 0.276s, 3715.57/s  (0.299s, 3424.97/s)  LR: 1.381e-05  Data: 0.000 (0.026)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.036 (2.036)  Loss:  0.4602 (0.4602)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.049 (0.236)  Loss:  0.6260 (0.9139)  Acc@1: 85.6132 (79.8020)  Acc@5: 97.6415 (94.7500)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-281.pth.tar', 79.80200008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-280.pth.tar', 79.7800000830078)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-279.pth.tar', 79.77200005859375)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-275.pth.tar', 79.77199998291016)

Train: 282 [   0/1251 (  0%)]  Loss: 2.955 (2.96)  Time: 2.194s,  466.77/s  (2.194s,  466.77/s)  LR: 1.381e-05  Data: 1.967 (1.967)
Train: 282 [  50/1251 (  4%)]  Loss: 2.746 (2.85)  Time: 0.293s, 3489.41/s  (0.322s, 3183.98/s)  LR: 1.377e-05  Data: 0.023 (0.069)
Train: 282 [ 100/1251 (  8%)]  Loss: 2.781 (2.83)  Time: 0.287s, 3572.52/s  (0.307s, 3331.74/s)  LR: 1.373e-05  Data: 0.020 (0.046)
Train: 282 [ 150/1251 ( 12%)]  Loss: 3.054 (2.88)  Time: 0.299s, 3428.05/s  (0.303s, 3375.50/s)  LR: 1.369e-05  Data: 0.020 (0.038)
Train: 282 [ 200/1251 ( 16%)]  Loss: 3.109 (2.93)  Time: 0.297s, 3442.32/s  (0.302s, 3395.01/s)  LR: 1.366e-05  Data: 0.020 (0.035)
Train: 282 [ 250/1251 ( 20%)]  Loss: 2.637 (2.88)  Time: 0.297s, 3443.53/s  (0.301s, 3402.10/s)  LR: 1.362e-05  Data: 0.022 (0.032)
Train: 282 [ 300/1251 ( 24%)]  Loss: 2.857 (2.88)  Time: 0.300s, 3415.19/s  (0.301s, 3405.85/s)  LR: 1.358e-05  Data: 0.023 (0.031)
Train: 282 [ 350/1251 ( 28%)]  Loss: 2.602 (2.84)  Time: 0.297s, 3444.50/s  (0.300s, 3409.57/s)  LR: 1.354e-05  Data: 0.023 (0.030)
Train: 282 [ 400/1251 ( 32%)]  Loss: 2.885 (2.85)  Time: 0.299s, 3425.91/s  (0.300s, 3412.18/s)  LR: 1.350e-05  Data: 0.025 (0.029)
Train: 282 [ 450/1251 ( 36%)]  Loss: 2.789 (2.84)  Time: 0.292s, 3503.69/s  (0.300s, 3412.62/s)  LR: 1.346e-05  Data: 0.022 (0.029)
Train: 282 [ 500/1251 ( 40%)]  Loss: 2.984 (2.85)  Time: 0.301s, 3403.39/s  (0.300s, 3412.69/s)  LR: 1.343e-05  Data: 0.026 (0.028)
Train: 282 [ 550/1251 ( 44%)]  Loss: 2.533 (2.83)  Time: 0.302s, 3395.35/s  (0.300s, 3413.71/s)  LR: 1.339e-05  Data: 0.022 (0.028)
Train: 282 [ 600/1251 ( 48%)]  Loss: 3.096 (2.85)  Time: 0.294s, 3478.55/s  (0.300s, 3413.41/s)  LR: 1.335e-05  Data: 0.018 (0.027)
Train: 282 [ 650/1251 ( 52%)]  Loss: 2.940 (2.85)  Time: 0.305s, 3355.25/s  (0.300s, 3412.99/s)  LR: 1.331e-05  Data: 0.022 (0.027)
Train: 282 [ 700/1251 ( 56%)]  Loss: 2.847 (2.85)  Time: 0.300s, 3416.67/s  (0.300s, 3412.24/s)  LR: 1.327e-05  Data: 0.022 (0.027)
Train: 282 [ 750/1251 ( 60%)]  Loss: 3.000 (2.86)  Time: 0.302s, 3395.81/s  (0.300s, 3412.08/s)  LR: 1.324e-05  Data: 0.023 (0.026)
Train: 282 [ 800/1251 ( 64%)]  Loss: 2.854 (2.86)  Time: 0.297s, 3443.86/s  (0.300s, 3412.08/s)  LR: 1.320e-05  Data: 0.026 (0.026)
Train: 282 [ 850/1251 ( 68%)]  Loss: 2.812 (2.86)  Time: 0.301s, 3405.43/s  (0.300s, 3412.02/s)  LR: 1.316e-05  Data: 0.028 (0.026)
Train: 282 [ 900/1251 ( 72%)]  Loss: 3.008 (2.87)  Time: 0.301s, 3402.39/s  (0.300s, 3411.91/s)  LR: 1.312e-05  Data: 0.023 (0.026)
Train: 282 [ 950/1251 ( 76%)]  Loss: 2.976 (2.87)  Time: 0.300s, 3410.32/s  (0.300s, 3411.77/s)  LR: 1.309e-05  Data: 0.024 (0.026)
Train: 282 [1000/1251 ( 80%)]  Loss: 2.609 (2.86)  Time: 0.297s, 3443.39/s  (0.300s, 3412.15/s)  LR: 1.305e-05  Data: 0.025 (0.026)
Train: 282 [1050/1251 ( 84%)]  Loss: 2.739 (2.86)  Time: 0.306s, 3341.27/s  (0.300s, 3411.53/s)  LR: 1.301e-05  Data: 0.020 (0.026)
Train: 282 [1100/1251 ( 88%)]  Loss: 2.908 (2.86)  Time: 0.302s, 3387.13/s  (0.300s, 3411.34/s)  LR: 1.297e-05  Data: 0.025 (0.026)
Train: 282 [1150/1251 ( 92%)]  Loss: 2.865 (2.86)  Time: 0.302s, 3386.26/s  (0.300s, 3410.68/s)  LR: 1.294e-05  Data: 0.021 (0.025)
Train: 282 [1200/1251 ( 96%)]  Loss: 2.591 (2.85)  Time: 0.305s, 3360.63/s  (0.300s, 3410.24/s)  LR: 1.290e-05  Data: 0.022 (0.025)
Train: 282 [1250/1251 (100%)]  Loss: 2.989 (2.85)  Time: 0.275s, 3728.36/s  (0.300s, 3411.57/s)  LR: 1.286e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.032 (2.032)  Loss:  0.4639 (0.4639)  Acc@1: 92.2852 (92.2852)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.041 (0.234)  Loss:  0.6411 (0.9178)  Acc@1: 85.8491 (79.8600)  Acc@5: 97.6415 (94.7300)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-282.pth.tar', 79.86000005859376)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-281.pth.tar', 79.80200008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-280.pth.tar', 79.7800000830078)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-279.pth.tar', 79.77200005859375)

Train: 283 [   0/1251 (  0%)]  Loss: 2.772 (2.77)  Time: 2.125s,  481.90/s  (2.125s,  481.90/s)  LR: 1.286e-05  Data: 1.899 (1.899)
Train: 283 [  50/1251 (  4%)]  Loss: 2.747 (2.76)  Time: 0.298s, 3435.08/s  (0.322s, 3180.34/s)  LR: 1.283e-05  Data: 0.024 (0.062)
Train: 283 [ 100/1251 (  8%)]  Loss: 2.727 (2.75)  Time: 0.297s, 3445.40/s  (0.309s, 3315.07/s)  LR: 1.279e-05  Data: 0.022 (0.043)
Train: 283 [ 150/1251 ( 12%)]  Loss: 2.670 (2.73)  Time: 0.303s, 3374.74/s  (0.305s, 3357.39/s)  LR: 1.275e-05  Data: 0.022 (0.036)
Train: 283 [ 200/1251 ( 16%)]  Loss: 2.699 (2.72)  Time: 0.295s, 3470.23/s  (0.303s, 3377.61/s)  LR: 1.272e-05  Data: 0.024 (0.033)
Train: 283 [ 250/1251 ( 20%)]  Loss: 2.830 (2.74)  Time: 0.295s, 3469.46/s  (0.302s, 3389.38/s)  LR: 1.268e-05  Data: 0.023 (0.031)
Train: 283 [ 300/1251 ( 24%)]  Loss: 2.859 (2.76)  Time: 0.300s, 3408.97/s  (0.302s, 3395.27/s)  LR: 1.264e-05  Data: 0.026 (0.030)
Train: 283 [ 350/1251 ( 28%)]  Loss: 2.869 (2.77)  Time: 0.295s, 3471.56/s  (0.301s, 3399.56/s)  LR: 1.261e-05  Data: 0.025 (0.029)
Train: 283 [ 400/1251 ( 32%)]  Loss: 2.904 (2.79)  Time: 0.299s, 3428.59/s  (0.301s, 3401.62/s)  LR: 1.257e-05  Data: 0.023 (0.028)
Train: 283 [ 450/1251 ( 36%)]  Loss: 2.963 (2.80)  Time: 0.298s, 3441.01/s  (0.301s, 3402.03/s)  LR: 1.253e-05  Data: 0.022 (0.028)
Train: 283 [ 500/1251 ( 40%)]  Loss: 2.636 (2.79)  Time: 0.305s, 3359.32/s  (0.301s, 3401.99/s)  LR: 1.250e-05  Data: 0.019 (0.027)
Train: 283 [ 550/1251 ( 44%)]  Loss: 3.020 (2.81)  Time: 0.295s, 3466.71/s  (0.301s, 3402.02/s)  LR: 1.246e-05  Data: 0.020 (0.027)
Train: 283 [ 600/1251 ( 48%)]  Loss: 2.947 (2.82)  Time: 0.301s, 3399.62/s  (0.301s, 3402.23/s)  LR: 1.243e-05  Data: 0.024 (0.027)
Train: 283 [ 650/1251 ( 52%)]  Loss: 2.753 (2.81)  Time: 0.295s, 3466.10/s  (0.301s, 3402.21/s)  LR: 1.239e-05  Data: 0.018 (0.026)
Train: 283 [ 700/1251 ( 56%)]  Loss: 2.901 (2.82)  Time: 0.301s, 3397.24/s  (0.301s, 3402.30/s)  LR: 1.235e-05  Data: 0.024 (0.026)
Train: 283 [ 750/1251 ( 60%)]  Loss: 3.012 (2.83)  Time: 0.298s, 3432.82/s  (0.301s, 3402.10/s)  LR: 1.232e-05  Data: 0.025 (0.026)
Train: 283 [ 800/1251 ( 64%)]  Loss: 2.654 (2.82)  Time: 0.297s, 3452.95/s  (0.301s, 3401.69/s)  LR: 1.228e-05  Data: 0.021 (0.026)
Train: 283 [ 850/1251 ( 68%)]  Loss: 2.498 (2.80)  Time: 0.296s, 3453.80/s  (0.301s, 3401.42/s)  LR: 1.225e-05  Data: 0.027 (0.026)
Train: 283 [ 900/1251 ( 72%)]  Loss: 2.952 (2.81)  Time: 0.303s, 3378.49/s  (0.301s, 3400.86/s)  LR: 1.221e-05  Data: 0.022 (0.026)
Train: 283 [ 950/1251 ( 76%)]  Loss: 2.740 (2.81)  Time: 0.301s, 3407.02/s  (0.301s, 3400.36/s)  LR: 1.218e-05  Data: 0.026 (0.025)
Train: 283 [1000/1251 ( 80%)]  Loss: 2.809 (2.81)  Time: 0.297s, 3453.12/s  (0.301s, 3399.59/s)  LR: 1.214e-05  Data: 0.022 (0.025)
Train: 283 [1050/1251 ( 84%)]  Loss: 2.839 (2.81)  Time: 0.305s, 3359.38/s  (0.301s, 3398.67/s)  LR: 1.211e-05  Data: 0.023 (0.025)
Train: 283 [1100/1251 ( 88%)]  Loss: 3.048 (2.82)  Time: 0.300s, 3415.85/s  (0.301s, 3398.31/s)  LR: 1.207e-05  Data: 0.021 (0.025)
Train: 283 [1150/1251 ( 92%)]  Loss: 2.636 (2.81)  Time: 0.300s, 3417.91/s  (0.301s, 3398.40/s)  LR: 1.204e-05  Data: 0.024 (0.025)
Train: 283 [1200/1251 ( 96%)]  Loss: 2.894 (2.82)  Time: 0.299s, 3424.11/s  (0.301s, 3397.96/s)  LR: 1.200e-05  Data: 0.022 (0.025)
Train: 283 [1250/1251 (100%)]  Loss: 2.916 (2.82)  Time: 0.276s, 3712.86/s  (0.301s, 3399.08/s)  LR: 1.197e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.015 (2.015)  Loss:  0.4624 (0.4624)  Acc@1: 91.7969 (91.7969)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.051 (0.233)  Loss:  0.6318 (0.9134)  Acc@1: 85.3774 (79.8320)  Acc@5: 97.5236 (94.7900)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-282.pth.tar', 79.86000005859376)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-283.pth.tar', 79.83200011230468)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-281.pth.tar', 79.80200008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-280.pth.tar', 79.7800000830078)

Train: 284 [   0/1251 (  0%)]  Loss: 2.745 (2.74)  Time: 2.262s,  452.66/s  (2.262s,  452.66/s)  LR: 1.197e-05  Data: 2.049 (2.049)
Train: 284 [  50/1251 (  4%)]  Loss: 3.030 (2.89)  Time: 0.290s, 3535.43/s  (0.316s, 3236.80/s)  LR: 1.193e-05  Data: 0.025 (0.066)
Train: 284 [ 100/1251 (  8%)]  Loss: 2.771 (2.85)  Time: 0.297s, 3453.52/s  (0.305s, 3355.12/s)  LR: 1.190e-05  Data: 0.025 (0.045)
Train: 284 [ 150/1251 ( 12%)]  Loss: 2.796 (2.84)  Time: 0.300s, 3412.70/s  (0.302s, 3385.55/s)  LR: 1.186e-05  Data: 0.016 (0.038)
Train: 284 [ 200/1251 ( 16%)]  Loss: 2.592 (2.79)  Time: 0.296s, 3453.78/s  (0.301s, 3396.65/s)  LR: 1.183e-05  Data: 0.022 (0.034)
Train: 284 [ 250/1251 ( 20%)]  Loss: 2.947 (2.81)  Time: 0.300s, 3416.20/s  (0.301s, 3401.92/s)  LR: 1.179e-05  Data: 0.020 (0.032)
Train: 284 [ 300/1251 ( 24%)]  Loss: 2.691 (2.80)  Time: 0.303s, 3381.88/s  (0.301s, 3402.91/s)  LR: 1.176e-05  Data: 0.024 (0.031)
Train: 284 [ 350/1251 ( 28%)]  Loss: 2.951 (2.82)  Time: 0.305s, 3357.90/s  (0.301s, 3402.81/s)  LR: 1.173e-05  Data: 0.025 (0.030)
Train: 284 [ 400/1251 ( 32%)]  Loss: 2.845 (2.82)  Time: 0.299s, 3423.00/s  (0.301s, 3403.01/s)  LR: 1.169e-05  Data: 0.021 (0.029)
Train: 284 [ 450/1251 ( 36%)]  Loss: 2.975 (2.83)  Time: 0.301s, 3406.38/s  (0.301s, 3403.11/s)  LR: 1.166e-05  Data: 0.024 (0.028)
Train: 284 [ 500/1251 ( 40%)]  Loss: 3.104 (2.86)  Time: 0.300s, 3412.56/s  (0.301s, 3402.97/s)  LR: 1.162e-05  Data: 0.022 (0.028)
Train: 284 [ 550/1251 ( 44%)]  Loss: 2.837 (2.86)  Time: 0.303s, 3377.39/s  (0.301s, 3401.60/s)  LR: 1.159e-05  Data: 0.022 (0.027)
Train: 284 [ 600/1251 ( 48%)]  Loss: 3.091 (2.87)  Time: 0.300s, 3411.60/s  (0.301s, 3400.44/s)  LR: 1.156e-05  Data: 0.024 (0.027)
Train: 284 [ 650/1251 ( 52%)]  Loss: 2.996 (2.88)  Time: 0.304s, 3368.10/s  (0.301s, 3398.84/s)  LR: 1.152e-05  Data: 0.022 (0.027)
Train: 284 [ 700/1251 ( 56%)]  Loss: 2.725 (2.87)  Time: 0.301s, 3401.33/s  (0.301s, 3398.22/s)  LR: 1.149e-05  Data: 0.022 (0.027)
Train: 284 [ 750/1251 ( 60%)]  Loss: 2.840 (2.87)  Time: 0.306s, 3345.58/s  (0.301s, 3397.26/s)  LR: 1.146e-05  Data: 0.021 (0.026)
Train: 284 [ 800/1251 ( 64%)]  Loss: 2.723 (2.86)  Time: 0.308s, 3328.24/s  (0.302s, 3396.26/s)  LR: 1.142e-05  Data: 0.024 (0.026)
Train: 284 [ 850/1251 ( 68%)]  Loss: 2.863 (2.86)  Time: 0.301s, 3397.36/s  (0.302s, 3396.34/s)  LR: 1.139e-05  Data: 0.025 (0.026)
Train: 284 [ 900/1251 ( 72%)]  Loss: 2.862 (2.86)  Time: 0.298s, 3431.35/s  (0.302s, 3395.02/s)  LR: 1.136e-05  Data: 0.023 (0.026)
Train: 284 [ 950/1251 ( 76%)]  Loss: 2.779 (2.86)  Time: 0.302s, 3394.73/s  (0.302s, 3394.30/s)  LR: 1.132e-05  Data: 0.024 (0.026)
Train: 284 [1000/1251 ( 80%)]  Loss: 2.754 (2.85)  Time: 0.310s, 3298.91/s  (0.302s, 3393.48/s)  LR: 1.129e-05  Data: 0.023 (0.026)
Train: 284 [1050/1251 ( 84%)]  Loss: 2.977 (2.86)  Time: 0.302s, 3391.87/s  (0.302s, 3392.54/s)  LR: 1.126e-05  Data: 0.028 (0.026)
Train: 284 [1100/1251 ( 88%)]  Loss: 2.784 (2.86)  Time: 0.301s, 3406.33/s  (0.302s, 3391.70/s)  LR: 1.122e-05  Data: 0.023 (0.025)
Train: 284 [1150/1251 ( 92%)]  Loss: 2.591 (2.84)  Time: 0.303s, 3377.66/s  (0.302s, 3391.15/s)  LR: 1.119e-05  Data: 0.022 (0.025)
Train: 284 [1200/1251 ( 96%)]  Loss: 2.964 (2.85)  Time: 0.304s, 3372.70/s  (0.302s, 3390.83/s)  LR: 1.116e-05  Data: 0.024 (0.025)
Train: 284 [1250/1251 (100%)]  Loss: 2.684 (2.84)  Time: 0.276s, 3714.23/s  (0.302s, 3392.52/s)  LR: 1.113e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.116 (2.116)  Loss:  0.4592 (0.4592)  Acc@1: 92.0898 (92.0898)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.058 (0.236)  Loss:  0.6245 (0.9103)  Acc@1: 85.9670 (79.8380)  Acc@5: 97.9953 (94.7500)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-282.pth.tar', 79.86000005859376)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-284.pth.tar', 79.83800010986329)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-283.pth.tar', 79.83200011230468)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-281.pth.tar', 79.80200008544922)

Train: 285 [   0/1251 (  0%)]  Loss: 3.120 (3.12)  Time: 2.398s,  427.08/s  (2.398s,  427.08/s)  LR: 1.112e-05  Data: 2.177 (2.177)
Train: 285 [  50/1251 (  4%)]  Loss: 2.951 (3.04)  Time: 0.303s, 3374.43/s  (0.324s, 3159.38/s)  LR: 1.109e-05  Data: 0.025 (0.067)
Train: 285 [ 100/1251 (  8%)]  Loss: 2.699 (2.92)  Time: 0.297s, 3452.93/s  (0.310s, 3298.57/s)  LR: 1.106e-05  Data: 0.021 (0.045)
Train: 285 [ 150/1251 ( 12%)]  Loss: 2.733 (2.88)  Time: 0.300s, 3414.73/s  (0.307s, 3337.89/s)  LR: 1.103e-05  Data: 0.024 (0.038)
Train: 285 [ 200/1251 ( 16%)]  Loss: 2.303 (2.76)  Time: 0.300s, 3408.16/s  (0.305s, 3355.39/s)  LR: 1.099e-05  Data: 0.023 (0.034)
Train: 285 [ 250/1251 ( 20%)]  Loss: 2.691 (2.75)  Time: 0.305s, 3361.10/s  (0.304s, 3363.32/s)  LR: 1.096e-05  Data: 0.025 (0.032)
Train: 285 [ 300/1251 ( 24%)]  Loss: 2.752 (2.75)  Time: 0.302s, 3385.73/s  (0.304s, 3369.27/s)  LR: 1.093e-05  Data: 0.022 (0.031)
Train: 285 [ 350/1251 ( 28%)]  Loss: 3.150 (2.80)  Time: 0.303s, 3376.24/s  (0.304s, 3372.77/s)  LR: 1.090e-05  Data: 0.028 (0.030)
Train: 285 [ 400/1251 ( 32%)]  Loss: 3.035 (2.83)  Time: 0.306s, 3346.26/s  (0.303s, 3374.62/s)  LR: 1.087e-05  Data: 0.024 (0.029)
Train: 285 [ 450/1251 ( 36%)]  Loss: 2.693 (2.81)  Time: 0.305s, 3362.35/s  (0.303s, 3374.95/s)  LR: 1.083e-05  Data: 0.025 (0.028)
Train: 285 [ 500/1251 ( 40%)]  Loss: 2.655 (2.80)  Time: 0.302s, 3396.25/s  (0.303s, 3375.12/s)  LR: 1.080e-05  Data: 0.023 (0.028)
Train: 285 [ 550/1251 ( 44%)]  Loss: 2.907 (2.81)  Time: 0.302s, 3389.41/s  (0.303s, 3375.55/s)  LR: 1.077e-05  Data: 0.022 (0.027)
Train: 285 [ 600/1251 ( 48%)]  Loss: 2.613 (2.79)  Time: 0.301s, 3405.37/s  (0.303s, 3376.54/s)  LR: 1.074e-05  Data: 0.024 (0.027)
Train: 285 [ 650/1251 ( 52%)]  Loss: 3.006 (2.81)  Time: 0.301s, 3401.16/s  (0.303s, 3376.34/s)  LR: 1.071e-05  Data: 0.022 (0.027)
Train: 285 [ 700/1251 ( 56%)]  Loss: 2.680 (2.80)  Time: 0.301s, 3398.47/s  (0.303s, 3376.39/s)  LR: 1.068e-05  Data: 0.023 (0.026)
Train: 285 [ 750/1251 ( 60%)]  Loss: 2.690 (2.79)  Time: 0.302s, 3388.23/s  (0.303s, 3375.58/s)  LR: 1.065e-05  Data: 0.022 (0.026)
Train: 285 [ 800/1251 ( 64%)]  Loss: 2.744 (2.79)  Time: 0.304s, 3365.72/s  (0.303s, 3375.92/s)  LR: 1.061e-05  Data: 0.025 (0.026)
Train: 285 [ 850/1251 ( 68%)]  Loss: 2.923 (2.80)  Time: 0.305s, 3359.29/s  (0.303s, 3375.98/s)  LR: 1.058e-05  Data: 0.026 (0.026)
Train: 285 [ 900/1251 ( 72%)]  Loss: 2.649 (2.79)  Time: 0.307s, 3333.77/s  (0.303s, 3375.71/s)  LR: 1.055e-05  Data: 0.023 (0.026)
Train: 285 [ 950/1251 ( 76%)]  Loss: 2.661 (2.78)  Time: 0.306s, 3348.81/s  (0.303s, 3375.17/s)  LR: 1.052e-05  Data: 0.023 (0.026)
Train: 285 [1000/1251 ( 80%)]  Loss: 2.867 (2.79)  Time: 0.309s, 3312.76/s  (0.303s, 3374.61/s)  LR: 1.049e-05  Data: 0.023 (0.026)
Train: 285 [1050/1251 ( 84%)]  Loss: 2.980 (2.80)  Time: 0.306s, 3349.50/s  (0.303s, 3374.08/s)  LR: 1.046e-05  Data: 0.022 (0.026)
Train: 285 [1100/1251 ( 88%)]  Loss: 2.953 (2.80)  Time: 0.303s, 3376.28/s  (0.304s, 3373.84/s)  LR: 1.043e-05  Data: 0.021 (0.025)
Train: 285 [1150/1251 ( 92%)]  Loss: 2.708 (2.80)  Time: 0.306s, 3348.17/s  (0.304s, 3373.66/s)  LR: 1.040e-05  Data: 0.026 (0.025)
Train: 285 [1200/1251 ( 96%)]  Loss: 3.069 (2.81)  Time: 0.303s, 3374.02/s  (0.304s, 3373.41/s)  LR: 1.037e-05  Data: 0.025 (0.025)
Train: 285 [1250/1251 (100%)]  Loss: 2.860 (2.81)  Time: 0.275s, 3721.33/s  (0.303s, 3374.78/s)  LR: 1.034e-05  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.179 (2.179)  Loss:  0.4626 (0.4626)  Acc@1: 92.0898 (92.0898)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.056 (0.236)  Loss:  0.6206 (0.9097)  Acc@1: 85.6132 (79.8720)  Acc@5: 97.6415 (94.7280)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-285.pth.tar', 79.87200008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-282.pth.tar', 79.86000005859376)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-284.pth.tar', 79.83800010986329)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-283.pth.tar', 79.83200011230468)

Train: 286 [   0/1251 (  0%)]  Loss: 2.877 (2.88)  Time: 1.982s,  516.54/s  (1.982s,  516.54/s)  LR: 1.034e-05  Data: 1.758 (1.758)
Train: 286 [  50/1251 (  4%)]  Loss: 2.495 (2.69)  Time: 0.298s, 3439.67/s  (0.325s, 3155.48/s)  LR: 1.031e-05  Data: 0.020 (0.063)
Train: 286 [ 100/1251 (  8%)]  Loss: 2.710 (2.69)  Time: 0.303s, 3379.38/s  (0.311s, 3296.47/s)  LR: 1.028e-05  Data: 0.025 (0.043)
Train: 286 [ 150/1251 ( 12%)]  Loss: 3.082 (2.79)  Time: 0.304s, 3362.99/s  (0.307s, 3336.03/s)  LR: 1.025e-05  Data: 0.025 (0.037)
Train: 286 [ 200/1251 ( 16%)]  Loss: 2.645 (2.76)  Time: 0.302s, 3396.20/s  (0.305s, 3353.83/s)  LR: 1.022e-05  Data: 0.023 (0.034)
Train: 286 [ 250/1251 ( 20%)]  Loss: 2.615 (2.74)  Time: 0.300s, 3409.65/s  (0.304s, 3363.94/s)  LR: 1.019e-05  Data: 0.025 (0.032)
Train: 286 [ 300/1251 ( 24%)]  Loss: 2.823 (2.75)  Time: 0.306s, 3346.73/s  (0.304s, 3370.24/s)  LR: 1.016e-05  Data: 0.023 (0.030)
Train: 286 [ 350/1251 ( 28%)]  Loss: 2.954 (2.78)  Time: 0.306s, 3344.81/s  (0.304s, 3371.63/s)  LR: 1.013e-05  Data: 0.026 (0.029)
Train: 286 [ 400/1251 ( 32%)]  Loss: 3.120 (2.81)  Time: 0.303s, 3381.58/s  (0.304s, 3372.94/s)  LR: 1.010e-05  Data: 0.022 (0.029)
Train: 286 [ 450/1251 ( 36%)]  Loss: 2.890 (2.82)  Time: 0.307s, 3335.25/s  (0.304s, 3373.74/s)  LR: 1.007e-05  Data: 0.027 (0.028)
Train: 286 [ 500/1251 ( 40%)]  Loss: 3.026 (2.84)  Time: 0.306s, 3341.35/s  (0.304s, 3373.51/s)  LR: 1.004e-05  Data: 0.026 (0.028)
Train: 286 [ 550/1251 ( 44%)]  Loss: 2.448 (2.81)  Time: 0.301s, 3400.84/s  (0.304s, 3373.20/s)  LR: 1.001e-05  Data: 0.025 (0.027)
Train: 286 [ 600/1251 ( 48%)]  Loss: 2.879 (2.81)  Time: 0.310s, 3306.54/s  (0.304s, 3373.07/s)  LR: 9.978e-06  Data: 0.024 (0.027)
Train: 286 [ 650/1251 ( 52%)]  Loss: 2.919 (2.82)  Time: 0.300s, 3408.52/s  (0.304s, 3372.95/s)  LR: 9.948e-06  Data: 0.021 (0.027)
Train: 286 [ 700/1251 ( 56%)]  Loss: 2.661 (2.81)  Time: 0.302s, 3388.88/s  (0.304s, 3372.99/s)  LR: 9.919e-06  Data: 0.026 (0.026)
Train: 286 [ 750/1251 ( 60%)]  Loss: 2.841 (2.81)  Time: 0.304s, 3366.97/s  (0.304s, 3373.18/s)  LR: 9.890e-06  Data: 0.019 (0.026)
Train: 286 [ 800/1251 ( 64%)]  Loss: 2.964 (2.82)  Time: 0.306s, 3346.59/s  (0.304s, 3372.82/s)  LR: 9.861e-06  Data: 0.026 (0.026)
Train: 286 [ 850/1251 ( 68%)]  Loss: 2.833 (2.82)  Time: 0.299s, 3420.57/s  (0.304s, 3371.91/s)  LR: 9.832e-06  Data: 0.024 (0.026)
Train: 286 [ 900/1251 ( 72%)]  Loss: 2.875 (2.82)  Time: 0.311s, 3296.34/s  (0.304s, 3371.35/s)  LR: 9.803e-06  Data: 0.023 (0.026)
Train: 286 [ 950/1251 ( 76%)]  Loss: 3.034 (2.83)  Time: 0.302s, 3387.92/s  (0.304s, 3370.83/s)  LR: 9.774e-06  Data: 0.022 (0.026)
Train: 286 [1000/1251 ( 80%)]  Loss: 2.940 (2.84)  Time: 0.303s, 3375.10/s  (0.304s, 3370.37/s)  LR: 9.745e-06  Data: 0.027 (0.026)
Train: 286 [1050/1251 ( 84%)]  Loss: 2.932 (2.84)  Time: 0.304s, 3373.88/s  (0.304s, 3370.43/s)  LR: 9.717e-06  Data: 0.019 (0.025)
Train: 286 [1100/1251 ( 88%)]  Loss: 2.604 (2.83)  Time: 0.307s, 3340.34/s  (0.304s, 3369.69/s)  LR: 9.688e-06  Data: 0.026 (0.025)
Train: 286 [1150/1251 ( 92%)]  Loss: 2.753 (2.83)  Time: 0.302s, 3388.19/s  (0.304s, 3369.41/s)  LR: 9.660e-06  Data: 0.023 (0.025)
Train: 286 [1200/1251 ( 96%)]  Loss: 2.451 (2.81)  Time: 0.303s, 3379.44/s  (0.304s, 3369.18/s)  LR: 9.631e-06  Data: 0.022 (0.025)
Train: 286 [1250/1251 (100%)]  Loss: 2.729 (2.81)  Time: 0.276s, 3706.17/s  (0.304s, 3370.79/s)  LR: 9.603e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.099 (2.099)  Loss:  0.4666 (0.4666)  Acc@1: 92.1875 (92.1875)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.058 (0.240)  Loss:  0.6260 (0.9172)  Acc@1: 85.2594 (79.7760)  Acc@5: 97.9953 (94.7040)
Train: 287 [   0/1251 (  0%)]  Loss: 2.626 (2.63)  Time: 2.291s,  446.88/s  (2.291s,  446.88/s)  LR: 9.602e-06  Data: 2.069 (2.069)
Train: 287 [  50/1251 (  4%)]  Loss: 2.850 (2.74)  Time: 0.301s, 3406.58/s  (0.326s, 3145.62/s)  LR: 9.574e-06  Data: 0.023 (0.065)
Train: 287 [ 100/1251 (  8%)]  Loss: 2.942 (2.81)  Time: 0.306s, 3348.09/s  (0.312s, 3285.05/s)  LR: 9.546e-06  Data: 0.025 (0.044)
Train: 287 [ 150/1251 ( 12%)]  Loss: 2.756 (2.79)  Time: 0.301s, 3405.74/s  (0.308s, 3324.24/s)  LR: 9.518e-06  Data: 0.017 (0.038)
Train: 287 [ 200/1251 ( 16%)]  Loss: 2.767 (2.79)  Time: 0.300s, 3411.71/s  (0.307s, 3340.50/s)  LR: 9.490e-06  Data: 0.022 (0.034)
Train: 287 [ 250/1251 ( 20%)]  Loss: 2.553 (2.75)  Time: 0.302s, 3388.79/s  (0.306s, 3349.82/s)  LR: 9.462e-06  Data: 0.023 (0.032)
Train: 287 [ 300/1251 ( 24%)]  Loss: 2.731 (2.75)  Time: 0.303s, 3375.59/s  (0.305s, 3353.89/s)  LR: 9.434e-06  Data: 0.025 (0.030)
Train: 287 [ 350/1251 ( 28%)]  Loss: 2.964 (2.77)  Time: 0.306s, 3350.46/s  (0.305s, 3356.25/s)  LR: 9.407e-06  Data: 0.023 (0.029)
Train: 287 [ 400/1251 ( 32%)]  Loss: 2.602 (2.75)  Time: 0.304s, 3368.04/s  (0.305s, 3358.86/s)  LR: 9.379e-06  Data: 0.023 (0.029)
Train: 287 [ 450/1251 ( 36%)]  Loss: 2.834 (2.76)  Time: 0.308s, 3324.04/s  (0.305s, 3358.86/s)  LR: 9.352e-06  Data: 0.024 (0.028)
Train: 287 [ 500/1251 ( 40%)]  Loss: 2.706 (2.76)  Time: 0.305s, 3354.92/s  (0.305s, 3359.07/s)  LR: 9.324e-06  Data: 0.025 (0.028)
Train: 287 [ 550/1251 ( 44%)]  Loss: 2.985 (2.78)  Time: 0.310s, 3304.41/s  (0.305s, 3359.95/s)  LR: 9.297e-06  Data: 0.027 (0.027)
Train: 287 [ 600/1251 ( 48%)]  Loss: 2.310 (2.74)  Time: 0.304s, 3372.83/s  (0.305s, 3359.44/s)  LR: 9.270e-06  Data: 0.022 (0.027)
Train: 287 [ 650/1251 ( 52%)]  Loss: 2.952 (2.76)  Time: 0.304s, 3363.13/s  (0.305s, 3358.60/s)  LR: 9.242e-06  Data: 0.022 (0.027)
Train: 287 [ 700/1251 ( 56%)]  Loss: 2.947 (2.77)  Time: 0.306s, 3351.57/s  (0.305s, 3358.79/s)  LR: 9.215e-06  Data: 0.024 (0.026)
Train: 287 [ 750/1251 ( 60%)]  Loss: 2.838 (2.77)  Time: 0.306s, 3348.84/s  (0.305s, 3358.64/s)  LR: 9.188e-06  Data: 0.024 (0.026)
Train: 287 [ 800/1251 ( 64%)]  Loss: 2.943 (2.78)  Time: 0.306s, 3351.13/s  (0.305s, 3358.32/s)  LR: 9.161e-06  Data: 0.026 (0.026)
Train: 287 [ 850/1251 ( 68%)]  Loss: 2.856 (2.79)  Time: 0.307s, 3336.90/s  (0.305s, 3357.88/s)  LR: 9.134e-06  Data: 0.025 (0.026)
Train: 287 [ 900/1251 ( 72%)]  Loss: 2.789 (2.79)  Time: 0.309s, 3318.98/s  (0.305s, 3357.59/s)  LR: 9.108e-06  Data: 0.023 (0.026)
Train: 287 [ 950/1251 ( 76%)]  Loss: 3.111 (2.80)  Time: 0.306s, 3343.38/s  (0.305s, 3356.79/s)  LR: 9.081e-06  Data: 0.023 (0.026)
Train: 287 [1000/1251 ( 80%)]  Loss: 2.813 (2.80)  Time: 0.307s, 3339.54/s  (0.305s, 3356.54/s)  LR: 9.055e-06  Data: 0.022 (0.025)
Train: 287 [1050/1251 ( 84%)]  Loss: 2.703 (2.80)  Time: 0.303s, 3383.39/s  (0.305s, 3356.12/s)  LR: 9.028e-06  Data: 0.028 (0.025)
Train: 287 [1100/1251 ( 88%)]  Loss: 2.700 (2.79)  Time: 0.310s, 3303.51/s  (0.305s, 3355.45/s)  LR: 9.002e-06  Data: 0.026 (0.025)
Train: 287 [1150/1251 ( 92%)]  Loss: 2.643 (2.79)  Time: 0.306s, 3350.26/s  (0.305s, 3355.49/s)  LR: 8.975e-06  Data: 0.027 (0.025)
Train: 287 [1200/1251 ( 96%)]  Loss: 2.832 (2.79)  Time: 0.304s, 3367.72/s  (0.305s, 3354.96/s)  LR: 8.949e-06  Data: 0.022 (0.025)
Train: 287 [1250/1251 (100%)]  Loss: 2.796 (2.79)  Time: 0.276s, 3708.96/s  (0.305s, 3356.49/s)  LR: 8.923e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.056 (2.056)  Loss:  0.4688 (0.4688)  Acc@1: 92.1875 (92.1875)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.053 (0.234)  Loss:  0.6318 (0.9107)  Acc@1: 85.0236 (79.8220)  Acc@5: 97.7594 (94.7720)
Train: 288 [   0/1251 (  0%)]  Loss: 2.535 (2.54)  Time: 1.934s,  529.48/s  (1.934s,  529.48/s)  LR: 8.922e-06  Data: 1.700 (1.700)
Train: 288 [  50/1251 (  4%)]  Loss: 2.927 (2.73)  Time: 0.303s, 3381.29/s  (0.323s, 3174.16/s)  LR: 8.896e-06  Data: 0.022 (0.065)
Train: 288 [ 100/1251 (  8%)]  Loss: 3.157 (2.87)  Time: 0.306s, 3350.86/s  (0.311s, 3295.74/s)  LR: 8.870e-06  Data: 0.022 (0.045)
Train: 288 [ 150/1251 ( 12%)]  Loss: 3.014 (2.91)  Time: 0.301s, 3396.65/s  (0.308s, 3328.61/s)  LR: 8.845e-06  Data: 0.020 (0.038)
Train: 288 [ 200/1251 ( 16%)]  Loss: 2.857 (2.90)  Time: 0.306s, 3342.39/s  (0.306s, 3341.55/s)  LR: 8.819e-06  Data: 0.022 (0.034)
Train: 288 [ 250/1251 ( 20%)]  Loss: 2.762 (2.88)  Time: 0.303s, 3377.82/s  (0.306s, 3348.51/s)  LR: 8.793e-06  Data: 0.021 (0.032)
Train: 288 [ 300/1251 ( 24%)]  Loss: 2.517 (2.82)  Time: 0.306s, 3346.88/s  (0.306s, 3350.80/s)  LR: 8.767e-06  Data: 0.022 (0.030)
Train: 288 [ 350/1251 ( 28%)]  Loss: 2.896 (2.83)  Time: 0.308s, 3326.46/s  (0.306s, 3351.87/s)  LR: 8.742e-06  Data: 0.023 (0.029)
Train: 288 [ 400/1251 ( 32%)]  Loss: 2.574 (2.80)  Time: 0.304s, 3372.19/s  (0.305s, 3352.53/s)  LR: 8.716e-06  Data: 0.024 (0.029)
Train: 288 [ 450/1251 ( 36%)]  Loss: 2.217 (2.75)  Time: 0.303s, 3374.23/s  (0.305s, 3353.39/s)  LR: 8.691e-06  Data: 0.019 (0.028)
Train: 288 [ 500/1251 ( 40%)]  Loss: 3.033 (2.77)  Time: 0.310s, 3300.62/s  (0.305s, 3352.89/s)  LR: 8.666e-06  Data: 0.024 (0.028)
Train: 288 [ 550/1251 ( 44%)]  Loss: 2.942 (2.79)  Time: 0.312s, 3286.10/s  (0.305s, 3352.26/s)  LR: 8.641e-06  Data: 0.024 (0.027)
Train: 288 [ 600/1251 ( 48%)]  Loss: 2.934 (2.80)  Time: 0.304s, 3363.20/s  (0.306s, 3351.75/s)  LR: 8.615e-06  Data: 0.024 (0.027)
Train: 288 [ 650/1251 ( 52%)]  Loss: 2.699 (2.79)  Time: 0.311s, 3292.88/s  (0.305s, 3352.14/s)  LR: 8.590e-06  Data: 0.023 (0.027)
Train: 288 [ 700/1251 ( 56%)]  Loss: 2.941 (2.80)  Time: 0.303s, 3381.50/s  (0.305s, 3351.90/s)  LR: 8.566e-06  Data: 0.022 (0.026)
Train: 288 [ 750/1251 ( 60%)]  Loss: 2.893 (2.81)  Time: 0.306s, 3344.01/s  (0.306s, 3351.53/s)  LR: 8.541e-06  Data: 0.025 (0.026)
Train: 288 [ 800/1251 ( 64%)]  Loss: 2.929 (2.81)  Time: 0.306s, 3350.03/s  (0.306s, 3350.30/s)  LR: 8.516e-06  Data: 0.023 (0.026)
Train: 288 [ 850/1251 ( 68%)]  Loss: 2.615 (2.80)  Time: 0.308s, 3327.66/s  (0.306s, 3349.58/s)  LR: 8.491e-06  Data: 0.024 (0.026)
Train: 288 [ 900/1251 ( 72%)]  Loss: 2.725 (2.80)  Time: 0.305s, 3356.49/s  (0.306s, 3349.15/s)  LR: 8.467e-06  Data: 0.021 (0.026)
Train: 288 [ 950/1251 ( 76%)]  Loss: 2.593 (2.79)  Time: 0.308s, 3320.33/s  (0.306s, 3348.08/s)  LR: 8.442e-06  Data: 0.022 (0.026)
Train: 288 [1000/1251 ( 80%)]  Loss: 2.494 (2.77)  Time: 0.310s, 3308.16/s  (0.306s, 3347.57/s)  LR: 8.418e-06  Data: 0.028 (0.026)
Train: 288 [1050/1251 ( 84%)]  Loss: 3.044 (2.79)  Time: 0.308s, 3325.17/s  (0.306s, 3346.62/s)  LR: 8.393e-06  Data: 0.023 (0.026)
Train: 288 [1100/1251 ( 88%)]  Loss: 2.823 (2.79)  Time: 0.304s, 3366.99/s  (0.306s, 3346.03/s)  LR: 8.369e-06  Data: 0.024 (0.025)
Train: 288 [1150/1251 ( 92%)]  Loss: 2.908 (2.79)  Time: 0.307s, 3334.14/s  (0.306s, 3345.55/s)  LR: 8.345e-06  Data: 0.021 (0.025)
Train: 288 [1200/1251 ( 96%)]  Loss: 2.618 (2.79)  Time: 0.308s, 3326.00/s  (0.306s, 3345.28/s)  LR: 8.321e-06  Data: 0.024 (0.025)
Train: 288 [1250/1251 (100%)]  Loss: 2.608 (2.78)  Time: 0.276s, 3712.70/s  (0.306s, 3346.34/s)  LR: 8.297e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.026 (2.026)  Loss:  0.4639 (0.4639)  Acc@1: 91.9922 (91.9922)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.054 (0.234)  Loss:  0.6211 (0.9083)  Acc@1: 85.4953 (79.8620)  Acc@5: 97.8774 (94.7860)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-285.pth.tar', 79.87200008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-288.pth.tar', 79.86200016357422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-282.pth.tar', 79.86000005859376)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-284.pth.tar', 79.83800010986329)

Train: 289 [   0/1251 (  0%)]  Loss: 2.742 (2.74)  Time: 2.476s,  413.57/s  (2.476s,  413.57/s)  LR: 8.297e-06  Data: 2.259 (2.259)
Train: 289 [  50/1251 (  4%)]  Loss: 2.639 (2.69)  Time: 0.300s, 3414.02/s  (0.329s, 3116.48/s)  LR: 8.273e-06  Data: 0.024 (0.068)
Train: 289 [ 100/1251 (  8%)]  Loss: 3.009 (2.80)  Time: 0.303s, 3373.98/s  (0.314s, 3256.43/s)  LR: 8.249e-06  Data: 0.028 (0.046)
Train: 289 [ 150/1251 ( 12%)]  Loss: 2.863 (2.81)  Time: 0.307s, 3330.86/s  (0.311s, 3296.41/s)  LR: 8.225e-06  Data: 0.024 (0.039)
Train: 289 [ 200/1251 ( 16%)]  Loss: 2.801 (2.81)  Time: 0.304s, 3372.65/s  (0.309s, 3314.54/s)  LR: 8.202e-06  Data: 0.022 (0.035)
Train: 289 [ 250/1251 ( 20%)]  Loss: 2.943 (2.83)  Time: 0.308s, 3320.46/s  (0.308s, 3322.96/s)  LR: 8.178e-06  Data: 0.023 (0.033)
Train: 289 [ 300/1251 ( 24%)]  Loss: 2.716 (2.82)  Time: 0.303s, 3375.93/s  (0.308s, 3326.90/s)  LR: 8.155e-06  Data: 0.020 (0.031)
Train: 289 [ 350/1251 ( 28%)]  Loss: 2.493 (2.78)  Time: 0.304s, 3373.71/s  (0.307s, 3331.41/s)  LR: 8.131e-06  Data: 0.026 (0.030)
Train: 289 [ 400/1251 ( 32%)]  Loss: 2.638 (2.76)  Time: 0.307s, 3332.14/s  (0.307s, 3332.99/s)  LR: 8.108e-06  Data: 0.026 (0.029)
Train: 289 [ 450/1251 ( 36%)]  Loss: 2.852 (2.77)  Time: 0.310s, 3301.35/s  (0.307s, 3333.19/s)  LR: 8.085e-06  Data: 0.024 (0.029)
Train: 289 [ 500/1251 ( 40%)]  Loss: 2.888 (2.78)  Time: 0.307s, 3336.78/s  (0.307s, 3332.98/s)  LR: 8.062e-06  Data: 0.021 (0.028)
Train: 289 [ 550/1251 ( 44%)]  Loss: 2.524 (2.76)  Time: 0.305s, 3360.53/s  (0.307s, 3333.31/s)  LR: 8.039e-06  Data: 0.019 (0.028)
Train: 289 [ 600/1251 ( 48%)]  Loss: 2.911 (2.77)  Time: 0.310s, 3306.74/s  (0.307s, 3332.98/s)  LR: 8.016e-06  Data: 0.023 (0.027)
Train: 289 [ 650/1251 ( 52%)]  Loss: 2.683 (2.76)  Time: 0.309s, 3315.72/s  (0.307s, 3333.16/s)  LR: 7.993e-06  Data: 0.025 (0.027)
Train: 289 [ 700/1251 ( 56%)]  Loss: 2.402 (2.74)  Time: 0.310s, 3308.26/s  (0.307s, 3333.73/s)  LR: 7.970e-06  Data: 0.027 (0.027)
Train: 289 [ 750/1251 ( 60%)]  Loss: 2.935 (2.75)  Time: 0.309s, 3310.87/s  (0.307s, 3333.48/s)  LR: 7.947e-06  Data: 0.020 (0.027)
Train: 289 [ 800/1251 ( 64%)]  Loss: 2.927 (2.76)  Time: 0.310s, 3303.97/s  (0.307s, 3333.15/s)  LR: 7.925e-06  Data: 0.027 (0.026)
Train: 289 [ 850/1251 ( 68%)]  Loss: 2.804 (2.76)  Time: 0.314s, 3266.25/s  (0.307s, 3332.91/s)  LR: 7.902e-06  Data: 0.025 (0.026)
Train: 289 [ 900/1251 ( 72%)]  Loss: 2.827 (2.77)  Time: 0.308s, 3323.58/s  (0.307s, 3332.86/s)  LR: 7.880e-06  Data: 0.018 (0.026)
Train: 289 [ 950/1251 ( 76%)]  Loss: 2.686 (2.76)  Time: 0.309s, 3313.69/s  (0.307s, 3333.15/s)  LR: 7.858e-06  Data: 0.022 (0.026)
Train: 289 [1000/1251 ( 80%)]  Loss: 2.874 (2.77)  Time: 0.309s, 3318.90/s  (0.307s, 3333.29/s)  LR: 7.835e-06  Data: 0.025 (0.026)
Train: 289 [1050/1251 ( 84%)]  Loss: 2.939 (2.78)  Time: 0.314s, 3266.00/s  (0.307s, 3333.22/s)  LR: 7.813e-06  Data: 0.021 (0.026)
Train: 289 [1100/1251 ( 88%)]  Loss: 2.849 (2.78)  Time: 0.306s, 3351.83/s  (0.307s, 3333.23/s)  LR: 7.791e-06  Data: 0.022 (0.026)
Train: 289 [1150/1251 ( 92%)]  Loss: 2.805 (2.78)  Time: 0.307s, 3335.77/s  (0.307s, 3333.83/s)  LR: 7.769e-06  Data: 0.023 (0.025)
Train: 289 [1200/1251 ( 96%)]  Loss: 2.743 (2.78)  Time: 0.312s, 3283.47/s  (0.307s, 3334.09/s)  LR: 7.747e-06  Data: 0.021 (0.025)
Train: 289 [1250/1251 (100%)]  Loss: 2.913 (2.78)  Time: 0.276s, 3707.63/s  (0.307s, 3336.01/s)  LR: 7.725e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.993 (1.993)  Loss:  0.4714 (0.4714)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.066 (0.236)  Loss:  0.6299 (0.9132)  Acc@1: 85.2594 (79.9380)  Acc@5: 97.6415 (94.7940)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-289.pth.tar', 79.93800006103515)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-285.pth.tar', 79.87200008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-288.pth.tar', 79.86200016357422)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-282.pth.tar', 79.86000005859376)

Train: 290 [   0/1251 (  0%)]  Loss: 2.658 (2.66)  Time: 2.006s,  510.52/s  (2.006s,  510.52/s)  LR: 7.725e-06  Data: 1.774 (1.774)
Train: 290 [  50/1251 (  4%)]  Loss: 2.473 (2.57)  Time: 0.304s, 3367.32/s  (0.328s, 3123.17/s)  LR: 7.703e-06  Data: 0.023 (0.061)
Train: 290 [ 100/1251 (  8%)]  Loss: 2.826 (2.65)  Time: 0.302s, 3396.19/s  (0.314s, 3263.70/s)  LR: 7.682e-06  Data: 0.032 (0.042)
Train: 290 [ 150/1251 ( 12%)]  Loss: 2.755 (2.68)  Time: 0.299s, 3425.33/s  (0.310s, 3305.46/s)  LR: 7.660e-06  Data: 0.016 (0.036)
Train: 290 [ 200/1251 ( 16%)]  Loss: 2.703 (2.68)  Time: 0.306s, 3345.31/s  (0.308s, 3321.36/s)  LR: 7.639e-06  Data: 0.024 (0.033)
Train: 290 [ 250/1251 ( 20%)]  Loss: 2.714 (2.69)  Time: 0.302s, 3390.66/s  (0.308s, 3328.48/s)  LR: 7.617e-06  Data: 0.025 (0.031)
Train: 290 [ 300/1251 ( 24%)]  Loss: 2.954 (2.73)  Time: 0.306s, 3343.04/s  (0.307s, 3333.62/s)  LR: 7.596e-06  Data: 0.021 (0.030)
Train: 290 [ 350/1251 ( 28%)]  Loss: 2.777 (2.73)  Time: 0.308s, 3325.53/s  (0.307s, 3337.06/s)  LR: 7.575e-06  Data: 0.026 (0.029)
Train: 290 [ 400/1251 ( 32%)]  Loss: 2.795 (2.74)  Time: 0.303s, 3382.68/s  (0.307s, 3339.93/s)  LR: 7.554e-06  Data: 0.022 (0.028)
Train: 290 [ 450/1251 ( 36%)]  Loss: 2.883 (2.75)  Time: 0.306s, 3341.39/s  (0.306s, 3341.75/s)  LR: 7.533e-06  Data: 0.026 (0.028)
Train: 290 [ 500/1251 ( 40%)]  Loss: 2.901 (2.77)  Time: 0.306s, 3341.83/s  (0.306s, 3342.13/s)  LR: 7.512e-06  Data: 0.021 (0.027)
Train: 290 [ 550/1251 ( 44%)]  Loss: 2.694 (2.76)  Time: 0.314s, 3258.15/s  (0.306s, 3343.35/s)  LR: 7.491e-06  Data: 0.024 (0.027)
Train: 290 [ 600/1251 ( 48%)]  Loss: 3.031 (2.78)  Time: 0.300s, 3414.46/s  (0.306s, 3343.75/s)  LR: 7.470e-06  Data: 0.025 (0.027)
Train: 290 [ 650/1251 ( 52%)]  Loss: 2.839 (2.79)  Time: 0.301s, 3403.97/s  (0.306s, 3343.86/s)  LR: 7.449e-06  Data: 0.021 (0.026)
Train: 290 [ 700/1251 ( 56%)]  Loss: 2.893 (2.79)  Time: 0.306s, 3349.36/s  (0.306s, 3343.84/s)  LR: 7.429e-06  Data: 0.023 (0.026)
Train: 290 [ 750/1251 ( 60%)]  Loss: 2.752 (2.79)  Time: 0.307s, 3333.72/s  (0.306s, 3343.83/s)  LR: 7.408e-06  Data: 0.025 (0.026)
Train: 290 [ 800/1251 ( 64%)]  Loss: 2.962 (2.80)  Time: 0.308s, 3328.51/s  (0.306s, 3344.64/s)  LR: 7.388e-06  Data: 0.022 (0.026)
Train: 290 [ 850/1251 ( 68%)]  Loss: 2.915 (2.81)  Time: 0.309s, 3309.73/s  (0.306s, 3344.75/s)  LR: 7.367e-06  Data: 0.022 (0.026)
Train: 290 [ 900/1251 ( 72%)]  Loss: 2.793 (2.81)  Time: 0.307s, 3339.62/s  (0.306s, 3344.89/s)  LR: 7.347e-06  Data: 0.026 (0.026)
Train: 290 [ 950/1251 ( 76%)]  Loss: 2.945 (2.81)  Time: 0.304s, 3369.34/s  (0.306s, 3345.33/s)  LR: 7.327e-06  Data: 0.025 (0.025)
Train: 290 [1000/1251 ( 80%)]  Loss: 2.875 (2.82)  Time: 0.302s, 3385.94/s  (0.306s, 3345.89/s)  LR: 7.307e-06  Data: 0.024 (0.025)
Train: 290 [1050/1251 ( 84%)]  Loss: 2.661 (2.81)  Time: 0.311s, 3287.98/s  (0.306s, 3346.17/s)  LR: 7.287e-06  Data: 0.024 (0.025)
Train: 290 [1100/1251 ( 88%)]  Loss: 2.373 (2.79)  Time: 0.306s, 3349.55/s  (0.306s, 3346.17/s)  LR: 7.267e-06  Data: 0.021 (0.025)
Train: 290 [1150/1251 ( 92%)]  Loss: 2.920 (2.80)  Time: 0.309s, 3314.30/s  (0.306s, 3346.62/s)  LR: 7.247e-06  Data: 0.026 (0.025)
Train: 290 [1200/1251 ( 96%)]  Loss: 3.019 (2.80)  Time: 0.303s, 3374.00/s  (0.306s, 3346.78/s)  LR: 7.228e-06  Data: 0.026 (0.025)
Train: 290 [1250/1251 (100%)]  Loss: 2.573 (2.80)  Time: 0.275s, 3720.75/s  (0.306s, 3348.68/s)  LR: 7.208e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.219 (2.219)  Loss:  0.4656 (0.4656)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.057 (0.238)  Loss:  0.6318 (0.9123)  Acc@1: 85.4953 (79.8960)  Acc@5: 97.9953 (94.7760)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-289.pth.tar', 79.93800006103515)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-290.pth.tar', 79.89600003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-285.pth.tar', 79.87200008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-288.pth.tar', 79.86200016357422)

Train: 291 [   0/1251 (  0%)]  Loss: 2.741 (2.74)  Time: 2.363s,  433.32/s  (2.363s,  433.32/s)  LR: 7.208e-06  Data: 2.133 (2.133)
Train: 291 [  50/1251 (  4%)]  Loss: 2.643 (2.69)  Time: 0.303s, 3384.27/s  (0.333s, 3076.40/s)  LR: 7.188e-06  Data: 0.027 (0.077)
Train: 291 [ 100/1251 (  8%)]  Loss: 2.983 (2.79)  Time: 0.299s, 3419.19/s  (0.316s, 3243.61/s)  LR: 7.169e-06  Data: 0.024 (0.050)
Train: 291 [ 150/1251 ( 12%)]  Loss: 2.908 (2.82)  Time: 0.301s, 3399.36/s  (0.311s, 3293.02/s)  LR: 7.149e-06  Data: 0.025 (0.042)
Train: 291 [ 200/1251 ( 16%)]  Loss: 2.869 (2.83)  Time: 0.304s, 3373.65/s  (0.309s, 3315.98/s)  LR: 7.130e-06  Data: 0.026 (0.037)
Train: 291 [ 250/1251 ( 20%)]  Loss: 2.485 (2.77)  Time: 0.303s, 3380.98/s  (0.308s, 3326.92/s)  LR: 7.111e-06  Data: 0.026 (0.034)
Train: 291 [ 300/1251 ( 24%)]  Loss: 2.771 (2.77)  Time: 0.305s, 3362.06/s  (0.307s, 3332.39/s)  LR: 7.092e-06  Data: 0.022 (0.033)
Train: 291 [ 350/1251 ( 28%)]  Loss: 2.941 (2.79)  Time: 0.306s, 3348.03/s  (0.307s, 3337.69/s)  LR: 7.072e-06  Data: 0.022 (0.031)
Train: 291 [ 400/1251 ( 32%)]  Loss: 2.869 (2.80)  Time: 0.306s, 3344.64/s  (0.307s, 3339.83/s)  LR: 7.054e-06  Data: 0.021 (0.030)
Train: 291 [ 450/1251 ( 36%)]  Loss: 2.803 (2.80)  Time: 0.304s, 3368.99/s  (0.306s, 3341.16/s)  LR: 7.035e-06  Data: 0.023 (0.030)
Train: 291 [ 500/1251 ( 40%)]  Loss: 3.063 (2.83)  Time: 0.304s, 3370.48/s  (0.306s, 3342.67/s)  LR: 7.016e-06  Data: 0.021 (0.029)
Train: 291 [ 550/1251 ( 44%)]  Loss: 2.743 (2.82)  Time: 0.308s, 3322.29/s  (0.306s, 3343.07/s)  LR: 6.997e-06  Data: 0.021 (0.028)
Train: 291 [ 600/1251 ( 48%)]  Loss: 2.767 (2.81)  Time: 0.308s, 3323.94/s  (0.306s, 3344.02/s)  LR: 6.979e-06  Data: 0.022 (0.028)
Train: 291 [ 650/1251 ( 52%)]  Loss: 2.868 (2.82)  Time: 0.304s, 3373.74/s  (0.306s, 3345.20/s)  LR: 6.960e-06  Data: 0.023 (0.028)
Train: 291 [ 700/1251 ( 56%)]  Loss: 2.934 (2.83)  Time: 0.300s, 3412.11/s  (0.306s, 3345.96/s)  LR: 6.942e-06  Data: 0.024 (0.027)
Train: 291 [ 750/1251 ( 60%)]  Loss: 2.879 (2.83)  Time: 0.304s, 3369.58/s  (0.306s, 3346.79/s)  LR: 6.923e-06  Data: 0.022 (0.027)
Train: 291 [ 800/1251 ( 64%)]  Loss: 2.715 (2.82)  Time: 0.304s, 3363.35/s  (0.306s, 3347.56/s)  LR: 6.905e-06  Data: 0.023 (0.027)
Train: 291 [ 850/1251 ( 68%)]  Loss: 2.766 (2.82)  Time: 0.306s, 3348.94/s  (0.306s, 3348.23/s)  LR: 6.887e-06  Data: 0.024 (0.027)
Train: 291 [ 900/1251 ( 72%)]  Loss: 2.960 (2.83)  Time: 0.305s, 3358.32/s  (0.306s, 3348.51/s)  LR: 6.869e-06  Data: 0.023 (0.026)
Train: 291 [ 950/1251 ( 76%)]  Loss: 3.097 (2.84)  Time: 0.305s, 3353.81/s  (0.306s, 3348.81/s)  LR: 6.851e-06  Data: 0.023 (0.026)
Train: 291 [1000/1251 ( 80%)]  Loss: 2.757 (2.84)  Time: 0.306s, 3348.45/s  (0.306s, 3349.12/s)  LR: 6.833e-06  Data: 0.026 (0.026)
Train: 291 [1050/1251 ( 84%)]  Loss: 2.802 (2.83)  Time: 0.308s, 3319.35/s  (0.306s, 3348.98/s)  LR: 6.815e-06  Data: 0.026 (0.026)
Train: 291 [1100/1251 ( 88%)]  Loss: 2.742 (2.83)  Time: 0.305s, 3354.50/s  (0.306s, 3349.63/s)  LR: 6.797e-06  Data: 0.022 (0.026)
Train: 291 [1150/1251 ( 92%)]  Loss: 3.017 (2.84)  Time: 0.306s, 3342.38/s  (0.306s, 3349.96/s)  LR: 6.780e-06  Data: 0.023 (0.026)
Train: 291 [1200/1251 ( 96%)]  Loss: 2.913 (2.84)  Time: 0.308s, 3323.61/s  (0.306s, 3350.43/s)  LR: 6.762e-06  Data: 0.025 (0.026)
Train: 291 [1250/1251 (100%)]  Loss: 2.988 (2.85)  Time: 0.276s, 3703.81/s  (0.305s, 3352.72/s)  LR: 6.745e-06  Data: 0.000 (0.026)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.068 (2.068)  Loss:  0.4646 (0.4646)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.057 (0.233)  Loss:  0.6313 (0.9126)  Acc@1: 85.7311 (79.9040)  Acc@5: 97.8774 (94.8120)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-289.pth.tar', 79.93800006103515)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-291.pth.tar', 79.90400000732421)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-290.pth.tar', 79.89600003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-285.pth.tar', 79.87200008544922)

Train: 292 [   0/1251 (  0%)]  Loss: 3.017 (3.02)  Time: 2.194s,  466.71/s  (2.194s,  466.71/s)  LR: 6.744e-06  Data: 1.970 (1.970)
Train: 292 [  50/1251 (  4%)]  Loss: 2.376 (2.70)  Time: 0.298s, 3432.93/s  (0.331s, 3098.29/s)  LR: 6.727e-06  Data: 0.023 (0.062)
Train: 292 [ 100/1251 (  8%)]  Loss: 2.593 (2.66)  Time: 0.300s, 3416.85/s  (0.315s, 3247.66/s)  LR: 6.710e-06  Data: 0.022 (0.043)
Train: 292 [ 150/1251 ( 12%)]  Loss: 2.632 (2.65)  Time: 0.301s, 3404.11/s  (0.310s, 3299.60/s)  LR: 6.693e-06  Data: 0.019 (0.036)
Train: 292 [ 200/1251 ( 16%)]  Loss: 2.831 (2.69)  Time: 0.304s, 3372.11/s  (0.308s, 3321.29/s)  LR: 6.675e-06  Data: 0.024 (0.033)
Train: 292 [ 250/1251 ( 20%)]  Loss: 2.590 (2.67)  Time: 0.307s, 3330.81/s  (0.307s, 3332.33/s)  LR: 6.658e-06  Data: 0.023 (0.031)
Train: 292 [ 300/1251 ( 24%)]  Loss: 2.780 (2.69)  Time: 0.308s, 3327.25/s  (0.307s, 3337.98/s)  LR: 6.641e-06  Data: 0.032 (0.030)
Train: 292 [ 350/1251 ( 28%)]  Loss: 2.621 (2.68)  Time: 0.304s, 3364.93/s  (0.306s, 3341.39/s)  LR: 6.625e-06  Data: 0.024 (0.029)
Train: 292 [ 400/1251 ( 32%)]  Loss: 2.850 (2.70)  Time: 0.301s, 3404.75/s  (0.306s, 3346.23/s)  LR: 6.608e-06  Data: 0.026 (0.028)
Train: 292 [ 450/1251 ( 36%)]  Loss: 2.592 (2.69)  Time: 0.300s, 3408.04/s  (0.306s, 3351.14/s)  LR: 6.591e-06  Data: 0.025 (0.028)
Train: 292 [ 500/1251 ( 40%)]  Loss: 2.953 (2.71)  Time: 0.307s, 3333.92/s  (0.305s, 3353.34/s)  LR: 6.575e-06  Data: 0.025 (0.027)
Train: 292 [ 550/1251 ( 44%)]  Loss: 2.765 (2.72)  Time: 0.303s, 3379.62/s  (0.305s, 3355.22/s)  LR: 6.558e-06  Data: 0.022 (0.027)
Train: 292 [ 600/1251 ( 48%)]  Loss: 2.941 (2.73)  Time: 0.304s, 3367.56/s  (0.305s, 3356.46/s)  LR: 6.542e-06  Data: 0.025 (0.027)
Train: 292 [ 650/1251 ( 52%)]  Loss: 2.992 (2.75)  Time: 0.305s, 3361.38/s  (0.305s, 3356.88/s)  LR: 6.525e-06  Data: 0.026 (0.026)
Train: 292 [ 700/1251 ( 56%)]  Loss: 2.816 (2.76)  Time: 0.308s, 3328.16/s  (0.305s, 3356.76/s)  LR: 6.509e-06  Data: 0.024 (0.026)
Train: 292 [ 750/1251 ( 60%)]  Loss: 3.019 (2.77)  Time: 0.306s, 3343.31/s  (0.305s, 3356.58/s)  LR: 6.493e-06  Data: 0.027 (0.026)
Train: 292 [ 800/1251 ( 64%)]  Loss: 3.123 (2.79)  Time: 0.305s, 3352.17/s  (0.305s, 3357.07/s)  LR: 6.477e-06  Data: 0.023 (0.026)
Train: 292 [ 850/1251 ( 68%)]  Loss: 3.008 (2.81)  Time: 0.308s, 3320.14/s  (0.305s, 3356.74/s)  LR: 6.461e-06  Data: 0.022 (0.026)
Train: 292 [ 900/1251 ( 72%)]  Loss: 3.109 (2.82)  Time: 0.308s, 3328.67/s  (0.305s, 3356.76/s)  LR: 6.445e-06  Data: 0.022 (0.026)
Train: 292 [ 950/1251 ( 76%)]  Loss: 2.958 (2.83)  Time: 0.308s, 3324.35/s  (0.305s, 3356.90/s)  LR: 6.429e-06  Data: 0.023 (0.026)
Train: 292 [1000/1251 ( 80%)]  Loss: 3.051 (2.84)  Time: 0.301s, 3399.06/s  (0.305s, 3356.87/s)  LR: 6.413e-06  Data: 0.027 (0.025)
Train: 292 [1050/1251 ( 84%)]  Loss: 2.876 (2.84)  Time: 0.305s, 3359.15/s  (0.305s, 3357.20/s)  LR: 6.398e-06  Data: 0.027 (0.025)
Train: 292 [1100/1251 ( 88%)]  Loss: 2.568 (2.83)  Time: 0.304s, 3371.82/s  (0.305s, 3357.37/s)  LR: 6.382e-06  Data: 0.023 (0.025)
Train: 292 [1150/1251 ( 92%)]  Loss: 2.758 (2.83)  Time: 0.304s, 3371.48/s  (0.305s, 3357.92/s)  LR: 6.367e-06  Data: 0.023 (0.025)
Train: 292 [1200/1251 ( 96%)]  Loss: 2.920 (2.83)  Time: 0.303s, 3374.15/s  (0.305s, 3358.12/s)  LR: 6.351e-06  Data: 0.024 (0.025)
Train: 292 [1250/1251 (100%)]  Loss: 3.018 (2.84)  Time: 0.275s, 3719.50/s  (0.305s, 3360.08/s)  LR: 6.336e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.147 (2.147)  Loss:  0.4626 (0.4626)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.060 (0.238)  Loss:  0.6318 (0.9141)  Acc@1: 85.4953 (79.9420)  Acc@5: 97.9953 (94.7640)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-292.pth.tar', 79.94200003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-289.pth.tar', 79.93800006103515)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-291.pth.tar', 79.90400000732421)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-290.pth.tar', 79.89600003417969)

Train: 293 [   0/1251 (  0%)]  Loss: 2.645 (2.65)  Time: 2.455s,  417.05/s  (2.455s,  417.05/s)  LR: 6.336e-06  Data: 2.227 (2.227)
Train: 293 [  50/1251 (  4%)]  Loss: 2.954 (2.80)  Time: 0.291s, 3517.59/s  (0.326s, 3138.76/s)  LR: 6.321e-06  Data: 0.023 (0.068)
Train: 293 [ 100/1251 (  8%)]  Loss: 2.856 (2.82)  Time: 0.297s, 3447.67/s  (0.312s, 3285.09/s)  LR: 6.305e-06  Data: 0.024 (0.046)
Train: 293 [ 150/1251 ( 12%)]  Loss: 2.677 (2.78)  Time: 0.299s, 3421.14/s  (0.308s, 3328.17/s)  LR: 6.290e-06  Data: 0.021 (0.038)
Train: 293 [ 200/1251 ( 16%)]  Loss: 2.584 (2.74)  Time: 0.301s, 3401.01/s  (0.306s, 3347.01/s)  LR: 6.275e-06  Data: 0.022 (0.035)
Train: 293 [ 250/1251 ( 20%)]  Loss: 2.812 (2.75)  Time: 0.307s, 3338.68/s  (0.305s, 3356.26/s)  LR: 6.261e-06  Data: 0.024 (0.033)
Train: 293 [ 300/1251 ( 24%)]  Loss: 2.855 (2.77)  Time: 0.307s, 3337.78/s  (0.305s, 3360.47/s)  LR: 6.246e-06  Data: 0.023 (0.031)
Train: 293 [ 350/1251 ( 28%)]  Loss: 2.456 (2.73)  Time: 0.304s, 3364.41/s  (0.304s, 3364.21/s)  LR: 6.231e-06  Data: 0.022 (0.030)
Train: 293 [ 400/1251 ( 32%)]  Loss: 2.676 (2.72)  Time: 0.305s, 3353.18/s  (0.304s, 3365.91/s)  LR: 6.217e-06  Data: 0.023 (0.029)
Train: 293 [ 450/1251 ( 36%)]  Loss: 2.866 (2.74)  Time: 0.303s, 3375.98/s  (0.304s, 3367.27/s)  LR: 6.202e-06  Data: 0.024 (0.029)
Train: 293 [ 500/1251 ( 40%)]  Loss: 3.044 (2.77)  Time: 0.303s, 3379.21/s  (0.304s, 3366.66/s)  LR: 6.188e-06  Data: 0.024 (0.028)
Train: 293 [ 550/1251 ( 44%)]  Loss: 2.613 (2.75)  Time: 0.305s, 3361.71/s  (0.304s, 3367.82/s)  LR: 6.173e-06  Data: 0.028 (0.028)
Train: 293 [ 600/1251 ( 48%)]  Loss: 2.678 (2.75)  Time: 0.301s, 3402.97/s  (0.304s, 3368.10/s)  LR: 6.159e-06  Data: 0.023 (0.027)
Train: 293 [ 650/1251 ( 52%)]  Loss: 2.980 (2.76)  Time: 0.299s, 3426.39/s  (0.304s, 3368.37/s)  LR: 6.145e-06  Data: 0.024 (0.027)
Train: 293 [ 700/1251 ( 56%)]  Loss: 2.895 (2.77)  Time: 0.304s, 3369.85/s  (0.304s, 3368.11/s)  LR: 6.131e-06  Data: 0.023 (0.027)
Train: 293 [ 750/1251 ( 60%)]  Loss: 2.935 (2.78)  Time: 0.306s, 3346.71/s  (0.304s, 3368.21/s)  LR: 6.117e-06  Data: 0.024 (0.027)
Train: 293 [ 800/1251 ( 64%)]  Loss: 2.824 (2.79)  Time: 0.301s, 3400.37/s  (0.304s, 3368.20/s)  LR: 6.103e-06  Data: 0.028 (0.026)
Train: 293 [ 850/1251 ( 68%)]  Loss: 2.781 (2.79)  Time: 0.311s, 3297.24/s  (0.304s, 3368.98/s)  LR: 6.089e-06  Data: 0.021 (0.026)
Train: 293 [ 900/1251 ( 72%)]  Loss: 2.852 (2.79)  Time: 0.300s, 3414.26/s  (0.304s, 3369.33/s)  LR: 6.075e-06  Data: 0.021 (0.026)
Train: 293 [ 950/1251 ( 76%)]  Loss: 2.924 (2.80)  Time: 0.305s, 3362.13/s  (0.304s, 3369.33/s)  LR: 6.062e-06  Data: 0.023 (0.026)
Train: 293 [1000/1251 ( 80%)]  Loss: 2.632 (2.79)  Time: 0.306s, 3343.12/s  (0.304s, 3369.26/s)  LR: 6.048e-06  Data: 0.025 (0.026)
Train: 293 [1050/1251 ( 84%)]  Loss: 3.060 (2.80)  Time: 0.306s, 3350.49/s  (0.304s, 3369.10/s)  LR: 6.035e-06  Data: 0.024 (0.026)
Train: 293 [1100/1251 ( 88%)]  Loss: 2.586 (2.79)  Time: 0.304s, 3370.55/s  (0.304s, 3369.00/s)  LR: 6.021e-06  Data: 0.021 (0.026)
Train: 293 [1150/1251 ( 92%)]  Loss: 2.766 (2.79)  Time: 0.305s, 3359.57/s  (0.304s, 3369.02/s)  LR: 6.008e-06  Data: 0.026 (0.026)
Train: 293 [1200/1251 ( 96%)]  Loss: 3.007 (2.80)  Time: 0.305s, 3360.57/s  (0.304s, 3369.21/s)  LR: 5.995e-06  Data: 0.028 (0.025)
Train: 293 [1250/1251 (100%)]  Loss: 2.818 (2.80)  Time: 0.275s, 3718.50/s  (0.304s, 3371.24/s)  LR: 5.982e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.009 (2.009)  Loss:  0.4604 (0.4604)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.053 (0.236)  Loss:  0.6406 (0.9117)  Acc@1: 85.1415 (79.8980)  Acc@5: 97.6415 (94.8280)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-292.pth.tar', 79.94200003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-289.pth.tar', 79.93800006103515)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-291.pth.tar', 79.90400000732421)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-293.pth.tar', 79.89800013916016)

Train: 294 [   0/1251 (  0%)]  Loss: 2.424 (2.42)  Time: 1.939s,  528.18/s  (1.939s,  528.18/s)  LR: 5.981e-06  Data: 1.714 (1.714)
Train: 294 [  50/1251 (  4%)]  Loss: 2.829 (2.63)  Time: 0.290s, 3525.57/s  (0.317s, 3233.34/s)  LR: 5.968e-06  Data: 0.021 (0.060)
Train: 294 [ 100/1251 (  8%)]  Loss: 2.988 (2.75)  Time: 0.301s, 3406.39/s  (0.306s, 3346.30/s)  LR: 5.955e-06  Data: 0.022 (0.042)
Train: 294 [ 150/1251 ( 12%)]  Loss: 2.882 (2.78)  Time: 0.296s, 3462.16/s  (0.304s, 3373.51/s)  LR: 5.943e-06  Data: 0.021 (0.036)
Train: 294 [ 200/1251 ( 16%)]  Loss: 2.666 (2.76)  Time: 0.300s, 3415.44/s  (0.303s, 3383.18/s)  LR: 5.930e-06  Data: 0.022 (0.033)
Train: 294 [ 250/1251 ( 20%)]  Loss: 2.983 (2.80)  Time: 0.307s, 3340.79/s  (0.302s, 3386.00/s)  LR: 5.917e-06  Data: 0.024 (0.031)
Train: 294 [ 300/1251 ( 24%)]  Loss: 3.036 (2.83)  Time: 0.302s, 3394.04/s  (0.302s, 3389.46/s)  LR: 5.905e-06  Data: 0.023 (0.030)
Train: 294 [ 350/1251 ( 28%)]  Loss: 2.789 (2.82)  Time: 0.306s, 3347.17/s  (0.302s, 3390.90/s)  LR: 5.892e-06  Data: 0.025 (0.029)
Train: 294 [ 400/1251 ( 32%)]  Loss: 2.675 (2.81)  Time: 0.299s, 3428.92/s  (0.302s, 3393.43/s)  LR: 5.880e-06  Data: 0.027 (0.028)
Train: 294 [ 450/1251 ( 36%)]  Loss: 2.847 (2.81)  Time: 0.298s, 3432.26/s  (0.302s, 3394.46/s)  LR: 5.867e-06  Data: 0.024 (0.028)
Train: 294 [ 500/1251 ( 40%)]  Loss: 3.060 (2.83)  Time: 0.306s, 3344.95/s  (0.302s, 3394.77/s)  LR: 5.855e-06  Data: 0.026 (0.027)
Train: 294 [ 550/1251 ( 44%)]  Loss: 2.731 (2.83)  Time: 0.302s, 3394.89/s  (0.302s, 3394.95/s)  LR: 5.843e-06  Data: 0.022 (0.027)
Train: 294 [ 600/1251 ( 48%)]  Loss: 2.768 (2.82)  Time: 0.309s, 3317.60/s  (0.302s, 3394.25/s)  LR: 5.831e-06  Data: 0.024 (0.027)
Train: 294 [ 650/1251 ( 52%)]  Loss: 2.915 (2.83)  Time: 0.302s, 3386.69/s  (0.302s, 3394.25/s)  LR: 5.819e-06  Data: 0.027 (0.027)
Train: 294 [ 700/1251 ( 56%)]  Loss: 3.029 (2.84)  Time: 0.301s, 3398.81/s  (0.302s, 3394.30/s)  LR: 5.807e-06  Data: 0.022 (0.026)
Train: 294 [ 750/1251 ( 60%)]  Loss: 2.917 (2.85)  Time: 0.303s, 3377.23/s  (0.302s, 3394.65/s)  LR: 5.795e-06  Data: 0.025 (0.026)
Train: 294 [ 800/1251 ( 64%)]  Loss: 2.869 (2.85)  Time: 0.305s, 3361.68/s  (0.302s, 3394.36/s)  LR: 5.783e-06  Data: 0.027 (0.026)
Train: 294 [ 850/1251 ( 68%)]  Loss: 3.049 (2.86)  Time: 0.301s, 3406.94/s  (0.302s, 3394.52/s)  LR: 5.772e-06  Data: 0.022 (0.026)
Train: 294 [ 900/1251 ( 72%)]  Loss: 3.073 (2.87)  Time: 0.305s, 3352.34/s  (0.302s, 3394.02/s)  LR: 5.760e-06  Data: 0.021 (0.026)
Train: 294 [ 950/1251 ( 76%)]  Loss: 2.797 (2.87)  Time: 0.309s, 3319.12/s  (0.302s, 3393.39/s)  LR: 5.749e-06  Data: 0.025 (0.026)
Train: 294 [1000/1251 ( 80%)]  Loss: 2.484 (2.85)  Time: 0.298s, 3432.99/s  (0.302s, 3392.85/s)  LR: 5.737e-06  Data: 0.019 (0.025)
Train: 294 [1050/1251 ( 84%)]  Loss: 2.939 (2.85)  Time: 0.309s, 3312.31/s  (0.302s, 3392.62/s)  LR: 5.726e-06  Data: 0.030 (0.025)
Train: 294 [1100/1251 ( 88%)]  Loss: 2.576 (2.84)  Time: 0.301s, 3403.36/s  (0.302s, 3393.04/s)  LR: 5.715e-06  Data: 0.022 (0.025)
Train: 294 [1150/1251 ( 92%)]  Loss: 2.716 (2.84)  Time: 0.300s, 3414.87/s  (0.302s, 3393.04/s)  LR: 5.704e-06  Data: 0.024 (0.025)
Train: 294 [1200/1251 ( 96%)]  Loss: 2.859 (2.84)  Time: 0.303s, 3376.54/s  (0.302s, 3393.08/s)  LR: 5.693e-06  Data: 0.022 (0.025)
Train: 294 [1250/1251 (100%)]  Loss: 2.886 (2.84)  Time: 0.276s, 3708.58/s  (0.302s, 3394.42/s)  LR: 5.682e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.056 (2.056)  Loss:  0.4680 (0.4680)  Acc@1: 92.2852 (92.2852)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.053 (0.237)  Loss:  0.6387 (0.9141)  Acc@1: 85.3774 (79.9080)  Acc@5: 97.8774 (94.7720)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-292.pth.tar', 79.94200003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-289.pth.tar', 79.93800006103515)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-294.pth.tar', 79.90800011230469)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-291.pth.tar', 79.90400000732421)

Train: 295 [   0/1251 (  0%)]  Loss: 2.799 (2.80)  Time: 2.140s,  478.59/s  (2.140s,  478.59/s)  LR: 5.682e-06  Data: 1.916 (1.916)
Train: 295 [  50/1251 (  4%)]  Loss: 2.961 (2.88)  Time: 0.291s, 3513.05/s  (0.322s, 3176.72/s)  LR: 5.671e-06  Data: 0.023 (0.062)
Train: 295 [ 100/1251 (  8%)]  Loss: 2.649 (2.80)  Time: 0.296s, 3453.94/s  (0.308s, 3319.50/s)  LR: 5.660e-06  Data: 0.023 (0.043)
Train: 295 [ 150/1251 ( 12%)]  Loss: 2.578 (2.75)  Time: 0.295s, 3473.60/s  (0.305s, 3358.93/s)  LR: 5.649e-06  Data: 0.025 (0.037)
Train: 295 [ 200/1251 ( 16%)]  Loss: 2.841 (2.77)  Time: 0.295s, 3465.57/s  (0.303s, 3376.59/s)  LR: 5.639e-06  Data: 0.028 (0.034)
Train: 295 [ 250/1251 ( 20%)]  Loss: 3.056 (2.81)  Time: 0.305s, 3352.36/s  (0.303s, 3384.07/s)  LR: 5.628e-06  Data: 0.022 (0.032)
Train: 295 [ 300/1251 ( 24%)]  Loss: 2.907 (2.83)  Time: 0.299s, 3429.53/s  (0.302s, 3389.31/s)  LR: 5.618e-06  Data: 0.024 (0.030)
Train: 295 [ 350/1251 ( 28%)]  Loss: 2.520 (2.79)  Time: 0.299s, 3424.97/s  (0.302s, 3391.93/s)  LR: 5.607e-06  Data: 0.025 (0.029)
Train: 295 [ 400/1251 ( 32%)]  Loss: 2.919 (2.80)  Time: 0.302s, 3387.51/s  (0.302s, 3393.83/s)  LR: 5.597e-06  Data: 0.023 (0.028)
Train: 295 [ 450/1251 ( 36%)]  Loss: 2.959 (2.82)  Time: 0.300s, 3409.35/s  (0.302s, 3393.93/s)  LR: 5.587e-06  Data: 0.024 (0.028)
Train: 295 [ 500/1251 ( 40%)]  Loss: 3.093 (2.84)  Time: 0.299s, 3421.35/s  (0.302s, 3394.70/s)  LR: 5.577e-06  Data: 0.027 (0.028)
Train: 295 [ 550/1251 ( 44%)]  Loss: 2.742 (2.84)  Time: 0.305s, 3357.69/s  (0.302s, 3394.41/s)  LR: 5.567e-06  Data: 0.026 (0.027)
Train: 295 [ 600/1251 ( 48%)]  Loss: 2.816 (2.83)  Time: 0.297s, 3452.29/s  (0.302s, 3394.58/s)  LR: 5.557e-06  Data: 0.024 (0.027)
Train: 295 [ 650/1251 ( 52%)]  Loss: 2.536 (2.81)  Time: 0.303s, 3378.50/s  (0.302s, 3394.44/s)  LR: 5.547e-06  Data: 0.022 (0.027)
Train: 295 [ 700/1251 ( 56%)]  Loss: 2.205 (2.77)  Time: 0.301s, 3398.51/s  (0.302s, 3393.88/s)  LR: 5.538e-06  Data: 0.022 (0.026)
Train: 295 [ 750/1251 ( 60%)]  Loss: 2.834 (2.78)  Time: 0.298s, 3439.04/s  (0.302s, 3394.43/s)  LR: 5.528e-06  Data: 0.020 (0.026)
Train: 295 [ 800/1251 ( 64%)]  Loss: 2.851 (2.78)  Time: 0.306s, 3348.83/s  (0.302s, 3393.91/s)  LR: 5.518e-06  Data: 0.023 (0.026)
Train: 295 [ 850/1251 ( 68%)]  Loss: 2.900 (2.79)  Time: 0.304s, 3363.44/s  (0.302s, 3393.69/s)  LR: 5.509e-06  Data: 0.023 (0.026)
Train: 295 [ 900/1251 ( 72%)]  Loss: 2.563 (2.78)  Time: 0.300s, 3408.43/s  (0.302s, 3393.34/s)  LR: 5.500e-06  Data: 0.022 (0.026)
Train: 295 [ 950/1251 ( 76%)]  Loss: 3.048 (2.79)  Time: 0.300s, 3408.89/s  (0.302s, 3393.24/s)  LR: 5.490e-06  Data: 0.025 (0.026)
Train: 295 [1000/1251 ( 80%)]  Loss: 2.883 (2.79)  Time: 0.300s, 3411.62/s  (0.302s, 3393.15/s)  LR: 5.481e-06  Data: 0.023 (0.026)
Train: 295 [1050/1251 ( 84%)]  Loss: 2.783 (2.79)  Time: 0.303s, 3384.32/s  (0.302s, 3392.78/s)  LR: 5.472e-06  Data: 0.023 (0.025)
Train: 295 [1100/1251 ( 88%)]  Loss: 3.039 (2.80)  Time: 0.306s, 3351.58/s  (0.302s, 3392.64/s)  LR: 5.463e-06  Data: 0.023 (0.025)
Train: 295 [1150/1251 ( 92%)]  Loss: 2.861 (2.81)  Time: 0.304s, 3363.67/s  (0.302s, 3392.74/s)  LR: 5.454e-06  Data: 0.024 (0.025)
Train: 295 [1200/1251 ( 96%)]  Loss: 2.606 (2.80)  Time: 0.301s, 3404.96/s  (0.302s, 3392.57/s)  LR: 5.445e-06  Data: 0.023 (0.025)
Train: 295 [1250/1251 (100%)]  Loss: 2.887 (2.80)  Time: 0.275s, 3725.38/s  (0.302s, 3394.02/s)  LR: 5.436e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.002 (2.002)  Loss:  0.4678 (0.4678)  Acc@1: 92.3828 (92.3828)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.058 (0.239)  Loss:  0.6309 (0.9114)  Acc@1: 85.7311 (79.9520)  Acc@5: 97.9953 (94.7820)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-295.pth.tar', 79.95200013671875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-292.pth.tar', 79.94200003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-289.pth.tar', 79.93800006103515)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-294.pth.tar', 79.90800011230469)

Train: 296 [   0/1251 (  0%)]  Loss: 2.639 (2.64)  Time: 2.147s,  476.96/s  (2.147s,  476.96/s)  LR: 5.436e-06  Data: 1.918 (1.918)
Train: 296 [  50/1251 (  4%)]  Loss: 2.773 (2.71)  Time: 0.301s, 3404.10/s  (0.326s, 3145.67/s)  LR: 5.428e-06  Data: 0.021 (0.062)
Train: 296 [ 100/1251 (  8%)]  Loss: 3.006 (2.81)  Time: 0.300s, 3417.31/s  (0.311s, 3292.21/s)  LR: 5.419e-06  Data: 0.026 (0.043)
Train: 296 [ 150/1251 ( 12%)]  Loss: 2.873 (2.82)  Time: 0.297s, 3445.07/s  (0.307s, 3336.36/s)  LR: 5.410e-06  Data: 0.022 (0.037)
Train: 296 [ 200/1251 ( 16%)]  Loss: 2.756 (2.81)  Time: 0.294s, 3477.85/s  (0.305s, 3360.74/s)  LR: 5.402e-06  Data: 0.022 (0.033)
Train: 296 [ 250/1251 ( 20%)]  Loss: 2.865 (2.82)  Time: 0.297s, 3444.51/s  (0.304s, 3372.09/s)  LR: 5.394e-06  Data: 0.023 (0.031)
Train: 296 [ 300/1251 ( 24%)]  Loss: 2.881 (2.83)  Time: 0.303s, 3378.20/s  (0.303s, 3380.58/s)  LR: 5.385e-06  Data: 0.020 (0.030)
Train: 296 [ 350/1251 ( 28%)]  Loss: 3.014 (2.85)  Time: 0.303s, 3377.30/s  (0.302s, 3386.88/s)  LR: 5.377e-06  Data: 0.025 (0.029)
Train: 296 [ 400/1251 ( 32%)]  Loss: 2.895 (2.86)  Time: 0.301s, 3398.30/s  (0.302s, 3391.68/s)  LR: 5.369e-06  Data: 0.022 (0.028)
Train: 296 [ 450/1251 ( 36%)]  Loss: 2.821 (2.85)  Time: 0.302s, 3385.97/s  (0.302s, 3393.07/s)  LR: 5.361e-06  Data: 0.025 (0.028)
Train: 296 [ 500/1251 ( 40%)]  Loss: 2.516 (2.82)  Time: 0.303s, 3377.71/s  (0.302s, 3394.47/s)  LR: 5.353e-06  Data: 0.021 (0.027)
Train: 296 [ 550/1251 ( 44%)]  Loss: 2.637 (2.81)  Time: 0.294s, 3487.21/s  (0.301s, 3396.94/s)  LR: 5.346e-06  Data: 0.025 (0.027)
Train: 296 [ 600/1251 ( 48%)]  Loss: 2.775 (2.80)  Time: 0.303s, 3384.64/s  (0.301s, 3398.41/s)  LR: 5.338e-06  Data: 0.021 (0.027)
Train: 296 [ 650/1251 ( 52%)]  Loss: 2.647 (2.79)  Time: 0.302s, 3396.04/s  (0.301s, 3399.50/s)  LR: 5.330e-06  Data: 0.024 (0.026)
Train: 296 [ 700/1251 ( 56%)]  Loss: 2.894 (2.80)  Time: 0.301s, 3399.74/s  (0.301s, 3400.31/s)  LR: 5.323e-06  Data: 0.022 (0.026)
Train: 296 [ 750/1251 ( 60%)]  Loss: 2.848 (2.80)  Time: 0.299s, 3422.03/s  (0.301s, 3401.72/s)  LR: 5.315e-06  Data: 0.021 (0.026)
Train: 296 [ 800/1251 ( 64%)]  Loss: 2.659 (2.79)  Time: 0.301s, 3399.72/s  (0.301s, 3402.18/s)  LR: 5.308e-06  Data: 0.022 (0.026)
Train: 296 [ 850/1251 ( 68%)]  Loss: 2.912 (2.80)  Time: 0.300s, 3413.99/s  (0.301s, 3403.12/s)  LR: 5.301e-06  Data: 0.023 (0.026)
Train: 296 [ 900/1251 ( 72%)]  Loss: 2.815 (2.80)  Time: 0.297s, 3444.36/s  (0.301s, 3403.54/s)  LR: 5.293e-06  Data: 0.022 (0.026)
Train: 296 [ 950/1251 ( 76%)]  Loss: 3.091 (2.82)  Time: 0.301s, 3399.46/s  (0.301s, 3403.75/s)  LR: 5.286e-06  Data: 0.023 (0.026)
Train: 296 [1000/1251 ( 80%)]  Loss: 2.652 (2.81)  Time: 0.299s, 3429.38/s  (0.301s, 3404.35/s)  LR: 5.279e-06  Data: 0.027 (0.025)
Train: 296 [1050/1251 ( 84%)]  Loss: 2.962 (2.81)  Time: 0.295s, 3472.97/s  (0.301s, 3404.66/s)  LR: 5.272e-06  Data: 0.022 (0.025)
Train: 296 [1100/1251 ( 88%)]  Loss: 2.524 (2.80)  Time: 0.306s, 3344.44/s  (0.301s, 3404.92/s)  LR: 5.265e-06  Data: 0.023 (0.025)
Train: 296 [1150/1251 ( 92%)]  Loss: 2.928 (2.81)  Time: 0.301s, 3406.13/s  (0.301s, 3404.71/s)  LR: 5.259e-06  Data: 0.022 (0.025)
Train: 296 [1200/1251 ( 96%)]  Loss: 2.859 (2.81)  Time: 0.306s, 3350.18/s  (0.301s, 3405.02/s)  LR: 5.252e-06  Data: 0.023 (0.025)
Train: 296 [1250/1251 (100%)]  Loss: 2.663 (2.80)  Time: 0.276s, 3710.72/s  (0.301s, 3407.38/s)  LR: 5.245e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.054 (2.054)  Loss:  0.4641 (0.4641)  Acc@1: 92.5781 (92.5781)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.057 (0.234)  Loss:  0.6343 (0.9075)  Acc@1: 85.3774 (79.9840)  Acc@5: 97.6415 (94.7960)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-296.pth.tar', 79.98400011230468)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-295.pth.tar', 79.95200013671875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-292.pth.tar', 79.94200003417969)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-289.pth.tar', 79.93800006103515)

Train: 297 [   0/1251 (  0%)]  Loss: 2.589 (2.59)  Time: 2.157s,  474.78/s  (2.157s,  474.78/s)  LR: 5.245e-06  Data: 1.931 (1.931)
Train: 297 [  50/1251 (  4%)]  Loss: 2.492 (2.54)  Time: 0.291s, 3523.88/s  (0.319s, 3211.00/s)  LR: 5.239e-06  Data: 0.027 (0.063)
Train: 297 [ 100/1251 (  8%)]  Loss: 2.557 (2.55)  Time: 0.299s, 3423.92/s  (0.306s, 3344.48/s)  LR: 5.232e-06  Data: 0.026 (0.043)
Train: 297 [ 150/1251 ( 12%)]  Loss: 3.046 (2.67)  Time: 0.300s, 3415.07/s  (0.303s, 3382.92/s)  LR: 5.226e-06  Data: 0.026 (0.037)
Train: 297 [ 200/1251 ( 16%)]  Loss: 2.637 (2.66)  Time: 0.295s, 3467.42/s  (0.301s, 3400.05/s)  LR: 5.220e-06  Data: 0.026 (0.033)
Train: 297 [ 250/1251 ( 20%)]  Loss: 2.748 (2.68)  Time: 0.300s, 3410.23/s  (0.300s, 3411.59/s)  LR: 5.214e-06  Data: 0.022 (0.031)
Train: 297 [ 300/1251 ( 24%)]  Loss: 2.702 (2.68)  Time: 0.297s, 3447.91/s  (0.300s, 3416.86/s)  LR: 5.208e-06  Data: 0.026 (0.030)
Train: 297 [ 350/1251 ( 28%)]  Loss: 2.847 (2.70)  Time: 0.300s, 3408.24/s  (0.299s, 3419.25/s)  LR: 5.202e-06  Data: 0.026 (0.029)
Train: 297 [ 400/1251 ( 32%)]  Loss: 2.753 (2.71)  Time: 0.294s, 3479.61/s  (0.299s, 3422.92/s)  LR: 5.196e-06  Data: 0.023 (0.028)
Train: 297 [ 450/1251 ( 36%)]  Loss: 2.811 (2.72)  Time: 0.298s, 3440.50/s  (0.299s, 3426.53/s)  LR: 5.190e-06  Data: 0.025 (0.028)
Train: 297 [ 500/1251 ( 40%)]  Loss: 3.028 (2.75)  Time: 0.292s, 3506.75/s  (0.299s, 3429.89/s)  LR: 5.184e-06  Data: 0.023 (0.027)
Train: 297 [ 550/1251 ( 44%)]  Loss: 2.964 (2.76)  Time: 0.306s, 3351.61/s  (0.298s, 3431.29/s)  LR: 5.179e-06  Data: 0.022 (0.027)
Train: 297 [ 600/1251 ( 48%)]  Loss: 2.804 (2.77)  Time: 0.302s, 3389.46/s  (0.298s, 3431.26/s)  LR: 5.173e-06  Data: 0.026 (0.027)
Train: 297 [ 650/1251 ( 52%)]  Loss: 2.777 (2.77)  Time: 0.297s, 3449.71/s  (0.298s, 3431.77/s)  LR: 5.168e-06  Data: 0.022 (0.027)
Train: 297 [ 700/1251 ( 56%)]  Loss: 2.773 (2.77)  Time: 0.293s, 3494.46/s  (0.298s, 3432.86/s)  LR: 5.162e-06  Data: 0.024 (0.026)
Train: 297 [ 750/1251 ( 60%)]  Loss: 2.464 (2.75)  Time: 0.292s, 3504.61/s  (0.298s, 3434.14/s)  LR: 5.157e-06  Data: 0.023 (0.026)
Train: 297 [ 800/1251 ( 64%)]  Loss: 2.683 (2.75)  Time: 0.301s, 3406.49/s  (0.298s, 3434.55/s)  LR: 5.152e-06  Data: 0.023 (0.026)
Train: 297 [ 850/1251 ( 68%)]  Loss: 2.768 (2.75)  Time: 0.291s, 3519.33/s  (0.298s, 3434.86/s)  LR: 5.147e-06  Data: 0.022 (0.026)
Train: 297 [ 900/1251 ( 72%)]  Loss: 2.827 (2.75)  Time: 0.296s, 3464.45/s  (0.298s, 3434.92/s)  LR: 5.142e-06  Data: 0.025 (0.026)
Train: 297 [ 950/1251 ( 76%)]  Loss: 2.579 (2.74)  Time: 0.297s, 3443.45/s  (0.298s, 3435.17/s)  LR: 5.137e-06  Data: 0.025 (0.026)
Train: 297 [1000/1251 ( 80%)]  Loss: 2.686 (2.74)  Time: 0.293s, 3491.68/s  (0.298s, 3435.55/s)  LR: 5.132e-06  Data: 0.026 (0.025)
Train: 297 [1050/1251 ( 84%)]  Loss: 2.589 (2.73)  Time: 0.305s, 3355.99/s  (0.298s, 3435.24/s)  LR: 5.127e-06  Data: 0.023 (0.025)
Train: 297 [1100/1251 ( 88%)]  Loss: 2.531 (2.72)  Time: 0.300s, 3408.82/s  (0.298s, 3435.25/s)  LR: 5.123e-06  Data: 0.023 (0.025)
Train: 297 [1150/1251 ( 92%)]  Loss: 2.861 (2.73)  Time: 0.301s, 3400.44/s  (0.298s, 3435.18/s)  LR: 5.118e-06  Data: 0.023 (0.025)
Train: 297 [1200/1251 ( 96%)]  Loss: 2.877 (2.74)  Time: 0.299s, 3420.33/s  (0.298s, 3435.06/s)  LR: 5.114e-06  Data: 0.027 (0.025)
Train: 297 [1250/1251 (100%)]  Loss: 2.814 (2.74)  Time: 0.276s, 3710.12/s  (0.298s, 3436.72/s)  LR: 5.109e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.070 (2.070)  Loss:  0.4666 (0.4666)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.040 (0.236)  Loss:  0.6357 (0.9121)  Acc@1: 85.4953 (79.9180)  Acc@5: 97.5236 (94.7620)
Train: 298 [   0/1251 (  0%)]  Loss: 2.691 (2.69)  Time: 2.119s,  483.34/s  (2.119s,  483.34/s)  LR: 5.109e-06  Data: 1.884 (1.884)
Train: 298 [  50/1251 (  4%)]  Loss: 3.049 (2.87)  Time: 0.284s, 3605.51/s  (0.317s, 3231.93/s)  LR: 5.105e-06  Data: 0.022 (0.062)
Train: 298 [ 100/1251 (  8%)]  Loss: 2.840 (2.86)  Time: 0.290s, 3534.59/s  (0.303s, 3375.41/s)  LR: 5.100e-06  Data: 0.018 (0.043)
Train: 298 [ 150/1251 ( 12%)]  Loss: 2.599 (2.79)  Time: 0.295s, 3469.21/s  (0.300s, 3411.56/s)  LR: 5.096e-06  Data: 0.021 (0.036)
Train: 298 [ 200/1251 ( 16%)]  Loss: 2.730 (2.78)  Time: 0.298s, 3440.82/s  (0.299s, 3426.54/s)  LR: 5.092e-06  Data: 0.025 (0.033)
Train: 298 [ 250/1251 ( 20%)]  Loss: 2.921 (2.80)  Time: 0.296s, 3458.92/s  (0.298s, 3433.96/s)  LR: 5.088e-06  Data: 0.022 (0.031)
Train: 298 [ 300/1251 ( 24%)]  Loss: 2.921 (2.82)  Time: 0.296s, 3462.09/s  (0.298s, 3438.42/s)  LR: 5.084e-06  Data: 0.024 (0.030)
Train: 298 [ 350/1251 ( 28%)]  Loss: 2.968 (2.84)  Time: 0.298s, 3441.77/s  (0.298s, 3441.35/s)  LR: 5.081e-06  Data: 0.022 (0.029)
Train: 298 [ 400/1251 ( 32%)]  Loss: 2.912 (2.85)  Time: 0.296s, 3455.79/s  (0.298s, 3441.03/s)  LR: 5.077e-06  Data: 0.022 (0.028)
Train: 298 [ 450/1251 ( 36%)]  Loss: 2.995 (2.86)  Time: 0.301s, 3405.06/s  (0.298s, 3441.60/s)  LR: 5.073e-06  Data: 0.024 (0.028)
Train: 298 [ 500/1251 ( 40%)]  Loss: 2.678 (2.85)  Time: 0.291s, 3521.80/s  (0.297s, 3442.72/s)  LR: 5.070e-06  Data: 0.027 (0.028)
Train: 298 [ 550/1251 ( 44%)]  Loss: 2.887 (2.85)  Time: 0.292s, 3507.38/s  (0.297s, 3443.34/s)  LR: 5.066e-06  Data: 0.024 (0.027)
Train: 298 [ 600/1251 ( 48%)]  Loss: 2.597 (2.83)  Time: 0.299s, 3427.03/s  (0.297s, 3443.98/s)  LR: 5.063e-06  Data: 0.023 (0.027)
Train: 298 [ 650/1251 ( 52%)]  Loss: 2.718 (2.82)  Time: 0.299s, 3429.77/s  (0.297s, 3445.46/s)  LR: 5.060e-06  Data: 0.021 (0.027)
Train: 298 [ 700/1251 ( 56%)]  Loss: 2.881 (2.83)  Time: 0.299s, 3426.30/s  (0.297s, 3446.08/s)  LR: 5.057e-06  Data: 0.021 (0.026)
Train: 298 [ 750/1251 ( 60%)]  Loss: 2.838 (2.83)  Time: 0.301s, 3401.26/s  (0.297s, 3446.33/s)  LR: 5.053e-06  Data: 0.025 (0.026)
Train: 298 [ 800/1251 ( 64%)]  Loss: 2.547 (2.81)  Time: 0.294s, 3486.30/s  (0.297s, 3446.33/s)  LR: 5.050e-06  Data: 0.023 (0.026)
Train: 298 [ 850/1251 ( 68%)]  Loss: 2.807 (2.81)  Time: 0.293s, 3494.23/s  (0.297s, 3446.77/s)  LR: 5.048e-06  Data: 0.025 (0.026)
Train: 298 [ 900/1251 ( 72%)]  Loss: 2.464 (2.79)  Time: 0.297s, 3443.70/s  (0.297s, 3446.54/s)  LR: 5.045e-06  Data: 0.026 (0.026)
Train: 298 [ 950/1251 ( 76%)]  Loss: 2.839 (2.79)  Time: 0.299s, 3423.75/s  (0.297s, 3446.69/s)  LR: 5.042e-06  Data: 0.025 (0.026)
Train: 298 [1000/1251 ( 80%)]  Loss: 2.973 (2.80)  Time: 0.301s, 3404.87/s  (0.297s, 3446.32/s)  LR: 5.039e-06  Data: 0.020 (0.026)
Train: 298 [1050/1251 ( 84%)]  Loss: 2.791 (2.80)  Time: 0.294s, 3486.10/s  (0.297s, 3446.36/s)  LR: 5.037e-06  Data: 0.021 (0.025)
Train: 298 [1100/1251 ( 88%)]  Loss: 3.007 (2.81)  Time: 0.302s, 3390.60/s  (0.297s, 3446.12/s)  LR: 5.034e-06  Data: 0.024 (0.025)
Train: 298 [1150/1251 ( 92%)]  Loss: 3.172 (2.83)  Time: 0.300s, 3411.25/s  (0.297s, 3445.82/s)  LR: 5.032e-06  Data: 0.024 (0.025)
Train: 298 [1200/1251 ( 96%)]  Loss: 2.658 (2.82)  Time: 0.296s, 3459.82/s  (0.297s, 3445.79/s)  LR: 5.030e-06  Data: 0.025 (0.025)
Train: 298 [1250/1251 (100%)]  Loss: 2.850 (2.82)  Time: 0.276s, 3713.50/s  (0.297s, 3447.66/s)  LR: 5.027e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.008 (2.008)  Loss:  0.4604 (0.4604)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.053 (0.239)  Loss:  0.6377 (0.9102)  Acc@1: 85.2594 (79.9080)  Acc@5: 97.6415 (94.8000)
Train: 299 [   0/1251 (  0%)]  Loss: 2.473 (2.47)  Time: 1.996s,  512.95/s  (1.996s,  512.95/s)  LR: 5.027e-06  Data: 1.777 (1.777)
Train: 299 [  50/1251 (  4%)]  Loss: 2.701 (2.59)  Time: 0.292s, 3504.58/s  (0.320s, 3201.04/s)  LR: 5.025e-06  Data: 0.022 (0.063)
Train: 299 [ 100/1251 (  8%)]  Loss: 2.842 (2.67)  Time: 0.293s, 3491.87/s  (0.306s, 3342.77/s)  LR: 5.023e-06  Data: 0.026 (0.044)
Train: 299 [ 150/1251 ( 12%)]  Loss: 2.882 (2.72)  Time: 0.289s, 3548.77/s  (0.302s, 3390.31/s)  LR: 5.021e-06  Data: 0.022 (0.037)
Train: 299 [ 200/1251 ( 16%)]  Loss: 2.800 (2.74)  Time: 0.290s, 3532.46/s  (0.300s, 3411.66/s)  LR: 5.019e-06  Data: 0.023 (0.033)
Train: 299 [ 250/1251 ( 20%)]  Loss: 3.045 (2.79)  Time: 0.297s, 3442.77/s  (0.299s, 3423.41/s)  LR: 5.017e-06  Data: 0.021 (0.031)
Train: 299 [ 300/1251 ( 24%)]  Loss: 2.933 (2.81)  Time: 0.298s, 3433.36/s  (0.299s, 3429.42/s)  LR: 5.016e-06  Data: 0.026 (0.030)
Train: 299 [ 350/1251 ( 28%)]  Loss: 3.049 (2.84)  Time: 0.296s, 3454.22/s  (0.298s, 3432.78/s)  LR: 5.014e-06  Data: 0.027 (0.029)
Train: 299 [ 400/1251 ( 32%)]  Loss: 2.486 (2.80)  Time: 0.293s, 3489.43/s  (0.298s, 3436.35/s)  LR: 5.013e-06  Data: 0.022 (0.029)
Train: 299 [ 450/1251 ( 36%)]  Loss: 2.750 (2.80)  Time: 0.293s, 3496.70/s  (0.298s, 3436.82/s)  LR: 5.011e-06  Data: 0.025 (0.028)
Train: 299 [ 500/1251 ( 40%)]  Loss: 2.721 (2.79)  Time: 0.292s, 3501.23/s  (0.298s, 3438.28/s)  LR: 5.010e-06  Data: 0.017 (0.028)
Train: 299 [ 550/1251 ( 44%)]  Loss: 2.883 (2.80)  Time: 0.293s, 3493.53/s  (0.298s, 3439.06/s)  LR: 5.009e-06  Data: 0.027 (0.027)
Train: 299 [ 600/1251 ( 48%)]  Loss: 3.022 (2.81)  Time: 0.298s, 3432.38/s  (0.298s, 3439.29/s)  LR: 5.007e-06  Data: 0.029 (0.027)
Train: 299 [ 650/1251 ( 52%)]  Loss: 2.725 (2.81)  Time: 0.297s, 3452.97/s  (0.298s, 3439.35/s)  LR: 5.006e-06  Data: 0.024 (0.027)
Train: 299 [ 700/1251 ( 56%)]  Loss: 2.747 (2.80)  Time: 0.296s, 3454.18/s  (0.298s, 3438.88/s)  LR: 5.005e-06  Data: 0.021 (0.026)
Train: 299 [ 750/1251 ( 60%)]  Loss: 2.903 (2.81)  Time: 0.300s, 3418.13/s  (0.298s, 3439.47/s)  LR: 5.004e-06  Data: 0.021 (0.026)
Train: 299 [ 800/1251 ( 64%)]  Loss: 2.918 (2.82)  Time: 0.298s, 3436.75/s  (0.298s, 3439.71/s)  LR: 5.004e-06  Data: 0.026 (0.026)
Train: 299 [ 850/1251 ( 68%)]  Loss: 2.797 (2.82)  Time: 0.294s, 3481.75/s  (0.298s, 3440.12/s)  LR: 5.003e-06  Data: 0.022 (0.026)
Train: 299 [ 900/1251 ( 72%)]  Loss: 2.912 (2.82)  Time: 0.294s, 3485.64/s  (0.298s, 3440.46/s)  LR: 5.002e-06  Data: 0.022 (0.026)
Train: 299 [ 950/1251 ( 76%)]  Loss: 2.653 (2.81)  Time: 0.306s, 3346.60/s  (0.298s, 3440.40/s)  LR: 5.002e-06  Data: 0.023 (0.026)
Train: 299 [1000/1251 ( 80%)]  Loss: 2.845 (2.81)  Time: 0.294s, 3477.55/s  (0.298s, 3440.98/s)  LR: 5.001e-06  Data: 0.022 (0.026)
Train: 299 [1050/1251 ( 84%)]  Loss: 2.706 (2.81)  Time: 0.299s, 3419.32/s  (0.298s, 3441.94/s)  LR: 5.001e-06  Data: 0.023 (0.025)
Train: 299 [1100/1251 ( 88%)]  Loss: 2.624 (2.80)  Time: 0.295s, 3465.98/s  (0.297s, 3442.33/s)  LR: 5.000e-06  Data: 0.021 (0.025)
Train: 299 [1150/1251 ( 92%)]  Loss: 3.107 (2.81)  Time: 0.295s, 3474.02/s  (0.297s, 3442.55/s)  LR: 5.000e-06  Data: 0.026 (0.025)
Train: 299 [1200/1251 ( 96%)]  Loss: 2.826 (2.81)  Time: 0.295s, 3466.02/s  (0.297s, 3442.77/s)  LR: 5.000e-06  Data: 0.024 (0.025)
Train: 299 [1250/1251 (100%)]  Loss: 2.522 (2.80)  Time: 0.277s, 3703.23/s  (0.297s, 3444.28/s)  LR: 5.000e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.015 (2.015)  Loss:  0.4607 (0.4607)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.054 (0.241)  Loss:  0.6382 (0.9095)  Acc@1: 84.9057 (79.8860)  Acc@5: 97.8774 (94.8180)
Train: 300 [   0/1251 (  0%)]  Loss: 2.836 (2.84)  Time: 2.228s,  459.67/s  (2.228s,  459.67/s)  LR: 5.000e-06  Data: 1.986 (1.986)
Train: 300 [  50/1251 (  4%)]  Loss: 2.935 (2.89)  Time: 0.284s, 3605.41/s  (0.322s, 3176.51/s)  LR: 5.000e-06  Data: 0.021 (0.064)
Train: 300 [ 100/1251 (  8%)]  Loss: 3.017 (2.93)  Time: 0.292s, 3511.83/s  (0.308s, 3326.04/s)  LR: 5.000e-06  Data: 0.023 (0.043)
Train: 300 [ 150/1251 ( 12%)]  Loss: 2.910 (2.92)  Time: 0.295s, 3474.63/s  (0.303s, 3377.41/s)  LR: 5.000e-06  Data: 0.023 (0.037)
Train: 300 [ 200/1251 ( 16%)]  Loss: 2.378 (2.82)  Time: 0.298s, 3435.78/s  (0.301s, 3396.73/s)  LR: 5.000e-06  Data: 0.028 (0.034)
Train: 300 [ 250/1251 ( 20%)]  Loss: 2.813 (2.81)  Time: 0.295s, 3468.97/s  (0.300s, 3409.33/s)  LR: 5.000e-06  Data: 0.024 (0.032)
Train: 300 [ 300/1251 ( 24%)]  Loss: 2.931 (2.83)  Time: 0.293s, 3500.18/s  (0.300s, 3415.65/s)  LR: 5.000e-06  Data: 0.026 (0.030)
Train: 300 [ 350/1251 ( 28%)]  Loss: 3.001 (2.85)  Time: 0.297s, 3442.11/s  (0.299s, 3421.81/s)  LR: 5.000e-06  Data: 0.025 (0.029)
Train: 300 [ 400/1251 ( 32%)]  Loss: 2.477 (2.81)  Time: 0.297s, 3450.27/s  (0.299s, 3423.84/s)  LR: 5.000e-06  Data: 0.027 (0.029)
Train: 300 [ 450/1251 ( 36%)]  Loss: 2.572 (2.79)  Time: 0.294s, 3479.68/s  (0.299s, 3425.89/s)  LR: 5.000e-06  Data: 0.024 (0.028)
Train: 300 [ 500/1251 ( 40%)]  Loss: 2.893 (2.80)  Time: 0.293s, 3488.98/s  (0.299s, 3427.67/s)  LR: 5.000e-06  Data: 0.026 (0.028)
Train: 300 [ 550/1251 ( 44%)]  Loss: 2.721 (2.79)  Time: 0.298s, 3441.73/s  (0.299s, 3429.69/s)  LR: 5.000e-06  Data: 0.022 (0.027)
Train: 300 [ 600/1251 ( 48%)]  Loss: 2.840 (2.79)  Time: 0.298s, 3437.17/s  (0.299s, 3429.90/s)  LR: 5.000e-06  Data: 0.022 (0.027)
Train: 300 [ 650/1251 ( 52%)]  Loss: 2.865 (2.80)  Time: 0.300s, 3408.14/s  (0.298s, 3430.90/s)  LR: 5.000e-06  Data: 0.022 (0.027)
Train: 300 [ 700/1251 ( 56%)]  Loss: 2.629 (2.79)  Time: 0.300s, 3408.19/s  (0.298s, 3431.07/s)  LR: 5.000e-06  Data: 0.024 (0.026)
Train: 300 [ 750/1251 ( 60%)]  Loss: 2.949 (2.80)  Time: 0.292s, 3505.44/s  (0.298s, 3431.73/s)  LR: 5.000e-06  Data: 0.022 (0.026)
Train: 300 [ 800/1251 ( 64%)]  Loss: 2.967 (2.81)  Time: 0.298s, 3434.11/s  (0.298s, 3432.05/s)  LR: 5.000e-06  Data: 0.018 (0.026)
Train: 300 [ 850/1251 ( 68%)]  Loss: 2.917 (2.81)  Time: 0.298s, 3438.45/s  (0.298s, 3431.71/s)  LR: 5.000e-06  Data: 0.025 (0.026)
Train: 300 [ 900/1251 ( 72%)]  Loss: 2.306 (2.79)  Time: 0.299s, 3428.57/s  (0.298s, 3431.32/s)  LR: 5.000e-06  Data: 0.022 (0.026)
Train: 300 [ 950/1251 ( 76%)]  Loss: 2.822 (2.79)  Time: 0.299s, 3427.79/s  (0.298s, 3431.49/s)  LR: 5.000e-06  Data: 0.023 (0.026)
Train: 300 [1000/1251 ( 80%)]  Loss: 2.736 (2.79)  Time: 0.294s, 3479.18/s  (0.298s, 3431.04/s)  LR: 5.000e-06  Data: 0.023 (0.026)
Train: 300 [1050/1251 ( 84%)]  Loss: 2.655 (2.78)  Time: 0.299s, 3423.66/s  (0.298s, 3430.79/s)  LR: 5.000e-06  Data: 0.023 (0.025)
Train: 300 [1100/1251 ( 88%)]  Loss: 2.642 (2.77)  Time: 0.301s, 3407.23/s  (0.299s, 3430.26/s)  LR: 5.000e-06  Data: 0.023 (0.025)
Train: 300 [1150/1251 ( 92%)]  Loss: 2.718 (2.77)  Time: 0.299s, 3421.46/s  (0.299s, 3430.08/s)  LR: 5.000e-06  Data: 0.027 (0.025)
Train: 300 [1200/1251 ( 96%)]  Loss: 2.769 (2.77)  Time: 0.297s, 3447.16/s  (0.299s, 3429.47/s)  LR: 5.000e-06  Data: 0.024 (0.025)
Train: 300 [1250/1251 (100%)]  Loss: 2.666 (2.77)  Time: 0.276s, 3706.41/s  (0.298s, 3430.76/s)  LR: 5.000e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.304 (2.304)  Loss:  0.4636 (0.4636)  Acc@1: 92.5781 (92.5781)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.044 (0.236)  Loss:  0.6362 (0.9117)  Acc@1: 85.3774 (79.9500)  Acc@5: 97.6415 (94.7840)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-296.pth.tar', 79.98400011230468)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-295.pth.tar', 79.95200013671875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-300.pth.tar', 79.95000011230469)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-292.pth.tar', 79.94200003417969)

Train: 301 [   0/1251 (  0%)]  Loss: 2.722 (2.72)  Time: 2.034s,  503.46/s  (2.034s,  503.46/s)  LR: 5.000e-06  Data: 1.803 (1.803)
Train: 301 [  50/1251 (  4%)]  Loss: 2.688 (2.71)  Time: 0.280s, 3662.65/s  (0.323s, 3172.44/s)  LR: 5.000e-06  Data: 0.022 (0.071)
Train: 301 [ 100/1251 (  8%)]  Loss: 3.039 (2.82)  Time: 0.293s, 3494.28/s  (0.306s, 3341.16/s)  LR: 5.000e-06  Data: 0.026 (0.048)
Train: 301 [ 150/1251 ( 12%)]  Loss: 2.924 (2.84)  Time: 0.290s, 3525.47/s  (0.302s, 3387.39/s)  LR: 5.000e-06  Data: 0.023 (0.040)
Train: 301 [ 200/1251 ( 16%)]  Loss: 3.138 (2.90)  Time: 0.290s, 3529.46/s  (0.300s, 3408.92/s)  LR: 5.000e-06  Data: 0.025 (0.036)
Train: 301 [ 250/1251 ( 20%)]  Loss: 2.985 (2.92)  Time: 0.295s, 3474.17/s  (0.300s, 3418.34/s)  LR: 5.000e-06  Data: 0.021 (0.033)
Train: 301 [ 300/1251 ( 24%)]  Loss: 2.625 (2.87)  Time: 0.295s, 3467.50/s  (0.299s, 3425.76/s)  LR: 5.000e-06  Data: 0.025 (0.032)
Train: 301 [ 350/1251 ( 28%)]  Loss: 2.598 (2.84)  Time: 0.295s, 3474.21/s  (0.299s, 3428.08/s)  LR: 5.000e-06  Data: 0.023 (0.030)
Train: 301 [ 400/1251 ( 32%)]  Loss: 2.742 (2.83)  Time: 0.293s, 3500.42/s  (0.298s, 3431.36/s)  LR: 5.000e-06  Data: 0.027 (0.030)
Train: 301 [ 450/1251 ( 36%)]  Loss: 2.638 (2.81)  Time: 0.299s, 3419.18/s  (0.298s, 3433.05/s)  LR: 5.000e-06  Data: 0.023 (0.029)
Train: 301 [ 500/1251 ( 40%)]  Loss: 2.844 (2.81)  Time: 0.303s, 3382.10/s  (0.298s, 3435.15/s)  LR: 5.000e-06  Data: 0.019 (0.028)
Train: 301 [ 550/1251 ( 44%)]  Loss: 2.870 (2.82)  Time: 0.297s, 3442.85/s  (0.298s, 3437.47/s)  LR: 5.000e-06  Data: 0.026 (0.028)
Train: 301 [ 600/1251 ( 48%)]  Loss: 2.790 (2.82)  Time: 0.292s, 3510.52/s  (0.298s, 3439.01/s)  LR: 5.000e-06  Data: 0.020 (0.028)
Train: 301 [ 650/1251 ( 52%)]  Loss: 2.637 (2.80)  Time: 0.304s, 3368.42/s  (0.298s, 3440.43/s)  LR: 5.000e-06  Data: 0.032 (0.027)
Train: 301 [ 700/1251 ( 56%)]  Loss: 2.885 (2.81)  Time: 0.293s, 3495.94/s  (0.298s, 3441.24/s)  LR: 5.000e-06  Data: 0.022 (0.027)
Train: 301 [ 750/1251 ( 60%)]  Loss: 2.640 (2.80)  Time: 0.294s, 3488.67/s  (0.297s, 3442.14/s)  LR: 5.000e-06  Data: 0.023 (0.027)
Train: 301 [ 800/1251 ( 64%)]  Loss: 2.564 (2.78)  Time: 0.304s, 3370.78/s  (0.297s, 3442.48/s)  LR: 5.000e-06  Data: 0.024 (0.027)
Train: 301 [ 850/1251 ( 68%)]  Loss: 2.860 (2.79)  Time: 0.295s, 3473.67/s  (0.297s, 3442.72/s)  LR: 5.000e-06  Data: 0.021 (0.026)
Train: 301 [ 900/1251 ( 72%)]  Loss: 2.505 (2.77)  Time: 0.301s, 3403.65/s  (0.297s, 3443.07/s)  LR: 5.000e-06  Data: 0.027 (0.026)
Train: 301 [ 950/1251 ( 76%)]  Loss: 2.852 (2.78)  Time: 0.297s, 3445.08/s  (0.297s, 3443.35/s)  LR: 5.000e-06  Data: 0.024 (0.026)
Train: 301 [1000/1251 ( 80%)]  Loss: 2.881 (2.78)  Time: 0.297s, 3446.25/s  (0.297s, 3443.39/s)  LR: 5.000e-06  Data: 0.021 (0.026)
Train: 301 [1050/1251 ( 84%)]  Loss: 2.886 (2.79)  Time: 0.295s, 3476.09/s  (0.297s, 3443.52/s)  LR: 5.000e-06  Data: 0.025 (0.026)
Train: 301 [1100/1251 ( 88%)]  Loss: 2.735 (2.78)  Time: 0.300s, 3409.69/s  (0.297s, 3443.42/s)  LR: 5.000e-06  Data: 0.025 (0.026)
Train: 301 [1150/1251 ( 92%)]  Loss: 2.645 (2.78)  Time: 0.299s, 3424.98/s  (0.297s, 3443.61/s)  LR: 5.000e-06  Data: 0.021 (0.026)
Train: 301 [1200/1251 ( 96%)]  Loss: 2.841 (2.78)  Time: 0.300s, 3411.85/s  (0.297s, 3443.80/s)  LR: 5.000e-06  Data: 0.028 (0.026)
Train: 301 [1250/1251 (100%)]  Loss: 2.686 (2.78)  Time: 0.273s, 3754.70/s  (0.297s, 3445.57/s)  LR: 5.000e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.035 (2.035)  Loss:  0.4570 (0.4570)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.066 (0.236)  Loss:  0.6328 (0.9092)  Acc@1: 85.3774 (79.9560)  Acc@5: 97.8774 (94.8080)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-296.pth.tar', 79.98400011230468)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-301.pth.tar', 79.9560001123047)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-295.pth.tar', 79.95200013671875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-300.pth.tar', 79.95000011230469)

Train: 302 [   0/1251 (  0%)]  Loss: 3.016 (3.02)  Time: 2.086s,  490.88/s  (2.086s,  490.88/s)  LR: 5.000e-06  Data: 1.867 (1.867)
Train: 302 [  50/1251 (  4%)]  Loss: 2.874 (2.95)  Time: 0.296s, 3462.58/s  (0.315s, 3249.03/s)  LR: 5.000e-06  Data: 0.021 (0.063)
Train: 302 [ 100/1251 (  8%)]  Loss: 2.958 (2.95)  Time: 0.290s, 3533.38/s  (0.303s, 3377.61/s)  LR: 5.000e-06  Data: 0.022 (0.043)
Train: 302 [ 150/1251 ( 12%)]  Loss: 2.862 (2.93)  Time: 0.293s, 3495.11/s  (0.300s, 3417.13/s)  LR: 5.000e-06  Data: 0.024 (0.037)
Train: 302 [ 200/1251 ( 16%)]  Loss: 3.117 (2.97)  Time: 0.290s, 3526.56/s  (0.298s, 3432.60/s)  LR: 5.000e-06  Data: 0.020 (0.033)
Train: 302 [ 250/1251 ( 20%)]  Loss: 3.228 (3.01)  Time: 0.292s, 3507.46/s  (0.298s, 3439.77/s)  LR: 5.000e-06  Data: 0.021 (0.031)
Train: 302 [ 300/1251 ( 24%)]  Loss: 2.616 (2.95)  Time: 0.294s, 3480.19/s  (0.297s, 3444.29/s)  LR: 5.000e-06  Data: 0.024 (0.030)
Train: 302 [ 350/1251 ( 28%)]  Loss: 3.047 (2.96)  Time: 0.293s, 3497.58/s  (0.297s, 3446.91/s)  LR: 5.000e-06  Data: 0.024 (0.029)
Train: 302 [ 400/1251 ( 32%)]  Loss: 2.699 (2.94)  Time: 0.294s, 3480.84/s  (0.297s, 3448.89/s)  LR: 5.000e-06  Data: 0.026 (0.028)
Train: 302 [ 450/1251 ( 36%)]  Loss: 2.393 (2.88)  Time: 0.292s, 3502.22/s  (0.297s, 3450.56/s)  LR: 5.000e-06  Data: 0.020 (0.028)
Train: 302 [ 500/1251 ( 40%)]  Loss: 2.551 (2.85)  Time: 0.305s, 3356.81/s  (0.297s, 3451.35/s)  LR: 5.000e-06  Data: 0.022 (0.027)
Train: 302 [ 550/1251 ( 44%)]  Loss: 2.956 (2.86)  Time: 0.304s, 3365.19/s  (0.297s, 3452.60/s)  LR: 5.000e-06  Data: 0.023 (0.027)
Train: 302 [ 600/1251 ( 48%)]  Loss: 2.764 (2.85)  Time: 0.300s, 3411.93/s  (0.297s, 3453.33/s)  LR: 5.000e-06  Data: 0.022 (0.027)
Train: 302 [ 650/1251 ( 52%)]  Loss: 2.698 (2.84)  Time: 0.292s, 3506.10/s  (0.297s, 3453.58/s)  LR: 5.000e-06  Data: 0.026 (0.027)
Train: 302 [ 700/1251 ( 56%)]  Loss: 3.030 (2.85)  Time: 0.299s, 3420.02/s  (0.297s, 3453.47/s)  LR: 5.000e-06  Data: 0.023 (0.026)
Train: 302 [ 750/1251 ( 60%)]  Loss: 2.865 (2.85)  Time: 0.300s, 3418.66/s  (0.296s, 3453.76/s)  LR: 5.000e-06  Data: 0.026 (0.026)
Train: 302 [ 800/1251 ( 64%)]  Loss: 2.828 (2.85)  Time: 0.300s, 3417.99/s  (0.297s, 3453.44/s)  LR: 5.000e-06  Data: 0.021 (0.026)
Train: 302 [ 850/1251 ( 68%)]  Loss: 2.598 (2.84)  Time: 0.301s, 3397.22/s  (0.297s, 3453.02/s)  LR: 5.000e-06  Data: 0.023 (0.026)
Train: 302 [ 900/1251 ( 72%)]  Loss: 2.703 (2.83)  Time: 0.299s, 3424.79/s  (0.297s, 3452.33/s)  LR: 5.000e-06  Data: 0.026 (0.026)
Train: 302 [ 950/1251 ( 76%)]  Loss: 2.848 (2.83)  Time: 0.299s, 3420.13/s  (0.297s, 3451.67/s)  LR: 5.000e-06  Data: 0.023 (0.026)
Train: 302 [1000/1251 ( 80%)]  Loss: 2.667 (2.82)  Time: 0.296s, 3455.39/s  (0.297s, 3450.94/s)  LR: 5.000e-06  Data: 0.025 (0.026)
Train: 302 [1050/1251 ( 84%)]  Loss: 2.933 (2.83)  Time: 0.302s, 3388.36/s  (0.297s, 3450.19/s)  LR: 5.000e-06  Data: 0.021 (0.025)
Train: 302 [1100/1251 ( 88%)]  Loss: 2.990 (2.84)  Time: 0.305s, 3360.86/s  (0.297s, 3449.78/s)  LR: 5.000e-06  Data: 0.022 (0.025)
Train: 302 [1150/1251 ( 92%)]  Loss: 2.833 (2.84)  Time: 0.303s, 3385.10/s  (0.297s, 3449.25/s)  LR: 5.000e-06  Data: 0.024 (0.025)
Train: 302 [1200/1251 ( 96%)]  Loss: 2.809 (2.84)  Time: 0.298s, 3435.44/s  (0.297s, 3448.56/s)  LR: 5.000e-06  Data: 0.025 (0.025)
Train: 302 [1250/1251 (100%)]  Loss: 3.054 (2.84)  Time: 0.276s, 3709.23/s  (0.297s, 3449.66/s)  LR: 5.000e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.022 (2.022)  Loss:  0.4604 (0.4604)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.055 (0.237)  Loss:  0.6333 (0.9091)  Acc@1: 85.7311 (79.9880)  Acc@5: 97.6415 (94.8020)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-302.pth.tar', 79.98800013671875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-296.pth.tar', 79.98400011230468)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-301.pth.tar', 79.9560001123047)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-295.pth.tar', 79.95200013671875)

Train: 303 [   0/1251 (  0%)]  Loss: 3.027 (3.03)  Time: 2.371s,  431.97/s  (2.371s,  431.97/s)  LR: 5.000e-06  Data: 2.148 (2.148)
Train: 303 [  50/1251 (  4%)]  Loss: 2.759 (2.89)  Time: 0.289s, 3547.94/s  (0.317s, 3227.33/s)  LR: 5.000e-06  Data: 0.023 (0.067)
Train: 303 [ 100/1251 (  8%)]  Loss: 2.933 (2.91)  Time: 0.296s, 3463.25/s  (0.305s, 3362.71/s)  LR: 5.000e-06  Data: 0.023 (0.045)
Train: 303 [ 150/1251 ( 12%)]  Loss: 2.594 (2.83)  Time: 0.291s, 3523.67/s  (0.301s, 3404.82/s)  LR: 5.000e-06  Data: 0.025 (0.038)
Train: 303 [ 200/1251 ( 16%)]  Loss: 2.910 (2.84)  Time: 0.298s, 3436.03/s  (0.299s, 3422.90/s)  LR: 5.000e-06  Data: 0.024 (0.034)
Train: 303 [ 250/1251 ( 20%)]  Loss: 2.672 (2.82)  Time: 0.299s, 3425.63/s  (0.299s, 3429.86/s)  LR: 5.000e-06  Data: 0.025 (0.032)
Train: 303 [ 300/1251 ( 24%)]  Loss: 2.654 (2.79)  Time: 0.289s, 3539.17/s  (0.298s, 3435.30/s)  LR: 5.000e-06  Data: 0.021 (0.031)
Train: 303 [ 350/1251 ( 28%)]  Loss: 3.047 (2.82)  Time: 0.295s, 3466.08/s  (0.298s, 3437.15/s)  LR: 5.000e-06  Data: 0.023 (0.030)
Train: 303 [ 400/1251 ( 32%)]  Loss: 2.918 (2.83)  Time: 0.297s, 3447.53/s  (0.298s, 3438.56/s)  LR: 5.000e-06  Data: 0.023 (0.029)
Train: 303 [ 450/1251 ( 36%)]  Loss: 2.701 (2.82)  Time: 0.305s, 3361.24/s  (0.298s, 3439.29/s)  LR: 5.000e-06  Data: 0.027 (0.028)
Train: 303 [ 500/1251 ( 40%)]  Loss: 2.656 (2.81)  Time: 0.298s, 3437.00/s  (0.298s, 3439.70/s)  LR: 5.000e-06  Data: 0.026 (0.028)
Train: 303 [ 550/1251 ( 44%)]  Loss: 2.732 (2.80)  Time: 0.298s, 3432.63/s  (0.298s, 3439.97/s)  LR: 5.000e-06  Data: 0.025 (0.028)
Train: 303 [ 600/1251 ( 48%)]  Loss: 2.690 (2.79)  Time: 0.298s, 3430.86/s  (0.298s, 3439.68/s)  LR: 5.000e-06  Data: 0.025 (0.027)
Train: 303 [ 650/1251 ( 52%)]  Loss: 2.969 (2.80)  Time: 0.304s, 3372.50/s  (0.298s, 3439.66/s)  LR: 5.000e-06  Data: 0.026 (0.027)
Train: 303 [ 700/1251 ( 56%)]  Loss: 2.560 (2.79)  Time: 0.297s, 3442.67/s  (0.298s, 3439.75/s)  LR: 5.000e-06  Data: 0.022 (0.027)
Train: 303 [ 750/1251 ( 60%)]  Loss: 3.028 (2.80)  Time: 0.297s, 3446.97/s  (0.298s, 3439.85/s)  LR: 5.000e-06  Data: 0.023 (0.026)
Train: 303 [ 800/1251 ( 64%)]  Loss: 2.778 (2.80)  Time: 0.293s, 3491.01/s  (0.298s, 3439.27/s)  LR: 5.000e-06  Data: 0.028 (0.026)
Train: 303 [ 850/1251 ( 68%)]  Loss: 2.702 (2.80)  Time: 0.299s, 3419.78/s  (0.298s, 3439.30/s)  LR: 5.000e-06  Data: 0.027 (0.026)
Train: 303 [ 900/1251 ( 72%)]  Loss: 3.119 (2.81)  Time: 0.299s, 3422.79/s  (0.298s, 3439.37/s)  LR: 5.000e-06  Data: 0.021 (0.026)
Train: 303 [ 950/1251 ( 76%)]  Loss: 2.540 (2.80)  Time: 0.298s, 3431.90/s  (0.298s, 3439.28/s)  LR: 5.000e-06  Data: 0.020 (0.026)
Train: 303 [1000/1251 ( 80%)]  Loss: 2.903 (2.80)  Time: 0.292s, 3504.37/s  (0.298s, 3439.41/s)  LR: 5.000e-06  Data: 0.025 (0.026)
Train: 303 [1050/1251 ( 84%)]  Loss: 3.071 (2.82)  Time: 0.308s, 3327.26/s  (0.298s, 3439.05/s)  LR: 5.000e-06  Data: 0.025 (0.026)
Train: 303 [1100/1251 ( 88%)]  Loss: 2.875 (2.82)  Time: 0.299s, 3420.53/s  (0.298s, 3438.88/s)  LR: 5.000e-06  Data: 0.026 (0.026)
Train: 303 [1150/1251 ( 92%)]  Loss: 2.472 (2.80)  Time: 0.300s, 3414.64/s  (0.298s, 3438.36/s)  LR: 5.000e-06  Data: 0.023 (0.025)
Train: 303 [1200/1251 ( 96%)]  Loss: 2.511 (2.79)  Time: 0.300s, 3413.46/s  (0.298s, 3437.43/s)  LR: 5.000e-06  Data: 0.024 (0.025)
Train: 303 [1250/1251 (100%)]  Loss: 2.678 (2.79)  Time: 0.276s, 3713.06/s  (0.298s, 3438.95/s)  LR: 5.000e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.183 (2.183)  Loss:  0.4631 (0.4631)  Acc@1: 92.4805 (92.4805)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.041 (0.237)  Loss:  0.6367 (0.9116)  Acc@1: 85.4953 (79.9460)  Acc@5: 97.5236 (94.7640)
Train: 304 [   0/1251 (  0%)]  Loss: 2.645 (2.64)  Time: 2.083s,  491.53/s  (2.083s,  491.53/s)  LR: 5.000e-06  Data: 1.858 (1.858)
Train: 304 [  50/1251 (  4%)]  Loss: 2.755 (2.70)  Time: 0.289s, 3544.69/s  (0.317s, 3225.78/s)  LR: 5.000e-06  Data: 0.023 (0.063)
Train: 304 [ 100/1251 (  8%)]  Loss: 3.042 (2.81)  Time: 0.291s, 3516.24/s  (0.304s, 3366.46/s)  LR: 5.000e-06  Data: 0.026 (0.043)
Train: 304 [ 150/1251 ( 12%)]  Loss: 2.760 (2.80)  Time: 0.295s, 3476.08/s  (0.301s, 3403.14/s)  LR: 5.000e-06  Data: 0.023 (0.037)
Train: 304 [ 200/1251 ( 16%)]  Loss: 2.694 (2.78)  Time: 0.299s, 3425.00/s  (0.300s, 3416.51/s)  LR: 5.000e-06  Data: 0.024 (0.033)
Train: 304 [ 250/1251 ( 20%)]  Loss: 2.810 (2.78)  Time: 0.299s, 3424.45/s  (0.299s, 3425.13/s)  LR: 5.000e-06  Data: 0.027 (0.031)
Train: 304 [ 300/1251 ( 24%)]  Loss: 2.825 (2.79)  Time: 0.302s, 3389.94/s  (0.299s, 3427.32/s)  LR: 5.000e-06  Data: 0.023 (0.030)
Train: 304 [ 350/1251 ( 28%)]  Loss: 3.082 (2.83)  Time: 0.297s, 3452.00/s  (0.299s, 3427.45/s)  LR: 5.000e-06  Data: 0.023 (0.029)
Train: 304 [ 400/1251 ( 32%)]  Loss: 2.989 (2.84)  Time: 0.300s, 3418.99/s  (0.299s, 3428.63/s)  LR: 5.000e-06  Data: 0.024 (0.029)
Train: 304 [ 450/1251 ( 36%)]  Loss: 2.788 (2.84)  Time: 0.298s, 3437.41/s  (0.299s, 3430.14/s)  LR: 5.000e-06  Data: 0.024 (0.028)
Train: 304 [ 500/1251 ( 40%)]  Loss: 2.971 (2.85)  Time: 0.301s, 3404.55/s  (0.298s, 3431.01/s)  LR: 5.000e-06  Data: 0.028 (0.027)
Train: 304 [ 550/1251 ( 44%)]  Loss: 2.788 (2.85)  Time: 0.299s, 3421.13/s  (0.299s, 3429.66/s)  LR: 5.000e-06  Data: 0.023 (0.027)
Train: 304 [ 600/1251 ( 48%)]  Loss: 2.880 (2.85)  Time: 0.293s, 3495.75/s  (0.299s, 3428.70/s)  LR: 5.000e-06  Data: 0.021 (0.027)
Train: 304 [ 650/1251 ( 52%)]  Loss: 2.966 (2.86)  Time: 0.305s, 3358.04/s  (0.299s, 3427.38/s)  LR: 5.000e-06  Data: 0.023 (0.027)
Train: 304 [ 700/1251 ( 56%)]  Loss: 2.786 (2.85)  Time: 0.301s, 3407.33/s  (0.299s, 3427.20/s)  LR: 5.000e-06  Data: 0.025 (0.026)
Train: 304 [ 750/1251 ( 60%)]  Loss: 2.616 (2.84)  Time: 0.304s, 3370.02/s  (0.299s, 3426.33/s)  LR: 5.000e-06  Data: 0.023 (0.026)
Train: 304 [ 800/1251 ( 64%)]  Loss: 2.719 (2.83)  Time: 0.300s, 3417.35/s  (0.299s, 3425.92/s)  LR: 5.000e-06  Data: 0.022 (0.026)
Train: 304 [ 850/1251 ( 68%)]  Loss: 2.464 (2.81)  Time: 0.296s, 3457.09/s  (0.299s, 3425.38/s)  LR: 5.000e-06  Data: 0.026 (0.026)
Train: 304 [ 900/1251 ( 72%)]  Loss: 2.823 (2.81)  Time: 0.298s, 3438.85/s  (0.299s, 3424.93/s)  LR: 5.000e-06  Data: 0.019 (0.026)
Train: 304 [ 950/1251 ( 76%)]  Loss: 2.504 (2.80)  Time: 0.301s, 3397.22/s  (0.299s, 3423.65/s)  LR: 5.000e-06  Data: 0.027 (0.026)
Train: 304 [1000/1251 ( 80%)]  Loss: 2.582 (2.79)  Time: 0.300s, 3418.51/s  (0.299s, 3423.05/s)  LR: 5.000e-06  Data: 0.021 (0.026)
Train: 304 [1050/1251 ( 84%)]  Loss: 2.624 (2.78)  Time: 0.297s, 3452.94/s  (0.299s, 3422.43/s)  LR: 5.000e-06  Data: 0.019 (0.025)
Train: 304 [1100/1251 ( 88%)]  Loss: 3.004 (2.79)  Time: 0.301s, 3399.97/s  (0.299s, 3421.70/s)  LR: 5.000e-06  Data: 0.027 (0.025)
Train: 304 [1150/1251 ( 92%)]  Loss: 2.538 (2.78)  Time: 0.300s, 3417.83/s  (0.299s, 3420.75/s)  LR: 5.000e-06  Data: 0.024 (0.025)
Train: 304 [1200/1251 ( 96%)]  Loss: 2.684 (2.77)  Time: 0.299s, 3420.45/s  (0.299s, 3420.33/s)  LR: 5.000e-06  Data: 0.024 (0.025)
Train: 304 [1250/1251 (100%)]  Loss: 2.979 (2.78)  Time: 0.276s, 3714.86/s  (0.299s, 3421.36/s)  LR: 5.000e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.096 (2.096)  Loss:  0.4644 (0.4644)  Acc@1: 92.1875 (92.1875)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.052 (0.235)  Loss:  0.6367 (0.9113)  Acc@1: 85.6132 (80.0040)  Acc@5: 97.7594 (94.7680)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-304.pth.tar', 80.00400008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-302.pth.tar', 79.98800013671875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-296.pth.tar', 79.98400011230468)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-301.pth.tar', 79.9560001123047)

Train: 305 [   0/1251 (  0%)]  Loss: 2.682 (2.68)  Time: 2.198s,  465.81/s  (2.198s,  465.81/s)  LR: 5.000e-06  Data: 1.971 (1.971)
Train: 305 [  50/1251 (  4%)]  Loss: 2.887 (2.78)  Time: 0.294s, 3482.79/s  (0.326s, 3143.79/s)  LR: 5.000e-06  Data: 0.024 (0.063)
Train: 305 [ 100/1251 (  8%)]  Loss: 2.592 (2.72)  Time: 0.296s, 3458.44/s  (0.311s, 3291.98/s)  LR: 5.000e-06  Data: 0.022 (0.043)
Train: 305 [ 150/1251 ( 12%)]  Loss: 2.851 (2.75)  Time: 0.307s, 3331.66/s  (0.306s, 3345.50/s)  LR: 5.000e-06  Data: 0.023 (0.037)
Train: 305 [ 200/1251 ( 16%)]  Loss: 2.917 (2.79)  Time: 0.301s, 3403.86/s  (0.304s, 3369.65/s)  LR: 5.000e-06  Data: 0.021 (0.033)
Train: 305 [ 250/1251 ( 20%)]  Loss: 2.845 (2.80)  Time: 0.303s, 3374.18/s  (0.303s, 3382.54/s)  LR: 5.000e-06  Data: 0.023 (0.032)
Train: 305 [ 300/1251 ( 24%)]  Loss: 2.953 (2.82)  Time: 0.296s, 3458.00/s  (0.302s, 3390.36/s)  LR: 5.000e-06  Data: 0.022 (0.030)
Train: 305 [ 350/1251 ( 28%)]  Loss: 2.740 (2.81)  Time: 0.299s, 3429.01/s  (0.301s, 3397.32/s)  LR: 5.000e-06  Data: 0.022 (0.029)
Train: 305 [ 400/1251 ( 32%)]  Loss: 2.908 (2.82)  Time: 0.303s, 3380.51/s  (0.301s, 3399.44/s)  LR: 5.000e-06  Data: 0.026 (0.028)
Train: 305 [ 450/1251 ( 36%)]  Loss: 2.622 (2.80)  Time: 0.301s, 3406.15/s  (0.301s, 3402.03/s)  LR: 5.000e-06  Data: 0.025 (0.028)
Train: 305 [ 500/1251 ( 40%)]  Loss: 2.623 (2.78)  Time: 0.297s, 3450.54/s  (0.301s, 3403.07/s)  LR: 5.000e-06  Data: 0.022 (0.027)
Train: 305 [ 550/1251 ( 44%)]  Loss: 2.637 (2.77)  Time: 0.302s, 3387.35/s  (0.301s, 3403.56/s)  LR: 5.000e-06  Data: 0.021 (0.027)
Train: 305 [ 600/1251 ( 48%)]  Loss: 2.615 (2.76)  Time: 0.300s, 3410.20/s  (0.301s, 3403.98/s)  LR: 5.000e-06  Data: 0.023 (0.027)
Train: 305 [ 650/1251 ( 52%)]  Loss: 2.833 (2.76)  Time: 0.296s, 3455.73/s  (0.301s, 3404.02/s)  LR: 5.000e-06  Data: 0.022 (0.027)
Train: 305 [ 700/1251 ( 56%)]  Loss: 2.683 (2.76)  Time: 0.304s, 3367.06/s  (0.301s, 3404.06/s)  LR: 5.000e-06  Data: 0.021 (0.026)
Train: 305 [ 750/1251 ( 60%)]  Loss: 2.810 (2.76)  Time: 0.301s, 3404.43/s  (0.301s, 3404.51/s)  LR: 5.000e-06  Data: 0.023 (0.026)
Train: 305 [ 800/1251 ( 64%)]  Loss: 2.957 (2.77)  Time: 0.301s, 3400.06/s  (0.301s, 3404.20/s)  LR: 5.000e-06  Data: 0.024 (0.026)
Train: 305 [ 850/1251 ( 68%)]  Loss: 2.786 (2.77)  Time: 0.300s, 3411.03/s  (0.301s, 3403.39/s)  LR: 5.000e-06  Data: 0.022 (0.026)
Train: 305 [ 900/1251 ( 72%)]  Loss: 2.903 (2.78)  Time: 0.300s, 3417.74/s  (0.301s, 3403.97/s)  LR: 5.000e-06  Data: 0.022 (0.026)
Train: 305 [ 950/1251 ( 76%)]  Loss: 2.726 (2.78)  Time: 0.304s, 3372.92/s  (0.301s, 3404.14/s)  LR: 5.000e-06  Data: 0.026 (0.026)
Train: 305 [1000/1251 ( 80%)]  Loss: 2.903 (2.78)  Time: 0.302s, 3392.69/s  (0.301s, 3404.08/s)  LR: 5.000e-06  Data: 0.023 (0.026)
Train: 305 [1050/1251 ( 84%)]  Loss: 2.626 (2.78)  Time: 0.302s, 3390.57/s  (0.301s, 3404.23/s)  LR: 5.000e-06  Data: 0.027 (0.025)
Train: 305 [1100/1251 ( 88%)]  Loss: 2.926 (2.78)  Time: 0.303s, 3384.28/s  (0.301s, 3404.35/s)  LR: 5.000e-06  Data: 0.023 (0.025)
Train: 305 [1150/1251 ( 92%)]  Loss: 2.703 (2.78)  Time: 0.302s, 3394.96/s  (0.301s, 3403.58/s)  LR: 5.000e-06  Data: 0.023 (0.025)
Train: 305 [1200/1251 ( 96%)]  Loss: 3.141 (2.79)  Time: 0.301s, 3397.71/s  (0.301s, 3403.72/s)  LR: 5.000e-06  Data: 0.027 (0.025)
Train: 305 [1250/1251 (100%)]  Loss: 2.969 (2.80)  Time: 0.275s, 3728.59/s  (0.301s, 3405.64/s)  LR: 5.000e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.967 (1.967)  Loss:  0.4612 (0.4612)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.047 (0.234)  Loss:  0.6353 (0.9072)  Acc@1: 85.4953 (80.0160)  Acc@5: 97.7594 (94.7960)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-305.pth.tar', 80.01600003417968)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-304.pth.tar', 80.00400008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-302.pth.tar', 79.98800013671875)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-296.pth.tar', 79.98400011230468)

Train: 306 [   0/1251 (  0%)]  Loss: 2.670 (2.67)  Time: 2.298s,  445.52/s  (2.298s,  445.52/s)  LR: 5.000e-06  Data: 2.064 (2.064)
Train: 306 [  50/1251 (  4%)]  Loss: 2.486 (2.58)  Time: 0.291s, 3513.74/s  (0.318s, 3219.53/s)  LR: 5.000e-06  Data: 0.024 (0.065)
Train: 306 [ 100/1251 (  8%)]  Loss: 2.727 (2.63)  Time: 0.291s, 3522.57/s  (0.306s, 3349.93/s)  LR: 5.000e-06  Data: 0.020 (0.045)
Train: 306 [ 150/1251 ( 12%)]  Loss: 2.793 (2.67)  Time: 0.299s, 3422.89/s  (0.303s, 3384.78/s)  LR: 5.000e-06  Data: 0.020 (0.037)
Train: 306 [ 200/1251 ( 16%)]  Loss: 2.686 (2.67)  Time: 0.302s, 3386.08/s  (0.301s, 3396.63/s)  LR: 5.000e-06  Data: 0.022 (0.034)
Train: 306 [ 250/1251 ( 20%)]  Loss: 2.741 (2.68)  Time: 0.298s, 3435.39/s  (0.301s, 3403.46/s)  LR: 5.000e-06  Data: 0.021 (0.032)
Train: 306 [ 300/1251 ( 24%)]  Loss: 2.607 (2.67)  Time: 0.306s, 3349.69/s  (0.301s, 3407.12/s)  LR: 5.000e-06  Data: 0.026 (0.031)
Train: 306 [ 350/1251 ( 28%)]  Loss: 2.877 (2.70)  Time: 0.309s, 3315.96/s  (0.300s, 3410.29/s)  LR: 5.000e-06  Data: 0.028 (0.030)
Train: 306 [ 400/1251 ( 32%)]  Loss: 2.909 (2.72)  Time: 0.304s, 3372.59/s  (0.300s, 3409.52/s)  LR: 5.000e-06  Data: 0.023 (0.029)
Train: 306 [ 450/1251 ( 36%)]  Loss: 2.813 (2.73)  Time: 0.294s, 3480.67/s  (0.300s, 3409.19/s)  LR: 5.000e-06  Data: 0.020 (0.028)
Train: 306 [ 500/1251 ( 40%)]  Loss: 3.120 (2.77)  Time: 0.303s, 3374.42/s  (0.300s, 3409.31/s)  LR: 5.000e-06  Data: 0.022 (0.028)
Train: 306 [ 550/1251 ( 44%)]  Loss: 2.526 (2.75)  Time: 0.300s, 3409.19/s  (0.300s, 3408.62/s)  LR: 5.000e-06  Data: 0.022 (0.027)
Train: 306 [ 600/1251 ( 48%)]  Loss: 2.978 (2.76)  Time: 0.301s, 3404.26/s  (0.301s, 3407.29/s)  LR: 5.000e-06  Data: 0.024 (0.027)
Train: 306 [ 650/1251 ( 52%)]  Loss: 2.866 (2.77)  Time: 0.307s, 3339.85/s  (0.301s, 3406.11/s)  LR: 5.000e-06  Data: 0.024 (0.027)
Train: 306 [ 700/1251 ( 56%)]  Loss: 2.470 (2.75)  Time: 0.303s, 3382.15/s  (0.301s, 3405.24/s)  LR: 5.000e-06  Data: 0.022 (0.027)
Train: 306 [ 750/1251 ( 60%)]  Loss: 2.651 (2.75)  Time: 0.303s, 3379.62/s  (0.301s, 3404.59/s)  LR: 5.000e-06  Data: 0.023 (0.026)
Train: 306 [ 800/1251 ( 64%)]  Loss: 2.909 (2.75)  Time: 0.300s, 3408.04/s  (0.301s, 3403.74/s)  LR: 5.000e-06  Data: 0.022 (0.026)
Train: 306 [ 850/1251 ( 68%)]  Loss: 2.861 (2.76)  Time: 0.301s, 3401.39/s  (0.301s, 3402.69/s)  LR: 5.000e-06  Data: 0.020 (0.026)
Train: 306 [ 900/1251 ( 72%)]  Loss: 3.010 (2.77)  Time: 0.303s, 3378.49/s  (0.301s, 3401.92/s)  LR: 5.000e-06  Data: 0.026 (0.026)
Train: 306 [ 950/1251 ( 76%)]  Loss: 2.785 (2.77)  Time: 0.302s, 3392.49/s  (0.301s, 3401.14/s)  LR: 5.000e-06  Data: 0.019 (0.026)
Train: 306 [1000/1251 ( 80%)]  Loss: 2.974 (2.78)  Time: 0.302s, 3386.06/s  (0.301s, 3399.77/s)  LR: 5.000e-06  Data: 0.025 (0.026)
Train: 306 [1050/1251 ( 84%)]  Loss: 2.666 (2.78)  Time: 0.305s, 3360.86/s  (0.301s, 3398.65/s)  LR: 5.000e-06  Data: 0.020 (0.026)
Train: 306 [1100/1251 ( 88%)]  Loss: 2.777 (2.78)  Time: 0.297s, 3444.82/s  (0.301s, 3397.98/s)  LR: 5.000e-06  Data: 0.018 (0.026)
Train: 306 [1150/1251 ( 92%)]  Loss: 3.096 (2.79)  Time: 0.308s, 3326.16/s  (0.301s, 3397.16/s)  LR: 5.000e-06  Data: 0.024 (0.025)
Train: 306 [1200/1251 ( 96%)]  Loss: 2.777 (2.79)  Time: 0.304s, 3370.77/s  (0.301s, 3396.80/s)  LR: 5.000e-06  Data: 0.024 (0.025)
Train: 306 [1250/1251 (100%)]  Loss: 2.357 (2.77)  Time: 0.276s, 3714.75/s  (0.301s, 3397.96/s)  LR: 5.000e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.031 (2.031)  Loss:  0.4668 (0.4668)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.053 (0.238)  Loss:  0.6294 (0.9134)  Acc@1: 85.6132 (79.9300)  Acc@5: 97.9953 (94.7960)
Train: 307 [   0/1251 (  0%)]  Loss: 2.972 (2.97)  Time: 2.190s,  467.51/s  (2.190s,  467.51/s)  LR: 5.000e-06  Data: 1.962 (1.962)
Train: 307 [  50/1251 (  4%)]  Loss: 2.749 (2.86)  Time: 0.292s, 3501.20/s  (0.322s, 3184.36/s)  LR: 5.000e-06  Data: 0.022 (0.064)
Train: 307 [ 100/1251 (  8%)]  Loss: 2.808 (2.84)  Time: 0.297s, 3453.03/s  (0.308s, 3320.59/s)  LR: 5.000e-06  Data: 0.022 (0.044)
Train: 307 [ 150/1251 ( 12%)]  Loss: 2.937 (2.87)  Time: 0.302s, 3395.09/s  (0.305s, 3356.42/s)  LR: 5.000e-06  Data: 0.021 (0.037)
Train: 307 [ 200/1251 ( 16%)]  Loss: 2.893 (2.87)  Time: 0.297s, 3446.28/s  (0.304s, 3371.85/s)  LR: 5.000e-06  Data: 0.022 (0.034)
Train: 307 [ 250/1251 ( 20%)]  Loss: 3.042 (2.90)  Time: 0.299s, 3424.88/s  (0.303s, 3379.17/s)  LR: 5.000e-06  Data: 0.022 (0.032)
Train: 307 [ 300/1251 ( 24%)]  Loss: 2.940 (2.91)  Time: 0.302s, 3395.10/s  (0.303s, 3383.58/s)  LR: 5.000e-06  Data: 0.022 (0.030)
Train: 307 [ 350/1251 ( 28%)]  Loss: 2.957 (2.91)  Time: 0.300s, 3418.02/s  (0.302s, 3386.16/s)  LR: 5.000e-06  Data: 0.022 (0.029)
Train: 307 [ 400/1251 ( 32%)]  Loss: 2.775 (2.90)  Time: 0.301s, 3396.46/s  (0.302s, 3388.14/s)  LR: 5.000e-06  Data: 0.027 (0.029)
Train: 307 [ 450/1251 ( 36%)]  Loss: 2.959 (2.90)  Time: 0.301s, 3399.56/s  (0.302s, 3387.83/s)  LR: 5.000e-06  Data: 0.026 (0.028)
Train: 307 [ 500/1251 ( 40%)]  Loss: 3.058 (2.92)  Time: 0.303s, 3374.33/s  (0.302s, 3388.12/s)  LR: 5.000e-06  Data: 0.021 (0.028)
Train: 307 [ 550/1251 ( 44%)]  Loss: 2.541 (2.89)  Time: 0.307s, 3340.21/s  (0.302s, 3387.83/s)  LR: 5.000e-06  Data: 0.021 (0.027)
Train: 307 [ 600/1251 ( 48%)]  Loss: 2.932 (2.89)  Time: 0.300s, 3410.35/s  (0.302s, 3387.47/s)  LR: 5.000e-06  Data: 0.025 (0.027)
Train: 307 [ 650/1251 ( 52%)]  Loss: 2.826 (2.88)  Time: 0.305s, 3358.32/s  (0.302s, 3387.24/s)  LR: 5.000e-06  Data: 0.022 (0.027)
Train: 307 [ 700/1251 ( 56%)]  Loss: 2.805 (2.88)  Time: 0.306s, 3346.90/s  (0.302s, 3386.31/s)  LR: 5.000e-06  Data: 0.022 (0.026)
Train: 307 [ 750/1251 ( 60%)]  Loss: 2.672 (2.87)  Time: 0.306s, 3346.27/s  (0.302s, 3385.22/s)  LR: 5.000e-06  Data: 0.024 (0.026)
Train: 307 [ 800/1251 ( 64%)]  Loss: 2.953 (2.87)  Time: 0.299s, 3427.46/s  (0.302s, 3385.13/s)  LR: 5.000e-06  Data: 0.022 (0.026)
Train: 307 [ 850/1251 ( 68%)]  Loss: 2.962 (2.88)  Time: 0.305s, 3354.01/s  (0.303s, 3385.08/s)  LR: 5.000e-06  Data: 0.024 (0.026)
Train: 307 [ 900/1251 ( 72%)]  Loss: 2.754 (2.87)  Time: 0.305s, 3352.28/s  (0.303s, 3385.01/s)  LR: 5.000e-06  Data: 0.025 (0.026)
Train: 307 [ 950/1251 ( 76%)]  Loss: 2.896 (2.87)  Time: 0.306s, 3349.35/s  (0.303s, 3384.61/s)  LR: 5.000e-06  Data: 0.023 (0.026)
Train: 307 [1000/1251 ( 80%)]  Loss: 2.793 (2.87)  Time: 0.304s, 3366.31/s  (0.303s, 3384.37/s)  LR: 5.000e-06  Data: 0.026 (0.026)
Train: 307 [1050/1251 ( 84%)]  Loss: 2.675 (2.86)  Time: 0.309s, 3316.09/s  (0.303s, 3383.87/s)  LR: 5.000e-06  Data: 0.026 (0.025)
Train: 307 [1100/1251 ( 88%)]  Loss: 2.700 (2.85)  Time: 0.302s, 3391.78/s  (0.303s, 3383.50/s)  LR: 5.000e-06  Data: 0.025 (0.025)
Train: 307 [1150/1251 ( 92%)]  Loss: 2.817 (2.85)  Time: 0.301s, 3402.07/s  (0.303s, 3383.16/s)  LR: 5.000e-06  Data: 0.022 (0.025)
Train: 307 [1200/1251 ( 96%)]  Loss: 2.916 (2.85)  Time: 0.303s, 3378.39/s  (0.303s, 3382.85/s)  LR: 5.000e-06  Data: 0.023 (0.025)
Train: 307 [1250/1251 (100%)]  Loss: 2.811 (2.85)  Time: 0.277s, 3697.59/s  (0.303s, 3384.05/s)  LR: 5.000e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.990 (1.990)  Loss:  0.4639 (0.4639)  Acc@1: 92.2852 (92.2852)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.046 (0.233)  Loss:  0.6309 (0.9116)  Acc@1: 85.0236 (79.9280)  Acc@5: 97.9953 (94.7760)
Train: 308 [   0/1251 (  0%)]  Loss: 2.846 (2.85)  Time: 1.979s,  517.50/s  (1.979s,  517.50/s)  LR: 5.000e-06  Data: 1.746 (1.746)
Train: 308 [  50/1251 (  4%)]  Loss: 2.909 (2.88)  Time: 0.299s, 3420.20/s  (0.320s, 3200.99/s)  LR: 5.000e-06  Data: 0.027 (0.059)
Train: 308 [ 100/1251 (  8%)]  Loss: 2.773 (2.84)  Time: 0.292s, 3510.43/s  (0.309s, 3317.48/s)  LR: 5.000e-06  Data: 0.021 (0.041)
Train: 308 [ 150/1251 ( 12%)]  Loss: 2.924 (2.86)  Time: 0.305s, 3357.21/s  (0.306s, 3348.70/s)  LR: 5.000e-06  Data: 0.025 (0.035)
Train: 308 [ 200/1251 ( 16%)]  Loss: 2.755 (2.84)  Time: 0.297s, 3450.39/s  (0.305s, 3361.11/s)  LR: 5.000e-06  Data: 0.021 (0.032)
Train: 308 [ 250/1251 ( 20%)]  Loss: 2.481 (2.78)  Time: 0.297s, 3449.30/s  (0.304s, 3368.14/s)  LR: 5.000e-06  Data: 0.020 (0.031)
Train: 308 [ 300/1251 ( 24%)]  Loss: 2.579 (2.75)  Time: 0.300s, 3411.31/s  (0.304s, 3370.97/s)  LR: 5.000e-06  Data: 0.027 (0.029)
Train: 308 [ 350/1251 ( 28%)]  Loss: 2.721 (2.75)  Time: 0.305s, 3361.26/s  (0.304s, 3371.89/s)  LR: 5.000e-06  Data: 0.026 (0.029)
Train: 308 [ 400/1251 ( 32%)]  Loss: 2.979 (2.77)  Time: 0.302s, 3389.46/s  (0.304s, 3372.24/s)  LR: 5.000e-06  Data: 0.023 (0.028)
Train: 308 [ 450/1251 ( 36%)]  Loss: 3.081 (2.80)  Time: 0.303s, 3379.93/s  (0.304s, 3373.19/s)  LR: 5.000e-06  Data: 0.023 (0.027)
Train: 308 [ 500/1251 ( 40%)]  Loss: 2.967 (2.82)  Time: 0.303s, 3384.52/s  (0.304s, 3373.37/s)  LR: 5.000e-06  Data: 0.022 (0.027)
Train: 308 [ 550/1251 ( 44%)]  Loss: 2.769 (2.82)  Time: 0.307s, 3331.82/s  (0.304s, 3372.50/s)  LR: 5.000e-06  Data: 0.027 (0.027)
Train: 308 [ 600/1251 ( 48%)]  Loss: 3.027 (2.83)  Time: 0.307s, 3333.44/s  (0.304s, 3371.67/s)  LR: 5.000e-06  Data: 0.022 (0.026)
Train: 308 [ 650/1251 ( 52%)]  Loss: 3.157 (2.85)  Time: 0.310s, 3303.43/s  (0.304s, 3370.53/s)  LR: 5.000e-06  Data: 0.022 (0.026)
Train: 308 [ 700/1251 ( 56%)]  Loss: 2.795 (2.85)  Time: 0.305s, 3354.86/s  (0.304s, 3369.72/s)  LR: 5.000e-06  Data: 0.022 (0.026)
Train: 308 [ 750/1251 ( 60%)]  Loss: 2.840 (2.85)  Time: 0.301s, 3405.26/s  (0.304s, 3369.95/s)  LR: 5.000e-06  Data: 0.022 (0.026)
Train: 308 [ 800/1251 ( 64%)]  Loss: 2.900 (2.85)  Time: 0.307s, 3330.61/s  (0.304s, 3369.40/s)  LR: 5.000e-06  Data: 0.023 (0.026)
Train: 308 [ 850/1251 ( 68%)]  Loss: 2.651 (2.84)  Time: 0.306s, 3342.84/s  (0.304s, 3368.30/s)  LR: 5.000e-06  Data: 0.025 (0.026)
Train: 308 [ 900/1251 ( 72%)]  Loss: 2.872 (2.84)  Time: 0.309s, 3317.04/s  (0.304s, 3367.72/s)  LR: 5.000e-06  Data: 0.023 (0.025)
Train: 308 [ 950/1251 ( 76%)]  Loss: 2.492 (2.83)  Time: 0.304s, 3370.23/s  (0.304s, 3366.95/s)  LR: 5.000e-06  Data: 0.021 (0.025)
Train: 308 [1000/1251 ( 80%)]  Loss: 2.869 (2.83)  Time: 0.306s, 3344.58/s  (0.304s, 3366.17/s)  LR: 5.000e-06  Data: 0.024 (0.025)
Train: 308 [1050/1251 ( 84%)]  Loss: 2.816 (2.83)  Time: 0.306s, 3345.35/s  (0.304s, 3365.28/s)  LR: 5.000e-06  Data: 0.021 (0.025)
Train: 308 [1100/1251 ( 88%)]  Loss: 2.653 (2.82)  Time: 0.312s, 3283.20/s  (0.304s, 3364.04/s)  LR: 5.000e-06  Data: 0.022 (0.025)
Train: 308 [1150/1251 ( 92%)]  Loss: 2.614 (2.81)  Time: 0.304s, 3369.96/s  (0.304s, 3363.18/s)  LR: 5.000e-06  Data: 0.023 (0.025)
Train: 308 [1200/1251 ( 96%)]  Loss: 3.075 (2.82)  Time: 0.300s, 3412.25/s  (0.305s, 3362.44/s)  LR: 5.000e-06  Data: 0.023 (0.025)
Train: 308 [1250/1251 (100%)]  Loss: 2.766 (2.82)  Time: 0.276s, 3706.52/s  (0.304s, 3363.79/s)  LR: 5.000e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.999 (1.999)  Loss:  0.4626 (0.4626)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.051 (0.243)  Loss:  0.6299 (0.9080)  Acc@1: 85.2594 (79.9480)  Acc@5: 97.8774 (94.8040)
Train: 309 [   0/1251 (  0%)]  Loss: 2.745 (2.75)  Time: 1.933s,  529.75/s  (1.933s,  529.75/s)  LR: 5.000e-06  Data: 1.693 (1.693)
Train: 309 [  50/1251 (  4%)]  Loss: 2.320 (2.53)  Time: 0.299s, 3424.84/s  (0.320s, 3196.60/s)  LR: 5.000e-06  Data: 0.022 (0.058)
Train: 309 [ 100/1251 (  8%)]  Loss: 2.460 (2.51)  Time: 0.305s, 3352.75/s  (0.310s, 3307.19/s)  LR: 5.000e-06  Data: 0.025 (0.041)
Train: 309 [ 150/1251 ( 12%)]  Loss: 2.698 (2.56)  Time: 0.307s, 3334.44/s  (0.307s, 3334.86/s)  LR: 5.000e-06  Data: 0.024 (0.035)
Train: 309 [ 200/1251 ( 16%)]  Loss: 3.004 (2.65)  Time: 0.304s, 3369.66/s  (0.306s, 3346.66/s)  LR: 5.000e-06  Data: 0.026 (0.032)
Train: 309 [ 250/1251 ( 20%)]  Loss: 2.918 (2.69)  Time: 0.308s, 3319.90/s  (0.306s, 3348.26/s)  LR: 5.000e-06  Data: 0.022 (0.030)
Train: 309 [ 300/1251 ( 24%)]  Loss: 2.770 (2.70)  Time: 0.308s, 3324.03/s  (0.306s, 3350.45/s)  LR: 5.000e-06  Data: 0.023 (0.029)
Train: 309 [ 350/1251 ( 28%)]  Loss: 2.639 (2.69)  Time: 0.311s, 3291.83/s  (0.305s, 3352.84/s)  LR: 5.000e-06  Data: 0.024 (0.028)
Train: 309 [ 400/1251 ( 32%)]  Loss: 2.788 (2.70)  Time: 0.307s, 3336.22/s  (0.305s, 3352.77/s)  LR: 5.000e-06  Data: 0.028 (0.028)
Train: 309 [ 450/1251 ( 36%)]  Loss: 3.031 (2.74)  Time: 0.304s, 3364.41/s  (0.305s, 3352.79/s)  LR: 5.000e-06  Data: 0.025 (0.027)
Train: 309 [ 500/1251 ( 40%)]  Loss: 2.874 (2.75)  Time: 0.304s, 3367.18/s  (0.305s, 3352.72/s)  LR: 5.000e-06  Data: 0.022 (0.027)
Train: 309 [ 550/1251 ( 44%)]  Loss: 2.629 (2.74)  Time: 0.307s, 3338.95/s  (0.306s, 3351.47/s)  LR: 5.000e-06  Data: 0.022 (0.027)
Train: 309 [ 600/1251 ( 48%)]  Loss: 2.378 (2.71)  Time: 0.309s, 3313.28/s  (0.306s, 3351.18/s)  LR: 5.000e-06  Data: 0.024 (0.026)
Train: 309 [ 650/1251 ( 52%)]  Loss: 2.956 (2.73)  Time: 0.311s, 3295.97/s  (0.306s, 3351.12/s)  LR: 5.000e-06  Data: 0.022 (0.026)
Train: 309 [ 700/1251 ( 56%)]  Loss: 2.781 (2.73)  Time: 0.302s, 3387.58/s  (0.306s, 3349.94/s)  LR: 5.000e-06  Data: 0.023 (0.026)
Train: 309 [ 750/1251 ( 60%)]  Loss: 2.920 (2.74)  Time: 0.308s, 3319.74/s  (0.306s, 3349.36/s)  LR: 5.000e-06  Data: 0.022 (0.026)
Train: 309 [ 800/1251 ( 64%)]  Loss: 2.642 (2.74)  Time: 0.310s, 3302.86/s  (0.306s, 3349.22/s)  LR: 5.000e-06  Data: 0.023 (0.026)
Train: 309 [ 850/1251 ( 68%)]  Loss: 3.002 (2.75)  Time: 0.308s, 3328.66/s  (0.306s, 3348.85/s)  LR: 5.000e-06  Data: 0.024 (0.025)
Train: 309 [ 900/1251 ( 72%)]  Loss: 2.770 (2.75)  Time: 0.305s, 3352.00/s  (0.306s, 3348.18/s)  LR: 5.000e-06  Data: 0.022 (0.025)
Train: 309 [ 950/1251 ( 76%)]  Loss: 2.877 (2.76)  Time: 0.308s, 3320.42/s  (0.306s, 3347.65/s)  LR: 5.000e-06  Data: 0.023 (0.025)
Train: 309 [1000/1251 ( 80%)]  Loss: 2.735 (2.76)  Time: 0.310s, 3298.90/s  (0.306s, 3346.58/s)  LR: 5.000e-06  Data: 0.027 (0.025)
Train: 309 [1050/1251 ( 84%)]  Loss: 2.868 (2.76)  Time: 0.307s, 3332.25/s  (0.306s, 3346.17/s)  LR: 5.000e-06  Data: 0.023 (0.025)
Train: 309 [1100/1251 ( 88%)]  Loss: 2.866 (2.77)  Time: 0.305s, 3355.34/s  (0.306s, 3345.88/s)  LR: 5.000e-06  Data: 0.022 (0.025)
Train: 309 [1150/1251 ( 92%)]  Loss: 2.843 (2.77)  Time: 0.313s, 3275.69/s  (0.306s, 3345.23/s)  LR: 5.000e-06  Data: 0.023 (0.025)
Train: 309 [1200/1251 ( 96%)]  Loss: 2.776 (2.77)  Time: 0.308s, 3329.56/s  (0.306s, 3344.73/s)  LR: 5.000e-06  Data: 0.022 (0.025)
Train: 309 [1250/1251 (100%)]  Loss: 2.825 (2.77)  Time: 0.275s, 3721.15/s  (0.306s, 3346.38/s)  LR: 5.000e-06  Data: 0.000 (0.025)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.024 (2.024)  Loss:  0.4702 (0.4702)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.042 (0.234)  Loss:  0.6406 (0.9151)  Acc@1: 85.6132 (79.9960)  Acc@5: 97.8774 (94.7980)
Current checkpoints:
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-305.pth.tar', 80.01600003417968)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-304.pth.tar', 80.00400008544922)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-309.pth.tar', 79.99600008544923)
 ('/workspace/akane/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224/checkpoint-302.pth.tar', 79.98800013671875)

*** Best metric: 80.01600003417968 (epoch 305)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: best_metric ▁
wandb:       epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:   eval_loss █▅▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   eval_top1 ▁▃▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████
wandb:   eval_top5 ▁▄▆▇▇▇▇▇▇▇▇▇████████████████████████████
wandb:  train_loss █▇▅▅▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: best_metric 80.016
wandb:       epoch 309
wandb:   eval_loss 0.91512
wandb:   eval_top1 79.996
wandb:   eval_top5 94.798
wandb:  train_loss 2.77367
wandb: 
wandb: 🚀 View run vit_small_patch16_224_augreg_in21k at: https://wandb.ai/compyle/wintome-dinat-s-IN1k/runs/20231202-173652-vit_small_patch16_224_augreg_in21k-224
wandb: ️⚡ View job at https://wandb.ai/compyle/wintome-dinat-s-IN1k/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNTMyNzI4Ng==/version_details/v13
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20231202_173654-20231202-173652-vit_small_patch16_224_augreg_in21k-224/logs

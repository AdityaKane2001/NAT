/home/users/akane/miniconda3/envs/nattome/lib/python3.10/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 7
Added key: store_based_barrier_key:1 to store for rank: 5
Added key: store_based_barrier_key:1 to store for rank: 6
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 1
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
WARNING: Unsupported operator aten::mul encountered 37 time(s)
WARNING: Unsupported operator aten::softmax encountered 12 time(s)
WARNING: Unsupported operator aten::add encountered 52 time(s)
WARNING: Unsupported operator aten::gelu encountered 12 time(s)
WARNING: Unsupported operator aten::rand encountered 22 time(s)
WARNING: Unsupported operator aten::floor_ encountered 22 time(s)
WARNING: Unsupported operator aten::div encountered 22 time(s)
WARNING: Unsupported operator aten::adaptive_avg_pool1d encountered 1 time(s)
Model nat_s_tiny created.
28.288M Params and 4.511GFLOPs
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
Using native Torch AMP. Training in mixed precision.
Using native Torch DistributedDataParallel.
Scheduled epochs: 310
wandb: Currently logged in as: compyle. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /workspace/akane/NAT/classification/wandb/run-20231009_134641-20231009-134638-nat_s_tiny-224
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run nat_s_tiny
wandb: ‚≠êÔ∏è View project at https://wandb.ai/compyle/wintome-dinat-s-IN1k
wandb: üöÄ View run at https://wandb.ai/compyle/wintome-dinat-s-IN1k/runs/20231009-134638-nat_s_tiny-224
Train: 0 [   0/1251 (  0%)]  Loss: 6.970 (6.97)  Time: 2.120s,  483.12/s  (2.120s,  483.12/s)  LR: 1.040e-06  Data: 1.320 (1.320)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this i0teration.
Train: 0 [  50/1251 (  4%)]  Loss: 6.943 (6.96)  Time: 0.404s, 2533.08/s  (0.451s, 2271.34/s)  LR: 3.036e-06  Data: 0.017 (0.041)
Train: 0 [ 100/1251 (  8%)]  Loss: 6.940 (6.95)  Time: 0.469s, 2181.46/s  (0.449s, 2283.06/s)  LR: 5.033e-06  Data: 0.013 (0.028)
Train: 0 [ 150/1251 ( 12%)]  Loss: 6.923 (6.94)  Time: 0.521s, 1963.97/s  (0.464s, 2208.04/s)  LR: 7.029e-06  Data: 0.014 (0.024)
Train: 0 [ 200/1251 ( 16%)]  Loss: 6.907 (6.94)  Time: 0.557s, 1839.90/s  (0.483s, 2120.92/s)  LR: 9.026e-06  Data: 0.014 (0.021)Train: 0 [ 250/1251 ( 20%)]  Loss: 6.906 (6.93)  Time: 0.590s, 1735.78/s  (0.502s, 2041.69/s)  LR: 1.102e-05  Data: 0.016 (0.020)
Train: 0 [ 300/1251 ( 24%)]  Loss: 6.885 (6.92)  Time: 0.609s, 1680.14/s  (0.519s, 1974.45/s)  LR: 1.302e-05  Data: 0.014 (0.019)
Train: 0 [ 350/1251 ( 28%)]  Loss: 6.884 (6.92)  Time: 0.626s, 1634.79/s  (0.533s, 1920.83/s)  LR: 1.501e-05  Data: 0.012 (0.018)
Train: 0 [ 400/1251 ( 32%)]  Loss: 6.878 (6.92)  Time: 0.652s, 1571.62/s  (0.546s, 1874.34/s)  LR: 1.701e-05  Data: 0.013 (0.018)
Train: 0 [ 450/1251 ( 36%)]  Loss: 6.866 (6.91)  Time: 0.659s, 1552.90/s  (0.558s, 1835.41/s)  LR: 1.901e-05  Data: 0.013 (0.017)
Train: 0 [ 500/1251 ( 40%)]  Loss: 6.866 (6.91)  Time: 0.665s, 1540.71/s  (0.568s, 1802.67/s)  LR: 2.100e-05  Data: 0.016 (0.017)
Train: 0 [ 550/1251 ( 44%)]  Loss: 6.837 (6.90)  Time: 0.682s, 1500.86/s  (0.577s, 1774.16/s)  LR: 2.300e-05  Data: 0.013 (0.017)
Train: 0 [ 600/1251 ( 48%)]  Loss: 6.854 (6.90)  Time: 0.682s, 1501.85/s  (0.585s, 1748.99/s)  LR: 2.500e-05  Data: 0.013 (0.017)
Train: 0 [ 650/1251 ( 52%)]  Loss: 6.852 (6.89)  Time: 0.678s, 1510.79/s  (0.593s, 1727.35/s)  LR: 2.699e-05  Data: 0.015 (0.016)
Train: 0 [ 700/1251 ( 56%)]  Loss: 6.839 (6.89)  Time: 0.694s, 1475.99/s  (0.599s, 1708.57/s)  LR: 2.899e-05  Data: 0.017 (0.016)
Train: 0 [ 750/1251 ( 60%)]  Loss: 6.838 (6.89)  Time: 0.693s, 1478.14/s  (0.605s, 1691.93/s)  LR: 3.099e-05  Data: 0.014 (0.016)
Train: 0 [ 800/1251 ( 64%)]  Loss: 6.825 (6.88)  Time: 0.688s, 1488.91/s  (0.610s, 1677.66/s)  LR: 3.298e-05  Data: 0.014 (0.016)
Train: 0 [ 850/1251 ( 68%)]  Loss: 6.809 (6.88)  Time: 0.683s, 1498.28/s  (0.615s, 1664.68/s)  LR: 3.498e-05  Data: 0.015 (0.016)
Train: 0 [ 900/1251 ( 72%)]  Loss: 6.782 (6.87)  Time: 0.696s, 1470.24/s  (0.619s, 1653.19/s)  LR: 3.698e-05  Data: 0.012 (0.016)
Train: 0 [ 950/1251 ( 76%)]  Loss: 6.809 (6.87)  Time: 0.698s, 1467.83/s  (0.623s, 1642.84/s)  LR: 3.897e-05  Data: 0.012 (0.016)
Train: 0 [1000/1251 ( 80%)]  Loss: 6.758 (6.87)  Time: 0.695s, 1473.20/s  (0.627s, 1633.56/s)  LR: 4.097e-05  Data: 0.013 (0.016)
Train: 0 [1050/1251 ( 84%)]  Loss: 6.746 (6.86)  Time: 0.699s, 1463.99/s  (0.630s, 1625.14/s)  LR: 4.296e-05  Data: 0.013 (0.015)
Train: 0 [1100/1251 ( 88%)]  Loss: 6.744 (6.85)  Time: 0.696s, 1470.76/s  (0.633s, 1617.54/s)  LR: 4.496e-05  Data: 0.014 (0.015)
Train: 0 [1150/1251 ( 92%)]  Loss: 6.716 (6.85)  Time: 0.692s, 1480.37/s  (0.636s, 1610.69/s)  LR: 4.696e-05  Data: 0.016 (0.015)
Train: 0 [1200/1251 ( 96%)]  Loss: 6.739 (6.84)  Time: 0.696s, 1470.28/s  (0.638s, 1604.28/s)  LR: 4.895e-05  Data: 0.013 (0.015)
Train: 0 [1250/1251 (100%)]  Loss: 6.720 (6.84)  Time: 0.685s, 1494.08/s  (0.641s, 1598.52/s)  LR: 5.095e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 4.466 (4.466)  Loss:  5.9727 (5.9727)  Acc@1:  1.0742 ( 1.0742)  Acc@5: 13.0859 (13.0859)
Test: [  48/48]  Time: 0.271 (0.367)  Loss:  5.6680 (6.3098)  Acc@1: 11.2028 ( 2.0520)  Acc@5: 23.9387 ( 6.8840)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-0.pth.tar', 2.052000018310547)

Train: 1 [   0/1251 (  0%)]  Loss: 6.635 (6.64)  Time: 3.758s,  272.48/s  (3.758s,  272.48/s)  LR: 5.099e-05  Data: 1.751 (1.751)
Train: 1 [  50/1251 (  4%)]  Loss: 6.635 (6.64)  Time: 0.669s, 1529.64/s  (0.723s, 1415.73/s)  LR: 5.299e-05  Data: 0.015 (0.049)
Train: 1 [ 100/1251 (  8%)]  Loss: 6.685 (6.65)  Time: 0.697s, 1469.76/s  (0.706s, 1451.19/s)  LR: 5.498e-05  Data: 0.017 (0.032)
Train: 1 [ 150/1251 ( 12%)]  Loss: 6.604 (6.64)  Time: 0.697s, 1469.48/s  (0.701s, 1460.30/s)  LR: 5.698e-05  Data: 0.013 (0.026)
Train: 1 [ 200/1251 ( 16%)]  Loss: 6.646 (6.64)  Time: 0.703s, 1457.22/s  (0.699s, 1464.33/s)  LR: 5.898e-05  Data: 0.013 (0.023)
Train: 1 [ 250/1251 ( 20%)]  Loss: 6.577 (6.63)  Time: 0.692s, 1480.79/s  (0.698s, 1467.39/s)  LR: 6.097e-05  Data: 0.012 (0.021)
Train: 1 [ 300/1251 ( 24%)]  Loss: 6.579 (6.62)  Time: 0.679s, 1508.93/s  (0.697s, 1469.04/s)  LR: 6.297e-05  Data: 0.013 (0.020)
Train: 1 [ 350/1251 ( 28%)]  Loss: 6.603 (6.62)  Time: 0.698s, 1467.04/s  (0.697s, 1470.19/s)  LR: 6.496e-05  Data: 0.012 (0.019)
Train: 1 [ 400/1251 ( 32%)]  Loss: 6.611 (6.62)  Time: 0.691s, 1481.36/s  (0.696s, 1471.32/s)  LR: 6.696e-05  Data: 0.013 (0.018)
Train: 1 [ 450/1251 ( 36%)]  Loss: 6.518 (6.61)  Time: 0.687s, 1491.19/s  (0.696s, 1471.31/s)  LR: 6.896e-05  Data: 0.013 (0.018)
Train: 1 [ 500/1251 ( 40%)]  Loss: 6.538 (6.60)  Time: 0.698s, 1467.30/s  (0.696s, 1471.26/s)  LR: 7.095e-05  Data: 0.013 (0.017)
Train: 1 [ 550/1251 ( 44%)]  Loss: 6.636 (6.61)  Time: 0.693s, 1478.59/s  (0.696s, 1470.91/s)  LR: 7.295e-05  Data: 0.013 (0.017)
Train: 1 [ 600/1251 ( 48%)]  Loss: 6.558 (6.60)  Time: 0.677s, 1512.06/s  (0.696s, 1470.88/s)  LR: 7.495e-05  Data: 0.014 (0.017)
Train: 1 [ 650/1251 ( 52%)]  Loss: 6.522 (6.60)  Time: 0.696s, 1470.89/s  (0.696s, 1470.85/s)  LR: 7.694e-05  Data: 0.013 (0.016)
Train: 1 [ 700/1251 ( 56%)]  Loss: 6.480 (6.59)  Time: 0.691s, 1481.63/s  (0.696s, 1470.94/s)  LR: 7.894e-05  Data: 0.012 (0.016)
Train: 1 [ 750/1251 ( 60%)]  Loss: 6.506 (6.58)  Time: 0.690s, 1483.01/s  (0.696s, 1471.10/s)  LR: 8.094e-05  Data: 0.011 (0.016)
Train: 1 [ 800/1251 ( 64%)]  Loss: 6.520 (6.58)  Time: 0.703s, 1456.08/s  (0.696s, 1471.18/s)  LR: 8.293e-05  Data: 0.013 (0.016)
Train: 1 [ 850/1251 ( 68%)]  Loss: 6.474 (6.57)  Time: 0.695s, 1473.42/s  (0.696s, 1471.31/s)  LR: 8.493e-05  Data: 0.012 (0.016)
Train: 1 [ 900/1251 ( 72%)]  Loss: 6.504 (6.57)  Time: 0.683s, 1499.21/s  (0.696s, 1471.33/s)  LR: 8.693e-05  Data: 0.013 (0.016)
Train: 1 [ 950/1251 ( 76%)]  Loss: 6.484 (6.57)  Time: 0.710s, 1442.77/s  (0.696s, 1470.96/s)  LR: 8.892e-05  Data: 0.014 (0.015)
Train: 1 [1000/1251 ( 80%)]  Loss: 6.558 (6.57)  Time: 0.692s, 1480.15/s  (0.696s, 1470.58/s)  LR: 9.092e-05  Data: 0.012 (0.015)
Train: 1 [1050/1251 ( 84%)]  Loss: 6.412 (6.56)  Time: 0.703s, 1457.08/s  (0.696s, 1470.45/s)  LR: 9.291e-05  Data: 0.014 (0.015)
Train: 1 [1100/1251 ( 88%)]  Loss: 6.379 (6.55)  Time: 0.696s, 1470.93/s  (0.697s, 1470.12/s)  LR: 9.491e-05  Data: 0.013 (0.015)
Train: 1 [1150/1251 ( 92%)]  Loss: 6.409 (6.54)  Time: 0.712s, 1438.34/s  (0.697s, 1469.81/s)  LR: 9.691e-05  Data: 0.016 (0.015)
Train: 1 [1200/1251 ( 96%)]  Loss: 6.512 (6.54)  Time: 0.689s, 1485.56/s  (0.697s, 1469.57/s)  LR: 9.890e-05  Data: 0.012 (0.015)
Train: 1 [1250/1251 (100%)]  Loss: 6.397 (6.54)  Time: 0.693s, 1478.63/s  (0.697s, 1469.47/s)  LR: 1.009e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.346 (3.346)  Loss:  5.3594 (5.3594)  Acc@1:  8.0078 ( 8.0078)  Acc@5: 22.7539 (22.7539)
Test: [  48/48]  Time: 0.180 (0.337)  Loss:  4.8555 (5.5310)  Acc@1: 20.1651 ( 6.4340)  Acc@5: 36.7925 (18.2120)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-1.pth.tar', 6.434000000610352)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-0.pth.tar', 2.052000018310547)

Train: 2 [   0/1251 (  0%)]  Loss: 6.285 (6.29)  Time: 3.466s,  295.41/s  (3.466s,  295.41/s)  LR: 1.009e-04  Data: 1.930 (1.930)
Train: 2 [  50/1251 (  4%)]  Loss: 6.377 (6.33)  Time: 0.691s, 1481.14/s  (0.722s, 1419.20/s)  LR: 1.029e-04  Data: 0.013 (0.051)
Train: 2 [ 100/1251 (  8%)]  Loss: 6.493 (6.39)  Time: 0.699s, 1465.23/s  (0.708s, 1446.70/s)  LR: 1.049e-04  Data: 0.012 (0.033)
Train: 2 [ 150/1251 ( 12%)]  Loss: 6.406 (6.39)  Time: 0.699s, 1465.08/s  (0.704s, 1455.28/s)  LR: 1.069e-04  Data: 0.013 (0.026)
Train: 2 [ 200/1251 ( 16%)]  Loss: 6.294 (6.37)  Time: 0.697s, 1469.45/s  (0.702s, 1458.87/s)  LR: 1.089e-04  Data: 0.013 (0.023)
Train: 2 [ 250/1251 ( 20%)]  Loss: 6.322 (6.36)  Time: 0.695s, 1473.57/s  (0.701s, 1461.23/s)  LR: 1.109e-04  Data: 0.013 (0.021)
Train: 2 [ 300/1251 ( 24%)]  Loss: 6.373 (6.36)  Time: 0.706s, 1450.66/s  (0.700s, 1462.99/s)  LR: 1.129e-04  Data: 0.013 (0.020)
Train: 2 [ 350/1251 ( 28%)]  Loss: 6.373 (6.37)  Time: 0.691s, 1481.04/s  (0.699s, 1464.42/s)  LR: 1.149e-04  Data: 0.012 (0.019)
Train: 2 [ 400/1251 ( 32%)]  Loss: 6.439 (6.37)  Time: 0.704s, 1455.48/s  (0.699s, 1464.80/s)  LR: 1.169e-04  Data: 0.013 (0.018)
Train: 2 [ 450/1251 ( 36%)]  Loss: 6.373 (6.37)  Time: 0.695s, 1474.41/s  (0.699s, 1465.39/s)  LR: 1.189e-04  Data: 0.013 (0.018)
Train: 2 [ 500/1251 ( 40%)]  Loss: 6.305 (6.37)  Time: 0.690s, 1483.38/s  (0.698s, 1466.23/s)  LR: 1.209e-04  Data: 0.013 (0.017)
Train: 2 [ 550/1251 ( 44%)]  Loss: 6.209 (6.35)  Time: 0.698s, 1466.53/s  (0.698s, 1466.38/s)  LR: 1.229e-04  Data: 0.015 (0.017)
Train: 2 [ 600/1251 ( 48%)]  Loss: 6.241 (6.35)  Time: 0.701s, 1460.04/s  (0.698s, 1466.67/s)  LR: 1.249e-04  Data: 0.014 (0.017)
Train: 2 [ 650/1251 ( 52%)]  Loss: 6.134 (6.33)  Time: 0.680s, 1505.85/s  (0.698s, 1467.40/s)  LR: 1.269e-04  Data: 0.012 (0.016)
Train: 2 [ 700/1251 ( 56%)]  Loss: 6.138 (6.32)  Time: 0.686s, 1493.65/s  (0.697s, 1468.14/s)  LR: 1.289e-04  Data: 0.013 (0.016)
Train: 2 [ 750/1251 ( 60%)]  Loss: 6.267 (6.31)  Time: 0.693s, 1477.35/s  (0.697s, 1468.58/s)  LR: 1.309e-04  Data: 0.011 (0.016)
Train: 2 [ 800/1251 ( 64%)]  Loss: 6.400 (6.32)  Time: 0.703s, 1455.73/s  (0.697s, 1469.08/s)  LR: 1.329e-04  Data: 0.014 (0.016)
Train: 2 [ 850/1251 ( 68%)]  Loss: 6.155 (6.31)  Time: 0.697s, 1469.26/s  (0.697s, 1469.41/s)  LR: 1.349e-04  Data: 0.013 (0.016)
Train: 2 [ 900/1251 ( 72%)]  Loss: 6.184 (6.30)  Time: 0.699s, 1464.99/s  (0.697s, 1469.94/s)  LR: 1.369e-04  Data: 0.014 (0.016)
Train: 2 [ 950/1251 ( 76%)]  Loss: 6.280 (6.30)  Time: 0.702s, 1459.31/s  (0.696s, 1470.31/s)  LR: 1.389e-04  Data: 0.013 (0.016)
Train: 2 [1000/1251 ( 80%)]  Loss: 6.198 (6.30)  Time: 0.696s, 1471.21/s  (0.696s, 1470.86/s)  LR: 1.409e-04  Data: 0.014 (0.015)
Train: 2 [1050/1251 ( 84%)]  Loss: 6.247 (6.30)  Time: 0.691s, 1481.97/s  (0.696s, 1471.16/s)  LR: 1.429e-04  Data: 0.013 (0.015)
Train: 2 [1100/1251 ( 88%)]  Loss: 6.198 (6.29)  Time: 0.695s, 1472.42/s  (0.696s, 1471.53/s)  LR: 1.449e-04  Data: 0.012 (0.015)
Train: 2 [1150/1251 ( 92%)]  Loss: 6.325 (6.29)  Time: 0.686s, 1492.18/s  (0.696s, 1471.80/s)  LR: 1.469e-04  Data: 0.013 (0.015)
Train: 2 [1200/1251 ( 96%)]  Loss: 6.081 (6.28)  Time: 0.687s, 1491.35/s  (0.696s, 1472.03/s)  LR: 1.489e-04  Data: 0.013 (0.015)
Train: 2 [1250/1251 (100%)]  Loss: 6.238 (6.28)  Time: 0.681s, 1503.89/s  (0.695s, 1472.48/s)  LR: 1.509e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.028 (3.028)  Loss:  4.3672 (4.3672)  Acc@1: 16.6016 (16.6016)  Acc@5: 45.2148 (45.2148)
Test: [  48/48]  Time: 0.173 (0.332)  Loss:  3.8086 (4.8972)  Acc@1: 37.6179 (12.5400)  Acc@5: 54.1274 (29.9460)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-2.pth.tar', 12.539999986572266)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-1.pth.tar', 6.434000000610352)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-0.pth.tar', 2.052000018310547)

Train: 3 [   0/1251 (  0%)]  Loss: 6.167 (6.17)  Time: 4.110s,  249.15/s  (4.110s,  249.15/s)  LR: 1.509e-04  Data: 1.689 (1.689)
Train: 3 [  50/1251 (  4%)]  Loss: 6.038 (6.10)  Time: 0.676s, 1515.39/s  (0.726s, 1409.92/s)  LR: 1.529e-04  Data: 0.015 (0.047)
Train: 3 [ 100/1251 (  8%)]  Loss: 6.236 (6.15)  Time: 0.698s, 1467.33/s  (0.708s, 1445.49/s)  LR: 1.549e-04  Data: 0.015 (0.030)
Train: 3 [ 150/1251 ( 12%)]  Loss: 6.085 (6.13)  Time: 0.700s, 1462.88/s  (0.703s, 1456.68/s)  LR: 1.569e-04  Data: 0.013 (0.025)
Train: 3 [ 200/1251 ( 16%)]  Loss: 6.142 (6.13)  Time: 0.701s, 1459.83/s  (0.700s, 1461.98/s)  LR: 1.589e-04  Data: 0.013 (0.022)
Train: 3 [ 250/1251 ( 20%)]  Loss: 5.992 (6.11)  Time: 0.686s, 1492.73/s  (0.699s, 1465.88/s)  LR: 1.609e-04  Data: 0.013 (0.020)
Train: 3 [ 300/1251 ( 24%)]  Loss: 5.923 (6.08)  Time: 0.684s, 1497.38/s  (0.697s, 1468.21/s)  LR: 1.629e-04  Data: 0.011 (0.019)
Train: 3 [ 350/1251 ( 28%)]  Loss: 6.095 (6.08)  Time: 0.693s, 1477.56/s  (0.697s, 1469.42/s)  LR: 1.649e-04  Data: 0.012 (0.018)
Train: 3 [ 400/1251 ( 32%)]  Loss: 6.248 (6.10)  Time: 0.686s, 1493.37/s  (0.696s, 1470.30/s)  LR: 1.669e-04  Data: 0.013 (0.018)
Train: 3 [ 450/1251 ( 36%)]  Loss: 6.045 (6.10)  Time: 0.702s, 1458.60/s  (0.696s, 1471.01/s)  LR: 1.689e-04  Data: 0.013 (0.017)
Train: 3 [ 500/1251 ( 40%)]  Loss: 6.120 (6.10)  Time: 0.704s, 1453.93/s  (0.696s, 1471.08/s)  LR: 1.709e-04  Data: 0.012 (0.017)
Train: 3 [ 550/1251 ( 44%)]  Loss: 6.062 (6.10)  Time: 0.694s, 1474.47/s  (0.696s, 1471.46/s)  LR: 1.729e-04  Data: 0.013 (0.017)
Train: 3 [ 600/1251 ( 48%)]  Loss: 5.960 (6.09)  Time: 0.693s, 1478.23/s  (0.696s, 1471.79/s)  LR: 1.748e-04  Data: 0.014 (0.016)
Train: 3 [ 650/1251 ( 52%)]  Loss: 5.869 (6.07)  Time: 0.705s, 1452.34/s  (0.696s, 1472.22/s)  LR: 1.768e-04  Data: 0.015 (0.016)
Train: 3 [ 700/1251 ( 56%)]  Loss: 5.942 (6.06)  Time: 0.694s, 1475.85/s  (0.695s, 1472.89/s)  LR: 1.788e-04  Data: 0.012 (0.016)
Train: 3 [ 750/1251 ( 60%)]  Loss: 6.116 (6.06)  Time: 0.694s, 1476.24/s  (0.695s, 1473.42/s)  LR: 1.808e-04  Data: 0.012 (0.016)
Train: 3 [ 800/1251 ( 64%)]  Loss: 6.067 (6.07)  Time: 0.690s, 1483.93/s  (0.695s, 1473.90/s)  LR: 1.828e-04  Data: 0.013 (0.016)
Train: 3 [ 850/1251 ( 68%)]  Loss: 6.143 (6.07)  Time: 0.697s, 1469.54/s  (0.695s, 1474.32/s)  LR: 1.848e-04  Data: 0.013 (0.015)
Train: 3 [ 900/1251 ( 72%)]  Loss: 6.053 (6.07)  Time: 0.691s, 1481.03/s  (0.694s, 1475.03/s)  LR: 1.868e-04  Data: 0.017 (0.015)
Train: 3 [ 950/1251 ( 76%)]  Loss: 5.990 (6.06)  Time: 0.691s, 1482.75/s  (0.694s, 1475.50/s)  LR: 1.888e-04  Data: 0.014 (0.015)
Train: 3 [1000/1251 ( 80%)]  Loss: 5.918 (6.06)  Time: 0.684s, 1497.67/s  (0.694s, 1475.86/s)  LR: 1.908e-04  Data: 0.012 (0.015)
Train: 3 [1050/1251 ( 84%)]  Loss: 6.164 (6.06)  Time: 0.699s, 1464.80/s  (0.694s, 1476.25/s)  LR: 1.928e-04  Data: 0.012 (0.015)
Train: 3 [1100/1251 ( 88%)]  Loss: 5.758 (6.05)  Time: 0.689s, 1485.74/s  (0.693s, 1476.83/s)  LR: 1.948e-04  Data: 0.015 (0.015)
Train: 3 [1150/1251 ( 92%)]  Loss: 5.901 (6.04)  Time: 0.683s, 1500.17/s  (0.693s, 1477.19/s)  LR: 1.968e-04  Data: 0.012 (0.015)
Train: 3 [1200/1251 ( 96%)]  Loss: 5.936 (6.04)  Time: 0.680s, 1506.39/s  (0.693s, 1477.73/s)  LR: 1.988e-04  Data: 0.012 (0.015)
Train: 3 [1250/1251 (100%)]  Loss: 6.021 (6.04)  Time: 0.683s, 1498.56/s  (0.693s, 1478.25/s)  LR: 2.008e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.009 (3.009)  Loss:  3.5332 (3.5332)  Acc@1: 27.8320 (27.8320)  Acc@5: 59.3750 (59.3750)
Test: [  48/48]  Time: 0.174 (0.324)  Loss:  3.0371 (4.2849)  Acc@1: 45.8726 (19.6580)  Acc@5: 64.5047 (40.9280)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-3.pth.tar', 19.658000017089844)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-2.pth.tar', 12.539999986572266)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-1.pth.tar', 6.434000000610352)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-0.pth.tar', 2.052000018310547)

Train: 4 [   0/1251 (  0%)]  Loss: 5.980 (5.98)  Time: 3.536s,  289.57/s  (3.536s,  289.57/s)  LR: 2.008e-04  Data: 1.883 (1.883)
Train: 4 [  50/1251 (  4%)]  Loss: 5.817 (5.90)  Time: 0.673s, 1521.54/s  (0.707s, 1449.06/s)  LR: 2.028e-04  Data: 0.013 (0.051)
Train: 4 [ 100/1251 (  8%)]  Loss: 5.861 (5.89)  Time: 0.686s, 1491.79/s  (0.693s, 1478.52/s)  LR: 2.048e-04  Data: 0.016 (0.032)
Train: 4 [ 150/1251 ( 12%)]  Loss: 5.996 (5.91)  Time: 0.682s, 1501.13/s  (0.689s, 1485.36/s)  LR: 2.068e-04  Data: 0.013 (0.026)
Train: 4 [ 200/1251 ( 16%)]  Loss: 6.019 (5.93)  Time: 0.681s, 1503.13/s  (0.688s, 1488.43/s)  LR: 2.088e-04  Data: 0.012 (0.023)
Train: 4 [ 250/1251 ( 20%)]  Loss: 5.957 (5.94)  Time: 0.683s, 1500.28/s  (0.687s, 1490.75/s)  LR: 2.108e-04  Data: 0.014 (0.021)
Train: 4 [ 300/1251 ( 24%)]  Loss: 5.807 (5.92)  Time: 0.683s, 1498.85/s  (0.687s, 1490.78/s)  LR: 2.128e-04  Data: 0.012 (0.020)
Train: 4 [ 350/1251 ( 28%)]  Loss: 5.821 (5.91)  Time: 0.680s, 1505.97/s  (0.687s, 1491.24/s)  LR: 2.148e-04  Data: 0.012 (0.019)
Train: 4 [ 400/1251 ( 32%)]  Loss: 5.932 (5.91)  Time: 0.689s, 1485.16/s  (0.687s, 1491.22/s)  LR: 2.168e-04  Data: 0.013 (0.018)
Train: 4 [ 450/1251 ( 36%)]  Loss: 6.006 (5.92)  Time: 0.675s, 1516.83/s  (0.687s, 1491.27/s)  LR: 2.188e-04  Data: 0.016 (0.018)
Train: 4 [ 500/1251 ( 40%)]  Loss: 5.847 (5.91)  Time: 0.684s, 1497.60/s  (0.687s, 1491.21/s)  LR: 2.208e-04  Data: 0.018 (0.017)
Train: 4 [ 550/1251 ( 44%)]  Loss: 5.991 (5.92)  Time: 0.686s, 1493.60/s  (0.687s, 1491.41/s)  LR: 2.228e-04  Data: 0.013 (0.017)
Train: 4 [ 600/1251 ( 48%)]  Loss: 5.917 (5.92)  Time: 0.689s, 1485.48/s  (0.686s, 1491.63/s)  LR: 2.248e-04  Data: 0.013 (0.017)
Train: 4 [ 650/1251 ( 52%)]  Loss: 5.805 (5.91)  Time: 0.686s, 1492.96/s  (0.687s, 1491.53/s)  LR: 2.268e-04  Data: 0.013 (0.016)
Train: 4 [ 700/1251 ( 56%)]  Loss: 5.842 (5.91)  Time: 0.687s, 1489.51/s  (0.686s, 1491.95/s)  LR: 2.288e-04  Data: 0.012 (0.016)
Train: 4 [ 750/1251 ( 60%)]  Loss: 5.940 (5.91)  Time: 0.681s, 1503.09/s  (0.686s, 1491.84/s)  LR: 2.308e-04  Data: 0.013 (0.016)
Train: 4 [ 800/1251 ( 64%)]  Loss: 5.802 (5.90)  Time: 0.675s, 1517.32/s  (0.686s, 1491.78/s)  LR: 2.328e-04  Data: 0.012 (0.016)
Train: 4 [ 850/1251 ( 68%)]  Loss: 5.817 (5.90)  Time: 0.685s, 1493.95/s  (0.686s, 1491.88/s)  LR: 2.348e-04  Data: 0.013 (0.016)
Train: 4 [ 900/1251 ( 72%)]  Loss: 5.954 (5.90)  Time: 0.686s, 1492.37/s  (0.686s, 1492.19/s)  LR: 2.368e-04  Data: 0.013 (0.016)
Train: 4 [ 950/1251 ( 76%)]  Loss: 5.897 (5.90)  Time: 0.680s, 1506.13/s  (0.686s, 1492.45/s)  LR: 2.388e-04  Data: 0.012 (0.016)
Train: 4 [1000/1251 ( 80%)]  Loss: 5.662 (5.89)  Time: 0.692s, 1479.94/s  (0.686s, 1492.38/s)  LR: 2.408e-04  Data: 0.013 (0.015)
Train: 4 [1050/1251 ( 84%)]  Loss: 6.039 (5.90)  Time: 0.689s, 1486.14/s  (0.686s, 1492.23/s)  LR: 2.428e-04  Data: 0.014 (0.015)
Train: 4 [1100/1251 ( 88%)]  Loss: 5.889 (5.90)  Time: 0.686s, 1493.69/s  (0.686s, 1492.35/s)  LR: 2.448e-04  Data: 0.012 (0.015)
Train: 4 [1150/1251 ( 92%)]  Loss: 5.820 (5.89)  Time: 0.674s, 1520.31/s  (0.686s, 1492.48/s)  LR: 2.468e-04  Data: 0.012 (0.015)
Train: 4 [1200/1251 ( 96%)]  Loss: 5.714 (5.89)  Time: 0.678s, 1511.03/s  (0.686s, 1492.79/s)  LR: 2.488e-04  Data: 0.013 (0.015)
Train: 4 [1250/1251 (100%)]  Loss: 5.667 (5.88)  Time: 0.673s, 1521.42/s  (0.686s, 1493.07/s)  LR: 2.508e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.763 (2.763)  Loss:  3.0332 (3.0332)  Acc@1: 37.5000 (37.5000)  Acc@5: 66.5039 (66.5039)
Test: [  48/48]  Time: 0.170 (0.339)  Loss:  2.4473 (3.8436)  Acc@1: 51.1793 (24.9780)  Acc@5: 73.1132 (48.4920)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-4.pth.tar', 24.97800012451172)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-3.pth.tar', 19.658000017089844)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-2.pth.tar', 12.539999986572266)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-1.pth.tar', 6.434000000610352)

Train: 5 [   0/1251 (  0%)]  Loss: 5.956 (5.96)  Time: 3.193s,  320.71/s  (3.193s,  320.71/s)  LR: 2.508e-04  Data: 1.875 (1.875)
Train: 5 [  50/1251 (  4%)]  Loss: 5.600 (5.78)  Time: 0.671s, 1526.90/s  (0.709s, 1443.72/s)  LR: 2.528e-04  Data: 0.014 (0.050)
Train: 5 [ 100/1251 (  8%)]  Loss: 5.790 (5.78)  Time: 0.672s, 1523.44/s  (0.693s, 1477.33/s)  LR: 2.548e-04  Data: 0.015 (0.032)
Train: 5 [ 150/1251 ( 12%)]  Loss: 5.709 (5.76)  Time: 0.675s, 1516.44/s  (0.689s, 1486.40/s)  LR: 2.568e-04  Data: 0.013 (0.026)
Train: 5 [ 200/1251 ( 16%)]  Loss: 5.838 (5.78)  Time: 0.676s, 1513.97/s  (0.687s, 1490.13/s)  LR: 2.588e-04  Data: 0.012 (0.023)
Train: 5 [ 250/1251 ( 20%)]  Loss: 5.631 (5.75)  Time: 0.681s, 1503.42/s  (0.687s, 1491.16/s)  LR: 2.608e-04  Data: 0.013 (0.021)
Train: 5 [ 300/1251 ( 24%)]  Loss: 5.802 (5.76)  Time: 0.688s, 1489.44/s  (0.687s, 1491.43/s)  LR: 2.628e-04  Data: 0.016 (0.020)
Train: 5 [ 350/1251 ( 28%)]  Loss: 5.866 (5.77)  Time: 0.694s, 1474.66/s  (0.686s, 1491.64/s)  LR: 2.648e-04  Data: 0.012 (0.019)
Train: 5 [ 400/1251 ( 32%)]  Loss: 5.754 (5.77)  Time: 0.682s, 1500.91/s  (0.686s, 1491.82/s)  LR: 2.668e-04  Data: 0.012 (0.018)
Train: 5 [ 450/1251 ( 36%)]  Loss: 5.665 (5.76)  Time: 0.685s, 1495.57/s  (0.686s, 1492.05/s)  LR: 2.688e-04  Data: 0.019 (0.018)
Train: 5 [ 500/1251 ( 40%)]  Loss: 5.827 (5.77)  Time: 0.692s, 1479.72/s  (0.686s, 1492.27/s)  LR: 2.708e-04  Data: 0.012 (0.017)
Train: 5 [ 550/1251 ( 44%)]  Loss: 5.698 (5.76)  Time: 0.686s, 1492.70/s  (0.686s, 1492.53/s)  LR: 2.728e-04  Data: 0.012 (0.017)
Train: 5 [ 600/1251 ( 48%)]  Loss: 5.635 (5.75)  Time: 0.685s, 1494.82/s  (0.686s, 1492.72/s)  LR: 2.747e-04  Data: 0.013 (0.017)
Train: 5 [ 650/1251 ( 52%)]  Loss: 5.876 (5.76)  Time: 0.691s, 1480.95/s  (0.686s, 1492.95/s)  LR: 2.767e-04  Data: 0.013 (0.017)
Train: 5 [ 700/1251 ( 56%)]  Loss: 5.396 (5.74)  Time: 0.692s, 1479.35/s  (0.686s, 1492.95/s)  LR: 2.787e-04  Data: 0.013 (0.016)
Train: 5 [ 750/1251 ( 60%)]  Loss: 5.747 (5.74)  Time: 0.696s, 1471.49/s  (0.686s, 1492.90/s)  LR: 2.807e-04  Data: 0.016 (0.016)
Train: 5 [ 800/1251 ( 64%)]  Loss: 5.869 (5.74)  Time: 0.680s, 1506.33/s  (0.686s, 1492.75/s)  LR: 2.827e-04  Data: 0.012 (0.016)
Train: 5 [ 850/1251 ( 68%)]  Loss: 5.769 (5.75)  Time: 0.689s, 1486.76/s  (0.686s, 1492.98/s)  LR: 2.847e-04  Data: 0.012 (0.016)
Train: 5 [ 900/1251 ( 72%)]  Loss: 5.796 (5.75)  Time: 0.674s, 1519.72/s  (0.686s, 1493.05/s)  LR: 2.867e-04  Data: 0.013 (0.016)
Train: 5 [ 950/1251 ( 76%)]  Loss: 5.674 (5.74)  Time: 0.682s, 1502.53/s  (0.686s, 1493.12/s)  LR: 2.887e-04  Data: 0.015 (0.016)
Train: 5 [1000/1251 ( 80%)]  Loss: 5.376 (5.73)  Time: 0.690s, 1483.75/s  (0.686s, 1493.23/s)  LR: 2.907e-04  Data: 0.015 (0.016)
Train: 5 [1050/1251 ( 84%)]  Loss: 5.608 (5.72)  Time: 0.688s, 1488.71/s  (0.686s, 1493.45/s)  LR: 2.927e-04  Data: 0.013 (0.015)
Train: 5 [1100/1251 ( 88%)]  Loss: 5.439 (5.71)  Time: 0.692s, 1479.44/s  (0.686s, 1493.60/s)  LR: 2.947e-04  Data: 0.012 (0.015)
Train: 5 [1150/1251 ( 92%)]  Loss: 5.481 (5.70)  Time: 0.678s, 1509.55/s  (0.686s, 1493.38/s)  LR: 2.967e-04  Data: 0.012 (0.015)
Train: 5 [1200/1251 ( 96%)]  Loss: 5.647 (5.70)  Time: 0.681s, 1503.57/s  (0.686s, 1493.42/s)  LR: 2.987e-04  Data: 0.013 (0.015)
Train: 5 [1250/1251 (100%)]  Loss: 5.397 (5.69)  Time: 0.672s, 1524.74/s  (0.686s, 1493.64/s)  LR: 3.007e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.022 (3.022)  Loss:  2.3438 (2.3438)  Acc@1: 48.8281 (48.8281)  Acc@5: 77.7344 (77.7344)
Test: [  48/48]  Time: 0.173 (0.335)  Loss:  2.2715 (3.4498)  Acc@1: 56.2500 (30.4260)  Acc@5: 74.0566 (55.0840)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-5.pth.tar', 30.426)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-4.pth.tar', 24.97800012451172)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-3.pth.tar', 19.658000017089844)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-2.pth.tar', 12.539999986572266)

Train: 6 [   0/1251 (  0%)]  Loss: 5.440 (5.44)  Time: 3.270s,  313.15/s  (3.270s,  313.15/s)  LR: 3.007e-04  Data: 1.617 (1.617)
Train: 6 [  50/1251 (  4%)]  Loss: 5.683 (5.56)  Time: 0.675s, 1516.94/s  (0.707s, 1449.09/s)  LR: 3.027e-04  Data: 0.011 (0.046)
Train: 6 [ 100/1251 (  8%)]  Loss: 5.545 (5.56)  Time: 0.680s, 1504.93/s  (0.692s, 1479.30/s)  LR: 3.047e-04  Data: 0.013 (0.031)
Train: 6 [ 150/1251 ( 12%)]  Loss: 5.123 (5.45)  Time: 0.673s, 1522.23/s  (0.689s, 1486.77/s)  LR: 3.067e-04  Data: 0.014 (0.025)
Train: 6 [ 200/1251 ( 16%)]  Loss: 5.504 (5.46)  Time: 0.680s, 1505.93/s  (0.687s, 1489.53/s)  LR: 3.087e-04  Data: 0.012 (0.022)
Train: 6 [ 250/1251 ( 20%)]  Loss: 5.577 (5.48)  Time: 0.677s, 1511.82/s  (0.687s, 1490.01/s)  LR: 3.107e-04  Data: 0.013 (0.021)
Train: 6 [ 300/1251 ( 24%)]  Loss: 5.534 (5.49)  Time: 0.680s, 1505.75/s  (0.687s, 1491.27/s)  LR: 3.127e-04  Data: 0.014 (0.019)
Train: 6 [ 350/1251 ( 28%)]  Loss: 5.640 (5.51)  Time: 0.682s, 1501.16/s  (0.686s, 1491.65/s)  LR: 3.147e-04  Data: 0.014 (0.019)
Train: 6 [ 400/1251 ( 32%)]  Loss: 5.499 (5.51)  Time: 0.696s, 1470.71/s  (0.686s, 1491.81/s)  LR: 3.167e-04  Data: 0.013 (0.018)
Train: 6 [ 450/1251 ( 36%)]  Loss: 5.490 (5.50)  Time: 0.684s, 1497.66/s  (0.687s, 1491.61/s)  LR: 3.187e-04  Data: 0.012 (0.018)
Train: 6 [ 500/1251 ( 40%)]  Loss: 5.482 (5.50)  Time: 0.675s, 1515.96/s  (0.686s, 1491.96/s)  LR: 3.207e-04  Data: 0.012 (0.017)
Train: 6 [ 550/1251 ( 44%)]  Loss: 5.388 (5.49)  Time: 0.674s, 1519.25/s  (0.686s, 1492.10/s)  LR: 3.227e-04  Data: 0.015 (0.017)
Train: 6 [ 600/1251 ( 48%)]  Loss: 5.615 (5.50)  Time: 0.682s, 1500.53/s  (0.686s, 1492.07/s)  LR: 3.247e-04  Data: 0.015 (0.017)
Train: 6 [ 650/1251 ( 52%)]  Loss: 5.590 (5.51)  Time: 0.686s, 1493.44/s  (0.686s, 1492.27/s)  LR: 3.267e-04  Data: 0.012 (0.017)
Train: 6 [ 700/1251 ( 56%)]  Loss: 5.593 (5.51)  Time: 0.691s, 1481.94/s  (0.686s, 1492.16/s)  LR: 3.287e-04  Data: 0.013 (0.016)
Train: 6 [ 750/1251 ( 60%)]  Loss: 5.743 (5.53)  Time: 0.687s, 1490.08/s  (0.686s, 1491.93/s)  LR: 3.307e-04  Data: 0.013 (0.016)
Train: 6 [ 800/1251 ( 64%)]  Loss: 5.724 (5.54)  Time: 0.685s, 1495.72/s  (0.686s, 1492.12/s)  LR: 3.327e-04  Data: 0.012 (0.016)
Train: 6 [ 850/1251 ( 68%)]  Loss: 5.545 (5.54)  Time: 0.691s, 1482.87/s  (0.686s, 1492.33/s)  LR: 3.347e-04  Data: 0.013 (0.016)
Train: 6 [ 900/1251 ( 72%)]  Loss: 5.484 (5.54)  Time: 0.687s, 1489.84/s  (0.686s, 1492.48/s)  LR: 3.367e-04  Data: 0.013 (0.016)
Train: 6 [ 950/1251 ( 76%)]  Loss: 5.512 (5.54)  Time: 0.695s, 1474.25/s  (0.686s, 1492.22/s)  LR: 3.387e-04  Data: 0.015 (0.016)
Train: 6 [1000/1251 ( 80%)]  Loss: 5.398 (5.53)  Time: 0.695s, 1472.89/s  (0.686s, 1492.23/s)  LR: 3.407e-04  Data: 0.013 (0.016)
Train: 6 [1050/1251 ( 84%)]  Loss: 5.433 (5.52)  Time: 0.683s, 1499.53/s  (0.686s, 1492.40/s)  LR: 3.427e-04  Data: 0.012 (0.016)
Train: 6 [1100/1251 ( 88%)]  Loss: 5.560 (5.53)  Time: 0.684s, 1497.69/s  (0.686s, 1492.59/s)  LR: 3.447e-04  Data: 0.013 (0.015)
Train: 6 [1150/1251 ( 92%)]  Loss: 5.534 (5.53)  Time: 0.686s, 1493.79/s  (0.686s, 1492.61/s)  LR: 3.467e-04  Data: 0.015 (0.015)
Train: 6 [1200/1251 ( 96%)]  Loss: 5.423 (5.52)  Time: 0.679s, 1509.14/s  (0.686s, 1492.86/s)  LR: 3.487e-04  Data: 0.015 (0.015)
Train: 6 [1250/1251 (100%)]  Loss: 5.791 (5.53)  Time: 0.684s, 1496.31/s  (0.686s, 1493.04/s)  LR: 3.507e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.726 (2.726)  Loss:  2.0859 (2.0859)  Acc@1: 55.9570 (55.9570)  Acc@5: 82.2266 (82.2266)
Test: [  48/48]  Time: 0.174 (0.324)  Loss:  1.8311 (3.2013)  Acc@1: 62.7359 (34.7360)  Acc@5: 80.4245 (60.0000)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-6.pth.tar', 34.7360000378418)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-5.pth.tar', 30.426)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-4.pth.tar', 24.97800012451172)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-3.pth.tar', 19.658000017089844)

Train: 7 [   0/1251 (  0%)]  Loss: 5.536 (5.54)  Time: 3.821s,  267.96/s  (3.821s,  267.96/s)  LR: 3.507e-04  Data: 2.064 (2.064)
Train: 7 [  50/1251 (  4%)]  Loss: 5.393 (5.46)  Time: 0.673s, 1520.53/s  (0.714s, 1433.49/s)  LR: 3.527e-04  Data: 0.013 (0.054)
Train: 7 [ 100/1251 (  8%)]  Loss: 5.312 (5.41)  Time: 0.686s, 1492.60/s  (0.698s, 1468.01/s)  LR: 3.547e-04  Data: 0.013 (0.035)
Train: 7 [ 150/1251 ( 12%)]  Loss: 5.450 (5.42)  Time: 0.688s, 1487.74/s  (0.693s, 1476.60/s)  LR: 3.567e-04  Data: 0.014 (0.028)
Train: 7 [ 200/1251 ( 16%)]  Loss: 5.514 (5.44)  Time: 0.688s, 1487.40/s  (0.692s, 1480.08/s)  LR: 3.587e-04  Data: 0.014 (0.024)
Train: 7 [ 250/1251 ( 20%)]  Loss: 5.419 (5.44)  Time: 0.687s, 1489.86/s  (0.691s, 1482.09/s)  LR: 3.607e-04  Data: 0.012 (0.022)
Train: 7 [ 300/1251 ( 24%)]  Loss: 5.002 (5.37)  Time: 0.685s, 1494.51/s  (0.690s, 1483.34/s)  LR: 3.627e-04  Data: 0.013 (0.021)
Train: 7 [ 350/1251 ( 28%)]  Loss: 5.493 (5.39)  Time: 0.677s, 1513.65/s  (0.689s, 1485.21/s)  LR: 3.647e-04  Data: 0.012 (0.020)
Train: 7 [ 400/1251 ( 32%)]  Loss: 5.593 (5.41)  Time: 0.690s, 1484.54/s  (0.689s, 1486.21/s)  LR: 3.667e-04  Data: 0.013 (0.019)
Train: 7 [ 450/1251 ( 36%)]  Loss: 5.309 (5.40)  Time: 0.682s, 1501.92/s  (0.689s, 1486.74/s)  LR: 3.687e-04  Data: 0.012 (0.018)
Train: 7 [ 500/1251 ( 40%)]  Loss: 5.174 (5.38)  Time: 0.681s, 1503.93/s  (0.689s, 1486.98/s)  LR: 3.707e-04  Data: 0.013 (0.018)
Train: 7 [ 550/1251 ( 44%)]  Loss: 5.495 (5.39)  Time: 0.684s, 1496.99/s  (0.688s, 1487.69/s)  LR: 3.727e-04  Data: 0.013 (0.018)
Train: 7 [ 600/1251 ( 48%)]  Loss: 5.051 (5.36)  Time: 0.696s, 1471.96/s  (0.688s, 1488.24/s)  LR: 3.746e-04  Data: 0.013 (0.017)
Train: 7 [ 650/1251 ( 52%)]  Loss: 5.268 (5.36)  Time: 0.678s, 1511.39/s  (0.688s, 1488.20/s)  LR: 3.766e-04  Data: 0.014 (0.017)
Train: 7 [ 700/1251 ( 56%)]  Loss: 5.459 (5.36)  Time: 0.684s, 1496.91/s  (0.688s, 1488.48/s)  LR: 3.786e-04  Data: 0.016 (0.017)
Train: 7 [ 750/1251 ( 60%)]  Loss: 5.380 (5.37)  Time: 0.683s, 1499.24/s  (0.688s, 1489.08/s)  LR: 3.806e-04  Data: 0.013 (0.017)
Train: 7 [ 800/1251 ( 64%)]  Loss: 5.230 (5.36)  Time: 0.692s, 1479.95/s  (0.688s, 1489.22/s)  LR: 3.826e-04  Data: 0.013 (0.016)
Train: 7 [ 850/1251 ( 68%)]  Loss: 5.288 (5.35)  Time: 0.689s, 1486.68/s  (0.687s, 1489.48/s)  LR: 3.846e-04  Data: 0.013 (0.016)
Train: 7 [ 900/1251 ( 72%)]  Loss: 5.399 (5.36)  Time: 0.683s, 1499.96/s  (0.687s, 1489.73/s)  LR: 3.866e-04  Data: 0.013 (0.016)
Train: 7 [ 950/1251 ( 76%)]  Loss: 5.161 (5.35)  Time: 0.690s, 1483.71/s  (0.687s, 1489.84/s)  LR: 3.886e-04  Data: 0.017 (0.016)
Train: 7 [1000/1251 ( 80%)]  Loss: 5.581 (5.36)  Time: 0.681s, 1503.63/s  (0.687s, 1490.01/s)  LR: 3.906e-04  Data: 0.012 (0.016)
Train: 7 [1050/1251 ( 84%)]  Loss: 5.451 (5.36)  Time: 0.689s, 1485.95/s  (0.687s, 1490.29/s)  LR: 3.926e-04  Data: 0.013 (0.016)
Train: 7 [1100/1251 ( 88%)]  Loss: 5.551 (5.37)  Time: 0.682s, 1501.14/s  (0.687s, 1490.52/s)  LR: 3.946e-04  Data: 0.015 (0.016)
Train: 7 [1150/1251 ( 92%)]  Loss: 5.286 (5.37)  Time: 0.683s, 1499.03/s  (0.687s, 1490.77/s)  LR: 3.966e-04  Data: 0.017 (0.016)
Train: 7 [1200/1251 ( 96%)]  Loss: 5.499 (5.37)  Time: 0.691s, 1481.62/s  (0.687s, 1490.92/s)  LR: 3.986e-04  Data: 0.013 (0.016)
Train: 7 [1250/1251 (100%)]  Loss: 5.468 (5.38)  Time: 0.672s, 1523.28/s  (0.687s, 1491.01/s)  LR: 4.006e-04  Data: 0.000 (0.016)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.035 (3.035)  Loss:  2.0156 (2.0156)  Acc@1: 57.0312 (57.0312)  Acc@5: 81.4453 (81.4453)
Test: [  48/48]  Time: 0.170 (0.331)  Loss:  1.6475 (2.9504)  Acc@1: 65.5660 (38.2340)  Acc@5: 83.6085 (64.0360)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-7.pth.tar', 38.2340000390625)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-6.pth.tar', 34.7360000378418)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-5.pth.tar', 30.426)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-4.pth.tar', 24.97800012451172)

Train: 8 [   0/1251 (  0%)]  Loss: 5.308 (5.31)  Time: 3.558s,  287.77/s  (3.558s,  287.77/s)  LR: 4.006e-04  Data: 1.924 (1.924)
Train: 8 [  50/1251 (  4%)]  Loss: 5.037 (5.17)  Time: 0.669s, 1530.60/s  (0.712s, 1438.78/s)  LR: 4.026e-04  Data: 0.015 (0.052)
Train: 8 [ 100/1251 (  8%)]  Loss: 5.299 (5.21)  Time: 0.670s, 1529.48/s  (0.694s, 1475.02/s)  LR: 4.046e-04  Data: 0.015 (0.033)
Train: 8 [ 150/1251 ( 12%)]  Loss: 5.392 (5.26)  Time: 0.685s, 1495.72/s  (0.690s, 1483.82/s)  LR: 4.066e-04  Data: 0.018 (0.027)
Train: 8 [ 200/1251 ( 16%)]  Loss: 5.626 (5.33)  Time: 0.683s, 1498.22/s  (0.688s, 1489.00/s)  LR: 4.086e-04  Data: 0.012 (0.024)
Train: 8 [ 250/1251 ( 20%)]  Loss: 5.190 (5.31)  Time: 0.671s, 1525.33/s  (0.686s, 1492.06/s)  LR: 4.106e-04  Data: 0.012 (0.022)
Train: 8 [ 300/1251 ( 24%)]  Loss: 4.975 (5.26)  Time: 0.674s, 1519.34/s  (0.686s, 1493.72/s)  LR: 4.126e-04  Data: 0.015 (0.021)
Train: 8 [ 350/1251 ( 28%)]  Loss: 5.028 (5.23)  Time: 0.675s, 1516.91/s  (0.685s, 1495.29/s)  LR: 4.146e-04  Data: 0.015 (0.020)
Train: 8 [ 400/1251 ( 32%)]  Loss: 5.156 (5.22)  Time: 0.678s, 1509.46/s  (0.684s, 1496.27/s)  LR: 4.166e-04  Data: 0.014 (0.019)
Train: 8 [ 450/1251 ( 36%)]  Loss: 5.384 (5.24)  Time: 0.684s, 1496.58/s  (0.684s, 1496.94/s)  LR: 4.186e-04  Data: 0.013 (0.018)
Train: 8 [ 500/1251 ( 40%)]  Loss: 5.409 (5.25)  Time: 0.676s, 1515.13/s  (0.684s, 1497.41/s)  LR: 4.206e-04  Data: 0.013 (0.018)
Train: 8 [ 550/1251 ( 44%)]  Loss: 5.173 (5.25)  Time: 0.676s, 1514.35/s  (0.684s, 1498.04/s)  LR: 4.226e-04  Data: 0.013 (0.018)
Train: 8 [ 600/1251 ( 48%)]  Loss: 5.145 (5.24)  Time: 0.687s, 1489.51/s  (0.684s, 1498.10/s)  LR: 4.246e-04  Data: 0.012 (0.017)
Train: 8 [ 650/1251 ( 52%)]  Loss: 5.138 (5.23)  Time: 0.687s, 1491.30/s  (0.683s, 1498.31/s)  LR: 4.266e-04  Data: 0.013 (0.017)
Train: 8 [ 700/1251 ( 56%)]  Loss: 5.510 (5.25)  Time: 0.690s, 1484.37/s  (0.683s, 1498.45/s)  LR: 4.286e-04  Data: 0.014 (0.017)
Train: 8 [ 750/1251 ( 60%)]  Loss: 5.332 (5.26)  Time: 0.678s, 1509.91/s  (0.683s, 1498.58/s)  LR: 4.306e-04  Data: 0.013 (0.017)
Train: 8 [ 800/1251 ( 64%)]  Loss: 5.133 (5.25)  Time: 0.691s, 1482.69/s  (0.683s, 1498.45/s)  LR: 4.326e-04  Data: 0.012 (0.016)
Train: 8 [ 850/1251 ( 68%)]  Loss: 4.748 (5.22)  Time: 0.684s, 1497.21/s  (0.683s, 1498.57/s)  LR: 4.346e-04  Data: 0.013 (0.016)
Train: 8 [ 900/1251 ( 72%)]  Loss: 5.079 (5.21)  Time: 0.687s, 1490.31/s  (0.683s, 1498.58/s)  LR: 4.366e-04  Data: 0.012 (0.016)
Train: 8 [ 950/1251 ( 76%)]  Loss: 5.316 (5.22)  Time: 0.692s, 1480.39/s  (0.683s, 1498.50/s)  LR: 4.386e-04  Data: 0.012 (0.016)
Train: 8 [1000/1251 ( 80%)]  Loss: 4.873 (5.20)  Time: 0.679s, 1507.33/s  (0.683s, 1498.33/s)  LR: 4.406e-04  Data: 0.015 (0.016)
Train: 8 [1050/1251 ( 84%)]  Loss: 5.161 (5.20)  Time: 0.688s, 1488.41/s  (0.683s, 1498.22/s)  LR: 4.426e-04  Data: 0.012 (0.016)
Train: 8 [1100/1251 ( 88%)]  Loss: 4.982 (5.19)  Time: 0.687s, 1491.40/s  (0.683s, 1498.29/s)  LR: 4.446e-04  Data: 0.013 (0.016)
Train: 8 [1150/1251 ( 92%)]  Loss: 5.221 (5.19)  Time: 0.675s, 1517.31/s  (0.683s, 1498.44/s)  LR: 4.466e-04  Data: 0.013 (0.016)
Train: 8 [1200/1251 ( 96%)]  Loss: 5.361 (5.20)  Time: 0.688s, 1487.82/s  (0.683s, 1498.43/s)  LR: 4.486e-04  Data: 0.014 (0.016)
Train: 8 [1250/1251 (100%)]  Loss: 5.309 (5.20)  Time: 0.684s, 1496.16/s  (0.683s, 1498.50/s)  LR: 4.506e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.769 (2.769)  Loss:  1.7930 (1.7930)  Acc@1: 61.6211 (61.6211)  Acc@5: 84.3750 (84.3750)
Test: [  48/48]  Time: 0.177 (0.326)  Loss:  1.7637 (2.7561)  Acc@1: 64.1509 (41.7020)  Acc@5: 83.7264 (67.4780)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-8.pth.tar', 41.70200007080078)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-7.pth.tar', 38.2340000390625)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-6.pth.tar', 34.7360000378418)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-5.pth.tar', 30.426)

Train: 9 [   0/1251 (  0%)]  Loss: 5.444 (5.44)  Time: 3.652s,  280.37/s  (3.652s,  280.37/s)  LR: 4.506e-04  Data: 1.868 (1.868)
Train: 9 [  50/1251 (  4%)]  Loss: 4.955 (5.20)  Time: 0.678s, 1510.86/s  (0.715s, 1431.49/s)  LR: 4.526e-04  Data: 0.013 (0.051)
Train: 9 [ 100/1251 (  8%)]  Loss: 5.264 (5.22)  Time: 0.686s, 1491.63/s  (0.698s, 1466.60/s)  LR: 4.546e-04  Data: 0.016 (0.033)
Train: 9 [ 150/1251 ( 12%)]  Loss: 4.902 (5.14)  Time: 0.684s, 1497.53/s  (0.693s, 1476.99/s)  LR: 4.566e-04  Data: 0.018 (0.027)
Train: 9 [ 200/1251 ( 16%)]  Loss: 5.143 (5.14)  Time: 0.692s, 1479.88/s  (0.692s, 1480.75/s)  LR: 4.586e-04  Data: 0.015 (0.023)
Train: 9 [ 250/1251 ( 20%)]  Loss: 5.071 (5.13)  Time: 0.686s, 1493.08/s  (0.690s, 1483.06/s)  LR: 4.606e-04  Data: 0.013 (0.022)
Train: 9 [ 300/1251 ( 24%)]  Loss: 5.213 (5.14)  Time: 0.694s, 1476.00/s  (0.690s, 1484.62/s)  LR: 4.626e-04  Data: 0.013 (0.020)
Train: 9 [ 350/1251 ( 28%)]  Loss: 5.017 (5.13)  Time: 0.687s, 1490.87/s  (0.689s, 1485.73/s)  LR: 4.646e-04  Data: 0.015 (0.019)
Train: 9 [ 400/1251 ( 32%)]  Loss: 5.043 (5.12)  Time: 0.680s, 1506.56/s  (0.689s, 1486.88/s)  LR: 4.666e-04  Data: 0.012 (0.019)
Train: 9 [ 450/1251 ( 36%)]  Loss: 5.181 (5.12)  Time: 0.685s, 1494.77/s  (0.688s, 1488.33/s)  LR: 4.686e-04  Data: 0.012 (0.018)
Train: 9 [ 500/1251 ( 40%)]  Loss: 5.120 (5.12)  Time: 0.686s, 1492.30/s  (0.688s, 1489.07/s)  LR: 4.706e-04  Data: 0.013 (0.018)
Train: 9 [ 550/1251 ( 44%)]  Loss: 5.028 (5.12)  Time: 0.684s, 1496.49/s  (0.687s, 1489.80/s)  LR: 4.726e-04  Data: 0.012 (0.017)
Train: 9 [ 600/1251 ( 48%)]  Loss: 5.616 (5.15)  Time: 0.687s, 1490.07/s  (0.687s, 1490.89/s)  LR: 4.745e-04  Data: 0.013 (0.017)
Train: 9 [ 650/1251 ( 52%)]  Loss: 5.205 (5.16)  Time: 0.685s, 1495.91/s  (0.686s, 1491.67/s)  LR: 4.765e-04  Data: 0.013 (0.017)
Train: 9 [ 700/1251 ( 56%)]  Loss: 5.235 (5.16)  Time: 0.684s, 1497.62/s  (0.686s, 1492.56/s)  LR: 4.785e-04  Data: 0.013 (0.017)
Train: 9 [ 750/1251 ( 60%)]  Loss: 4.662 (5.13)  Time: 0.681s, 1502.58/s  (0.686s, 1493.13/s)  LR: 4.805e-04  Data: 0.014 (0.017)
Train: 9 [ 800/1251 ( 64%)]  Loss: 5.421 (5.15)  Time: 0.691s, 1481.27/s  (0.686s, 1493.53/s)  LR: 4.825e-04  Data: 0.012 (0.016)
Train: 9 [ 850/1251 ( 68%)]  Loss: 5.233 (5.15)  Time: 0.683s, 1498.48/s  (0.686s, 1493.67/s)  LR: 4.845e-04  Data: 0.013 (0.016)
Train: 9 [ 900/1251 ( 72%)]  Loss: 5.145 (5.15)  Time: 0.685s, 1495.14/s  (0.685s, 1493.90/s)  LR: 4.865e-04  Data: 0.015 (0.016)
Train: 9 [ 950/1251 ( 76%)]  Loss: 5.173 (5.15)  Time: 0.690s, 1483.94/s  (0.685s, 1494.32/s)  LR: 4.885e-04  Data: 0.014 (0.016)
Train: 9 [1000/1251 ( 80%)]  Loss: 4.978 (5.15)  Time: 0.687s, 1490.84/s  (0.685s, 1494.30/s)  LR: 4.905e-04  Data: 0.014 (0.016)
Train: 9 [1050/1251 ( 84%)]  Loss: 4.899 (5.13)  Time: 0.686s, 1491.71/s  (0.685s, 1494.24/s)  LR: 4.925e-04  Data: 0.012 (0.016)
Train: 9 [1100/1251 ( 88%)]  Loss: 5.554 (5.15)  Time: 0.685s, 1495.55/s  (0.685s, 1494.39/s)  LR: 4.945e-04  Data: 0.014 (0.016)
Train: 9 [1150/1251 ( 92%)]  Loss: 4.964 (5.14)  Time: 0.691s, 1482.06/s  (0.685s, 1494.39/s)  LR: 4.965e-04  Data: 0.014 (0.016)
Train: 9 [1200/1251 ( 96%)]  Loss: 5.096 (5.14)  Time: 0.673s, 1522.58/s  (0.685s, 1494.41/s)  LR: 4.985e-04  Data: 0.013 (0.016)
Train: 9 [1250/1251 (100%)]  Loss: 4.997 (5.14)  Time: 0.679s, 1508.57/s  (0.685s, 1494.47/s)  LR: 5.005e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.777 (2.777)  Loss:  1.6123 (1.6123)  Acc@1: 66.7969 (66.7969)  Acc@5: 86.3281 (86.3281)
Test: [  48/48]  Time: 0.173 (0.330)  Loss:  1.4844 (2.5939)  Acc@1: 69.4576 (44.5200)  Acc@5: 84.9057 (70.4660)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-9.pth.tar', 44.52000004882812)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-8.pth.tar', 41.70200007080078)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-7.pth.tar', 38.2340000390625)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-6.pth.tar', 34.7360000378418)

Train: 10 [   0/1251 (  0%)]  Loss: 4.820 (4.82)  Time: 3.371s,  303.78/s  (3.371s,  303.78/s)  LR: 5.005e-04  Data: 1.745 (1.745)
Train: 10 [  50/1251 (  4%)]  Loss: 5.177 (5.00)  Time: 0.678s, 1510.84/s  (0.712s, 1437.60/s)  LR: 5.025e-04  Data: 0.013 (0.048)
Train: 10 [ 100/1251 (  8%)]  Loss: 4.889 (4.96)  Time: 0.690s, 1483.46/s  (0.696s, 1471.63/s)  LR: 5.045e-04  Data: 0.013 (0.031)
Train: 10 [ 150/1251 ( 12%)]  Loss: 5.284 (5.04)  Time: 0.679s, 1507.83/s  (0.691s, 1481.31/s)  LR: 5.065e-04  Data: 0.016 (0.026)
Train: 10 [ 200/1251 ( 16%)]  Loss: 5.082 (5.05)  Time: 0.667s, 1534.71/s  (0.689s, 1485.85/s)  LR: 5.085e-04  Data: 0.013 (0.023)
Train: 10 [ 250/1251 ( 20%)]  Loss: 5.206 (5.08)  Time: 0.682s, 1500.93/s  (0.688s, 1488.27/s)  LR: 5.105e-04  Data: 0.013 (0.021)
Train: 10 [ 300/1251 ( 24%)]  Loss: 5.472 (5.13)  Time: 0.680s, 1505.50/s  (0.687s, 1490.11/s)  LR: 5.125e-04  Data: 0.013 (0.020)
Train: 10 [ 350/1251 ( 28%)]  Loss: 5.272 (5.15)  Time: 0.696s, 1472.27/s  (0.687s, 1491.37/s)  LR: 5.145e-04  Data: 0.012 (0.019)
Train: 10 [ 400/1251 ( 32%)]  Loss: 5.091 (5.14)  Time: 0.685s, 1493.86/s  (0.686s, 1492.48/s)  LR: 5.165e-04  Data: 0.013 (0.018)
Train: 10 [ 450/1251 ( 36%)]  Loss: 5.352 (5.16)  Time: 0.676s, 1515.84/s  (0.685s, 1493.81/s)  LR: 5.185e-04  Data: 0.013 (0.018)
Train: 10 [ 500/1251 ( 40%)]  Loss: 5.272 (5.17)  Time: 0.688s, 1488.03/s  (0.685s, 1494.56/s)  LR: 5.205e-04  Data: 0.014 (0.017)
Train: 10 [ 550/1251 ( 44%)]  Loss: 5.127 (5.17)  Time: 0.688s, 1489.18/s  (0.685s, 1495.19/s)  LR: 5.225e-04  Data: 0.012 (0.017)
Train: 10 [ 600/1251 ( 48%)]  Loss: 5.179 (5.17)  Time: 0.676s, 1515.07/s  (0.685s, 1495.67/s)  LR: 5.245e-04  Data: 0.013 (0.017)
Train: 10 [ 650/1251 ( 52%)]  Loss: 5.107 (5.17)  Time: 0.675s, 1516.16/s  (0.684s, 1496.02/s)  LR: 5.265e-04  Data: 0.015 (0.017)
Train: 10 [ 700/1251 ( 56%)]  Loss: 5.037 (5.16)  Time: 0.672s, 1522.76/s  (0.684s, 1496.53/s)  LR: 5.285e-04  Data: 0.013 (0.016)
Train: 10 [ 750/1251 ( 60%)]  Loss: 4.611 (5.12)  Time: 0.678s, 1510.76/s  (0.684s, 1497.23/s)  LR: 5.305e-04  Data: 0.014 (0.016)
Train: 10 [ 800/1251 ( 64%)]  Loss: 4.811 (5.11)  Time: 0.676s, 1513.81/s  (0.684s, 1497.41/s)  LR: 5.325e-04  Data: 0.015 (0.016)
Train: 10 [ 850/1251 ( 68%)]  Loss: 4.724 (5.08)  Time: 0.683s, 1499.53/s  (0.684s, 1497.85/s)  LR: 5.345e-04  Data: 0.016 (0.016)
Train: 10 [ 900/1251 ( 72%)]  Loss: 5.193 (5.09)  Time: 0.678s, 1509.41/s  (0.683s, 1498.30/s)  LR: 5.365e-04  Data: 0.014 (0.016)
Train: 10 [ 950/1251 ( 76%)]  Loss: 4.952 (5.08)  Time: 0.682s, 1501.08/s  (0.683s, 1498.57/s)  LR: 5.385e-04  Data: 0.012 (0.016)
Train: 10 [1000/1251 ( 80%)]  Loss: 5.243 (5.09)  Time: 0.673s, 1520.49/s  (0.683s, 1498.75/s)  LR: 5.405e-04  Data: 0.013 (0.016)
Train: 10 [1050/1251 ( 84%)]  Loss: 4.845 (5.08)  Time: 0.676s, 1515.39/s  (0.683s, 1499.10/s)  LR: 5.425e-04  Data: 0.012 (0.016)
Train: 10 [1100/1251 ( 88%)]  Loss: 5.134 (5.08)  Time: 0.681s, 1503.20/s  (0.683s, 1499.30/s)  LR: 5.445e-04  Data: 0.014 (0.016)
Train: 10 [1150/1251 ( 92%)]  Loss: 4.854 (5.07)  Time: 0.675s, 1516.51/s  (0.683s, 1499.38/s)  LR: 5.465e-04  Data: 0.015 (0.016)
Train: 10 [1200/1251 ( 96%)]  Loss: 4.950 (5.07)  Time: 0.680s, 1505.29/s  (0.683s, 1499.81/s)  LR: 5.485e-04  Data: 0.012 (0.015)
Train: 10 [1250/1251 (100%)]  Loss: 4.973 (5.06)  Time: 0.674s, 1520.18/s  (0.683s, 1500.13/s)  LR: 5.505e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.775 (2.775)  Loss:  1.4561 (1.4561)  Acc@1: 68.3594 (68.3594)  Acc@5: 88.8672 (88.8672)
Test: [  48/48]  Time: 0.170 (0.322)  Loss:  1.4590 (2.4260)  Acc@1: 69.9292 (47.4400)  Acc@5: 85.6132 (73.0520)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-10.pth.tar', 47.43999999511719)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-9.pth.tar', 44.52000004882812)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-8.pth.tar', 41.70200007080078)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-7.pth.tar', 38.2340000390625)

Train: 11 [   0/1251 (  0%)]  Loss: 4.956 (4.96)  Time: 3.711s,  275.92/s  (3.711s,  275.92/s)  LR: 5.505e-04  Data: 1.825 (1.825)
Train: 11 [  50/1251 (  4%)]  Loss: 4.902 (4.93)  Time: 0.669s, 1530.11/s  (0.711s, 1439.86/s)  LR: 5.525e-04  Data: 0.013 (0.050)
Train: 11 [ 100/1251 (  8%)]  Loss: 5.189 (5.02)  Time: 0.680s, 1505.31/s  (0.693s, 1477.51/s)  LR: 5.545e-04  Data: 0.013 (0.032)
Train: 11 [ 150/1251 ( 12%)]  Loss: 4.718 (4.94)  Time: 0.682s, 1500.89/s  (0.688s, 1488.08/s)  LR: 5.565e-04  Data: 0.013 (0.026)
Train: 11 [ 200/1251 ( 16%)]  Loss: 4.851 (4.92)  Time: 0.678s, 1510.46/s  (0.686s, 1492.56/s)  LR: 5.585e-04  Data: 0.013 (0.023)
Train: 11 [ 250/1251 ( 20%)]  Loss: 5.062 (4.95)  Time: 0.680s, 1505.99/s  (0.685s, 1495.74/s)  LR: 5.605e-04  Data: 0.012 (0.021)
Train: 11 [ 300/1251 ( 24%)]  Loss: 5.231 (4.99)  Time: 0.683s, 1500.36/s  (0.684s, 1497.01/s)  LR: 5.625e-04  Data: 0.013 (0.020)
Train: 11 [ 350/1251 ( 28%)]  Loss: 5.173 (5.01)  Time: 0.677s, 1511.45/s  (0.683s, 1498.20/s)  LR: 5.645e-04  Data: 0.012 (0.019)
Train: 11 [ 400/1251 ( 32%)]  Loss: 5.140 (5.02)  Time: 0.674s, 1520.22/s  (0.683s, 1499.07/s)  LR: 5.665e-04  Data: 0.012 (0.019)
Train: 11 [ 450/1251 ( 36%)]  Loss: 5.063 (5.03)  Time: 0.690s, 1483.67/s  (0.683s, 1499.25/s)  LR: 5.685e-04  Data: 0.015 (0.018)
Train: 11 [ 500/1251 ( 40%)]  Loss: 4.999 (5.03)  Time: 0.679s, 1508.22/s  (0.683s, 1499.13/s)  LR: 5.705e-04  Data: 0.016 (0.018)
Train: 11 [ 550/1251 ( 44%)]  Loss: 5.180 (5.04)  Time: 0.689s, 1485.26/s  (0.683s, 1498.69/s)  LR: 5.725e-04  Data: 0.016 (0.017)
Train: 11 [ 600/1251 ( 48%)]  Loss: 5.069 (5.04)  Time: 0.690s, 1484.25/s  (0.683s, 1498.42/s)  LR: 5.744e-04  Data: 0.012 (0.017)
Train: 11 [ 650/1251 ( 52%)]  Loss: 4.737 (5.02)  Time: 0.684s, 1496.05/s  (0.683s, 1498.18/s)  LR: 5.764e-04  Data: 0.012 (0.017)
Train: 11 [ 700/1251 ( 56%)]  Loss: 4.974 (5.02)  Time: 0.680s, 1506.50/s  (0.683s, 1498.34/s)  LR: 5.784e-04  Data: 0.012 (0.017)
Train: 11 [ 750/1251 ( 60%)]  Loss: 4.854 (5.01)  Time: 0.684s, 1496.02/s  (0.683s, 1498.52/s)  LR: 5.804e-04  Data: 0.014 (0.016)
Train: 11 [ 800/1251 ( 64%)]  Loss: 4.871 (5.00)  Time: 0.676s, 1514.04/s  (0.683s, 1498.64/s)  LR: 5.824e-04  Data: 0.014 (0.016)
Train: 11 [ 850/1251 ( 68%)]  Loss: 5.241 (5.01)  Time: 0.685s, 1495.09/s  (0.683s, 1498.81/s)  LR: 5.844e-04  Data: 0.016 (0.016)
Train: 11 [ 900/1251 ( 72%)]  Loss: 4.860 (5.00)  Time: 0.678s, 1509.58/s  (0.683s, 1498.81/s)  LR: 5.864e-04  Data: 0.013 (0.016)
Train: 11 [ 950/1251 ( 76%)]  Loss: 4.686 (4.99)  Time: 0.670s, 1528.63/s  (0.683s, 1498.63/s)  LR: 5.884e-04  Data: 0.014 (0.016)
Train: 11 [1000/1251 ( 80%)]  Loss: 5.069 (4.99)  Time: 0.676s, 1515.20/s  (0.683s, 1498.32/s)  LR: 5.904e-04  Data: 0.012 (0.016)
Train: 11 [1050/1251 ( 84%)]  Loss: 4.898 (4.99)  Time: 0.680s, 1505.92/s  (0.683s, 1498.30/s)  LR: 5.924e-04  Data: 0.016 (0.016)
Train: 11 [1100/1251 ( 88%)]  Loss: 5.166 (5.00)  Time: 0.680s, 1505.52/s  (0.683s, 1498.45/s)  LR: 5.944e-04  Data: 0.014 (0.016)
Train: 11 [1150/1251 ( 92%)]  Loss: 4.716 (4.98)  Time: 0.678s, 1510.52/s  (0.683s, 1498.49/s)  LR: 5.964e-04  Data: 0.015 (0.016)
Train: 11 [1200/1251 ( 96%)]  Loss: 4.993 (4.98)  Time: 0.687s, 1490.81/s  (0.683s, 1498.39/s)  LR: 5.984e-04  Data: 0.012 (0.015)
Train: 11 [1250/1251 (100%)]  Loss: 4.862 (4.98)  Time: 0.680s, 1505.61/s  (0.683s, 1498.44/s)  LR: 6.004e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.178 (3.178)  Loss:  1.3574 (1.3574)  Acc@1: 71.0938 (71.0938)  Acc@5: 91.1133 (91.1133)
Test: [  48/48]  Time: 0.176 (0.329)  Loss:  1.4082 (2.2940)  Acc@1: 70.0472 (50.0040)  Acc@5: 87.9717 (75.1600)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-11.pth.tar', 50.00400004638672)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-10.pth.tar', 47.43999999511719)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-9.pth.tar', 44.52000004882812)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-8.pth.tar', 41.70200007080078)

Train: 12 [   0/1251 (  0%)]  Loss: 4.494 (4.49)  Time: 3.366s,  304.20/s  (3.366s,  304.20/s)  LR: 6.004e-04  Data: 1.972 (1.972)
Train: 12 [  50/1251 (  4%)]  Loss: 5.131 (4.81)  Time: 0.676s, 1515.87/s  (0.706s, 1451.42/s)  LR: 6.024e-04  Data: 0.012 (0.053)
Train: 12 [ 100/1251 (  8%)]  Loss: 4.618 (4.75)  Time: 0.678s, 1510.60/s  (0.691s, 1481.49/s)  LR: 6.044e-04  Data: 0.014 (0.034)
Train: 12 [ 150/1251 ( 12%)]  Loss: 4.801 (4.76)  Time: 0.682s, 1500.74/s  (0.688s, 1488.91/s)  LR: 6.064e-04  Data: 0.012 (0.027)
Train: 12 [ 200/1251 ( 16%)]  Loss: 4.690 (4.75)  Time: 0.687s, 1490.93/s  (0.686s, 1492.02/s)  LR: 6.084e-04  Data: 0.013 (0.024)
Train: 12 [ 250/1251 ( 20%)]  Loss: 4.932 (4.78)  Time: 0.677s, 1511.95/s  (0.686s, 1493.59/s)  LR: 6.104e-04  Data: 0.013 (0.022)
Train: 12 [ 300/1251 ( 24%)]  Loss: 5.016 (4.81)  Time: 0.689s, 1485.95/s  (0.685s, 1494.15/s)  LR: 6.124e-04  Data: 0.017 (0.021)
Train: 12 [ 350/1251 ( 28%)]  Loss: 5.000 (4.84)  Time: 0.692s, 1480.18/s  (0.685s, 1494.67/s)  LR: 6.144e-04  Data: 0.012 (0.020)
Train: 12 [ 400/1251 ( 32%)]  Loss: 4.759 (4.83)  Time: 0.689s, 1485.52/s  (0.685s, 1495.57/s)  LR: 6.164e-04  Data: 0.015 (0.019)
Train: 12 [ 450/1251 ( 36%)]  Loss: 4.816 (4.83)  Time: 0.682s, 1502.37/s  (0.685s, 1495.82/s)  LR: 6.184e-04  Data: 0.013 (0.018)
Train: 12 [ 500/1251 ( 40%)]  Loss: 5.256 (4.86)  Time: 0.693s, 1477.48/s  (0.684s, 1496.08/s)  LR: 6.204e-04  Data: 0.012 (0.018)
Train: 12 [ 550/1251 ( 44%)]  Loss: 4.686 (4.85)  Time: 0.696s, 1471.75/s  (0.684s, 1496.14/s)  LR: 6.224e-04  Data: 0.013 (0.018)
Train: 12 [ 600/1251 ( 48%)]  Loss: 5.078 (4.87)  Time: 0.693s, 1477.91/s  (0.684s, 1496.30/s)  LR: 6.244e-04  Data: 0.016 (0.017)
Train: 12 [ 650/1251 ( 52%)]  Loss: 4.873 (4.87)  Time: 0.682s, 1502.56/s  (0.684s, 1496.54/s)  LR: 6.264e-04  Data: 0.014 (0.017)
Train: 12 [ 700/1251 ( 56%)]  Loss: 4.816 (4.86)  Time: 0.695s, 1472.74/s  (0.684s, 1496.67/s)  LR: 6.284e-04  Data: 0.016 (0.017)
Train: 12 [ 750/1251 ( 60%)]  Loss: 5.020 (4.87)  Time: 0.683s, 1500.11/s  (0.684s, 1496.53/s)  LR: 6.304e-04  Data: 0.012 (0.017)
Train: 12 [ 800/1251 ( 64%)]  Loss: 4.838 (4.87)  Time: 0.676s, 1514.10/s  (0.684s, 1496.76/s)  LR: 6.324e-04  Data: 0.012 (0.016)
Train: 12 [ 850/1251 ( 68%)]  Loss: 4.835 (4.87)  Time: 0.685s, 1494.01/s  (0.684s, 1496.94/s)  LR: 6.344e-04  Data: 0.013 (0.016)
Train: 12 [ 900/1251 ( 72%)]  Loss: 4.443 (4.85)  Time: 0.692s, 1478.89/s  (0.684s, 1497.19/s)  LR: 6.364e-04  Data: 0.016 (0.016)
Train: 12 [ 950/1251 ( 76%)]  Loss: 4.476 (4.83)  Time: 0.688s, 1488.00/s  (0.684s, 1497.38/s)  LR: 6.384e-04  Data: 0.013 (0.016)
Train: 12 [1000/1251 ( 80%)]  Loss: 4.687 (4.82)  Time: 0.691s, 1481.72/s  (0.684s, 1497.29/s)  LR: 6.404e-04  Data: 0.014 (0.016)
Train: 12 [1050/1251 ( 84%)]  Loss: 4.894 (4.83)  Time: 0.696s, 1471.91/s  (0.684s, 1497.56/s)  LR: 6.424e-04  Data: 0.013 (0.016)
Train: 12 [1100/1251 ( 88%)]  Loss: 4.883 (4.83)  Time: 0.684s, 1496.61/s  (0.684s, 1497.71/s)  LR: 6.444e-04  Data: 0.014 (0.016)
Train: 12 [1150/1251 ( 92%)]  Loss: 4.958 (4.83)  Time: 0.688s, 1489.14/s  (0.684s, 1497.88/s)  LR: 6.464e-04  Data: 0.015 (0.016)
Train: 12 [1200/1251 ( 96%)]  Loss: 4.774 (4.83)  Time: 0.684s, 1497.42/s  (0.684s, 1498.09/s)  LR: 6.484e-04  Data: 0.012 (0.016)
Train: 12 [1250/1251 (100%)]  Loss: 5.011 (4.84)  Time: 0.669s, 1530.71/s  (0.683s, 1498.54/s)  LR: 6.504e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.895 (2.895)  Loss:  1.2559 (1.2559)  Acc@1: 70.9961 (70.9961)  Acc@5: 91.7969 (91.7969)
Test: [  48/48]  Time: 0.168 (0.325)  Loss:  1.2764 (2.2331)  Acc@1: 73.9387 (51.6540)  Acc@5: 89.5047 (76.7940)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-12.pth.tar', 51.65400005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-11.pth.tar', 50.00400004638672)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-10.pth.tar', 47.43999999511719)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-9.pth.tar', 44.52000004882812)

Train: 13 [   0/1251 (  0%)]  Loss: 5.031 (5.03)  Time: 3.757s,  272.57/s  (3.757s,  272.57/s)  LR: 6.504e-04  Data: 1.604 (1.604)
Train: 13 [  50/1251 (  4%)]  Loss: 5.221 (5.13)  Time: 0.664s, 1541.47/s  (0.709s, 1444.29/s)  LR: 6.524e-04  Data: 0.013 (0.046)
Train: 13 [ 100/1251 (  8%)]  Loss: 5.074 (5.11)  Time: 0.682s, 1500.86/s  (0.693s, 1478.31/s)  LR: 6.544e-04  Data: 0.013 (0.030)
Train: 13 [ 150/1251 ( 12%)]  Loss: 5.082 (5.10)  Time: 0.695s, 1473.24/s  (0.689s, 1486.85/s)  LR: 6.564e-04  Data: 0.016 (0.025)
Train: 13 [ 200/1251 ( 16%)]  Loss: 4.480 (4.98)  Time: 0.679s, 1507.23/s  (0.686s, 1491.85/s)  LR: 6.584e-04  Data: 0.013 (0.022)
Train: 13 [ 250/1251 ( 20%)]  Loss: 4.830 (4.95)  Time: 0.688s, 1488.39/s  (0.685s, 1494.41/s)  LR: 6.604e-04  Data: 0.014 (0.020)
Train: 13 [ 300/1251 ( 24%)]  Loss: 5.056 (4.97)  Time: 0.682s, 1500.61/s  (0.685s, 1494.95/s)  LR: 6.624e-04  Data: 0.013 (0.019)
Train: 13 [ 350/1251 ( 28%)]  Loss: 5.036 (4.98)  Time: 0.686s, 1491.70/s  (0.685s, 1494.96/s)  LR: 6.644e-04  Data: 0.013 (0.019)
Train: 13 [ 400/1251 ( 32%)]  Loss: 4.984 (4.98)  Time: 0.684s, 1498.17/s  (0.685s, 1495.05/s)  LR: 6.664e-04  Data: 0.010 (0.018)
Train: 13 [ 450/1251 ( 36%)]  Loss: 5.190 (5.00)  Time: 0.682s, 1501.59/s  (0.685s, 1495.37/s)  LR: 6.684e-04  Data: 0.012 (0.018)
Train: 13 [ 500/1251 ( 40%)]  Loss: 4.859 (4.99)  Time: 0.680s, 1505.84/s  (0.685s, 1495.84/s)  LR: 6.704e-04  Data: 0.021 (0.017)
Train: 13 [ 550/1251 ( 44%)]  Loss: 4.954 (4.98)  Time: 0.681s, 1503.88/s  (0.684s, 1496.32/s)  LR: 6.724e-04  Data: 0.014 (0.017)
Train: 13 [ 600/1251 ( 48%)]  Loss: 4.835 (4.97)  Time: 0.677s, 1513.09/s  (0.684s, 1496.85/s)  LR: 6.743e-04  Data: 0.013 (0.017)
Train: 13 [ 650/1251 ( 52%)]  Loss: 4.646 (4.95)  Time: 0.681s, 1503.30/s  (0.684s, 1496.90/s)  LR: 6.763e-04  Data: 0.016 (0.016)
Train: 13 [ 700/1251 ( 56%)]  Loss: 4.890 (4.94)  Time: 0.685s, 1494.65/s  (0.684s, 1497.02/s)  LR: 6.783e-04  Data: 0.014 (0.016)
Train: 13 [ 750/1251 ( 60%)]  Loss: 4.736 (4.93)  Time: 0.672s, 1522.73/s  (0.684s, 1497.35/s)  LR: 6.803e-04  Data: 0.014 (0.016)
Train: 13 [ 800/1251 ( 64%)]  Loss: 4.960 (4.93)  Time: 0.680s, 1505.24/s  (0.684s, 1497.53/s)  LR: 6.823e-04  Data: 0.012 (0.016)
Train: 13 [ 850/1251 ( 68%)]  Loss: 5.149 (4.95)  Time: 0.679s, 1507.43/s  (0.684s, 1497.64/s)  LR: 6.843e-04  Data: 0.013 (0.016)
Train: 13 [ 900/1251 ( 72%)]  Loss: 4.427 (4.92)  Time: 0.686s, 1491.74/s  (0.684s, 1497.66/s)  LR: 6.863e-04  Data: 0.013 (0.016)
Train: 13 [ 950/1251 ( 76%)]  Loss: 4.864 (4.92)  Time: 0.684s, 1496.96/s  (0.684s, 1497.85/s)  LR: 6.883e-04  Data: 0.013 (0.016)
Train: 13 [1000/1251 ( 80%)]  Loss: 4.837 (4.91)  Time: 0.678s, 1510.39/s  (0.684s, 1497.98/s)  LR: 6.903e-04  Data: 0.013 (0.016)
Train: 13 [1050/1251 ( 84%)]  Loss: 4.804 (4.91)  Time: 0.689s, 1487.17/s  (0.684s, 1497.93/s)  LR: 6.923e-04  Data: 0.012 (0.015)
Train: 13 [1100/1251 ( 88%)]  Loss: 5.002 (4.91)  Time: 0.685s, 1494.20/s  (0.684s, 1497.94/s)  LR: 6.943e-04  Data: 0.013 (0.015)
Train: 13 [1150/1251 ( 92%)]  Loss: 4.870 (4.91)  Time: 0.680s, 1506.17/s  (0.684s, 1497.88/s)  LR: 6.963e-04  Data: 0.016 (0.015)
Train: 13 [1200/1251 ( 96%)]  Loss: 4.979 (4.91)  Time: 0.687s, 1489.46/s  (0.683s, 1498.23/s)  LR: 6.983e-04  Data: 0.013 (0.015)
Train: 13 [1250/1251 (100%)]  Loss: 4.780 (4.91)  Time: 0.674s, 1518.51/s  (0.683s, 1498.40/s)  LR: 7.003e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.821 (2.821)  Loss:  1.1826 (1.1826)  Acc@1: 74.6094 (74.6094)  Acc@5: 92.0898 (92.0898)
Test: [  48/48]  Time: 0.175 (0.325)  Loss:  1.2334 (2.1443)  Acc@1: 74.6462 (53.2560)  Acc@5: 89.9764 (78.1680)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-13.pth.tar', 53.256000104980465)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-12.pth.tar', 51.65400005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-11.pth.tar', 50.00400004638672)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-10.pth.tar', 47.43999999511719)

Train: 14 [   0/1251 (  0%)]  Loss: 4.729 (4.73)  Time: 3.648s,  280.68/s  (3.648s,  280.68/s)  LR: 7.003e-04  Data: 2.105 (2.105)
Train: 14 [  50/1251 (  4%)]  Loss: 4.658 (4.69)  Time: 0.670s, 1528.08/s  (0.711s, 1439.52/s)  LR: 7.023e-04  Data: 0.013 (0.055)
Train: 14 [ 100/1251 (  8%)]  Loss: 4.987 (4.79)  Time: 0.676s, 1515.02/s  (0.691s, 1481.00/s)  LR: 7.043e-04  Data: 0.013 (0.035)
Train: 14 [ 150/1251 ( 12%)]  Loss: 4.593 (4.74)  Time: 0.678s, 1509.82/s  (0.687s, 1491.44/s)  LR: 7.063e-04  Data: 0.012 (0.028)
Train: 14 [ 200/1251 ( 16%)]  Loss: 4.725 (4.74)  Time: 0.674s, 1519.22/s  (0.684s, 1496.48/s)  LR: 7.083e-04  Data: 0.013 (0.025)
Train: 14 [ 250/1251 ( 20%)]  Loss: 5.107 (4.80)  Time: 0.682s, 1500.57/s  (0.683s, 1498.58/s)  LR: 7.103e-04  Data: 0.013 (0.023)
Train: 14 [ 300/1251 ( 24%)]  Loss: 5.027 (4.83)  Time: 0.671s, 1525.80/s  (0.683s, 1499.04/s)  LR: 7.123e-04  Data: 0.013 (0.021)
Train: 14 [ 350/1251 ( 28%)]  Loss: 4.558 (4.80)  Time: 0.672s, 1524.54/s  (0.683s, 1499.41/s)  LR: 7.143e-04  Data: 0.015 (0.020)
Train: 14 [ 400/1251 ( 32%)]  Loss: 4.896 (4.81)  Time: 0.678s, 1510.06/s  (0.683s, 1499.76/s)  LR: 7.163e-04  Data: 0.013 (0.019)
Train: 14 [ 450/1251 ( 36%)]  Loss: 4.427 (4.77)  Time: 0.675s, 1518.07/s  (0.683s, 1499.94/s)  LR: 7.183e-04  Data: 0.015 (0.019)
Train: 14 [ 500/1251 ( 40%)]  Loss: 4.701 (4.76)  Time: 0.674s, 1519.89/s  (0.683s, 1500.13/s)  LR: 7.203e-04  Data: 0.017 (0.018)
Train: 14 [ 550/1251 ( 44%)]  Loss: 4.640 (4.75)  Time: 0.685s, 1494.71/s  (0.682s, 1500.43/s)  LR: 7.223e-04  Data: 0.016 (0.018)
Train: 14 [ 600/1251 ( 48%)]  Loss: 4.858 (4.76)  Time: 0.680s, 1506.64/s  (0.682s, 1501.06/s)  LR: 7.243e-04  Data: 0.013 (0.018)
Train: 14 [ 650/1251 ( 52%)]  Loss: 4.885 (4.77)  Time: 0.685s, 1494.53/s  (0.682s, 1501.33/s)  LR: 7.263e-04  Data: 0.014 (0.017)
Train: 14 [ 700/1251 ( 56%)]  Loss: 4.870 (4.78)  Time: 0.675s, 1516.24/s  (0.682s, 1501.72/s)  LR: 7.283e-04  Data: 0.013 (0.017)
Train: 14 [ 750/1251 ( 60%)]  Loss: 5.003 (4.79)  Time: 0.676s, 1513.83/s  (0.682s, 1502.06/s)  LR: 7.303e-04  Data: 0.016 (0.017)
Train: 14 [ 800/1251 ( 64%)]  Loss: 4.622 (4.78)  Time: 0.686s, 1491.73/s  (0.682s, 1502.11/s)  LR: 7.323e-04  Data: 0.014 (0.017)
Train: 14 [ 850/1251 ( 68%)]  Loss: 4.581 (4.77)  Time: 0.676s, 1515.11/s  (0.682s, 1502.23/s)  LR: 7.343e-04  Data: 0.017 (0.017)
Train: 14 [ 900/1251 ( 72%)]  Loss: 5.184 (4.79)  Time: 0.667s, 1535.74/s  (0.682s, 1502.36/s)  LR: 7.363e-04  Data: 0.016 (0.017)
Train: 14 [ 950/1251 ( 76%)]  Loss: 4.874 (4.80)  Time: 0.676s, 1514.81/s  (0.682s, 1502.55/s)  LR: 7.383e-04  Data: 0.012 (0.016)
Train: 14 [1000/1251 ( 80%)]  Loss: 4.512 (4.78)  Time: 0.676s, 1514.29/s  (0.682s, 1502.46/s)  LR: 7.403e-04  Data: 0.013 (0.016)
Train: 14 [1050/1251 ( 84%)]  Loss: 4.625 (4.78)  Time: 0.673s, 1521.71/s  (0.681s, 1502.75/s)  LR: 7.423e-04  Data: 0.013 (0.016)
Train: 14 [1100/1251 ( 88%)]  Loss: 5.023 (4.79)  Time: 0.671s, 1527.15/s  (0.681s, 1502.93/s)  LR: 7.443e-04  Data: 0.011 (0.016)
Train: 14 [1150/1251 ( 92%)]  Loss: 4.835 (4.79)  Time: 0.683s, 1498.31/s  (0.681s, 1502.93/s)  LR: 7.463e-04  Data: 0.017 (0.016)
Train: 14 [1200/1251 ( 96%)]  Loss: 4.726 (4.79)  Time: 0.675s, 1517.33/s  (0.681s, 1503.32/s)  LR: 7.483e-04  Data: 0.014 (0.016)
Train: 14 [1250/1251 (100%)]  Loss: 4.546 (4.78)  Time: 0.661s, 1549.25/s  (0.681s, 1503.58/s)  LR: 7.503e-04  Data: 0.000 (0.016)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.831 (2.831)  Loss:  1.1377 (1.1377)  Acc@1: 75.7812 (75.7812)  Acc@5: 92.6758 (92.6758)
Test: [  48/48]  Time: 0.168 (0.331)  Loss:  1.2637 (2.0362)  Acc@1: 73.9387 (55.1200)  Acc@5: 89.9764 (79.6000)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-14.pth.tar', 55.119999926757814)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-13.pth.tar', 53.256000104980465)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-12.pth.tar', 51.65400005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-11.pth.tar', 50.00400004638672)

Train: 15 [   0/1251 (  0%)]  Loss: 4.578 (4.58)  Time: 3.851s,  265.91/s  (3.851s,  265.91/s)  LR: 7.503e-04  Data: 1.737 (1.737)
Train: 15 [  50/1251 (  4%)]  Loss: 4.878 (4.73)  Time: 0.681s, 1503.58/s  (0.709s, 1444.69/s)  LR: 7.523e-04  Data: 0.013 (0.048)
Train: 15 [ 100/1251 (  8%)]  Loss: 4.659 (4.70)  Time: 0.672s, 1523.10/s  (0.693s, 1478.45/s)  LR: 7.543e-04  Data: 0.014 (0.031)
Train: 15 [ 150/1251 ( 12%)]  Loss: 4.817 (4.73)  Time: 0.688s, 1489.24/s  (0.689s, 1486.72/s)  LR: 7.563e-04  Data: 0.013 (0.026)
Train: 15 [ 200/1251 ( 16%)]  Loss: 4.575 (4.70)  Time: 0.680s, 1505.43/s  (0.686s, 1491.69/s)  LR: 7.583e-04  Data: 0.012 (0.023)
Train: 15 [ 250/1251 ( 20%)]  Loss: 4.653 (4.69)  Time: 0.680s, 1506.35/s  (0.685s, 1493.89/s)  LR: 7.603e-04  Data: 0.014 (0.021)
Train: 15 [ 300/1251 ( 24%)]  Loss: 4.751 (4.70)  Time: 0.677s, 1511.51/s  (0.685s, 1495.53/s)  LR: 7.623e-04  Data: 0.015 (0.020)
Train: 15 [ 350/1251 ( 28%)]  Loss: 4.689 (4.70)  Time: 0.683s, 1498.54/s  (0.685s, 1495.54/s)  LR: 7.643e-04  Data: 0.017 (0.019)
Train: 15 [ 400/1251 ( 32%)]  Loss: 4.655 (4.69)  Time: 0.679s, 1508.73/s  (0.685s, 1495.32/s)  LR: 7.663e-04  Data: 0.013 (0.018)
Train: 15 [ 450/1251 ( 36%)]  Loss: 4.304 (4.66)  Time: 0.678s, 1509.41/s  (0.685s, 1495.79/s)  LR: 7.683e-04  Data: 0.016 (0.018)
Train: 15 [ 500/1251 ( 40%)]  Loss: 4.160 (4.61)  Time: 0.679s, 1507.51/s  (0.685s, 1495.86/s)  LR: 7.703e-04  Data: 0.013 (0.017)
Train: 15 [ 550/1251 ( 44%)]  Loss: 4.875 (4.63)  Time: 0.668s, 1531.85/s  (0.684s, 1496.10/s)  LR: 7.723e-04  Data: 0.016 (0.017)
Train: 15 [ 600/1251 ( 48%)]  Loss: 4.557 (4.63)  Time: 0.688s, 1488.62/s  (0.684s, 1496.07/s)  LR: 7.742e-04  Data: 0.012 (0.017)
Train: 15 [ 650/1251 ( 52%)]  Loss: 4.789 (4.64)  Time: 0.691s, 1481.39/s  (0.684s, 1496.11/s)  LR: 7.762e-04  Data: 0.012 (0.017)
Train: 15 [ 700/1251 ( 56%)]  Loss: 4.588 (4.64)  Time: 0.682s, 1500.70/s  (0.684s, 1496.11/s)  LR: 7.782e-04  Data: 0.013 (0.017)
Train: 15 [ 750/1251 ( 60%)]  Loss: 4.800 (4.65)  Time: 0.680s, 1506.16/s  (0.684s, 1496.02/s)  LR: 7.802e-04  Data: 0.013 (0.016)
Train: 15 [ 800/1251 ( 64%)]  Loss: 4.738 (4.65)  Time: 0.678s, 1509.45/s  (0.684s, 1496.40/s)  LR: 7.822e-04  Data: 0.014 (0.016)
Train: 15 [ 850/1251 ( 68%)]  Loss: 4.441 (4.64)  Time: 0.688s, 1488.39/s  (0.684s, 1496.57/s)  LR: 7.842e-04  Data: 0.011 (0.016)
Train: 15 [ 900/1251 ( 72%)]  Loss: 4.718 (4.64)  Time: 0.688s, 1487.76/s  (0.684s, 1496.63/s)  LR: 7.862e-04  Data: 0.012 (0.016)
Train: 15 [ 950/1251 ( 76%)]  Loss: 4.619 (4.64)  Time: 0.687s, 1489.72/s  (0.684s, 1496.80/s)  LR: 7.882e-04  Data: 0.013 (0.016)
Train: 15 [1000/1251 ( 80%)]  Loss: 4.453 (4.63)  Time: 0.673s, 1522.44/s  (0.684s, 1497.04/s)  LR: 7.902e-04  Data: 0.013 (0.016)
Train: 15 [1050/1251 ( 84%)]  Loss: 4.332 (4.62)  Time: 0.678s, 1509.34/s  (0.684s, 1497.25/s)  LR: 7.922e-04  Data: 0.013 (0.016)
Train: 15 [1100/1251 ( 88%)]  Loss: 4.373 (4.61)  Time: 0.687s, 1490.72/s  (0.684s, 1497.40/s)  LR: 7.942e-04  Data: 0.013 (0.016)
Train: 15 [1150/1251 ( 92%)]  Loss: 4.709 (4.61)  Time: 0.684s, 1496.83/s  (0.684s, 1497.50/s)  LR: 7.962e-04  Data: 0.012 (0.016)
Train: 15 [1200/1251 ( 96%)]  Loss: 4.384 (4.60)  Time: 0.676s, 1513.97/s  (0.684s, 1497.55/s)  LR: 7.982e-04  Data: 0.013 (0.015)
Train: 15 [1250/1251 (100%)]  Loss: 4.664 (4.61)  Time: 0.678s, 1511.05/s  (0.684s, 1497.68/s)  LR: 8.002e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.727 (2.727)  Loss:  1.0957 (1.0957)  Acc@1: 76.1719 (76.1719)  Acc@5: 92.0898 (92.0898)
Test: [  48/48]  Time: 0.169 (0.329)  Loss:  1.1113 (1.9765)  Acc@1: 76.8868 (56.0820)  Acc@5: 90.3302 (80.6440)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-15.pth.tar', 56.082000043945314)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-14.pth.tar', 55.119999926757814)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-13.pth.tar', 53.256000104980465)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-12.pth.tar', 51.65400005615234)

Train: 16 [   0/1251 (  0%)]  Loss: 4.837 (4.84)  Time: 3.961s,  258.52/s  (3.961s,  258.52/s)  LR: 8.002e-04  Data: 1.795 (1.795)
Train: 16 [  50/1251 (  4%)]  Loss: 4.763 (4.80)  Time: 0.663s, 1545.23/s  (0.712s, 1437.26/s)  LR: 8.022e-04  Data: 0.012 (0.049)
Train: 16 [ 100/1251 (  8%)]  Loss: 5.120 (4.91)  Time: 0.682s, 1500.44/s  (0.696s, 1472.07/s)  LR: 8.042e-04  Data: 0.013 (0.031)
Train: 16 [ 150/1251 ( 12%)]  Loss: 4.860 (4.90)  Time: 0.684s, 1497.77/s  (0.691s, 1480.89/s)  LR: 8.062e-04  Data: 0.013 (0.026)
Train: 16 [ 200/1251 ( 16%)]  Loss: 4.958 (4.91)  Time: 0.680s, 1506.87/s  (0.689s, 1485.85/s)  LR: 8.082e-04  Data: 0.012 (0.023)
Train: 16 [ 250/1251 ( 20%)]  Loss: 4.616 (4.86)  Time: 0.683s, 1499.22/s  (0.688s, 1488.58/s)  LR: 8.102e-04  Data: 0.014 (0.021)
Train: 16 [ 300/1251 ( 24%)]  Loss: 4.875 (4.86)  Time: 0.684s, 1497.68/s  (0.687s, 1490.73/s)  LR: 8.122e-04  Data: 0.011 (0.020)
Train: 16 [ 350/1251 ( 28%)]  Loss: 4.791 (4.85)  Time: 0.686s, 1493.08/s  (0.686s, 1492.15/s)  LR: 8.142e-04  Data: 0.013 (0.019)
Train: 16 [ 400/1251 ( 32%)]  Loss: 4.659 (4.83)  Time: 0.688s, 1488.61/s  (0.686s, 1492.98/s)  LR: 8.162e-04  Data: 0.012 (0.018)
Train: 16 [ 450/1251 ( 36%)]  Loss: 4.046 (4.75)  Time: 0.669s, 1530.74/s  (0.686s, 1493.28/s)  LR: 8.182e-04  Data: 0.012 (0.018)
Train: 16 [ 500/1251 ( 40%)]  Loss: 4.469 (4.73)  Time: 0.687s, 1490.94/s  (0.686s, 1493.48/s)  LR: 8.202e-04  Data: 0.014 (0.017)
Train: 16 [ 550/1251 ( 44%)]  Loss: 4.511 (4.71)  Time: 0.691s, 1482.04/s  (0.685s, 1493.84/s)  LR: 8.222e-04  Data: 0.016 (0.017)
Train: 16 [ 600/1251 ( 48%)]  Loss: 4.620 (4.70)  Time: 0.686s, 1493.63/s  (0.685s, 1494.24/s)  LR: 8.242e-04  Data: 0.013 (0.017)
Train: 16 [ 650/1251 ( 52%)]  Loss: 4.593 (4.69)  Time: 0.684s, 1498.14/s  (0.685s, 1494.34/s)  LR: 8.262e-04  Data: 0.012 (0.017)
Train: 16 [ 700/1251 ( 56%)]  Loss: 4.462 (4.68)  Time: 0.683s, 1498.39/s  (0.685s, 1494.48/s)  LR: 8.282e-04  Data: 0.016 (0.016)
Train: 16 [ 750/1251 ( 60%)]  Loss: 4.430 (4.66)  Time: 0.683s, 1499.82/s  (0.685s, 1494.66/s)  LR: 8.302e-04  Data: 0.015 (0.016)
Train: 16 [ 800/1251 ( 64%)]  Loss: 4.684 (4.66)  Time: 0.691s, 1481.73/s  (0.685s, 1494.52/s)  LR: 8.322e-04  Data: 0.013 (0.016)
Train: 16 [ 850/1251 ( 68%)]  Loss: 4.502 (4.66)  Time: 0.690s, 1483.86/s  (0.685s, 1494.20/s)  LR: 8.342e-04  Data: 0.015 (0.016)
Train: 16 [ 900/1251 ( 72%)]  Loss: 4.565 (4.65)  Time: 0.686s, 1492.51/s  (0.685s, 1494.13/s)  LR: 8.362e-04  Data: 0.014 (0.016)
Train: 16 [ 950/1251 ( 76%)]  Loss: 4.253 (4.63)  Time: 0.691s, 1482.05/s  (0.685s, 1494.08/s)  LR: 8.382e-04  Data: 0.014 (0.016)
Train: 16 [1000/1251 ( 80%)]  Loss: 4.651 (4.63)  Time: 0.684s, 1497.98/s  (0.685s, 1494.45/s)  LR: 8.402e-04  Data: 0.013 (0.016)
Train: 16 [1050/1251 ( 84%)]  Loss: 4.799 (4.64)  Time: 0.675s, 1517.16/s  (0.685s, 1494.71/s)  LR: 8.422e-04  Data: 0.013 (0.016)
Train: 16 [1100/1251 ( 88%)]  Loss: 4.579 (4.64)  Time: 0.680s, 1505.42/s  (0.685s, 1495.01/s)  LR: 8.442e-04  Data: 0.013 (0.016)
Train: 16 [1150/1251 ( 92%)]  Loss: 4.897 (4.65)  Time: 0.673s, 1522.66/s  (0.685s, 1495.31/s)  LR: 8.462e-04  Data: 0.013 (0.015)
Train: 16 [1200/1251 ( 96%)]  Loss: 4.326 (4.63)  Time: 0.674s, 1518.79/s  (0.685s, 1495.62/s)  LR: 8.482e-04  Data: 0.013 (0.015)
Train: 16 [1250/1251 (100%)]  Loss: 4.660 (4.64)  Time: 0.679s, 1509.03/s  (0.685s, 1495.85/s)  LR: 8.502e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.866 (2.866)  Loss:  1.0596 (1.0596)  Acc@1: 75.7812 (75.7812)  Acc@5: 92.6758 (92.6758)
Test: [  48/48]  Time: 0.169 (0.324)  Loss:  1.1855 (1.9221)  Acc@1: 74.7642 (57.0080)  Acc@5: 90.5660 (81.4860)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-16.pth.tar', 57.00800002685547)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-15.pth.tar', 56.082000043945314)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-14.pth.tar', 55.119999926757814)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-13.pth.tar', 53.256000104980465)

Train: 17 [   0/1251 (  0%)]  Loss: 4.554 (4.55)  Time: 3.406s,  300.64/s  (3.406s,  300.64/s)  LR: 8.502e-04  Data: 1.728 (1.728)
Train: 17 [  50/1251 (  4%)]  Loss: 4.534 (4.54)  Time: 0.682s, 1502.47/s  (0.704s, 1453.53/s)  LR: 8.522e-04  Data: 0.013 (0.048)
Train: 17 [ 100/1251 (  8%)]  Loss: 4.214 (4.43)  Time: 0.680s, 1506.92/s  (0.692s, 1480.12/s)  LR: 8.542e-04  Data: 0.015 (0.031)
Train: 17 [ 150/1251 ( 12%)]  Loss: 4.672 (4.49)  Time: 0.679s, 1507.27/s  (0.689s, 1485.46/s)  LR: 8.562e-04  Data: 0.012 (0.026)
Train: 17 [ 200/1251 ( 16%)]  Loss: 4.069 (4.41)  Time: 0.683s, 1500.10/s  (0.688s, 1489.25/s)  LR: 8.582e-04  Data: 0.013 (0.023)
Train: 17 [ 250/1251 ( 20%)]  Loss: 4.772 (4.47)  Time: 0.690s, 1483.28/s  (0.687s, 1490.25/s)  LR: 8.602e-04  Data: 0.013 (0.021)
Train: 17 [ 300/1251 ( 24%)]  Loss: 4.502 (4.47)  Time: 0.680s, 1506.17/s  (0.687s, 1491.21/s)  LR: 8.622e-04  Data: 0.014 (0.020)
Train: 17 [ 350/1251 ( 28%)]  Loss: 4.351 (4.46)  Time: 0.682s, 1502.32/s  (0.686s, 1492.14/s)  LR: 8.642e-04  Data: 0.013 (0.019)
Train: 17 [ 400/1251 ( 32%)]  Loss: 4.453 (4.46)  Time: 0.685s, 1494.89/s  (0.686s, 1493.11/s)  LR: 8.662e-04  Data: 0.013 (0.018)
Train: 17 [ 450/1251 ( 36%)]  Loss: 4.517 (4.46)  Time: 0.677s, 1513.25/s  (0.685s, 1493.84/s)  LR: 8.682e-04  Data: 0.012 (0.018)
Train: 17 [ 500/1251 ( 40%)]  Loss: 4.459 (4.46)  Time: 0.684s, 1497.73/s  (0.685s, 1494.03/s)  LR: 8.702e-04  Data: 0.013 (0.018)
Train: 17 [ 550/1251 ( 44%)]  Loss: 4.569 (4.47)  Time: 0.682s, 1502.25/s  (0.685s, 1494.59/s)  LR: 8.722e-04  Data: 0.017 (0.017)
Train: 17 [ 600/1251 ( 48%)]  Loss: 4.556 (4.48)  Time: 0.681s, 1503.55/s  (0.685s, 1494.78/s)  LR: 8.741e-04  Data: 0.012 (0.017)
Train: 17 [ 650/1251 ( 52%)]  Loss: 4.579 (4.49)  Time: 0.676s, 1514.42/s  (0.685s, 1495.13/s)  LR: 8.761e-04  Data: 0.016 (0.017)
Train: 17 [ 700/1251 ( 56%)]  Loss: 4.420 (4.48)  Time: 0.687s, 1489.87/s  (0.685s, 1495.56/s)  LR: 8.781e-04  Data: 0.013 (0.017)
Train: 17 [ 750/1251 ( 60%)]  Loss: 4.720 (4.50)  Time: 0.681s, 1504.43/s  (0.684s, 1496.34/s)  LR: 8.801e-04  Data: 0.012 (0.016)
Train: 17 [ 800/1251 ( 64%)]  Loss: 4.336 (4.49)  Time: 0.683s, 1499.19/s  (0.684s, 1496.73/s)  LR: 8.821e-04  Data: 0.013 (0.016)
Train: 17 [ 850/1251 ( 68%)]  Loss: 4.399 (4.48)  Time: 0.685s, 1494.92/s  (0.684s, 1497.21/s)  LR: 8.841e-04  Data: 0.014 (0.016)
Train: 17 [ 900/1251 ( 72%)]  Loss: 4.764 (4.50)  Time: 0.694s, 1475.44/s  (0.684s, 1497.45/s)  LR: 8.861e-04  Data: 0.012 (0.016)
Train: 17 [ 950/1251 ( 76%)]  Loss: 4.584 (4.50)  Time: 0.678s, 1510.79/s  (0.684s, 1497.81/s)  LR: 8.881e-04  Data: 0.017 (0.016)
Train: 17 [1000/1251 ( 80%)]  Loss: 4.543 (4.50)  Time: 0.678s, 1509.97/s  (0.683s, 1498.31/s)  LR: 8.901e-04  Data: 0.012 (0.016)
Train: 17 [1050/1251 ( 84%)]  Loss: 4.669 (4.51)  Time: 0.672s, 1522.73/s  (0.683s, 1498.63/s)  LR: 8.921e-04  Data: 0.013 (0.016)
Train: 17 [1100/1251 ( 88%)]  Loss: 4.558 (4.51)  Time: 0.686s, 1492.58/s  (0.683s, 1499.21/s)  LR: 8.941e-04  Data: 0.013 (0.016)
Train: 17 [1150/1251 ( 92%)]  Loss: 4.476 (4.51)  Time: 0.677s, 1513.05/s  (0.683s, 1499.65/s)  LR: 8.961e-04  Data: 0.015 (0.016)
Train: 17 [1200/1251 ( 96%)]  Loss: 4.591 (4.51)  Time: 0.690s, 1483.90/s  (0.683s, 1499.98/s)  LR: 8.981e-04  Data: 0.013 (0.016)
Train: 17 [1250/1251 (100%)]  Loss: 4.603 (4.52)  Time: 0.658s, 1555.63/s  (0.683s, 1500.27/s)  LR: 9.001e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.924 (2.924)  Loss:  1.1367 (1.1367)  Acc@1: 77.3438 (77.3438)  Acc@5: 93.7500 (93.7500)
Test: [  48/48]  Time: 0.166 (0.338)  Loss:  1.0234 (1.8939)  Acc@1: 77.7123 (58.1340)  Acc@5: 92.3349 (82.1120)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-17.pth.tar', 58.134000014648436)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-16.pth.tar', 57.00800002685547)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-15.pth.tar', 56.082000043945314)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-14.pth.tar', 55.119999926757814)

Train: 18 [   0/1251 (  0%)]  Loss: 4.310 (4.31)  Time: 3.519s,  290.95/s  (3.519s,  290.95/s)  LR: 9.001e-04  Data: 1.499 (1.499)
Train: 18 [  50/1251 (  4%)]  Loss: 4.604 (4.46)  Time: 0.659s, 1552.89/s  (0.700s, 1463.46/s)  LR: 9.021e-04  Data: 0.015 (0.043)
Train: 18 [ 100/1251 (  8%)]  Loss: 4.602 (4.51)  Time: 0.680s, 1504.97/s  (0.688s, 1488.11/s)  LR: 9.041e-04  Data: 0.017 (0.029)
Train: 18 [ 150/1251 ( 12%)]  Loss: 4.638 (4.54)  Time: 0.685s, 1495.49/s  (0.685s, 1494.67/s)  LR: 9.061e-04  Data: 0.012 (0.024)
Train: 18 [ 200/1251 ( 16%)]  Loss: 4.709 (4.57)  Time: 0.684s, 1496.19/s  (0.684s, 1496.90/s)  LR: 9.081e-04  Data: 0.013 (0.022)
Train: 18 [ 250/1251 ( 20%)]  Loss: 4.908 (4.63)  Time: 0.689s, 1485.64/s  (0.683s, 1498.18/s)  LR: 9.101e-04  Data: 0.013 (0.020)
Train: 18 [ 300/1251 ( 24%)]  Loss: 4.590 (4.62)  Time: 0.688s, 1487.90/s  (0.683s, 1498.55/s)  LR: 9.121e-04  Data: 0.017 (0.019)
Train: 18 [ 350/1251 ( 28%)]  Loss: 4.662 (4.63)  Time: 0.691s, 1481.98/s  (0.684s, 1498.11/s)  LR: 9.141e-04  Data: 0.013 (0.018)
Train: 18 [ 400/1251 ( 32%)]  Loss: 4.591 (4.62)  Time: 0.687s, 1490.00/s  (0.683s, 1498.25/s)  LR: 9.161e-04  Data: 0.012 (0.018)
Train: 18 [ 450/1251 ( 36%)]  Loss: 4.754 (4.64)  Time: 0.679s, 1508.40/s  (0.683s, 1498.62/s)  LR: 9.181e-04  Data: 0.015 (0.017)
Train: 18 [ 500/1251 ( 40%)]  Loss: 4.170 (4.59)  Time: 0.686s, 1491.82/s  (0.683s, 1498.83/s)  LR: 9.201e-04  Data: 0.014 (0.017)
Train: 18 [ 550/1251 ( 44%)]  Loss: 4.387 (4.58)  Time: 0.672s, 1524.08/s  (0.683s, 1499.39/s)  LR: 9.221e-04  Data: 0.015 (0.017)
Train: 18 [ 600/1251 ( 48%)]  Loss: 4.334 (4.56)  Time: 0.686s, 1492.79/s  (0.683s, 1499.79/s)  LR: 9.241e-04  Data: 0.014 (0.017)
Train: 18 [ 650/1251 ( 52%)]  Loss: 4.326 (4.54)  Time: 0.689s, 1485.17/s  (0.683s, 1500.14/s)  LR: 9.261e-04  Data: 0.014 (0.016)
Train: 18 [ 700/1251 ( 56%)]  Loss: 4.500 (4.54)  Time: 0.682s, 1502.06/s  (0.682s, 1500.84/s)  LR: 9.281e-04  Data: 0.013 (0.016)
Train: 18 [ 750/1251 ( 60%)]  Loss: 4.782 (4.55)  Time: 0.675s, 1516.94/s  (0.682s, 1501.43/s)  LR: 9.301e-04  Data: 0.013 (0.016)
Train: 18 [ 800/1251 ( 64%)]  Loss: 4.639 (4.56)  Time: 0.673s, 1521.30/s  (0.682s, 1501.80/s)  LR: 9.321e-04  Data: 0.017 (0.016)
Train: 18 [ 850/1251 ( 68%)]  Loss: 4.765 (4.57)  Time: 0.681s, 1503.61/s  (0.682s, 1502.09/s)  LR: 9.341e-04  Data: 0.013 (0.016)
Train: 18 [ 900/1251 ( 72%)]  Loss: 4.352 (4.56)  Time: 0.670s, 1527.98/s  (0.682s, 1502.56/s)  LR: 9.361e-04  Data: 0.013 (0.016)
Train: 18 [ 950/1251 ( 76%)]  Loss: 4.412 (4.55)  Time: 0.676s, 1513.75/s  (0.681s, 1502.89/s)  LR: 9.381e-04  Data: 0.014 (0.016)
Train: 18 [1000/1251 ( 80%)]  Loss: 4.213 (4.54)  Time: 0.672s, 1523.80/s  (0.681s, 1503.19/s)  LR: 9.401e-04  Data: 0.013 (0.016)
Train: 18 [1050/1251 ( 84%)]  Loss: 4.470 (4.53)  Time: 0.680s, 1506.52/s  (0.681s, 1503.46/s)  LR: 9.421e-04  Data: 0.012 (0.016)
Train: 18 [1100/1251 ( 88%)]  Loss: 4.738 (4.54)  Time: 0.680s, 1506.65/s  (0.681s, 1503.90/s)  LR: 9.441e-04  Data: 0.013 (0.015)
Train: 18 [1150/1251 ( 92%)]  Loss: 4.465 (4.54)  Time: 0.684s, 1496.09/s  (0.681s, 1503.99/s)  LR: 9.461e-04  Data: 0.012 (0.015)
Train: 18 [1200/1251 ( 96%)]  Loss: 4.347 (4.53)  Time: 0.676s, 1515.89/s  (0.681s, 1504.39/s)  LR: 9.481e-04  Data: 0.013 (0.015)
Train: 18 [1250/1251 (100%)]  Loss: 4.995 (4.55)  Time: 0.659s, 1554.42/s  (0.681s, 1504.77/s)  LR: 9.501e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.795 (2.795)  Loss:  1.1309 (1.1309)  Acc@1: 76.3672 (76.3672)  Acc@5: 93.2617 (93.2617)
Test: [  48/48]  Time: 0.172 (0.328)  Loss:  1.1299 (1.8983)  Acc@1: 77.8302 (58.7480)  Acc@5: 91.7453 (82.7580)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-18.pth.tar', 58.74800006591797)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-17.pth.tar', 58.134000014648436)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-16.pth.tar', 57.00800002685547)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-15.pth.tar', 56.082000043945314)

Train: 19 [   0/1251 (  0%)]  Loss: 4.304 (4.30)  Time: 3.307s,  309.62/s  (3.307s,  309.62/s)  LR: 9.501e-04  Data: 2.141 (2.141)
Train: 19 [  50/1251 (  4%)]  Loss: 4.440 (4.37)  Time: 0.665s, 1539.66/s  (0.697s, 1469.55/s)  LR: 9.521e-04  Data: 0.012 (0.056)
Train: 19 [ 100/1251 (  8%)]  Loss: 4.457 (4.40)  Time: 0.681s, 1502.97/s  (0.684s, 1496.68/s)  LR: 9.541e-04  Data: 0.013 (0.036)
Train: 19 [ 150/1251 ( 12%)]  Loss: 4.470 (4.42)  Time: 0.678s, 1510.83/s  (0.681s, 1502.95/s)  LR: 9.561e-04  Data: 0.012 (0.028)
Train: 19 [ 200/1251 ( 16%)]  Loss: 4.660 (4.47)  Time: 0.674s, 1518.35/s  (0.680s, 1505.12/s)  LR: 9.581e-04  Data: 0.013 (0.025)
Train: 19 [ 250/1251 ( 20%)]  Loss: 4.588 (4.49)  Time: 0.671s, 1525.52/s  (0.680s, 1506.13/s)  LR: 9.601e-04  Data: 0.012 (0.023)
Train: 19 [ 300/1251 ( 24%)]  Loss: 4.455 (4.48)  Time: 0.687s, 1490.71/s  (0.680s, 1506.89/s)  LR: 9.621e-04  Data: 0.014 (0.021)
Train: 19 [ 350/1251 ( 28%)]  Loss: 3.895 (4.41)  Time: 0.680s, 1505.36/s  (0.680s, 1506.49/s)  LR: 9.641e-04  Data: 0.013 (0.020)
Train: 19 [ 400/1251 ( 32%)]  Loss: 4.587 (4.43)  Time: 0.688s, 1488.10/s  (0.680s, 1506.01/s)  LR: 9.661e-04  Data: 0.014 (0.019)
Train: 19 [ 450/1251 ( 36%)]  Loss: 4.299 (4.42)  Time: 0.692s, 1480.24/s  (0.680s, 1505.35/s)  LR: 9.681e-04  Data: 0.013 (0.019)
Train: 19 [ 500/1251 ( 40%)]  Loss: 4.757 (4.45)  Time: 0.680s, 1505.90/s  (0.680s, 1505.54/s)  LR: 9.701e-04  Data: 0.014 (0.018)
Train: 19 [ 550/1251 ( 44%)]  Loss: 4.665 (4.46)  Time: 0.686s, 1491.94/s  (0.680s, 1505.23/s)  LR: 9.721e-04  Data: 0.013 (0.018)
Train: 19 [ 600/1251 ( 48%)]  Loss: 4.186 (4.44)  Time: 0.686s, 1493.09/s  (0.680s, 1505.06/s)  LR: 9.740e-04  Data: 0.016 (0.018)
Train: 19 [ 650/1251 ( 52%)]  Loss: 4.321 (4.43)  Time: 0.677s, 1512.61/s  (0.680s, 1505.31/s)  LR: 9.760e-04  Data: 0.013 (0.017)
Train: 19 [ 700/1251 ( 56%)]  Loss: 4.565 (4.44)  Time: 0.675s, 1517.52/s  (0.680s, 1505.40/s)  LR: 9.780e-04  Data: 0.012 (0.017)
Train: 19 [ 750/1251 ( 60%)]  Loss: 4.352 (4.44)  Time: 0.680s, 1504.96/s  (0.680s, 1505.17/s)  LR: 9.800e-04  Data: 0.012 (0.017)
Train: 19 [ 800/1251 ( 64%)]  Loss: 4.386 (4.43)  Time: 0.684s, 1497.59/s  (0.680s, 1504.91/s)  LR: 9.820e-04  Data: 0.013 (0.017)
Train: 19 [ 850/1251 ( 68%)]  Loss: 4.361 (4.43)  Time: 0.682s, 1501.67/s  (0.680s, 1504.81/s)  LR: 9.840e-04  Data: 0.014 (0.017)
Train: 19 [ 900/1251 ( 72%)]  Loss: 4.674 (4.44)  Time: 0.682s, 1500.75/s  (0.681s, 1504.75/s)  LR: 9.860e-04  Data: 0.017 (0.017)
Train: 19 [ 950/1251 ( 76%)]  Loss: 4.552 (4.45)  Time: 0.682s, 1502.33/s  (0.681s, 1504.66/s)  LR: 9.880e-04  Data: 0.015 (0.016)
Train: 19 [1000/1251 ( 80%)]  Loss: 4.535 (4.45)  Time: 0.689s, 1485.20/s  (0.680s, 1504.78/s)  LR: 9.900e-04  Data: 0.013 (0.016)
Train: 19 [1050/1251 ( 84%)]  Loss: 4.639 (4.46)  Time: 0.677s, 1512.02/s  (0.680s, 1504.79/s)  LR: 9.920e-04  Data: 0.014 (0.016)
Train: 19 [1100/1251 ( 88%)]  Loss: 4.377 (4.46)  Time: 0.667s, 1534.95/s  (0.680s, 1505.02/s)  LR: 9.940e-04  Data: 0.014 (0.016)
Train: 19 [1150/1251 ( 92%)]  Loss: 4.834 (4.47)  Time: 0.682s, 1501.99/s  (0.680s, 1505.08/s)  LR: 9.960e-04  Data: 0.014 (0.016)
Train: 19 [1200/1251 ( 96%)]  Loss: 4.626 (4.48)  Time: 0.687s, 1490.08/s  (0.680s, 1505.37/s)  LR: 9.980e-04  Data: 0.012 (0.016)
Train: 19 [1250/1251 (100%)]  Loss: 4.705 (4.49)  Time: 0.660s, 1552.01/s  (0.680s, 1505.61/s)  LR: 9.891e-04  Data: 0.000 (0.016)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.704 (2.704)  Loss:  1.0088 (1.0088)  Acc@1: 79.4922 (79.4922)  Acc@5: 93.0664 (93.0664)
Test: [  48/48]  Time: 0.165 (0.320)  Loss:  1.0547 (1.7825)  Acc@1: 76.0613 (60.2320)  Acc@5: 90.6840 (83.5540)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-19.pth.tar', 60.23200007324219)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-18.pth.tar', 58.74800006591797)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-17.pth.tar', 58.134000014648436)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-16.pth.tar', 57.00800002685547)

Train: 20 [   0/1251 (  0%)]  Loss: 4.520 (4.52)  Time: 4.096s,  249.99/s  (4.096s,  249.99/s)  LR: 9.891e-04  Data: 1.897 (1.897)
Train: 20 [  50/1251 (  4%)]  Loss: 4.205 (4.36)  Time: 0.672s, 1523.29/s  (0.708s, 1446.73/s)  LR: 9.891e-04  Data: 0.014 (0.051)
Train: 20 [ 100/1251 (  8%)]  Loss: 4.099 (4.27)  Time: 0.678s, 1511.27/s  (0.690s, 1484.38/s)  LR: 9.890e-04  Data: 0.013 (0.033)
Train: 20 [ 150/1251 ( 12%)]  Loss: 4.756 (4.39)  Time: 0.671s, 1525.50/s  (0.685s, 1495.38/s)  LR: 9.890e-04  Data: 0.016 (0.027)
Train: 20 [ 200/1251 ( 16%)]  Loss: 4.279 (4.37)  Time: 0.671s, 1527.14/s  (0.682s, 1501.67/s)  LR: 9.890e-04  Data: 0.012 (0.024)
Train: 20 [ 250/1251 ( 20%)]  Loss: 4.424 (4.38)  Time: 0.673s, 1520.50/s  (0.681s, 1503.75/s)  LR: 9.889e-04  Data: 0.013 (0.022)
Train: 20 [ 300/1251 ( 24%)]  Loss: 4.649 (4.42)  Time: 0.674s, 1520.28/s  (0.680s, 1504.89/s)  LR: 9.889e-04  Data: 0.014 (0.020)
Train: 20 [ 350/1251 ( 28%)]  Loss: 4.354 (4.41)  Time: 0.693s, 1477.80/s  (0.680s, 1505.82/s)  LR: 9.888e-04  Data: 0.013 (0.020)
Train: 20 [ 400/1251 ( 32%)]  Loss: 4.853 (4.46)  Time: 0.684s, 1497.14/s  (0.680s, 1506.20/s)  LR: 9.888e-04  Data: 0.012 (0.019)
Train: 20 [ 450/1251 ( 36%)]  Loss: 4.394 (4.45)  Time: 0.673s, 1521.32/s  (0.679s, 1507.00/s)  LR: 9.887e-04  Data: 0.014 (0.018)
Train: 20 [ 500/1251 ( 40%)]  Loss: 4.093 (4.42)  Time: 0.684s, 1496.30/s  (0.679s, 1507.11/s)  LR: 9.887e-04  Data: 0.014 (0.018)
Train: 20 [ 550/1251 ( 44%)]  Loss: 4.745 (4.45)  Time: 0.675s, 1517.06/s  (0.679s, 1507.31/s)  LR: 9.886e-04  Data: 0.013 (0.018)
Train: 20 [ 600/1251 ( 48%)]  Loss: 4.353 (4.44)  Time: 0.676s, 1513.97/s  (0.679s, 1507.45/s)  LR: 9.886e-04  Data: 0.017 (0.017)
Train: 20 [ 650/1251 ( 52%)]  Loss: 4.750 (4.46)  Time: 0.675s, 1517.10/s  (0.679s, 1507.60/s)  LR: 9.886e-04  Data: 0.014 (0.017)
Train: 20 [ 700/1251 ( 56%)]  Loss: 4.772 (4.48)  Time: 0.668s, 1533.19/s  (0.679s, 1507.65/s)  LR: 9.885e-04  Data: 0.013 (0.017)
Train: 20 [ 750/1251 ( 60%)]  Loss: 4.601 (4.49)  Time: 0.682s, 1502.23/s  (0.679s, 1507.55/s)  LR: 9.885e-04  Data: 0.014 (0.017)
Train: 20 [ 800/1251 ( 64%)]  Loss: 4.228 (4.47)  Time: 0.681s, 1502.70/s  (0.679s, 1507.36/s)  LR: 9.884e-04  Data: 0.013 (0.016)
Train: 20 [ 850/1251 ( 68%)]  Loss: 4.327 (4.47)  Time: 0.669s, 1531.51/s  (0.679s, 1507.55/s)  LR: 9.884e-04  Data: 0.013 (0.016)
Train: 20 [ 900/1251 ( 72%)]  Loss: 4.204 (4.45)  Time: 0.668s, 1532.28/s  (0.679s, 1507.59/s)  LR: 9.883e-04  Data: 0.017 (0.016)
Train: 20 [ 950/1251 ( 76%)]  Loss: 4.464 (4.45)  Time: 0.670s, 1528.24/s  (0.679s, 1507.65/s)  LR: 9.883e-04  Data: 0.018 (0.016)
Train: 20 [1000/1251 ( 80%)]  Loss: 4.255 (4.44)  Time: 0.674s, 1519.01/s  (0.679s, 1507.71/s)  LR: 9.882e-04  Data: 0.013 (0.016)
Train: 20 [1050/1251 ( 84%)]  Loss: 4.615 (4.45)  Time: 0.675s, 1517.96/s  (0.679s, 1507.90/s)  LR: 9.882e-04  Data: 0.012 (0.016)
Train: 20 [1100/1251 ( 88%)]  Loss: 4.547 (4.46)  Time: 0.688s, 1488.34/s  (0.679s, 1507.99/s)  LR: 9.882e-04  Data: 0.020 (0.016)
Train: 20 [1150/1251 ( 92%)]  Loss: 4.006 (4.44)  Time: 0.684s, 1497.97/s  (0.679s, 1508.11/s)  LR: 9.881e-04  Data: 0.013 (0.016)
Train: 20 [1200/1251 ( 96%)]  Loss: 4.344 (4.43)  Time: 0.682s, 1502.53/s  (0.679s, 1508.26/s)  LR: 9.881e-04  Data: 0.014 (0.016)
Train: 20 [1250/1251 (100%)]  Loss: 4.386 (4.43)  Time: 0.660s, 1552.53/s  (0.679s, 1508.71/s)  LR: 9.880e-04  Data: 0.000 (0.016)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.923 (2.923)  Loss:  0.9092 (0.9092)  Acc@1: 79.1992 (79.1992)  Acc@5: 94.9219 (94.9219)
Test: [  48/48]  Time: 0.168 (0.328)  Loss:  1.0039 (1.7032)  Acc@1: 77.7123 (60.6540)  Acc@5: 91.9811 (84.1160)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-20.pth.tar', 60.65400001464844)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-19.pth.tar', 60.23200007324219)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-18.pth.tar', 58.74800006591797)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-17.pth.tar', 58.134000014648436)

Train: 21 [   0/1251 (  0%)]  Loss: 4.558 (4.56)  Time: 4.118s,  248.67/s  (4.118s,  248.67/s)  LR: 9.880e-04  Data: 1.933 (1.933)
Train: 21 [  50/1251 (  4%)]  Loss: 4.134 (4.35)  Time: 0.663s, 1543.63/s  (0.706s, 1450.28/s)  LR: 9.880e-04  Data: 0.013 (0.052)
Train: 21 [ 100/1251 (  8%)]  Loss: 4.510 (4.40)  Time: 0.676s, 1514.65/s  (0.689s, 1486.49/s)  LR: 9.879e-04  Data: 0.013 (0.033)
Train: 21 [ 150/1251 ( 12%)]  Loss: 4.613 (4.45)  Time: 0.676s, 1513.97/s  (0.684s, 1496.47/s)  LR: 9.879e-04  Data: 0.012 (0.027)
Train: 21 [ 200/1251 ( 16%)]  Loss: 4.194 (4.40)  Time: 0.680s, 1505.58/s  (0.683s, 1500.03/s)  LR: 9.878e-04  Data: 0.016 (0.024)
Train: 21 [ 250/1251 ( 20%)]  Loss: 4.218 (4.37)  Time: 0.676s, 1515.72/s  (0.682s, 1501.96/s)  LR: 9.878e-04  Data: 0.013 (0.022)
Train: 21 [ 300/1251 ( 24%)]  Loss: 4.246 (4.35)  Time: 0.682s, 1501.38/s  (0.681s, 1504.07/s)  LR: 9.877e-04  Data: 0.012 (0.021)
Train: 21 [ 350/1251 ( 28%)]  Loss: 4.344 (4.35)  Time: 0.670s, 1528.12/s  (0.680s, 1505.41/s)  LR: 9.877e-04  Data: 0.018 (0.020)
Train: 21 [ 400/1251 ( 32%)]  Loss: 4.161 (4.33)  Time: 0.675s, 1517.17/s  (0.680s, 1505.88/s)  LR: 9.877e-04  Data: 0.013 (0.019)
Train: 21 [ 450/1251 ( 36%)]  Loss: 4.326 (4.33)  Time: 0.676s, 1515.32/s  (0.680s, 1505.90/s)  LR: 9.876e-04  Data: 0.013 (0.018)
Train: 21 [ 500/1251 ( 40%)]  Loss: 4.386 (4.34)  Time: 0.674s, 1518.24/s  (0.680s, 1505.63/s)  LR: 9.876e-04  Data: 0.021 (0.018)
Train: 21 [ 550/1251 ( 44%)]  Loss: 4.733 (4.37)  Time: 0.684s, 1498.01/s  (0.680s, 1506.16/s)  LR: 9.875e-04  Data: 0.012 (0.018)
Train: 21 [ 600/1251 ( 48%)]  Loss: 4.450 (4.37)  Time: 0.674s, 1519.91/s  (0.680s, 1506.75/s)  LR: 9.875e-04  Data: 0.014 (0.017)
Train: 21 [ 650/1251 ( 52%)]  Loss: 4.600 (4.39)  Time: 0.691s, 1481.38/s  (0.680s, 1506.47/s)  LR: 9.874e-04  Data: 0.013 (0.017)
Train: 21 [ 700/1251 ( 56%)]  Loss: 4.400 (4.39)  Time: 0.681s, 1504.76/s  (0.680s, 1505.98/s)  LR: 9.874e-04  Data: 0.014 (0.017)
Train: 21 [ 750/1251 ( 60%)]  Loss: 4.316 (4.39)  Time: 0.688s, 1488.69/s  (0.680s, 1505.55/s)  LR: 9.873e-04  Data: 0.012 (0.017)
Train: 21 [ 800/1251 ( 64%)]  Loss: 4.209 (4.38)  Time: 0.681s, 1503.21/s  (0.680s, 1505.42/s)  LR: 9.873e-04  Data: 0.015 (0.017)
Train: 21 [ 850/1251 ( 68%)]  Loss: 4.297 (4.37)  Time: 0.682s, 1502.21/s  (0.680s, 1505.35/s)  LR: 9.872e-04  Data: 0.013 (0.016)
Train: 21 [ 900/1251 ( 72%)]  Loss: 4.485 (4.38)  Time: 0.678s, 1510.44/s  (0.680s, 1505.24/s)  LR: 9.872e-04  Data: 0.013 (0.016)
Train: 21 [ 950/1251 ( 76%)]  Loss: 4.149 (4.37)  Time: 0.683s, 1499.86/s  (0.680s, 1505.06/s)  LR: 9.871e-04  Data: 0.012 (0.016)
Train: 21 [1000/1251 ( 80%)]  Loss: 4.507 (4.37)  Time: 0.682s, 1500.56/s  (0.681s, 1504.72/s)  LR: 9.871e-04  Data: 0.012 (0.016)
Train: 21 [1050/1251 ( 84%)]  Loss: 4.216 (4.37)  Time: 0.680s, 1506.84/s  (0.681s, 1504.62/s)  LR: 9.870e-04  Data: 0.013 (0.016)
Train: 21 [1100/1251 ( 88%)]  Loss: 4.368 (4.37)  Time: 0.686s, 1491.64/s  (0.681s, 1504.74/s)  LR: 9.870e-04  Data: 0.014 (0.016)
Train: 21 [1150/1251 ( 92%)]  Loss: 4.530 (4.37)  Time: 0.693s, 1476.76/s  (0.680s, 1504.84/s)  LR: 9.870e-04  Data: 0.012 (0.016)
Train: 21 [1200/1251 ( 96%)]  Loss: 4.524 (4.38)  Time: 0.677s, 1512.90/s  (0.680s, 1504.84/s)  LR: 9.869e-04  Data: 0.012 (0.016)
Train: 21 [1250/1251 (100%)]  Loss: 4.367 (4.38)  Time: 0.671s, 1526.84/s  (0.680s, 1505.03/s)  LR: 9.869e-04  Data: 0.000 (0.016)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.818 (2.818)  Loss:  1.0781 (1.0781)  Acc@1: 79.3945 (79.3945)  Acc@5: 94.5312 (94.5312)
Test: [  48/48]  Time: 0.170 (0.334)  Loss:  0.9917 (1.7342)  Acc@1: 77.9481 (61.8480)  Acc@5: 93.8679 (84.8980)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-21.pth.tar', 61.84799998779297)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-20.pth.tar', 60.65400001464844)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-19.pth.tar', 60.23200007324219)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-18.pth.tar', 58.74800006591797)

Train: 22 [   0/1251 (  0%)]  Loss: 4.389 (4.39)  Time: 4.106s,  249.37/s  (4.106s,  249.37/s)  LR: 9.869e-04  Data: 1.895 (1.895)
Train: 22 [  50/1251 (  4%)]  Loss: 4.285 (4.34)  Time: 0.667s, 1534.31/s  (0.717s, 1428.07/s)  LR: 9.868e-04  Data: 0.014 (0.051)
Train: 22 [ 100/1251 (  8%)]  Loss: 4.027 (4.23)  Time: 0.672s, 1524.20/s  (0.694s, 1475.38/s)  LR: 9.868e-04  Data: 0.013 (0.033)
Train: 22 [ 150/1251 ( 12%)]  Loss: 4.606 (4.33)  Time: 0.688s, 1487.50/s  (0.687s, 1489.60/s)  LR: 9.867e-04  Data: 0.012 (0.027)
Train: 22 [ 200/1251 ( 16%)]  Loss: 4.294 (4.32)  Time: 0.677s, 1512.14/s  (0.685s, 1495.08/s)  LR: 9.867e-04  Data: 0.013 (0.024)
Train: 22 [ 250/1251 ( 20%)]  Loss: 4.194 (4.30)  Time: 0.672s, 1524.43/s  (0.684s, 1497.95/s)  LR: 9.866e-04  Data: 0.013 (0.022)
Train: 22 [ 300/1251 ( 24%)]  Loss: 4.290 (4.30)  Time: 0.686s, 1492.99/s  (0.683s, 1499.89/s)  LR: 9.866e-04  Data: 0.013 (0.021)
Train: 22 [ 350/1251 ( 28%)]  Loss: 4.223 (4.29)  Time: 0.685s, 1495.89/s  (0.682s, 1500.66/s)  LR: 9.865e-04  Data: 0.014 (0.020)
Train: 22 [ 400/1251 ( 32%)]  Loss: 4.153 (4.27)  Time: 0.689s, 1487.28/s  (0.682s, 1501.75/s)  LR: 9.865e-04  Data: 0.013 (0.019)
Train: 22 [ 450/1251 ( 36%)]  Loss: 4.259 (4.27)  Time: 0.673s, 1520.47/s  (0.682s, 1502.51/s)  LR: 9.864e-04  Data: 0.013 (0.018)
Train: 22 [ 500/1251 ( 40%)]  Loss: 4.323 (4.28)  Time: 0.678s, 1509.75/s  (0.682s, 1502.48/s)  LR: 9.864e-04  Data: 0.016 (0.018)
Train: 22 [ 550/1251 ( 44%)]  Loss: 4.418 (4.29)  Time: 0.679s, 1509.17/s  (0.681s, 1502.94/s)  LR: 9.863e-04  Data: 0.012 (0.018)
Train: 22 [ 600/1251 ( 48%)]  Loss: 4.593 (4.31)  Time: 0.681s, 1503.54/s  (0.681s, 1503.57/s)  LR: 9.863e-04  Data: 0.013 (0.017)
Train: 22 [ 650/1251 ( 52%)]  Loss: 4.388 (4.32)  Time: 0.680s, 1505.76/s  (0.681s, 1503.63/s)  LR: 9.862e-04  Data: 0.013 (0.017)
Train: 22 [ 700/1251 ( 56%)]  Loss: 4.148 (4.31)  Time: 0.675s, 1516.27/s  (0.681s, 1503.98/s)  LR: 9.862e-04  Data: 0.014 (0.017)
Train: 22 [ 750/1251 ( 60%)]  Loss: 4.392 (4.31)  Time: 0.671s, 1525.14/s  (0.681s, 1504.44/s)  LR: 9.861e-04  Data: 0.014 (0.017)
Train: 22 [ 800/1251 ( 64%)]  Loss: 4.536 (4.32)  Time: 0.682s, 1502.54/s  (0.681s, 1504.71/s)  LR: 9.861e-04  Data: 0.013 (0.017)
Train: 22 [ 850/1251 ( 68%)]  Loss: 4.659 (4.34)  Time: 0.678s, 1510.55/s  (0.680s, 1504.98/s)  LR: 9.860e-04  Data: 0.013 (0.017)
Train: 22 [ 900/1251 ( 72%)]  Loss: 4.273 (4.34)  Time: 0.681s, 1504.28/s  (0.680s, 1505.02/s)  LR: 9.860e-04  Data: 0.016 (0.016)
Train: 22 [ 950/1251 ( 76%)]  Loss: 4.301 (4.34)  Time: 0.675s, 1516.90/s  (0.680s, 1505.27/s)  LR: 9.859e-04  Data: 0.017 (0.016)
Train: 22 [1000/1251 ( 80%)]  Loss: 4.318 (4.34)  Time: 0.688s, 1488.42/s  (0.680s, 1505.37/s)  LR: 9.859e-04  Data: 0.012 (0.016)
Train: 22 [1050/1251 ( 84%)]  Loss: 4.440 (4.34)  Time: 0.680s, 1506.49/s  (0.680s, 1505.59/s)  LR: 9.858e-04  Data: 0.018 (0.016)
Train: 22 [1100/1251 ( 88%)]  Loss: 4.388 (4.34)  Time: 0.687s, 1491.58/s  (0.680s, 1505.73/s)  LR: 9.858e-04  Data: 0.013 (0.016)
Train: 22 [1150/1251 ( 92%)]  Loss: 4.242 (4.34)  Time: 0.681s, 1504.23/s  (0.680s, 1506.00/s)  LR: 9.857e-04  Data: 0.012 (0.016)
Train: 22 [1200/1251 ( 96%)]  Loss: 4.446 (4.34)  Time: 0.674s, 1518.66/s  (0.680s, 1506.40/s)  LR: 9.857e-04  Data: 0.013 (0.016)
Train: 22 [1250/1251 (100%)]  Loss: 4.336 (4.34)  Time: 0.661s, 1549.60/s  (0.680s, 1506.88/s)  LR: 9.856e-04  Data: 0.000 (0.016)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.687 (2.687)  Loss:  0.8701 (0.8701)  Acc@1: 81.2500 (81.2500)  Acc@5: 94.9219 (94.9219)
Test: [  48/48]  Time: 0.169 (0.323)  Loss:  0.9814 (1.6248)  Acc@1: 78.6557 (62.7300)  Acc@5: 92.3349 (85.4560)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-22.pth.tar', 62.73000003662109)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-21.pth.tar', 61.84799998779297)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-20.pth.tar', 60.65400001464844)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-19.pth.tar', 60.23200007324219)

Train: 23 [   0/1251 (  0%)]  Loss: 4.360 (4.36)  Time: 3.950s,  259.25/s  (3.950s,  259.25/s)  LR: 9.856e-04  Data: 1.697 (1.697)
Train: 23 [  50/1251 (  4%)]  Loss: 4.103 (4.23)  Time: 0.671s, 1525.19/s  (0.701s, 1460.75/s)  LR: 9.856e-04  Data: 0.013 (0.048)
Train: 23 [ 100/1251 (  8%)]  Loss: 4.510 (4.32)  Time: 0.686s, 1491.83/s  (0.688s, 1489.44/s)  LR: 9.855e-04  Data: 0.013 (0.031)
Train: 23 [ 150/1251 ( 12%)]  Loss: 4.338 (4.33)  Time: 0.670s, 1529.32/s  (0.684s, 1497.50/s)  LR: 9.855e-04  Data: 0.017 (0.026)
Train: 23 [ 200/1251 ( 16%)]  Loss: 4.258 (4.31)  Time: 0.677s, 1511.63/s  (0.683s, 1499.99/s)  LR: 9.854e-04  Data: 0.016 (0.023)
Train: 23 [ 250/1251 ( 20%)]  Loss: 4.201 (4.30)  Time: 0.681s, 1504.42/s  (0.682s, 1500.57/s)  LR: 9.854e-04  Data: 0.013 (0.021)
Train: 23 [ 300/1251 ( 24%)]  Loss: 4.315 (4.30)  Time: 0.680s, 1505.84/s  (0.682s, 1501.23/s)  LR: 9.853e-04  Data: 0.017 (0.020)
Train: 23 [ 350/1251 ( 28%)]  Loss: 4.318 (4.30)  Time: 0.685s, 1495.80/s  (0.682s, 1501.17/s)  LR: 9.853e-04  Data: 0.013 (0.019)
Train: 23 [ 400/1251 ( 32%)]  Loss: 4.233 (4.29)  Time: 0.690s, 1483.66/s  (0.682s, 1501.37/s)  LR: 9.852e-04  Data: 0.013 (0.019)
Train: 23 [ 450/1251 ( 36%)]  Loss: 4.370 (4.30)  Time: 0.693s, 1478.38/s  (0.682s, 1500.82/s)  LR: 9.852e-04  Data: 0.013 (0.018)
Train: 23 [ 500/1251 ( 40%)]  Loss: 4.115 (4.28)  Time: 0.680s, 1504.86/s  (0.682s, 1500.70/s)  LR: 9.851e-04  Data: 0.016 (0.018)
Train: 23 [ 550/1251 ( 44%)]  Loss: 4.143 (4.27)  Time: 0.679s, 1508.92/s  (0.682s, 1501.09/s)  LR: 9.851e-04  Data: 0.012 (0.017)
Train: 23 [ 600/1251 ( 48%)]  Loss: 4.480 (4.29)  Time: 0.682s, 1501.48/s  (0.682s, 1500.66/s)  LR: 9.850e-04  Data: 0.018 (0.017)
Train: 23 [ 650/1251 ( 52%)]  Loss: 4.195 (4.28)  Time: 0.676s, 1514.27/s  (0.683s, 1500.34/s)  LR: 9.850e-04  Data: 0.012 (0.017)
Train: 23 [ 700/1251 ( 56%)]  Loss: 4.584 (4.30)  Time: 0.680s, 1506.70/s  (0.683s, 1500.37/s)  LR: 9.849e-04  Data: 0.013 (0.017)
Train: 23 [ 750/1251 ( 60%)]  Loss: 3.944 (4.28)  Time: 0.678s, 1509.49/s  (0.682s, 1500.72/s)  LR: 9.849e-04  Data: 0.012 (0.017)
Train: 23 [ 800/1251 ( 64%)]  Loss: 3.860 (4.25)  Time: 0.675s, 1517.32/s  (0.682s, 1501.14/s)  LR: 9.848e-04  Data: 0.014 (0.016)
Train: 23 [ 850/1251 ( 68%)]  Loss: 4.176 (4.25)  Time: 0.684s, 1498.04/s  (0.682s, 1501.43/s)  LR: 9.848e-04  Data: 0.014 (0.016)
Train: 23 [ 900/1251 ( 72%)]  Loss: 4.182 (4.25)  Time: 0.676s, 1515.27/s  (0.682s, 1501.83/s)  LR: 9.847e-04  Data: 0.014 (0.016)
Train: 23 [ 950/1251 ( 76%)]  Loss: 4.192 (4.24)  Time: 0.686s, 1493.57/s  (0.682s, 1502.24/s)  LR: 9.847e-04  Data: 0.012 (0.016)
Train: 23 [1000/1251 ( 80%)]  Loss: 4.462 (4.25)  Time: 0.674s, 1518.24/s  (0.682s, 1502.43/s)  LR: 9.846e-04  Data: 0.013 (0.016)
Train: 23 [1050/1251 ( 84%)]  Loss: 3.797 (4.23)  Time: 0.684s, 1496.91/s  (0.682s, 1502.51/s)  LR: 9.846e-04  Data: 0.016 (0.016)
Train: 23 [1100/1251 ( 88%)]  Loss: 4.187 (4.23)  Time: 0.686s, 1493.52/s  (0.682s, 1502.56/s)  LR: 9.845e-04  Data: 0.013 (0.016)
Train: 23 [1150/1251 ( 92%)]  Loss: 3.930 (4.22)  Time: 0.670s, 1528.76/s  (0.681s, 1502.85/s)  LR: 9.845e-04  Data: 0.015 (0.016)
Train: 23 [1200/1251 ( 96%)]  Loss: 4.307 (4.22)  Time: 0.693s, 1477.48/s  (0.681s, 1502.78/s)  LR: 9.844e-04  Data: 0.013 (0.016)
Train: 23 [1250/1251 (100%)]  Loss: 4.214 (4.22)  Time: 0.669s, 1531.45/s  (0.681s, 1503.19/s)  LR: 9.844e-04  Data: 0.000 (0.016)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.739 (2.739)  Loss:  0.8760 (0.8760)  Acc@1: 81.9336 (81.9336)  Acc@5: 95.1172 (95.1172)
Test: [  48/48]  Time: 0.167 (0.324)  Loss:  1.0771 (1.6564)  Acc@1: 79.2453 (63.5660)  Acc@5: 92.6887 (85.8620)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-23.pth.tar', 63.56600003417969)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-22.pth.tar', 62.73000003662109)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-21.pth.tar', 61.84799998779297)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-20.pth.tar', 60.65400001464844)

Train: 24 [   0/1251 (  0%)]  Loss: 4.171 (4.17)  Time: 3.832s,  267.21/s  (3.832s,  267.21/s)  LR: 9.844e-04  Data: 1.912 (1.912)
Train: 24 [  50/1251 (  4%)]  Loss: 4.482 (4.33)  Time: 0.676s, 1514.90/s  (0.705s, 1451.77/s)  LR: 9.843e-04  Data: 0.015 (0.052)
Train: 24 [ 100/1251 (  8%)]  Loss: 4.251 (4.30)  Time: 0.668s, 1533.14/s  (0.688s, 1488.38/s)  LR: 9.843e-04  Data: 0.012 (0.034)
Train: 24 [ 150/1251 ( 12%)]  Loss: 4.448 (4.34)  Time: 0.681s, 1502.67/s  (0.683s, 1498.83/s)  LR: 9.842e-04  Data: 0.014 (0.027)
Train: 24 [ 200/1251 ( 16%)]  Loss: 4.533 (4.38)  Time: 0.681s, 1503.85/s  (0.681s, 1503.42/s)  LR: 9.842e-04  Data: 0.013 (0.024)
Train: 24 [ 250/1251 ( 20%)]  Loss: 4.304 (4.36)  Time: 0.680s, 1506.24/s  (0.681s, 1504.71/s)  LR: 9.841e-04  Data: 0.013 (0.022)
Train: 24 [ 300/1251 ( 24%)]  Loss: 4.879 (4.44)  Time: 0.679s, 1507.63/s  (0.680s, 1505.07/s)  LR: 9.841e-04  Data: 0.018 (0.021)
Train: 24 [ 350/1251 ( 28%)]  Loss: 4.303 (4.42)  Time: 0.683s, 1499.22/s  (0.680s, 1504.93/s)  LR: 9.840e-04  Data: 0.017 (0.020)
Train: 24 [ 400/1251 ( 32%)]  Loss: 3.994 (4.37)  Time: 0.678s, 1509.71/s  (0.680s, 1504.88/s)  LR: 9.840e-04  Data: 0.014 (0.019)
Train: 24 [ 450/1251 ( 36%)]  Loss: 4.619 (4.40)  Time: 0.683s, 1499.03/s  (0.681s, 1504.65/s)  LR: 9.839e-04  Data: 0.015 (0.019)
Train: 24 [ 500/1251 ( 40%)]  Loss: 4.249 (4.38)  Time: 0.686s, 1492.57/s  (0.681s, 1504.49/s)  LR: 9.838e-04  Data: 0.012 (0.018)
Train: 24 [ 550/1251 ( 44%)]  Loss: 4.430 (4.39)  Time: 0.679s, 1507.93/s  (0.681s, 1504.68/s)  LR: 9.838e-04  Data: 0.015 (0.018)
Train: 24 [ 600/1251 ( 48%)]  Loss: 4.586 (4.40)  Time: 0.679s, 1507.76/s  (0.681s, 1504.31/s)  LR: 9.837e-04  Data: 0.018 (0.018)
Train: 24 [ 650/1251 ( 52%)]  Loss: 4.387 (4.40)  Time: 0.694s, 1476.38/s  (0.681s, 1504.24/s)  LR: 9.837e-04  Data: 0.013 (0.017)
Train: 24 [ 700/1251 ( 56%)]  Loss: 4.284 (4.39)  Time: 0.678s, 1510.70/s  (0.681s, 1504.00/s)  LR: 9.836e-04  Data: 0.016 (0.017)
Train: 24 [ 750/1251 ( 60%)]  Loss: 4.406 (4.40)  Time: 0.687s, 1490.64/s  (0.681s, 1504.45/s)  LR: 9.836e-04  Data: 0.017 (0.017)
Train: 24 [ 800/1251 ( 64%)]  Loss: 4.539 (4.40)  Time: 0.687s, 1490.97/s  (0.681s, 1504.65/s)  LR: 9.835e-04  Data: 0.013 (0.017)
Train: 24 [ 850/1251 ( 68%)]  Loss: 4.218 (4.39)  Time: 0.677s, 1512.58/s  (0.681s, 1504.74/s)  LR: 9.835e-04  Data: 0.012 (0.017)
Train: 24 [ 900/1251 ( 72%)]  Loss: 4.131 (4.38)  Time: 0.686s, 1493.28/s  (0.680s, 1504.88/s)  LR: 9.834e-04  Data: 0.016 (0.017)
Train: 24 [ 950/1251 ( 76%)]  Loss: 4.396 (4.38)  Time: 0.684s, 1496.44/s  (0.680s, 1505.12/s)  LR: 9.834e-04  Data: 0.012 (0.017)
Train: 24 [1000/1251 ( 80%)]  Loss: 4.104 (4.37)  Time: 0.672s, 1524.51/s  (0.680s, 1505.10/s)  LR: 9.833e-04  Data: 0.012 (0.017)
Train: 24 [1050/1251 ( 84%)]  Loss: 4.280 (4.36)  Time: 0.688s, 1488.93/s  (0.680s, 1505.28/s)  LR: 9.833e-04  Data: 0.013 (0.016)
Train: 24 [1100/1251 ( 88%)]  Loss: 4.380 (4.36)  Time: 0.676s, 1515.38/s  (0.680s, 1505.43/s)  LR: 9.832e-04  Data: 0.014 (0.016)
Train: 24 [1150/1251 ( 92%)]  Loss: 4.296 (4.36)  Time: 0.669s, 1529.74/s  (0.680s, 1505.59/s)  LR: 9.832e-04  Data: 0.015 (0.016)
Train: 24 [1200/1251 ( 96%)]  Loss: 3.942 (4.34)  Time: 0.687s, 1490.82/s  (0.680s, 1505.82/s)  LR: 9.831e-04  Data: 0.013 (0.016)
Train: 24 [1250/1251 (100%)]  Loss: 4.566 (4.35)  Time: 0.658s, 1555.28/s  (0.680s, 1506.22/s)  LR: 9.830e-04  Data: 0.000 (0.016)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.757 (2.757)  Loss:  0.8154 (0.8154)  Acc@1: 82.5195 (82.5195)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.169 (0.328)  Loss:  0.9375 (1.6021)  Acc@1: 78.4198 (64.0460)  Acc@5: 94.1038 (86.2940)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-24.pth.tar', 64.04600006347657)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-23.pth.tar', 63.56600003417969)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-22.pth.tar', 62.73000003662109)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-21.pth.tar', 61.84799998779297)

Train: 25 [   0/1251 (  0%)]  Loss: 4.371 (4.37)  Time: 3.320s,  308.42/s  (3.320s,  308.42/s)  LR: 9.830e-04  Data: 1.855 (1.855)
Train: 25 [  50/1251 (  4%)]  Loss: 4.191 (4.28)  Time: 0.679s, 1508.59/s  (0.704s, 1454.44/s)  LR: 9.830e-04  Data: 0.014 (0.051)
Train: 25 [ 100/1251 (  8%)]  Loss: 4.123 (4.23)  Time: 0.670s, 1529.25/s  (0.688s, 1489.09/s)  LR: 9.829e-04  Data: 0.013 (0.033)
Train: 25 [ 150/1251 ( 12%)]  Loss: 4.326 (4.25)  Time: 0.682s, 1501.83/s  (0.683s, 1498.54/s)  LR: 9.829e-04  Data: 0.014 (0.027)
Train: 25 [ 200/1251 ( 16%)]  Loss: 4.213 (4.24)  Time: 0.669s, 1531.01/s  (0.681s, 1503.11/s)  LR: 9.828e-04  Data: 0.013 (0.024)
Train: 25 [ 250/1251 ( 20%)]  Loss: 4.289 (4.25)  Time: 0.670s, 1528.42/s  (0.680s, 1505.72/s)  LR: 9.828e-04  Data: 0.014 (0.022)
Train: 25 [ 300/1251 ( 24%)]  Loss: 4.222 (4.25)  Time: 0.686s, 1493.63/s  (0.679s, 1507.03/s)  LR: 9.827e-04  Data: 0.012 (0.021)
Train: 25 [ 350/1251 ( 28%)]  Loss: 4.432 (4.27)  Time: 0.681s, 1504.73/s  (0.679s, 1507.66/s)  LR: 9.827e-04  Data: 0.012 (0.020)
Train: 25 [ 400/1251 ( 32%)]  Loss: 4.258 (4.27)  Time: 0.669s, 1530.30/s  (0.679s, 1508.21/s)  LR: 9.826e-04  Data: 0.015 (0.019)
Train: 25 [ 450/1251 ( 36%)]  Loss: 3.879 (4.23)  Time: 0.672s, 1524.18/s  (0.679s, 1508.82/s)  LR: 9.826e-04  Data: 0.014 (0.019)
Train: 25 [ 500/1251 ( 40%)]  Loss: 4.424 (4.25)  Time: 0.681s, 1503.35/s  (0.678s, 1509.28/s)  LR: 9.825e-04  Data: 0.017 (0.019)
Train: 25 [ 550/1251 ( 44%)]  Loss: 4.229 (4.25)  Time: 0.676s, 1514.66/s  (0.678s, 1509.72/s)  LR: 9.824e-04  Data: 0.018 (0.018)
Train: 25 [ 600/1251 ( 48%)]  Loss: 4.489 (4.26)  Time: 0.675s, 1517.10/s  (0.678s, 1509.89/s)  LR: 9.824e-04  Data: 0.015 (0.018)
Train: 25 [ 650/1251 ( 52%)]  Loss: 4.416 (4.28)  Time: 0.677s, 1512.28/s  (0.678s, 1510.27/s)  LR: 9.823e-04  Data: 0.013 (0.018)
Train: 25 [ 700/1251 ( 56%)]  Loss: 4.421 (4.29)  Time: 0.680s, 1506.61/s  (0.678s, 1510.39/s)  LR: 9.823e-04  Data: 0.017 (0.017)
Train: 25 [ 750/1251 ( 60%)]  Loss: 4.491 (4.30)  Time: 0.682s, 1502.20/s  (0.678s, 1510.57/s)  LR: 9.822e-04  Data: 0.012 (0.017)
Train: 25 [ 800/1251 ( 64%)]  Loss: 4.329 (4.30)  Time: 0.682s, 1500.52/s  (0.678s, 1511.10/s)  LR: 9.822e-04  Data: 0.015 (0.017)
Train: 25 [ 850/1251 ( 68%)]  Loss: 4.097 (4.29)  Time: 0.679s, 1508.11/s  (0.677s, 1511.48/s)  LR: 9.821e-04  Data: 0.012 (0.017)
Train: 25 [ 900/1251 ( 72%)]  Loss: 3.940 (4.27)  Time: 0.676s, 1514.72/s  (0.677s, 1511.94/s)  LR: 9.821e-04  Data: 0.013 (0.017)
Train: 25 [ 950/1251 ( 76%)]  Loss: 4.009 (4.26)  Time: 0.673s, 1522.18/s  (0.677s, 1512.24/s)  LR: 9.820e-04  Data: 0.015 (0.017)
Train: 25 [1000/1251 ( 80%)]  Loss: 4.504 (4.27)  Time: 0.671s, 1526.11/s  (0.677s, 1512.48/s)  LR: 9.820e-04  Data: 0.016 (0.016)
Train: 25 [1050/1251 ( 84%)]  Loss: 4.469 (4.28)  Time: 0.680s, 1505.78/s  (0.677s, 1512.46/s)  LR: 9.819e-04  Data: 0.013 (0.016)
Train: 25 [1100/1251 ( 88%)]  Loss: 4.356 (4.28)  Time: 0.667s, 1534.29/s  (0.677s, 1512.70/s)  LR: 9.818e-04  Data: 0.017 (0.016)
Train: 25 [1150/1251 ( 92%)]  Loss: 4.142 (4.28)  Time: 0.674s, 1519.25/s  (0.677s, 1512.75/s)  LR: 9.818e-04  Data: 0.014 (0.016)
Train: 25 [1200/1251 ( 96%)]  Loss: 4.586 (4.29)  Time: 0.669s, 1530.77/s  (0.677s, 1512.91/s)  LR: 9.817e-04  Data: 0.012 (0.016)
Train: 25 [1250/1251 (100%)]  Loss: 4.400 (4.29)  Time: 0.662s, 1547.49/s  (0.677s, 1512.83/s)  LR: 9.817e-04  Data: 0.000 (0.016)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.712 (2.712)  Loss:  0.8779 (0.8779)  Acc@1: 82.6172 (82.6172)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.170 (0.330)  Loss:  0.9268 (1.5814)  Acc@1: 81.1321 (64.7920)  Acc@5: 93.9859 (86.7440)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-25.pth.tar', 64.792000078125)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-24.pth.tar', 64.04600006347657)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-23.pth.tar', 63.56600003417969)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-22.pth.tar', 62.73000003662109)

Train: 26 [   0/1251 (  0%)]  Loss: 4.217 (4.22)  Time: 3.117s,  328.55/s  (3.117s,  328.55/s)  LR: 9.817e-04  Data: 1.867 (1.867)
Train: 26 [  50/1251 (  4%)]  Loss: 4.317 (4.27)  Time: 0.666s, 1537.18/s  (0.702s, 1458.70/s)  LR: 9.816e-04  Data: 0.013 (0.051)
Train: 26 [ 100/1251 (  8%)]  Loss: 4.023 (4.19)  Time: 0.692s, 1480.51/s  (0.689s, 1485.78/s)  LR: 9.816e-04  Data: 0.014 (0.033)
Train: 26 [ 150/1251 ( 12%)]  Loss: 4.085 (4.16)  Time: 0.678s, 1510.92/s  (0.685s, 1494.32/s)  LR: 9.815e-04  Data: 0.012 (0.027)
Train: 26 [ 200/1251 ( 16%)]  Loss: 4.341 (4.20)  Time: 0.673s, 1521.77/s  (0.683s, 1499.93/s)  LR: 9.814e-04  Data: 0.012 (0.024)
Train: 26 [ 250/1251 ( 20%)]  Loss: 4.552 (4.26)  Time: 0.675s, 1517.38/s  (0.681s, 1502.72/s)  LR: 9.814e-04  Data: 0.016 (0.022)
Train: 26 [ 300/1251 ( 24%)]  Loss: 4.104 (4.23)  Time: 0.670s, 1529.26/s  (0.681s, 1504.43/s)  LR: 9.813e-04  Data: 0.012 (0.021)
Train: 26 [ 350/1251 ( 28%)]  Loss: 4.364 (4.25)  Time: 0.677s, 1512.44/s  (0.680s, 1505.01/s)  LR: 9.813e-04  Data: 0.014 (0.020)
Train: 26 [ 400/1251 ( 32%)]  Loss: 3.929 (4.21)  Time: 0.676s, 1515.38/s  (0.680s, 1505.74/s)  LR: 9.812e-04  Data: 0.012 (0.019)
Train: 26 [ 450/1251 ( 36%)]  Loss: 4.426 (4.24)  Time: 0.671s, 1525.65/s  (0.680s, 1505.92/s)  LR: 9.812e-04  Data: 0.017 (0.018)
Train: 26 [ 500/1251 ( 40%)]  Loss: 4.141 (4.23)  Time: 0.671s, 1525.11/s  (0.680s, 1506.54/s)  LR: 9.811e-04  Data: 0.012 (0.018)
Train: 26 [ 550/1251 ( 44%)]  Loss: 4.322 (4.24)  Time: 0.675s, 1517.34/s  (0.679s, 1507.07/s)  LR: 9.811e-04  Data: 0.014 (0.018)
Train: 26 [ 600/1251 ( 48%)]  Loss: 4.344 (4.24)  Time: 0.678s, 1510.59/s  (0.679s, 1507.37/s)  LR: 9.810e-04  Data: 0.016 (0.017)
Train: 26 [ 650/1251 ( 52%)]  Loss: 4.412 (4.26)  Time: 0.664s, 1542.14/s  (0.679s, 1507.81/s)  LR: 9.809e-04  Data: 0.013 (0.017)
Train: 26 [ 700/1251 ( 56%)]  Loss: 4.555 (4.28)  Time: 0.676s, 1515.38/s  (0.679s, 1508.16/s)  LR: 9.809e-04  Data: 0.014 (0.017)
Train: 26 [ 750/1251 ( 60%)]  Loss: 4.234 (4.27)  Time: 0.685s, 1494.60/s  (0.679s, 1508.41/s)  LR: 9.808e-04  Data: 0.015 (0.017)
Train: 26 [ 800/1251 ( 64%)]  Loss: 4.067 (4.26)  Time: 0.685s, 1495.01/s  (0.679s, 1508.65/s)  LR: 9.808e-04  Data: 0.013 (0.017)
Train: 26 [ 850/1251 ( 68%)]  Loss: 3.858 (4.24)  Time: 0.675s, 1516.45/s  (0.679s, 1508.86/s)  LR: 9.807e-04  Data: 0.014 (0.016)
Train: 26 [ 900/1251 ( 72%)]  Loss: 3.976 (4.22)  Time: 0.676s, 1514.38/s  (0.679s, 1508.96/s)  LR: 9.807e-04  Data: 0.015 (0.016)
Train: 26 [ 950/1251 ( 76%)]  Loss: 4.184 (4.22)  Time: 0.671s, 1526.44/s  (0.679s, 1508.99/s)  LR: 9.806e-04  Data: 0.012 (0.016)
Train: 26 [1000/1251 ( 80%)]  Loss: 4.090 (4.22)  Time: 0.673s, 1522.35/s  (0.679s, 1508.76/s)  LR: 9.805e-04  Data: 0.013 (0.016)
Train: 26 [1050/1251 ( 84%)]  Loss: 4.413 (4.23)  Time: 0.671s, 1527.00/s  (0.679s, 1508.72/s)  LR: 9.805e-04  Data: 0.013 (0.016)
Train: 26 [1100/1251 ( 88%)]  Loss: 4.509 (4.24)  Time: 0.683s, 1499.93/s  (0.679s, 1508.69/s)  LR: 9.804e-04  Data: 0.014 (0.016)
Train: 26 [1150/1251 ( 92%)]  Loss: 4.099 (4.23)  Time: 0.685s, 1494.95/s  (0.679s, 1508.48/s)  LR: 9.804e-04  Data: 0.015 (0.016)
Train: 26 [1200/1251 ( 96%)]  Loss: 4.454 (4.24)  Time: 0.678s, 1509.95/s  (0.679s, 1508.29/s)  LR: 9.803e-04  Data: 0.016 (0.016)
Train: 26 [1250/1251 (100%)]  Loss: 4.703 (4.26)  Time: 0.662s, 1546.20/s  (0.679s, 1508.31/s)  LR: 9.802e-04  Data: 0.000 (0.016)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.177 (3.177)  Loss:  0.7876 (0.7876)  Acc@1: 83.1055 (83.1055)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.170 (0.324)  Loss:  0.8257 (1.5654)  Acc@1: 82.3113 (65.6560)  Acc@5: 94.5755 (87.1860)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-26.pth.tar', 65.65600007324218)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-25.pth.tar', 64.792000078125)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-24.pth.tar', 64.04600006347657)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-23.pth.tar', 63.56600003417969)

Train: 27 [   0/1251 (  0%)]  Loss: 4.451 (4.45)  Time: 3.477s,  294.48/s  (3.477s,  294.48/s)  LR: 9.802e-04  Data: 2.050 (2.050)
Train: 27 [  50/1251 (  4%)]  Loss: 4.436 (4.44)  Time: 0.669s, 1531.33/s  (0.696s, 1470.92/s)  LR: 9.802e-04  Data: 0.013 (0.054)
Train: 27 [ 100/1251 (  8%)]  Loss: 4.327 (4.40)  Time: 0.686s, 1492.62/s  (0.684s, 1497.23/s)  LR: 9.801e-04  Data: 0.014 (0.034)
Train: 27 [ 150/1251 ( 12%)]  Loss: 3.939 (4.29)  Time: 0.683s, 1500.30/s  (0.681s, 1502.57/s)  LR: 9.801e-04  Data: 0.012 (0.028)
Train: 27 [ 200/1251 ( 16%)]  Loss: 4.061 (4.24)  Time: 0.667s, 1534.15/s  (0.680s, 1504.96/s)  LR: 9.800e-04  Data: 0.013 (0.024)
Train: 27 [ 250/1251 ( 20%)]  Loss: 4.292 (4.25)  Time: 0.673s, 1521.61/s  (0.679s, 1507.14/s)  LR: 9.800e-04  Data: 0.014 (0.022)
Train: 27 [ 300/1251 ( 24%)]  Loss: 4.088 (4.23)  Time: 0.672s, 1522.99/s  (0.679s, 1508.37/s)  LR: 9.799e-04  Data: 0.014 (0.021)
Train: 27 [ 350/1251 ( 28%)]  Loss: 3.984 (4.20)  Time: 0.686s, 1492.55/s  (0.679s, 1508.93/s)  LR: 9.798e-04  Data: 0.012 (0.020)
Train: 27 [ 400/1251 ( 32%)]  Loss: 4.446 (4.22)  Time: 0.678s, 1510.04/s  (0.679s, 1509.16/s)  LR: 9.798e-04  Data: 0.014 (0.019)
Train: 27 [ 450/1251 ( 36%)]  Loss: 4.304 (4.23)  Time: 0.676s, 1514.76/s  (0.679s, 1508.55/s)  LR: 9.797e-04  Data: 0.013 (0.019)
Train: 27 [ 500/1251 ( 40%)]  Loss: 4.332 (4.24)  Time: 0.676s, 1515.01/s  (0.679s, 1508.63/s)  LR: 9.797e-04  Data: 0.014 (0.018)
Train: 27 [ 550/1251 ( 44%)]  Loss: 4.275 (4.24)  Time: 0.683s, 1500.15/s  (0.679s, 1508.09/s)  LR: 9.796e-04  Data: 0.012 (0.018)
Train: 27 [ 600/1251 ( 48%)]  Loss: 4.272 (4.25)  Time: 0.679s, 1508.30/s  (0.679s, 1507.56/s)  LR: 9.795e-04  Data: 0.013 (0.017)
Train: 27 [ 650/1251 ( 52%)]  Loss: 4.208 (4.24)  Time: 0.678s, 1510.53/s  (0.679s, 1507.54/s)  LR: 9.795e-04  Data: 0.015 (0.017)
Train: 27 [ 700/1251 ( 56%)]  Loss: 3.850 (4.22)  Time: 0.665s, 1540.95/s  (0.679s, 1507.51/s)  LR: 9.794e-04  Data: 0.016 (0.017)
Train: 27 [ 750/1251 ( 60%)]  Loss: 4.494 (4.23)  Time: 0.680s, 1506.55/s  (0.679s, 1507.64/s)  LR: 9.794e-04  Data: 0.013 (0.017)
Train: 27 [ 800/1251 ( 64%)]  Loss: 3.960 (4.22)  Time: 0.675s, 1517.95/s  (0.679s, 1507.64/s)  LR: 9.793e-04  Data: 0.013 (0.017)
Train: 27 [ 850/1251 ( 68%)]  Loss: 4.109 (4.21)  Time: 0.692s, 1480.63/s  (0.679s, 1507.46/s)  LR: 9.792e-04  Data: 0.013 (0.017)
Train: 27 [ 900/1251 ( 72%)]  Loss: 4.136 (4.21)  Time: 0.680s, 1506.81/s  (0.679s, 1507.23/s)  LR: 9.792e-04  Data: 0.013 (0.016)
Train: 27 [ 950/1251 ( 76%)]  Loss: 4.238 (4.21)  Time: 0.678s, 1510.19/s  (0.680s, 1506.91/s)  LR: 9.791e-04  Data: 0.014 (0.016)
Train: 27 [1000/1251 ( 80%)]  Loss: 4.144 (4.21)  Time: 0.681s, 1502.68/s  (0.680s, 1506.91/s)  LR: 9.791e-04  Data: 0.013 (0.016)
Train: 27 [1050/1251 ( 84%)]  Loss: 4.429 (4.22)  Time: 0.680s, 1504.79/s  (0.680s, 1506.83/s)  LR: 9.790e-04  Data: 0.012 (0.016)
Train: 27 [1100/1251 ( 88%)]  Loss: 4.244 (4.22)  Time: 0.675s, 1516.49/s  (0.680s, 1506.59/s)  LR: 9.789e-04  Data: 0.016 (0.016)
Train: 27 [1150/1251 ( 92%)]  Loss: 4.216 (4.22)  Time: 0.693s, 1476.91/s  (0.680s, 1506.41/s)  LR: 9.789e-04  Data: 0.014 (0.016)
Train: 27 [1200/1251 ( 96%)]  Loss: 4.026 (4.21)  Time: 0.677s, 1513.56/s  (0.680s, 1506.42/s)  LR: 9.788e-04  Data: 0.013 (0.016)
Train: 27 [1250/1251 (100%)]  Loss: 3.833 (4.20)  Time: 0.672s, 1524.65/s  (0.680s, 1506.71/s)  LR: 9.788e-04  Data: 0.000 (0.016)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.978 (2.978)  Loss:  0.7671 (0.7671)  Acc@1: 83.3984 (83.3984)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.170 (0.329)  Loss:  0.8301 (1.4948)  Acc@1: 80.7783 (65.9940)  Acc@5: 95.0472 (87.4940)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-27.pth.tar', 65.99400005371093)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-26.pth.tar', 65.65600007324218)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-25.pth.tar', 64.792000078125)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-24.pth.tar', 64.04600006347657)

Train: 28 [   0/1251 (  0%)]  Loss: 4.178 (4.18)  Time: 3.957s,  258.77/s  (3.957s,  258.77/s)  LR: 9.788e-04  Data: 1.958 (1.958)
Train: 28 [  50/1251 (  4%)]  Loss: 4.260 (4.22)  Time: 0.671s, 1525.37/s  (0.705s, 1452.41/s)  LR: 9.787e-04  Data: 0.014 (0.053)
Train: 28 [ 100/1251 (  8%)]  Loss: 4.455 (4.30)  Time: 0.679s, 1507.62/s  (0.690s, 1484.63/s)  LR: 9.786e-04  Data: 0.014 (0.034)
Train: 28 [ 150/1251 ( 12%)]  Loss: 4.176 (4.27)  Time: 0.679s, 1509.11/s  (0.686s, 1493.00/s)  LR: 9.786e-04  Data: 0.013 (0.027)
Train: 28 [ 200/1251 ( 16%)]  Loss: 3.910 (4.20)  Time: 0.674s, 1519.52/s  (0.685s, 1495.13/s)  LR: 9.785e-04  Data: 0.013 (0.024)
Train: 28 [ 250/1251 ( 20%)]  Loss: 3.933 (4.15)  Time: 0.693s, 1478.36/s  (0.684s, 1496.74/s)  LR: 9.785e-04  Data: 0.013 (0.022)
Train: 28 [ 300/1251 ( 24%)]  Loss: 4.149 (4.15)  Time: 0.673s, 1522.67/s  (0.684s, 1496.89/s)  LR: 9.784e-04  Data: 0.016 (0.021)
Train: 28 [ 350/1251 ( 28%)]  Loss: 4.374 (4.18)  Time: 0.679s, 1508.58/s  (0.684s, 1497.66/s)  LR: 9.783e-04  Data: 0.012 (0.020)
Train: 28 [ 400/1251 ( 32%)]  Loss: 4.176 (4.18)  Time: 0.670s, 1529.00/s  (0.683s, 1498.74/s)  LR: 9.783e-04  Data: 0.014 (0.019)
Train: 28 [ 450/1251 ( 36%)]  Loss: 4.051 (4.17)  Time: 0.673s, 1521.03/s  (0.683s, 1499.35/s)  LR: 9.782e-04  Data: 0.012 (0.019)
Train: 28 [ 500/1251 ( 40%)]  Loss: 4.036 (4.15)  Time: 0.682s, 1500.82/s  (0.683s, 1500.20/s)  LR: 9.782e-04  Data: 0.012 (0.018)
Train: 28 [ 550/1251 ( 44%)]  Loss: 4.044 (4.15)  Time: 0.691s, 1481.14/s  (0.682s, 1500.63/s)  LR: 9.781e-04  Data: 0.013 (0.018)
Train: 28 [ 600/1251 ( 48%)]  Loss: 4.241 (4.15)  Time: 0.682s, 1501.68/s  (0.682s, 1500.81/s)  LR: 9.780e-04  Data: 0.018 (0.017)
Train: 28 [ 650/1251 ( 52%)]  Loss: 4.067 (4.15)  Time: 0.672s, 1522.86/s  (0.682s, 1500.72/s)  LR: 9.780e-04  Data: 0.011 (0.017)
Train: 28 [ 700/1251 ( 56%)]  Loss: 4.387 (4.16)  Time: 0.684s, 1497.77/s  (0.682s, 1501.31/s)  LR: 9.779e-04  Data: 0.013 (0.017)
Train: 28 [ 750/1251 ( 60%)]  Loss: 4.460 (4.18)  Time: 0.687s, 1489.47/s  (0.682s, 1501.73/s)  LR: 9.779e-04  Data: 0.013 (0.017)
Train: 28 [ 800/1251 ( 64%)]  Loss: 4.476 (4.20)  Time: 0.678s, 1510.88/s  (0.682s, 1502.30/s)  LR: 9.778e-04  Data: 0.012 (0.017)
Train: 28 [ 850/1251 ( 68%)]  Loss: 3.992 (4.19)  Time: 0.677s, 1512.81/s  (0.681s, 1502.70/s)  LR: 9.777e-04  Data: 0.013 (0.016)
Train: 28 [ 900/1251 ( 72%)]  Loss: 4.148 (4.18)  Time: 0.672s, 1524.56/s  (0.681s, 1503.09/s)  LR: 9.777e-04  Data: 0.012 (0.016)
Train: 28 [ 950/1251 ( 76%)]  Loss: 4.069 (4.18)  Time: 0.685s, 1493.96/s  (0.681s, 1503.49/s)  LR: 9.776e-04  Data: 0.017 (0.016)
Train: 28 [1000/1251 ( 80%)]  Loss: 4.200 (4.18)  Time: 0.682s, 1500.65/s  (0.681s, 1503.75/s)  LR: 9.775e-04  Data: 0.012 (0.016)
Train: 28 [1050/1251 ( 84%)]  Loss: 4.317 (4.19)  Time: 0.684s, 1496.34/s  (0.681s, 1503.99/s)  LR: 9.775e-04  Data: 0.017 (0.016)
Train: 28 [1100/1251 ( 88%)]  Loss: 4.028 (4.18)  Time: 0.683s, 1500.32/s  (0.681s, 1504.36/s)  LR: 9.774e-04  Data: 0.013 (0.016)
Train: 28 [1150/1251 ( 92%)]  Loss: 3.990 (4.17)  Time: 0.674s, 1519.73/s  (0.680s, 1504.90/s)  LR: 9.774e-04  Data: 0.013 (0.016)
Train: 28 [1200/1251 ( 96%)]  Loss: 4.208 (4.17)  Time: 0.666s, 1538.65/s  (0.680s, 1505.36/s)  LR: 9.773e-04  Data: 0.016 (0.016)
Train: 28 [1250/1251 (100%)]  Loss: 4.357 (4.18)  Time: 0.672s, 1523.96/s  (0.680s, 1505.85/s)  LR: 9.772e-04  Data: 0.000 (0.016)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.793 (2.793)  Loss:  0.7109 (0.7109)  Acc@1: 83.8867 (83.8867)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.162 (0.328)  Loss:  0.8945 (1.4392)  Acc@1: 80.3066 (66.3900)  Acc@5: 93.8679 (87.8260)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-28.pth.tar', 66.39000010742187)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-27.pth.tar', 65.99400005371093)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-26.pth.tar', 65.65600007324218)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-25.pth.tar', 64.792000078125)

Train: 29 [   0/1251 (  0%)]  Loss: 3.847 (3.85)  Time: 3.464s,  295.63/s  (3.464s,  295.63/s)  LR: 9.772e-04  Data: 2.024 (2.024)
Train: 29 [  50/1251 (  4%)]  Loss: 4.335 (4.09)  Time: 0.669s, 1530.62/s  (0.701s, 1459.97/s)  LR: 9.772e-04  Data: 0.013 (0.054)
Train: 29 [ 100/1251 (  8%)]  Loss: 4.257 (4.15)  Time: 0.674s, 1519.71/s  (0.686s, 1492.79/s)  LR: 9.771e-04  Data: 0.014 (0.034)
Train: 29 [ 150/1251 ( 12%)]  Loss: 3.600 (4.01)  Time: 0.684s, 1497.77/s  (0.682s, 1501.45/s)  LR: 9.770e-04  Data: 0.012 (0.028)
Train: 29 [ 200/1251 ( 16%)]  Loss: 4.052 (4.02)  Time: 0.683s, 1498.94/s  (0.681s, 1504.39/s)  LR: 9.770e-04  Data: 0.014 (0.024)
Train: 29 [ 250/1251 ( 20%)]  Loss: 3.911 (4.00)  Time: 0.676s, 1515.81/s  (0.680s, 1506.08/s)  LR: 9.769e-04  Data: 0.013 (0.022)
Train: 29 [ 300/1251 ( 24%)]  Loss: 4.500 (4.07)  Time: 0.680s, 1504.92/s  (0.679s, 1507.55/s)  LR: 9.769e-04  Data: 0.018 (0.021)
Train: 29 [ 350/1251 ( 28%)]  Loss: 3.983 (4.06)  Time: 0.667s, 1535.85/s  (0.679s, 1508.75/s)  LR: 9.768e-04  Data: 0.013 (0.020)
Train: 29 [ 400/1251 ( 32%)]  Loss: 4.187 (4.07)  Time: 0.683s, 1500.12/s  (0.678s, 1509.38/s)  LR: 9.767e-04  Data: 0.017 (0.019)
Train: 29 [ 450/1251 ( 36%)]  Loss: 4.098 (4.08)  Time: 0.667s, 1534.70/s  (0.678s, 1509.48/s)  LR: 9.767e-04  Data: 0.016 (0.019)
Train: 29 [ 500/1251 ( 40%)]  Loss: 4.125 (4.08)  Time: 0.671s, 1525.48/s  (0.678s, 1509.28/s)  LR: 9.766e-04  Data: 0.014 (0.018)
Train: 29 [ 550/1251 ( 44%)]  Loss: 3.963 (4.07)  Time: 0.690s, 1484.80/s  (0.678s, 1509.29/s)  LR: 9.765e-04  Data: 0.015 (0.018)
Train: 29 [ 600/1251 ( 48%)]  Loss: 4.077 (4.07)  Time: 0.671s, 1526.99/s  (0.679s, 1509.12/s)  LR: 9.765e-04  Data: 0.013 (0.018)
Train: 29 [ 650/1251 ( 52%)]  Loss: 4.099 (4.07)  Time: 0.669s, 1530.41/s  (0.679s, 1508.99/s)  LR: 9.764e-04  Data: 0.013 (0.017)
Train: 29 [ 700/1251 ( 56%)]  Loss: 4.239 (4.08)  Time: 0.673s, 1520.73/s  (0.679s, 1508.94/s)  LR: 9.764e-04  Data: 0.013 (0.017)
Train: 29 [ 750/1251 ( 60%)]  Loss: 4.548 (4.11)  Time: 0.673s, 1522.57/s  (0.679s, 1508.73/s)  LR: 9.763e-04  Data: 0.014 (0.017)
Train: 29 [ 800/1251 ( 64%)]  Loss: 4.313 (4.13)  Time: 0.677s, 1513.42/s  (0.679s, 1508.89/s)  LR: 9.762e-04  Data: 0.013 (0.017)
Train: 29 [ 850/1251 ( 68%)]  Loss: 4.074 (4.12)  Time: 0.672s, 1523.34/s  (0.679s, 1508.89/s)  LR: 9.762e-04  Data: 0.013 (0.017)
Train: 29 [ 900/1251 ( 72%)]  Loss: 4.511 (4.14)  Time: 0.677s, 1511.46/s  (0.679s, 1509.03/s)  LR: 9.761e-04  Data: 0.012 (0.016)
Train: 29 [ 950/1251 ( 76%)]  Loss: 4.382 (4.16)  Time: 0.675s, 1515.95/s  (0.679s, 1509.02/s)  LR: 9.760e-04  Data: 0.014 (0.016)
Train: 29 [1000/1251 ( 80%)]  Loss: 3.656 (4.13)  Time: 0.684s, 1496.72/s  (0.679s, 1508.86/s)  LR: 9.760e-04  Data: 0.013 (0.016)
Train: 29 [1050/1251 ( 84%)]  Loss: 4.199 (4.13)  Time: 0.679s, 1508.05/s  (0.679s, 1508.72/s)  LR: 9.759e-04  Data: 0.013 (0.016)
Train: 29 [1100/1251 ( 88%)]  Loss: 3.516 (4.11)  Time: 0.681s, 1504.42/s  (0.679s, 1508.72/s)  LR: 9.758e-04  Data: 0.014 (0.016)
Train: 29 [1150/1251 ( 92%)]  Loss: 4.122 (4.11)  Time: 0.671s, 1525.56/s  (0.679s, 1508.79/s)  LR: 9.758e-04  Data: 0.014 (0.016)
Train: 29 [1200/1251 ( 96%)]  Loss: 4.204 (4.11)  Time: 0.671s, 1526.60/s  (0.679s, 1508.90/s)  LR: 9.757e-04  Data: 0.014 (0.016)
Train: 29 [1250/1251 (100%)]  Loss: 3.962 (4.11)  Time: 0.680s, 1505.85/s  (0.679s, 1509.02/s)  LR: 9.757e-04  Data: 0.000 (0.016)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.843 (2.843)  Loss:  0.7910 (0.7910)  Acc@1: 83.8867 (83.8867)  Acc@5: 95.8984 (95.8984)
Test: [  48/48]  Time: 0.166 (0.332)  Loss:  0.7686 (1.4639)  Acc@1: 83.2547 (66.7580)  Acc@5: 95.6368 (88.1700)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-29.pth.tar', 66.75800009521484)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-28.pth.tar', 66.39000010742187)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-27.pth.tar', 65.99400005371093)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-26.pth.tar', 65.65600007324218)

Train: 30 [   0/1251 (  0%)]  Loss: 4.152 (4.15)  Time: 3.709s,  276.09/s  (3.709s,  276.09/s)  LR: 9.756e-04  Data: 1.954 (1.954)
Train: 30 [  50/1251 (  4%)]  Loss: 4.247 (4.20)  Time: 0.676s, 1514.46/s  (0.706s, 1450.51/s)  LR: 9.756e-04  Data: 0.013 (0.052)
Train: 30 [ 100/1251 (  8%)]  Loss: 4.297 (4.23)  Time: 0.680s, 1505.86/s  (0.691s, 1482.47/s)  LR: 9.755e-04  Data: 0.014 (0.034)
Train: 30 [ 150/1251 ( 12%)]  Loss: 4.511 (4.30)  Time: 0.682s, 1502.01/s  (0.687s, 1490.22/s)  LR: 9.755e-04  Data: 0.013 (0.027)
Train: 30 [ 200/1251 ( 16%)]  Loss: 4.122 (4.27)  Time: 0.669s, 1530.39/s  (0.685s, 1494.19/s)  LR: 9.754e-04  Data: 0.016 (0.024)
Train: 30 [ 250/1251 ( 20%)]  Loss: 4.269 (4.27)  Time: 0.671s, 1525.44/s  (0.684s, 1497.56/s)  LR: 9.753e-04  Data: 0.013 (0.022)
Train: 30 [ 300/1251 ( 24%)]  Loss: 4.087 (4.24)  Time: 0.683s, 1499.04/s  (0.683s, 1499.74/s)  LR: 9.753e-04  Data: 0.014 (0.021)
Train: 30 [ 350/1251 ( 28%)]  Loss: 4.067 (4.22)  Time: 0.692s, 1480.50/s  (0.682s, 1500.52/s)  LR: 9.752e-04  Data: 0.011 (0.020)
Train: 30 [ 400/1251 ( 32%)]  Loss: 3.761 (4.17)  Time: 0.670s, 1528.88/s  (0.682s, 1501.09/s)  LR: 9.751e-04  Data: 0.013 (0.019)
Train: 30 [ 450/1251 ( 36%)]  Loss: 4.121 (4.16)  Time: 0.681s, 1503.29/s  (0.682s, 1501.56/s)  LR: 9.751e-04  Data: 0.013 (0.018)
Train: 30 [ 500/1251 ( 40%)]  Loss: 4.383 (4.18)  Time: 0.680s, 1505.63/s  (0.682s, 1501.15/s)  LR: 9.750e-04  Data: 0.015 (0.018)
Train: 30 [ 550/1251 ( 44%)]  Loss: 4.311 (4.19)  Time: 0.675s, 1516.92/s  (0.682s, 1500.81/s)  LR: 9.749e-04  Data: 0.013 (0.018)
Train: 30 [ 600/1251 ( 48%)]  Loss: 3.941 (4.17)  Time: 0.679s, 1508.83/s  (0.682s, 1500.55/s)  LR: 9.749e-04  Data: 0.013 (0.017)
Train: 30 [ 650/1251 ( 52%)]  Loss: 4.103 (4.17)  Time: 0.683s, 1499.55/s  (0.682s, 1500.43/s)  LR: 9.748e-04  Data: 0.013 (0.017)
Train: 30 [ 700/1251 ( 56%)]  Loss: 3.966 (4.16)  Time: 0.672s, 1524.64/s  (0.682s, 1500.67/s)  LR: 9.747e-04  Data: 0.018 (0.017)
Train: 30 [ 750/1251 ( 60%)]  Loss: 4.250 (4.16)  Time: 0.684s, 1497.64/s  (0.682s, 1500.76/s)  LR: 9.747e-04  Data: 0.013 (0.017)
Train: 30 [ 800/1251 ( 64%)]  Loss: 4.327 (4.17)  Time: 0.684s, 1497.50/s  (0.682s, 1500.78/s)  LR: 9.746e-04  Data: 0.014 (0.016)
Train: 30 [ 850/1251 ( 68%)]  Loss: 4.302 (4.18)  Time: 0.684s, 1497.22/s  (0.682s, 1500.78/s)  LR: 9.745e-04  Data: 0.013 (0.016)
Train: 30 [ 900/1251 ( 72%)]  Loss: 4.395 (4.19)  Time: 0.673s, 1522.30/s  (0.682s, 1500.66/s)  LR: 9.745e-04  Data: 0.016 (0.016)
Train: 30 [ 950/1251 ( 76%)]  Loss: 4.358 (4.20)  Time: 0.680s, 1506.32/s  (0.682s, 1500.91/s)  LR: 9.744e-04  Data: 0.018 (0.016)
Train: 30 [1000/1251 ( 80%)]  Loss: 4.400 (4.21)  Time: 0.701s, 1460.44/s  (0.682s, 1500.63/s)  LR: 9.743e-04  Data: 0.012 (0.016)
Train: 30 [1050/1251 ( 84%)]  Loss: 4.572 (4.22)  Time: 0.673s, 1521.36/s  (0.682s, 1500.63/s)  LR: 9.743e-04  Data: 0.013 (0.016)
Train: 30 [1100/1251 ( 88%)]  Loss: 4.384 (4.23)  Time: 0.679s, 1508.33/s  (0.682s, 1500.61/s)  LR: 9.742e-04  Data: 0.013 (0.016)
Train: 30 [1150/1251 ( 92%)]  Loss: 3.911 (4.22)  Time: 0.694s, 1474.48/s  (0.682s, 1500.53/s)  LR: 9.741e-04  Data: 0.013 (0.016)
Train: 30 [1200/1251 ( 96%)]  Loss: 4.017 (4.21)  Time: 0.678s, 1509.96/s  (0.682s, 1500.71/s)  LR: 9.741e-04  Data: 0.013 (0.016)
Train: 30 [1250/1251 (100%)]  Loss: 4.070 (4.20)  Time: 0.661s, 1549.48/s  (0.682s, 1500.83/s)  LR: 9.740e-04  Data: 0.000 (0.016)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.240 (3.240)  Loss:  0.8203 (0.8203)  Acc@1: 81.0547 (81.0547)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.167 (0.331)  Loss:  0.7944 (1.4391)  Acc@1: 82.0755 (67.2540)  Acc@5: 95.4009 (88.2920)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-30.pth.tar', 67.25399997070312)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-29.pth.tar', 66.75800009521484)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-28.pth.tar', 66.39000010742187)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-27.pth.tar', 65.99400005371093)

Train: 31 [   0/1251 (  0%)]  Loss: 4.148 (4.15)  Time: 3.479s,  294.30/s  (3.479s,  294.30/s)  LR: 9.740e-04  Data: 1.586 (1.586)
Train: 31 [  50/1251 (  4%)]  Loss: 3.838 (3.99)  Time: 0.674s, 1519.59/s  (0.698s, 1466.99/s)  LR: 9.739e-04  Data: 0.013 (0.045)
Train: 31 [ 100/1251 (  8%)]  Loss: 3.819 (3.93)  Time: 0.684s, 1497.74/s  (0.686s, 1492.03/s)  LR: 9.739e-04  Data: 0.014 (0.030)
Train: 31 [ 150/1251 ( 12%)]  Loss: 4.321 (4.03)  Time: 0.676s, 1513.71/s  (0.684s, 1498.00/s)  LR: 9.738e-04  Data: 0.012 (0.025)
Train: 31 [ 200/1251 ( 16%)]  Loss: 4.028 (4.03)  Time: 0.679s, 1508.50/s  (0.682s, 1500.48/s)  LR: 9.737e-04  Data: 0.012 (0.022)
Train: 31 [ 250/1251 ( 20%)]  Loss: 3.774 (3.99)  Time: 0.671s, 1525.39/s  (0.682s, 1500.93/s)  LR: 9.737e-04  Data: 0.013 (0.020)
Train: 31 [ 300/1251 ( 24%)]  Loss: 4.363 (4.04)  Time: 0.673s, 1522.22/s  (0.682s, 1502.01/s)  LR: 9.736e-04  Data: 0.012 (0.019)
Train: 31 [ 350/1251 ( 28%)]  Loss: 4.131 (4.05)  Time: 0.670s, 1528.52/s  (0.682s, 1502.44/s)  LR: 9.735e-04  Data: 0.018 (0.018)
Train: 31 [ 400/1251 ( 32%)]  Loss: 4.221 (4.07)  Time: 0.682s, 1501.21/s  (0.681s, 1503.15/s)  LR: 9.735e-04  Data: 0.015 (0.018)
Train: 31 [ 450/1251 ( 36%)]  Loss: 4.132 (4.08)  Time: 0.678s, 1509.47/s  (0.681s, 1503.07/s)  LR: 9.734e-04  Data: 0.013 (0.017)
Train: 31 [ 500/1251 ( 40%)]  Loss: 3.996 (4.07)  Time: 0.668s, 1532.06/s  (0.681s, 1503.36/s)  LR: 9.733e-04  Data: 0.014 (0.017)
Train: 31 [ 550/1251 ( 44%)]  Loss: 4.192 (4.08)  Time: 0.677s, 1511.64/s  (0.681s, 1503.46/s)  LR: 9.733e-04  Data: 0.014 (0.017)
Train: 31 [ 600/1251 ( 48%)]  Loss: 3.977 (4.07)  Time: 0.683s, 1498.34/s  (0.681s, 1503.76/s)  LR: 9.732e-04  Data: 0.014 (0.016)
Train: 31 [ 650/1251 ( 52%)]  Loss: 4.050 (4.07)  Time: 0.676s, 1514.28/s  (0.681s, 1503.84/s)  LR: 9.731e-04  Data: 0.013 (0.016)
Train: 31 [ 700/1251 ( 56%)]  Loss: 4.011 (4.07)  Time: 0.685s, 1493.87/s  (0.681s, 1503.92/s)  LR: 9.731e-04  Data: 0.018 (0.016)
Train: 31 [ 750/1251 ( 60%)]  Loss: 3.973 (4.06)  Time: 0.684s, 1498.07/s  (0.681s, 1504.25/s)  LR: 9.730e-04  Data: 0.013 (0.016)
Train: 31 [ 800/1251 ( 64%)]  Loss: 4.255 (4.07)  Time: 0.688s, 1488.54/s  (0.681s, 1504.38/s)  LR: 9.729e-04  Data: 0.013 (0.016)
Train: 31 [ 850/1251 ( 68%)]  Loss: 4.273 (4.08)  Time: 0.681s, 1504.36/s  (0.681s, 1504.51/s)  LR: 9.729e-04  Data: 0.013 (0.016)
Train: 31 [ 900/1251 ( 72%)]  Loss: 4.285 (4.09)  Time: 0.691s, 1482.89/s  (0.681s, 1504.62/s)  LR: 9.728e-04  Data: 0.014 (0.016)
Train: 31 [ 950/1251 ( 76%)]  Loss: 4.306 (4.10)  Time: 0.673s, 1520.51/s  (0.680s, 1504.90/s)  LR: 9.727e-04  Data: 0.014 (0.015)
Train: 31 [1000/1251 ( 80%)]  Loss: 3.977 (4.10)  Time: 0.675s, 1517.13/s  (0.680s, 1504.92/s)  LR: 9.727e-04  Data: 0.012 (0.015)
Train: 31 [1050/1251 ( 84%)]  Loss: 4.103 (4.10)  Time: 0.674s, 1518.52/s  (0.680s, 1505.19/s)  LR: 9.726e-04  Data: 0.013 (0.015)
Train: 31 [1100/1251 ( 88%)]  Loss: 4.268 (4.11)  Time: 0.691s, 1482.22/s  (0.680s, 1505.35/s)  LR: 9.725e-04  Data: 0.015 (0.015)
Train: 31 [1150/1251 ( 92%)]  Loss: 4.092 (4.11)  Time: 0.676s, 1515.87/s  (0.680s, 1505.39/s)  LR: 9.725e-04  Data: 0.013 (0.015)
Train: 31 [1200/1251 ( 96%)]  Loss: 4.035 (4.10)  Time: 0.682s, 1502.01/s  (0.680s, 1505.51/s)  LR: 9.724e-04  Data: 0.013 (0.015)
Train: 31 [1250/1251 (100%)]  Loss: 4.192 (4.11)  Time: 0.671s, 1526.07/s  (0.680s, 1505.60/s)  LR: 9.723e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.752 (2.752)  Loss:  0.7676 (0.7676)  Acc@1: 84.9609 (84.9609)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.178 (0.329)  Loss:  0.8027 (1.4076)  Acc@1: 82.3113 (67.9680)  Acc@5: 94.8113 (88.6860)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-31.pth.tar', 67.9680000732422)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-30.pth.tar', 67.25399997070312)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-29.pth.tar', 66.75800009521484)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-28.pth.tar', 66.39000010742187)

Train: 32 [   0/1251 (  0%)]  Loss: 3.833 (3.83)  Time: 3.499s,  292.69/s  (3.499s,  292.69/s)  LR: 9.723e-04  Data: 1.690 (1.690)
Train: 32 [  50/1251 (  4%)]  Loss: 4.052 (3.94)  Time: 0.670s, 1528.45/s  (0.702s, 1459.10/s)  LR: 9.723e-04  Data: 0.015 (0.047)
Train: 32 [ 100/1251 (  8%)]  Loss: 3.921 (3.94)  Time: 0.672s, 1523.12/s  (0.686s, 1491.80/s)  LR: 9.722e-04  Data: 0.013 (0.030)
Train: 32 [ 150/1251 ( 12%)]  Loss: 3.802 (3.90)  Time: 0.677s, 1513.28/s  (0.683s, 1499.30/s)  LR: 9.721e-04  Data: 0.012 (0.025)
Train: 32 [ 200/1251 ( 16%)]  Loss: 4.030 (3.93)  Time: 0.675s, 1517.52/s  (0.682s, 1502.49/s)  LR: 9.721e-04  Data: 0.013 (0.022)
Train: 32 [ 250/1251 ( 20%)]  Loss: 4.363 (4.00)  Time: 0.676s, 1513.68/s  (0.681s, 1504.16/s)  LR: 9.720e-04  Data: 0.013 (0.020)
Train: 32 [ 300/1251 ( 24%)]  Loss: 3.733 (3.96)  Time: 0.672s, 1522.86/s  (0.680s, 1505.38/s)  LR: 9.719e-04  Data: 0.012 (0.019)
Train: 32 [ 350/1251 ( 28%)]  Loss: 4.165 (3.99)  Time: 0.673s, 1521.58/s  (0.680s, 1506.49/s)  LR: 9.718e-04  Data: 0.016 (0.018)
Train: 32 [ 400/1251 ( 32%)]  Loss: 3.959 (3.98)  Time: 0.680s, 1506.31/s  (0.680s, 1506.71/s)  LR: 9.718e-04  Data: 0.012 (0.018)
Train: 32 [ 450/1251 ( 36%)]  Loss: 4.316 (4.02)  Time: 0.682s, 1502.49/s  (0.680s, 1506.90/s)  LR: 9.717e-04  Data: 0.014 (0.017)
Train: 32 [ 500/1251 ( 40%)]  Loss: 4.047 (4.02)  Time: 0.679s, 1509.12/s  (0.679s, 1507.09/s)  LR: 9.716e-04  Data: 0.013 (0.017)
Train: 32 [ 550/1251 ( 44%)]  Loss: 3.997 (4.02)  Time: 0.672s, 1523.02/s  (0.679s, 1507.50/s)  LR: 9.716e-04  Data: 0.014 (0.017)
Train: 32 [ 600/1251 ( 48%)]  Loss: 4.307 (4.04)  Time: 0.673s, 1522.22/s  (0.679s, 1508.09/s)  LR: 9.715e-04  Data: 0.013 (0.016)
Train: 32 [ 650/1251 ( 52%)]  Loss: 4.341 (4.06)  Time: 0.681s, 1502.91/s  (0.679s, 1508.24/s)  LR: 9.714e-04  Data: 0.013 (0.016)
Train: 32 [ 700/1251 ( 56%)]  Loss: 3.957 (4.05)  Time: 0.686s, 1493.36/s  (0.679s, 1508.53/s)  LR: 9.714e-04  Data: 0.013 (0.016)
Train: 32 [ 750/1251 ( 60%)]  Loss: 4.068 (4.06)  Time: 0.683s, 1500.10/s  (0.679s, 1508.81/s)  LR: 9.713e-04  Data: 0.013 (0.016)
Train: 32 [ 800/1251 ( 64%)]  Loss: 4.202 (4.06)  Time: 0.675s, 1518.08/s  (0.679s, 1508.67/s)  LR: 9.712e-04  Data: 0.013 (0.016)
Train: 32 [ 850/1251 ( 68%)]  Loss: 4.151 (4.07)  Time: 0.676s, 1515.51/s  (0.679s, 1508.70/s)  LR: 9.711e-04  Data: 0.014 (0.015)
Train: 32 [ 900/1251 ( 72%)]  Loss: 4.096 (4.07)  Time: 0.669s, 1531.75/s  (0.679s, 1508.98/s)  LR: 9.711e-04  Data: 0.013 (0.015)
Train: 32 [ 950/1251 ( 76%)]  Loss: 4.363 (4.09)  Time: 0.675s, 1517.04/s  (0.678s, 1509.21/s)  LR: 9.710e-04  Data: 0.015 (0.015)
Train: 32 [1000/1251 ( 80%)]  Loss: 3.957 (4.08)  Time: 0.673s, 1520.63/s  (0.678s, 1509.48/s)  LR: 9.709e-04  Data: 0.013 (0.015)
Train: 32 [1050/1251 ( 84%)]  Loss: 4.040 (4.08)  Time: 0.673s, 1520.83/s  (0.678s, 1509.55/s)  LR: 9.709e-04  Data: 0.014 (0.015)
Train: 32 [1100/1251 ( 88%)]  Loss: 4.497 (4.10)  Time: 0.675s, 1517.99/s  (0.678s, 1509.76/s)  LR: 9.708e-04  Data: 0.012 (0.015)
Train: 32 [1150/1251 ( 92%)]  Loss: 3.917 (4.09)  Time: 0.665s, 1540.02/s  (0.678s, 1509.99/s)  LR: 9.707e-04  Data: 0.013 (0.015)
Train: 32 [1200/1251 ( 96%)]  Loss: 3.998 (4.08)  Time: 0.675s, 1516.26/s  (0.678s, 1510.24/s)  LR: 9.707e-04  Data: 0.013 (0.015)
Train: 32 [1250/1251 (100%)]  Loss: 4.318 (4.09)  Time: 0.671s, 1527.05/s  (0.678s, 1510.66/s)  LR: 9.706e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.807 (2.807)  Loss:  0.7471 (0.7471)  Acc@1: 85.6445 (85.6445)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.170 (0.320)  Loss:  0.8486 (1.4334)  Acc@1: 81.8396 (67.6480)  Acc@5: 94.9292 (88.6940)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-31.pth.tar', 67.9680000732422)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-32.pth.tar', 67.64799986816406)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-30.pth.tar', 67.25399997070312)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-29.pth.tar', 66.75800009521484)

Train: 33 [   0/1251 (  0%)]  Loss: 4.445 (4.45)  Time: 3.719s,  275.37/s  (3.719s,  275.37/s)  LR: 9.706e-04  Data: 1.639 (1.639)
Train: 33 [  50/1251 (  4%)]  Loss: 4.370 (4.41)  Time: 0.665s, 1539.60/s  (0.703s, 1457.24/s)  LR: 9.705e-04  Data: 0.014 (0.046)
Train: 33 [ 100/1251 (  8%)]  Loss: 4.418 (4.41)  Time: 0.673s, 1520.92/s  (0.685s, 1494.51/s)  LR: 9.704e-04  Data: 0.014 (0.030)
Train: 33 [ 150/1251 ( 12%)]  Loss: 4.047 (4.32)  Time: 0.669s, 1530.26/s  (0.680s, 1505.16/s)  LR: 9.704e-04  Data: 0.015 (0.025)
Train: 33 [ 200/1251 ( 16%)]  Loss: 4.577 (4.37)  Time: 0.668s, 1532.31/s  (0.678s, 1509.97/s)  LR: 9.703e-04  Data: 0.013 (0.022)
Train: 33 [ 250/1251 ( 20%)]  Loss: 4.537 (4.40)  Time: 0.682s, 1500.57/s  (0.677s, 1512.99/s)  LR: 9.702e-04  Data: 0.013 (0.020)
Train: 33 [ 300/1251 ( 24%)]  Loss: 4.454 (4.41)  Time: 0.675s, 1515.92/s  (0.676s, 1514.46/s)  LR: 9.702e-04  Data: 0.016 (0.019)
Train: 33 [ 350/1251 ( 28%)]  Loss: 4.032 (4.36)  Time: 0.676s, 1514.88/s  (0.676s, 1514.93/s)  LR: 9.701e-04  Data: 0.014 (0.019)
Train: 33 [ 400/1251 ( 32%)]  Loss: 4.094 (4.33)  Time: 0.672s, 1523.26/s  (0.676s, 1515.31/s)  LR: 9.700e-04  Data: 0.012 (0.018)
Train: 33 [ 450/1251 ( 36%)]  Loss: 4.193 (4.32)  Time: 0.673s, 1520.53/s  (0.676s, 1515.42/s)  LR: 9.699e-04  Data: 0.013 (0.017)
Train: 33 [ 500/1251 ( 40%)]  Loss: 4.198 (4.31)  Time: 0.685s, 1495.65/s  (0.676s, 1515.63/s)  LR: 9.699e-04  Data: 0.013 (0.017)
Train: 33 [ 550/1251 ( 44%)]  Loss: 3.809 (4.26)  Time: 0.674s, 1519.68/s  (0.676s, 1515.70/s)  LR: 9.698e-04  Data: 0.013 (0.017)
Train: 33 [ 600/1251 ( 48%)]  Loss: 4.141 (4.25)  Time: 0.672s, 1524.60/s  (0.675s, 1516.02/s)  LR: 9.697e-04  Data: 0.014 (0.016)
Train: 33 [ 650/1251 ( 52%)]  Loss: 4.089 (4.24)  Time: 0.682s, 1500.55/s  (0.676s, 1515.79/s)  LR: 9.697e-04  Data: 0.013 (0.016)
Train: 33 [ 700/1251 ( 56%)]  Loss: 3.861 (4.22)  Time: 0.676s, 1515.02/s  (0.676s, 1515.26/s)  LR: 9.696e-04  Data: 0.013 (0.016)
Train: 33 [ 750/1251 ( 60%)]  Loss: 3.841 (4.19)  Time: 0.681s, 1503.86/s  (0.676s, 1515.04/s)  LR: 9.695e-04  Data: 0.014 (0.016)
Train: 33 [ 800/1251 ( 64%)]  Loss: 4.413 (4.21)  Time: 0.684s, 1496.00/s  (0.676s, 1514.74/s)  LR: 9.694e-04  Data: 0.015 (0.016)
Train: 33 [ 850/1251 ( 68%)]  Loss: 3.958 (4.19)  Time: 0.684s, 1496.47/s  (0.676s, 1514.34/s)  LR: 9.694e-04  Data: 0.016 (0.016)
Train: 33 [ 900/1251 ( 72%)]  Loss: 4.264 (4.20)  Time: 0.675s, 1516.53/s  (0.676s, 1514.06/s)  LR: 9.693e-04  Data: 0.014 (0.016)
Train: 33 [ 950/1251 ( 76%)]  Loss: 3.434 (4.16)  Time: 0.679s, 1508.37/s  (0.676s, 1513.95/s)  LR: 9.692e-04  Data: 0.012 (0.015)
Train: 33 [1000/1251 ( 80%)]  Loss: 4.322 (4.17)  Time: 0.672s, 1524.57/s  (0.676s, 1514.26/s)  LR: 9.692e-04  Data: 0.012 (0.015)
Train: 33 [1050/1251 ( 84%)]  Loss: 4.245 (4.17)  Time: 0.669s, 1530.16/s  (0.676s, 1514.40/s)  LR: 9.691e-04  Data: 0.013 (0.015)
Train: 33 [1100/1251 ( 88%)]  Loss: 4.053 (4.16)  Time: 0.671s, 1526.12/s  (0.676s, 1514.34/s)  LR: 9.690e-04  Data: 0.013 (0.015)
Train: 33 [1150/1251 ( 92%)]  Loss: 4.123 (4.16)  Time: 0.677s, 1513.59/s  (0.676s, 1514.30/s)  LR: 9.689e-04  Data: 0.012 (0.015)
Train: 33 [1200/1251 ( 96%)]  Loss: 3.931 (4.15)  Time: 0.675s, 1516.26/s  (0.676s, 1513.96/s)  LR: 9.689e-04  Data: 0.013 (0.015)
Train: 33 [1250/1251 (100%)]  Loss: 3.964 (4.15)  Time: 0.669s, 1530.53/s  (0.676s, 1513.88/s)  LR: 9.688e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.828 (2.828)  Loss:  0.7061 (0.7061)  Acc@1: 85.5469 (85.5469)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.166 (0.326)  Loss:  0.7886 (1.3859)  Acc@1: 80.5424 (68.0560)  Acc@5: 95.1651 (88.8560)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-33.pth.tar', 68.05599995117187)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-31.pth.tar', 67.9680000732422)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-32.pth.tar', 67.64799986816406)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-30.pth.tar', 67.25399997070312)

Train: 34 [   0/1251 (  0%)]  Loss: 4.235 (4.24)  Time: 3.222s,  317.81/s  (3.222s,  317.81/s)  LR: 9.688e-04  Data: 1.636 (1.636)
Train: 34 [  50/1251 (  4%)]  Loss: 4.216 (4.23)  Time: 0.665s, 1539.16/s  (0.697s, 1469.33/s)  LR: 9.687e-04  Data: 0.012 (0.045)
Train: 34 [ 100/1251 (  8%)]  Loss: 4.073 (4.17)  Time: 0.677s, 1512.91/s  (0.686s, 1493.68/s)  LR: 9.687e-04  Data: 0.013 (0.030)
Train: 34 [ 150/1251 ( 12%)]  Loss: 4.038 (4.14)  Time: 0.678s, 1510.28/s  (0.682s, 1500.51/s)  LR: 9.686e-04  Data: 0.014 (0.024)
Train: 34 [ 200/1251 ( 16%)]  Loss: 3.532 (4.02)  Time: 0.676s, 1514.66/s  (0.681s, 1503.15/s)  LR: 9.685e-04  Data: 0.013 (0.022)
Train: 34 [ 250/1251 ( 20%)]  Loss: 4.144 (4.04)  Time: 0.674s, 1518.65/s  (0.680s, 1504.79/s)  LR: 9.684e-04  Data: 0.014 (0.020)
Train: 34 [ 300/1251 ( 24%)]  Loss: 4.279 (4.07)  Time: 0.678s, 1511.07/s  (0.680s, 1505.38/s)  LR: 9.684e-04  Data: 0.013 (0.019)
Train: 34 [ 350/1251 ( 28%)]  Loss: 3.916 (4.05)  Time: 0.683s, 1499.49/s  (0.680s, 1504.92/s)  LR: 9.683e-04  Data: 0.012 (0.018)
Train: 34 [ 400/1251 ( 32%)]  Loss: 4.084 (4.06)  Time: 0.681s, 1503.27/s  (0.681s, 1504.52/s)  LR: 9.682e-04  Data: 0.014 (0.018)
Train: 34 [ 450/1251 ( 36%)]  Loss: 4.146 (4.07)  Time: 0.677s, 1511.95/s  (0.681s, 1504.62/s)  LR: 9.681e-04  Data: 0.013 (0.017)
Train: 34 [ 500/1251 ( 40%)]  Loss: 3.734 (4.04)  Time: 0.690s, 1483.11/s  (0.680s, 1505.19/s)  LR: 9.681e-04  Data: 0.015 (0.017)
Train: 34 [ 550/1251 ( 44%)]  Loss: 4.252 (4.05)  Time: 0.681s, 1504.73/s  (0.680s, 1505.89/s)  LR: 9.680e-04  Data: 0.012 (0.017)
Train: 34 [ 600/1251 ( 48%)]  Loss: 4.018 (4.05)  Time: 0.685s, 1494.64/s  (0.680s, 1506.16/s)  LR: 9.679e-04  Data: 0.012 (0.016)
Train: 34 [ 650/1251 ( 52%)]  Loss: 3.947 (4.04)  Time: 0.680s, 1505.17/s  (0.680s, 1506.55/s)  LR: 9.678e-04  Data: 0.014 (0.016)
Train: 34 [ 700/1251 ( 56%)]  Loss: 4.035 (4.04)  Time: 0.686s, 1492.10/s  (0.680s, 1506.54/s)  LR: 9.678e-04  Data: 0.012 (0.016)
Train: 34 [ 750/1251 ( 60%)]  Loss: 4.124 (4.05)  Time: 0.684s, 1497.59/s  (0.680s, 1506.74/s)  LR: 9.677e-04  Data: 0.018 (0.016)
Train: 34 [ 800/1251 ( 64%)]  Loss: 4.105 (4.05)  Time: 0.690s, 1483.63/s  (0.680s, 1506.78/s)  LR: 9.676e-04  Data: 0.013 (0.016)
Train: 34 [ 850/1251 ( 68%)]  Loss: 4.002 (4.05)  Time: 0.685s, 1494.37/s  (0.680s, 1506.88/s)  LR: 9.676e-04  Data: 0.015 (0.016)
Train: 34 [ 900/1251 ( 72%)]  Loss: 3.768 (4.03)  Time: 0.685s, 1495.33/s  (0.680s, 1506.86/s)  LR: 9.675e-04  Data: 0.016 (0.016)
Train: 34 [ 950/1251 ( 76%)]  Loss: 4.225 (4.04)  Time: 0.675s, 1517.58/s  (0.680s, 1506.76/s)  LR: 9.674e-04  Data: 0.013 (0.016)
Train: 34 [1000/1251 ( 80%)]  Loss: 3.996 (4.04)  Time: 0.684s, 1497.66/s  (0.680s, 1506.65/s)  LR: 9.673e-04  Data: 0.014 (0.015)
Train: 34 [1050/1251 ( 84%)]  Loss: 3.918 (4.04)  Time: 0.690s, 1483.00/s  (0.680s, 1506.60/s)  LR: 9.673e-04  Data: 0.013 (0.015)
Train: 34 [1100/1251 ( 88%)]  Loss: 4.035 (4.04)  Time: 0.670s, 1529.26/s  (0.680s, 1506.96/s)  LR: 9.672e-04  Data: 0.013 (0.015)
Train: 34 [1150/1251 ( 92%)]  Loss: 4.115 (4.04)  Time: 0.672s, 1523.66/s  (0.679s, 1507.33/s)  LR: 9.671e-04  Data: 0.013 (0.015)
Train: 34 [1200/1251 ( 96%)]  Loss: 3.965 (4.04)  Time: 0.683s, 1498.86/s  (0.679s, 1507.57/s)  LR: 9.670e-04  Data: 0.015 (0.015)
Train: 34 [1250/1251 (100%)]  Loss: 4.398 (4.05)  Time: 0.666s, 1537.29/s  (0.679s, 1507.87/s)  LR: 9.670e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.865 (2.865)  Loss:  0.6802 (0.6802)  Acc@1: 86.5234 (86.5234)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.168 (0.322)  Loss:  0.8193 (1.3698)  Acc@1: 82.1934 (68.7240)  Acc@5: 95.0472 (89.1240)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-34.pth.tar', 68.72400002197266)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-33.pth.tar', 68.05599995117187)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-31.pth.tar', 67.9680000732422)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-32.pth.tar', 67.64799986816406)

Train: 35 [   0/1251 (  0%)]  Loss: 4.111 (4.11)  Time: 3.119s,  328.32/s  (3.119s,  328.32/s)  LR: 9.670e-04  Data: 1.738 (1.738)
Train: 35 [  50/1251 (  4%)]  Loss: 3.438 (3.77)  Time: 0.672s, 1524.43/s  (0.697s, 1469.84/s)  LR: 9.669e-04  Data: 0.013 (0.048)
Train: 35 [ 100/1251 (  8%)]  Loss: 4.090 (3.88)  Time: 0.673s, 1520.73/s  (0.684s, 1497.11/s)  LR: 9.668e-04  Data: 0.012 (0.031)
Train: 35 [ 150/1251 ( 12%)]  Loss: 4.028 (3.92)  Time: 0.660s, 1552.34/s  (0.681s, 1504.24/s)  LR: 9.667e-04  Data: 0.013 (0.026)
Train: 35 [ 200/1251 ( 16%)]  Loss: 4.129 (3.96)  Time: 0.683s, 1499.21/s  (0.680s, 1505.56/s)  LR: 9.667e-04  Data: 0.014 (0.023)
Train: 35 [ 250/1251 ( 20%)]  Loss: 3.650 (3.91)  Time: 0.680s, 1505.28/s  (0.680s, 1505.85/s)  LR: 9.666e-04  Data: 0.016 (0.021)
Train: 35 [ 300/1251 ( 24%)]  Loss: 3.844 (3.90)  Time: 0.672s, 1523.46/s  (0.680s, 1505.79/s)  LR: 9.665e-04  Data: 0.014 (0.020)
Train: 35 [ 350/1251 ( 28%)]  Loss: 4.182 (3.93)  Time: 0.683s, 1499.55/s  (0.680s, 1505.38/s)  LR: 9.664e-04  Data: 0.013 (0.019)
Train: 35 [ 400/1251 ( 32%)]  Loss: 4.188 (3.96)  Time: 0.663s, 1543.49/s  (0.680s, 1506.06/s)  LR: 9.664e-04  Data: 0.014 (0.018)
Train: 35 [ 450/1251 ( 36%)]  Loss: 3.969 (3.96)  Time: 0.683s, 1498.39/s  (0.680s, 1506.24/s)  LR: 9.663e-04  Data: 0.012 (0.018)
Train: 35 [ 500/1251 ( 40%)]  Loss: 4.218 (3.99)  Time: 0.673s, 1520.53/s  (0.680s, 1506.11/s)  LR: 9.662e-04  Data: 0.013 (0.017)
Train: 35 [ 550/1251 ( 44%)]  Loss: 3.979 (3.99)  Time: 0.692s, 1479.79/s  (0.680s, 1506.28/s)  LR: 9.661e-04  Data: 0.015 (0.017)
Train: 35 [ 600/1251 ( 48%)]  Loss: 4.289 (4.01)  Time: 0.685s, 1494.52/s  (0.680s, 1506.30/s)  LR: 9.661e-04  Data: 0.013 (0.017)
Train: 35 [ 650/1251 ( 52%)]  Loss: 4.177 (4.02)  Time: 0.682s, 1501.01/s  (0.680s, 1506.46/s)  LR: 9.660e-04  Data: 0.011 (0.017)
Train: 35 [ 700/1251 ( 56%)]  Loss: 4.093 (4.03)  Time: 0.681s, 1503.90/s  (0.680s, 1506.61/s)  LR: 9.659e-04  Data: 0.013 (0.016)
Train: 35 [ 750/1251 ( 60%)]  Loss: 3.939 (4.02)  Time: 0.677s, 1512.97/s  (0.680s, 1506.79/s)  LR: 9.658e-04  Data: 0.016 (0.016)
Train: 35 [ 800/1251 ( 64%)]  Loss: 4.318 (4.04)  Time: 0.674s, 1520.28/s  (0.679s, 1507.12/s)  LR: 9.658e-04  Data: 0.012 (0.016)
Train: 35 [ 850/1251 ( 68%)]  Loss: 4.373 (4.06)  Time: 0.680s, 1505.00/s  (0.679s, 1507.26/s)  LR: 9.657e-04  Data: 0.013 (0.016)
Train: 35 [ 900/1251 ( 72%)]  Loss: 3.878 (4.05)  Time: 0.675s, 1516.25/s  (0.679s, 1507.34/s)  LR: 9.656e-04  Data: 0.013 (0.016)
Train: 35 [ 950/1251 ( 76%)]  Loss: 3.934 (4.04)  Time: 0.671s, 1526.13/s  (0.679s, 1507.34/s)  LR: 9.655e-04  Data: 0.014 (0.016)
Train: 35 [1000/1251 ( 80%)]  Loss: 4.296 (4.05)  Time: 0.673s, 1521.90/s  (0.679s, 1507.19/s)  LR: 9.654e-04  Data: 0.013 (0.015)
Train: 35 [1050/1251 ( 84%)]  Loss: 4.286 (4.06)  Time: 0.684s, 1497.61/s  (0.679s, 1507.25/s)  LR: 9.654e-04  Data: 0.013 (0.015)
Train: 35 [1100/1251 ( 88%)]  Loss: 3.854 (4.05)  Time: 0.673s, 1521.27/s  (0.679s, 1507.33/s)  LR: 9.653e-04  Data: 0.014 (0.015)
Train: 35 [1150/1251 ( 92%)]  Loss: 3.604 (4.04)  Time: 0.688s, 1487.91/s  (0.679s, 1507.23/s)  LR: 9.652e-04  Data: 0.014 (0.015)
Train: 35 [1200/1251 ( 96%)]  Loss: 4.064 (4.04)  Time: 0.679s, 1507.00/s  (0.679s, 1507.58/s)  LR: 9.651e-04  Data: 0.016 (0.015)
Train: 35 [1250/1251 (100%)]  Loss: 3.944 (4.03)  Time: 0.668s, 1533.52/s  (0.679s, 1507.47/s)  LR: 9.651e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.977 (2.977)  Loss:  0.6987 (0.6987)  Acc@1: 85.2539 (85.2539)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.173 (0.323)  Loss:  0.8584 (1.3720)  Acc@1: 82.4293 (68.7700)  Acc@5: 93.8679 (88.9960)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-35.pth.tar', 68.77000012451172)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-34.pth.tar', 68.72400002197266)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-33.pth.tar', 68.05599995117187)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-31.pth.tar', 67.9680000732422)

Train: 36 [   0/1251 (  0%)]  Loss: 4.088 (4.09)  Time: 3.605s,  284.05/s  (3.605s,  284.05/s)  LR: 9.651e-04  Data: 1.653 (1.653)
Train: 36 [  50/1251 (  4%)]  Loss: 3.946 (4.02)  Time: 0.673s, 1520.91/s  (0.703s, 1456.26/s)  LR: 9.650e-04  Data: 0.013 (0.046)
Train: 36 [ 100/1251 (  8%)]  Loss: 4.198 (4.08)  Time: 0.678s, 1509.43/s  (0.689s, 1486.65/s)  LR: 9.649e-04  Data: 0.016 (0.030)
Train: 36 [ 150/1251 ( 12%)]  Loss: 3.723 (3.99)  Time: 0.681s, 1504.01/s  (0.685s, 1495.30/s)  LR: 9.648e-04  Data: 0.013 (0.025)
Train: 36 [ 200/1251 ( 16%)]  Loss: 4.223 (4.04)  Time: 0.679s, 1507.96/s  (0.683s, 1499.40/s)  LR: 9.648e-04  Data: 0.015 (0.022)
Train: 36 [ 250/1251 ( 20%)]  Loss: 3.950 (4.02)  Time: 0.679s, 1507.44/s  (0.682s, 1501.37/s)  LR: 9.647e-04  Data: 0.013 (0.020)
Train: 36 [ 300/1251 ( 24%)]  Loss: 4.041 (4.02)  Time: 0.675s, 1516.62/s  (0.681s, 1502.83/s)  LR: 9.646e-04  Data: 0.013 (0.019)
Train: 36 [ 350/1251 ( 28%)]  Loss: 3.838 (4.00)  Time: 0.687s, 1490.13/s  (0.681s, 1503.64/s)  LR: 9.645e-04  Data: 0.013 (0.018)
Train: 36 [ 400/1251 ( 32%)]  Loss: 4.359 (4.04)  Time: 0.677s, 1513.37/s  (0.681s, 1504.20/s)  LR: 9.644e-04  Data: 0.015 (0.018)
Train: 36 [ 450/1251 ( 36%)]  Loss: 3.989 (4.04)  Time: 0.670s, 1528.10/s  (0.680s, 1504.94/s)  LR: 9.644e-04  Data: 0.016 (0.017)
Train: 36 [ 500/1251 ( 40%)]  Loss: 4.184 (4.05)  Time: 0.682s, 1502.55/s  (0.680s, 1505.43/s)  LR: 9.643e-04  Data: 0.013 (0.017)
Train: 36 [ 550/1251 ( 44%)]  Loss: 4.248 (4.07)  Time: 0.684s, 1498.14/s  (0.680s, 1505.61/s)  LR: 9.642e-04  Data: 0.012 (0.017)
Train: 36 [ 600/1251 ( 48%)]  Loss: 4.201 (4.08)  Time: 0.683s, 1498.55/s  (0.680s, 1506.25/s)  LR: 9.641e-04  Data: 0.012 (0.016)
Train: 36 [ 650/1251 ( 52%)]  Loss: 3.952 (4.07)  Time: 0.669s, 1530.24/s  (0.680s, 1506.48/s)  LR: 9.641e-04  Data: 0.013 (0.016)
Train: 36 [ 700/1251 ( 56%)]  Loss: 3.864 (4.05)  Time: 0.671s, 1525.96/s  (0.680s, 1506.93/s)  LR: 9.640e-04  Data: 0.015 (0.016)
Train: 36 [ 750/1251 ( 60%)]  Loss: 4.129 (4.06)  Time: 0.683s, 1498.59/s  (0.679s, 1507.00/s)  LR: 9.639e-04  Data: 0.017 (0.016)
Train: 36 [ 800/1251 ( 64%)]  Loss: 4.177 (4.07)  Time: 0.688s, 1488.92/s  (0.679s, 1507.14/s)  LR: 9.638e-04  Data: 0.013 (0.016)
Train: 36 [ 850/1251 ( 68%)]  Loss: 3.915 (4.06)  Time: 0.669s, 1530.38/s  (0.679s, 1507.24/s)  LR: 9.637e-04  Data: 0.013 (0.015)
Train: 36 [ 900/1251 ( 72%)]  Loss: 3.721 (4.04)  Time: 0.682s, 1500.85/s  (0.679s, 1507.37/s)  LR: 9.637e-04  Data: 0.012 (0.015)
Train: 36 [ 950/1251 ( 76%)]  Loss: 4.067 (4.04)  Time: 0.675s, 1518.00/s  (0.679s, 1507.39/s)  LR: 9.636e-04  Data: 0.012 (0.015)
Train: 36 [1000/1251 ( 80%)]  Loss: 4.180 (4.05)  Time: 0.676s, 1514.07/s  (0.679s, 1507.45/s)  LR: 9.635e-04  Data: 0.014 (0.015)
Train: 36 [1050/1251 ( 84%)]  Loss: 4.155 (4.05)  Time: 0.674s, 1518.64/s  (0.679s, 1507.55/s)  LR: 9.634e-04  Data: 0.013 (0.015)
Train: 36 [1100/1251 ( 88%)]  Loss: 3.701 (4.04)  Time: 0.668s, 1532.58/s  (0.679s, 1507.70/s)  LR: 9.634e-04  Data: 0.013 (0.015)
Train: 36 [1150/1251 ( 92%)]  Loss: 4.065 (4.04)  Time: 0.670s, 1528.72/s  (0.679s, 1508.05/s)  LR: 9.633e-04  Data: 0.013 (0.015)
Train: 36 [1200/1251 ( 96%)]  Loss: 3.736 (4.03)  Time: 0.676s, 1515.41/s  (0.679s, 1508.24/s)  LR: 9.632e-04  Data: 0.012 (0.015)
Train: 36 [1250/1251 (100%)]  Loss: 4.022 (4.03)  Time: 0.664s, 1541.97/s  (0.679s, 1508.63/s)  LR: 9.631e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.847 (2.847)  Loss:  0.6865 (0.6865)  Acc@1: 84.8633 (84.8633)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.167 (0.330)  Loss:  0.7739 (1.3186)  Acc@1: 82.3113 (69.0940)  Acc@5: 95.7547 (89.4480)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-36.pth.tar', 69.09400007324219)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-35.pth.tar', 68.77000012451172)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-34.pth.tar', 68.72400002197266)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-33.pth.tar', 68.05599995117187)

Train: 37 [   0/1251 (  0%)]  Loss: 3.816 (3.82)  Time: 3.719s,  275.33/s  (3.719s,  275.33/s)  LR: 9.631e-04  Data: 1.618 (1.618)
Train: 37 [  50/1251 (  4%)]  Loss: 4.094 (3.96)  Time: 0.665s, 1540.99/s  (0.708s, 1445.52/s)  LR: 9.630e-04  Data: 0.013 (0.046)
Train: 37 [ 100/1251 (  8%)]  Loss: 3.855 (3.92)  Time: 0.672s, 1522.90/s  (0.690s, 1484.42/s)  LR: 9.630e-04  Data: 0.016 (0.030)
Train: 37 [ 150/1251 ( 12%)]  Loss: 4.298 (4.02)  Time: 0.668s, 1531.87/s  (0.685s, 1495.76/s)  LR: 9.629e-04  Data: 0.013 (0.024)
Train: 37 [ 200/1251 ( 16%)]  Loss: 4.146 (4.04)  Time: 0.676s, 1515.52/s  (0.683s, 1499.98/s)  LR: 9.628e-04  Data: 0.016 (0.022)
Train: 37 [ 250/1251 ( 20%)]  Loss: 4.298 (4.08)  Time: 0.667s, 1534.99/s  (0.682s, 1502.02/s)  LR: 9.627e-04  Data: 0.014 (0.020)
Train: 37 [ 300/1251 ( 24%)]  Loss: 4.061 (4.08)  Time: 0.685s, 1494.32/s  (0.682s, 1502.24/s)  LR: 9.626e-04  Data: 0.020 (0.019)
Train: 37 [ 350/1251 ( 28%)]  Loss: 4.018 (4.07)  Time: 0.671s, 1527.04/s  (0.681s, 1503.47/s)  LR: 9.626e-04  Data: 0.013 (0.018)
Train: 37 [ 400/1251 ( 32%)]  Loss: 3.819 (4.04)  Time: 0.682s, 1501.21/s  (0.681s, 1503.64/s)  LR: 9.625e-04  Data: 0.015 (0.018)
Train: 37 [ 450/1251 ( 36%)]  Loss: 3.936 (4.03)  Time: 0.686s, 1493.39/s  (0.681s, 1503.77/s)  LR: 9.624e-04  Data: 0.013 (0.017)
Train: 37 [ 500/1251 ( 40%)]  Loss: 3.881 (4.02)  Time: 0.679s, 1507.43/s  (0.681s, 1503.98/s)  LR: 9.623e-04  Data: 0.012 (0.017)
Train: 37 [ 550/1251 ( 44%)]  Loss: 4.300 (4.04)  Time: 0.683s, 1498.65/s  (0.681s, 1503.92/s)  LR: 9.622e-04  Data: 0.016 (0.017)
Train: 37 [ 600/1251 ( 48%)]  Loss: 4.114 (4.05)  Time: 0.676s, 1515.91/s  (0.681s, 1504.58/s)  LR: 9.622e-04  Data: 0.011 (0.016)
Train: 37 [ 650/1251 ( 52%)]  Loss: 3.994 (4.05)  Time: 0.668s, 1532.19/s  (0.680s, 1505.35/s)  LR: 9.621e-04  Data: 0.016 (0.016)
Train: 37 [ 700/1251 ( 56%)]  Loss: 3.986 (4.04)  Time: 0.671s, 1526.45/s  (0.680s, 1505.75/s)  LR: 9.620e-04  Data: 0.014 (0.016)
Train: 37 [ 750/1251 ( 60%)]  Loss: 4.228 (4.05)  Time: 0.691s, 1482.24/s  (0.680s, 1506.07/s)  LR: 9.619e-04  Data: 0.013 (0.016)
Train: 37 [ 800/1251 ( 64%)]  Loss: 3.925 (4.05)  Time: 0.683s, 1499.14/s  (0.680s, 1506.15/s)  LR: 9.618e-04  Data: 0.016 (0.016)
Train: 37 [ 850/1251 ( 68%)]  Loss: 4.051 (4.05)  Time: 0.685s, 1495.50/s  (0.680s, 1506.18/s)  LR: 9.618e-04  Data: 0.013 (0.015)
Train: 37 [ 900/1251 ( 72%)]  Loss: 4.318 (4.06)  Time: 0.683s, 1500.29/s  (0.680s, 1506.18/s)  LR: 9.617e-04  Data: 0.014 (0.015)
Train: 37 [ 950/1251 ( 76%)]  Loss: 3.878 (4.05)  Time: 0.681s, 1504.71/s  (0.680s, 1506.21/s)  LR: 9.616e-04  Data: 0.017 (0.015)
Train: 37 [1000/1251 ( 80%)]  Loss: 3.774 (4.04)  Time: 0.681s, 1504.37/s  (0.680s, 1506.15/s)  LR: 9.615e-04  Data: 0.013 (0.015)
Train: 37 [1050/1251 ( 84%)]  Loss: 4.025 (4.04)  Time: 0.680s, 1505.30/s  (0.680s, 1506.20/s)  LR: 9.614e-04  Data: 0.014 (0.015)
Train: 37 [1100/1251 ( 88%)]  Loss: 4.075 (4.04)  Time: 0.675s, 1516.99/s  (0.680s, 1506.30/s)  LR: 9.614e-04  Data: 0.013 (0.015)
Train: 37 [1150/1251 ( 92%)]  Loss: 3.811 (4.03)  Time: 0.686s, 1493.16/s  (0.680s, 1506.51/s)  LR: 9.613e-04  Data: 0.013 (0.015)
Train: 37 [1200/1251 ( 96%)]  Loss: 3.849 (4.02)  Time: 0.676s, 1515.16/s  (0.680s, 1506.78/s)  LR: 9.612e-04  Data: 0.014 (0.015)
Train: 37 [1250/1251 (100%)]  Loss: 4.009 (4.02)  Time: 0.661s, 1548.51/s  (0.680s, 1506.92/s)  LR: 9.611e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.700 (2.700)  Loss:  0.6675 (0.6675)  Acc@1: 87.0117 (87.0117)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.175 (0.331)  Loss:  0.7725 (1.3309)  Acc@1: 83.6085 (69.3660)  Acc@5: 95.4009 (89.6020)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-37.pth.tar', 69.36599999023437)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-36.pth.tar', 69.09400007324219)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-35.pth.tar', 68.77000012451172)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-34.pth.tar', 68.72400002197266)

Train: 38 [   0/1251 (  0%)]  Loss: 4.259 (4.26)  Time: 3.436s,  297.99/s  (3.436s,  297.99/s)  LR: 9.611e-04  Data: 1.678 (1.678)
Train: 38 [  50/1251 (  4%)]  Loss: 4.006 (4.13)  Time: 0.679s, 1508.67/s  (0.707s, 1448.34/s)  LR: 9.610e-04  Data: 0.014 (0.046)
Train: 38 [ 100/1251 (  8%)]  Loss: 3.838 (4.03)  Time: 0.671s, 1525.44/s  (0.690s, 1483.09/s)  LR: 9.610e-04  Data: 0.013 (0.030)
Train: 38 [ 150/1251 ( 12%)]  Loss: 4.340 (4.11)  Time: 0.673s, 1520.86/s  (0.686s, 1492.85/s)  LR: 9.609e-04  Data: 0.011 (0.025)
Train: 38 [ 200/1251 ( 16%)]  Loss: 4.185 (4.13)  Time: 0.695s, 1473.53/s  (0.684s, 1496.85/s)  LR: 9.608e-04  Data: 0.013 (0.022)
Train: 38 [ 250/1251 ( 20%)]  Loss: 3.695 (4.05)  Time: 0.690s, 1483.09/s  (0.683s, 1499.08/s)  LR: 9.607e-04  Data: 0.013 (0.020)
Train: 38 [ 300/1251 ( 24%)]  Loss: 3.805 (4.02)  Time: 0.672s, 1523.16/s  (0.682s, 1501.35/s)  LR: 9.606e-04  Data: 0.015 (0.019)
Train: 38 [ 350/1251 ( 28%)]  Loss: 4.242 (4.05)  Time: 0.682s, 1500.76/s  (0.682s, 1502.12/s)  LR: 9.606e-04  Data: 0.012 (0.018)
Train: 38 [ 400/1251 ( 32%)]  Loss: 4.025 (4.04)  Time: 0.671s, 1525.09/s  (0.681s, 1503.04/s)  LR: 9.605e-04  Data: 0.013 (0.018)
Train: 38 [ 450/1251 ( 36%)]  Loss: 3.932 (4.03)  Time: 0.669s, 1529.77/s  (0.681s, 1503.82/s)  LR: 9.604e-04  Data: 0.014 (0.017)
Train: 38 [ 500/1251 ( 40%)]  Loss: 4.002 (4.03)  Time: 0.671s, 1526.04/s  (0.681s, 1504.51/s)  LR: 9.603e-04  Data: 0.013 (0.017)
Train: 38 [ 550/1251 ( 44%)]  Loss: 4.098 (4.04)  Time: 0.672s, 1524.33/s  (0.680s, 1505.18/s)  LR: 9.602e-04  Data: 0.013 (0.016)
Train: 38 [ 600/1251 ( 48%)]  Loss: 3.946 (4.03)  Time: 0.684s, 1497.10/s  (0.680s, 1505.45/s)  LR: 9.602e-04  Data: 0.013 (0.016)
Train: 38 [ 650/1251 ( 52%)]  Loss: 4.068 (4.03)  Time: 0.678s, 1509.92/s  (0.680s, 1506.03/s)  LR: 9.601e-04  Data: 0.013 (0.016)
Train: 38 [ 700/1251 ( 56%)]  Loss: 4.046 (4.03)  Time: 0.676s, 1513.91/s  (0.680s, 1506.70/s)  LR: 9.600e-04  Data: 0.016 (0.016)
Train: 38 [ 750/1251 ( 60%)]  Loss: 3.662 (4.01)  Time: 0.682s, 1502.43/s  (0.679s, 1507.13/s)  LR: 9.599e-04  Data: 0.013 (0.016)
Train: 38 [ 800/1251 ( 64%)]  Loss: 4.121 (4.02)  Time: 0.675s, 1516.34/s  (0.679s, 1507.66/s)  LR: 9.598e-04  Data: 0.013 (0.016)
Train: 38 [ 850/1251 ( 68%)]  Loss: 4.221 (4.03)  Time: 0.681s, 1503.34/s  (0.679s, 1507.96/s)  LR: 9.597e-04  Data: 0.013 (0.015)
Train: 38 [ 900/1251 ( 72%)]  Loss: 3.950 (4.02)  Time: 0.680s, 1505.74/s  (0.679s, 1508.35/s)  LR: 9.597e-04  Data: 0.012 (0.015)
Train: 38 [ 950/1251 ( 76%)]  Loss: 3.968 (4.02)  Time: 0.685s, 1495.55/s  (0.679s, 1508.59/s)  LR: 9.596e-04  Data: 0.012 (0.015)
Train: 38 [1000/1251 ( 80%)]  Loss: 4.135 (4.03)  Time: 0.679s, 1508.72/s  (0.679s, 1508.91/s)  LR: 9.595e-04  Data: 0.015 (0.015)
Train: 38 [1050/1251 ( 84%)]  Loss: 3.800 (4.02)  Time: 0.677s, 1512.33/s  (0.679s, 1509.02/s)  LR: 9.594e-04  Data: 0.013 (0.015)
Train: 38 [1100/1251 ( 88%)]  Loss: 3.954 (4.01)  Time: 0.681s, 1503.87/s  (0.679s, 1508.87/s)  LR: 9.593e-04  Data: 0.013 (0.015)
Train: 38 [1150/1251 ( 92%)]  Loss: 4.154 (4.02)  Time: 0.677s, 1512.08/s  (0.679s, 1508.89/s)  LR: 9.592e-04  Data: 0.012 (0.015)
Train: 38 [1200/1251 ( 96%)]  Loss: 4.020 (4.02)  Time: 0.674s, 1518.21/s  (0.679s, 1508.75/s)  LR: 9.592e-04  Data: 0.013 (0.015)
Train: 38 [1250/1251 (100%)]  Loss: 4.053 (4.02)  Time: 0.669s, 1530.21/s  (0.679s, 1508.94/s)  LR: 9.591e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.967 (2.967)  Loss:  0.7051 (0.7051)  Acc@1: 85.0586 (85.0586)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.166 (0.331)  Loss:  0.7314 (1.3282)  Acc@1: 83.0189 (69.6560)  Acc@5: 96.8160 (89.6380)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-38.pth.tar', 69.65599999267579)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-37.pth.tar', 69.36599999023437)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-36.pth.tar', 69.09400007324219)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-35.pth.tar', 68.77000012451172)

Train: 39 [   0/1251 (  0%)]  Loss: 3.997 (4.00)  Time: 3.527s,  290.36/s  (3.527s,  290.36/s)  LR: 9.591e-04  Data: 1.683 (1.683)
Train: 39 [  50/1251 (  4%)]  Loss: 3.998 (4.00)  Time: 0.658s, 1556.07/s  (0.698s, 1466.00/s)  LR: 9.590e-04  Data: 0.018 (0.047)
Train: 39 [ 100/1251 (  8%)]  Loss: 4.125 (4.04)  Time: 0.669s, 1531.39/s  (0.684s, 1496.11/s)  LR: 9.589e-04  Data: 0.015 (0.031)
Train: 39 [ 150/1251 ( 12%)]  Loss: 3.666 (3.95)  Time: 0.677s, 1511.95/s  (0.681s, 1503.41/s)  LR: 9.588e-04  Data: 0.013 (0.025)
Train: 39 [ 200/1251 ( 16%)]  Loss: 3.898 (3.94)  Time: 0.683s, 1500.09/s  (0.680s, 1506.22/s)  LR: 9.587e-04  Data: 0.016 (0.022)
Train: 39 [ 250/1251 ( 20%)]  Loss: 4.284 (3.99)  Time: 0.668s, 1531.81/s  (0.679s, 1507.72/s)  LR: 9.587e-04  Data: 0.013 (0.020)
Train: 39 [ 300/1251 ( 24%)]  Loss: 3.573 (3.93)  Time: 0.674s, 1519.87/s  (0.679s, 1509.21/s)  LR: 9.586e-04  Data: 0.012 (0.019)
Train: 39 [ 350/1251 ( 28%)]  Loss: 3.544 (3.89)  Time: 0.674s, 1518.35/s  (0.678s, 1509.71/s)  LR: 9.585e-04  Data: 0.016 (0.018)
Train: 39 [ 400/1251 ( 32%)]  Loss: 4.021 (3.90)  Time: 0.680s, 1506.66/s  (0.678s, 1510.68/s)  LR: 9.584e-04  Data: 0.013 (0.018)
Train: 39 [ 450/1251 ( 36%)]  Loss: 4.184 (3.93)  Time: 0.678s, 1510.06/s  (0.678s, 1510.61/s)  LR: 9.583e-04  Data: 0.013 (0.017)
Train: 39 [ 500/1251 ( 40%)]  Loss: 3.806 (3.92)  Time: 0.665s, 1540.17/s  (0.678s, 1511.26/s)  LR: 9.583e-04  Data: 0.014 (0.017)
Train: 39 [ 550/1251 ( 44%)]  Loss: 3.906 (3.92)  Time: 0.678s, 1510.29/s  (0.677s, 1511.62/s)  LR: 9.582e-04  Data: 0.013 (0.016)
Train: 39 [ 600/1251 ( 48%)]  Loss: 3.954 (3.92)  Time: 0.680s, 1505.43/s  (0.677s, 1512.11/s)  LR: 9.581e-04  Data: 0.014 (0.016)
Train: 39 [ 650/1251 ( 52%)]  Loss: 4.323 (3.95)  Time: 0.677s, 1512.52/s  (0.677s, 1512.22/s)  LR: 9.580e-04  Data: 0.012 (0.016)
Train: 39 [ 700/1251 ( 56%)]  Loss: 3.948 (3.95)  Time: 0.672s, 1524.61/s  (0.677s, 1512.46/s)  LR: 9.579e-04  Data: 0.013 (0.016)
Train: 39 [ 750/1251 ( 60%)]  Loss: 4.300 (3.97)  Time: 0.686s, 1491.89/s  (0.677s, 1512.74/s)  LR: 9.578e-04  Data: 0.015 (0.016)
Train: 39 [ 800/1251 ( 64%)]  Loss: 3.822 (3.96)  Time: 0.686s, 1491.92/s  (0.677s, 1512.68/s)  LR: 9.577e-04  Data: 0.013 (0.015)
Train: 39 [ 850/1251 ( 68%)]  Loss: 3.631 (3.94)  Time: 0.684s, 1497.87/s  (0.677s, 1512.90/s)  LR: 9.577e-04  Data: 0.012 (0.015)
Train: 39 [ 900/1251 ( 72%)]  Loss: 4.001 (3.95)  Time: 0.677s, 1511.73/s  (0.677s, 1513.25/s)  LR: 9.576e-04  Data: 0.013 (0.015)
Train: 39 [ 950/1251 ( 76%)]  Loss: 3.972 (3.95)  Time: 0.669s, 1531.72/s  (0.677s, 1513.57/s)  LR: 9.575e-04  Data: 0.012 (0.015)
Train: 39 [1000/1251 ( 80%)]  Loss: 4.091 (3.95)  Time: 0.671s, 1525.05/s  (0.676s, 1513.75/s)  LR: 9.574e-04  Data: 0.013 (0.015)
Train: 39 [1050/1251 ( 84%)]  Loss: 4.059 (3.96)  Time: 0.682s, 1501.31/s  (0.676s, 1514.05/s)  LR: 9.573e-04  Data: 0.012 (0.015)
Train: 39 [1100/1251 ( 88%)]  Loss: 4.083 (3.96)  Time: 0.675s, 1516.54/s  (0.676s, 1514.22/s)  LR: 9.572e-04  Data: 0.012 (0.015)
Train: 39 [1150/1251 ( 92%)]  Loss: 3.931 (3.96)  Time: 0.669s, 1530.21/s  (0.676s, 1514.49/s)  LR: 9.572e-04  Data: 0.012 (0.015)
Train: 39 [1200/1251 ( 96%)]  Loss: 3.930 (3.96)  Time: 0.676s, 1515.83/s  (0.676s, 1514.59/s)  LR: 9.571e-04  Data: 0.013 (0.015)
Train: 39 [1250/1251 (100%)]  Loss: 4.312 (3.98)  Time: 0.648s, 1580.25/s  (0.676s, 1514.90/s)  LR: 9.570e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.752 (2.752)  Loss:  0.6650 (0.6650)  Acc@1: 86.3281 (86.3281)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.167 (0.323)  Loss:  0.7671 (1.3190)  Acc@1: 83.2547 (69.6880)  Acc@5: 95.5189 (89.9380)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-39.pth.tar', 69.68800009521485)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-38.pth.tar', 69.65599999267579)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-37.pth.tar', 69.36599999023437)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-36.pth.tar', 69.09400007324219)

Train: 40 [   0/1251 (  0%)]  Loss: 4.003 (4.00)  Time: 4.292s,  238.60/s  (4.292s,  238.60/s)  LR: 9.570e-04  Data: 1.691 (1.691)
Train: 40 [  50/1251 (  4%)]  Loss: 4.105 (4.05)  Time: 0.672s, 1524.56/s  (0.707s, 1449.39/s)  LR: 9.569e-04  Data: 0.014 (0.047)
Train: 40 [ 100/1251 (  8%)]  Loss: 4.339 (4.15)  Time: 0.668s, 1533.04/s  (0.687s, 1491.37/s)  LR: 9.568e-04  Data: 0.013 (0.030)
Train: 40 [ 150/1251 ( 12%)]  Loss: 4.248 (4.17)  Time: 0.680s, 1505.41/s  (0.681s, 1502.83/s)  LR: 9.567e-04  Data: 0.012 (0.025)
Train: 40 [ 200/1251 ( 16%)]  Loss: 3.902 (4.12)  Time: 0.683s, 1498.64/s  (0.680s, 1506.04/s)  LR: 9.566e-04  Data: 0.015 (0.022)
Train: 40 [ 250/1251 ( 20%)]  Loss: 3.995 (4.10)  Time: 0.678s, 1509.84/s  (0.679s, 1508.52/s)  LR: 9.566e-04  Data: 0.015 (0.020)
Train: 40 [ 300/1251 ( 24%)]  Loss: 3.756 (4.05)  Time: 0.680s, 1506.69/s  (0.678s, 1509.39/s)  LR: 9.565e-04  Data: 0.016 (0.019)
Train: 40 [ 350/1251 ( 28%)]  Loss: 3.932 (4.04)  Time: 0.669s, 1531.38/s  (0.678s, 1510.28/s)  LR: 9.564e-04  Data: 0.013 (0.018)
Train: 40 [ 400/1251 ( 32%)]  Loss: 3.970 (4.03)  Time: 0.678s, 1509.66/s  (0.678s, 1510.83/s)  LR: 9.563e-04  Data: 0.013 (0.018)
Train: 40 [ 450/1251 ( 36%)]  Loss: 4.209 (4.05)  Time: 0.677s, 1511.55/s  (0.678s, 1511.05/s)  LR: 9.562e-04  Data: 0.014 (0.017)
Train: 40 [ 500/1251 ( 40%)]  Loss: 3.790 (4.02)  Time: 0.677s, 1512.08/s  (0.678s, 1511.15/s)  LR: 9.561e-04  Data: 0.013 (0.017)
Train: 40 [ 550/1251 ( 44%)]  Loss: 4.235 (4.04)  Time: 0.688s, 1488.16/s  (0.678s, 1510.89/s)  LR: 9.561e-04  Data: 0.014 (0.017)
Train: 40 [ 600/1251 ( 48%)]  Loss: 4.351 (4.06)  Time: 0.681s, 1503.23/s  (0.678s, 1510.66/s)  LR: 9.560e-04  Data: 0.012 (0.016)
Train: 40 [ 650/1251 ( 52%)]  Loss: 3.680 (4.04)  Time: 0.665s, 1539.02/s  (0.678s, 1510.66/s)  LR: 9.559e-04  Data: 0.016 (0.016)
Train: 40 [ 700/1251 ( 56%)]  Loss: 3.985 (4.03)  Time: 0.675s, 1516.81/s  (0.678s, 1510.56/s)  LR: 9.558e-04  Data: 0.012 (0.016)
Train: 40 [ 750/1251 ( 60%)]  Loss: 3.906 (4.03)  Time: 0.679s, 1508.20/s  (0.678s, 1510.76/s)  LR: 9.557e-04  Data: 0.012 (0.016)
Train: 40 [ 800/1251 ( 64%)]  Loss: 4.198 (4.04)  Time: 0.681s, 1504.55/s  (0.678s, 1511.00/s)  LR: 9.556e-04  Data: 0.013 (0.016)
Train: 40 [ 850/1251 ( 68%)]  Loss: 4.087 (4.04)  Time: 0.675s, 1516.78/s  (0.678s, 1511.07/s)  LR: 9.555e-04  Data: 0.013 (0.015)
Train: 40 [ 900/1251 ( 72%)]  Loss: 4.325 (4.05)  Time: 0.676s, 1515.01/s  (0.678s, 1511.11/s)  LR: 9.554e-04  Data: 0.013 (0.015)
Train: 40 [ 950/1251 ( 76%)]  Loss: 4.196 (4.06)  Time: 0.685s, 1494.22/s  (0.678s, 1511.15/s)  LR: 9.554e-04  Data: 0.013 (0.015)
Train: 40 [1000/1251 ( 80%)]  Loss: 4.105 (4.06)  Time: 0.672s, 1524.00/s  (0.678s, 1511.31/s)  LR: 9.553e-04  Data: 0.013 (0.015)
Train: 40 [1050/1251 ( 84%)]  Loss: 4.193 (4.07)  Time: 0.675s, 1516.81/s  (0.678s, 1511.36/s)  LR: 9.552e-04  Data: 0.014 (0.015)
Train: 40 [1100/1251 ( 88%)]  Loss: 3.867 (4.06)  Time: 0.679s, 1508.09/s  (0.677s, 1511.52/s)  LR: 9.551e-04  Data: 0.013 (0.015)
Train: 40 [1150/1251 ( 92%)]  Loss: 3.600 (4.04)  Time: 0.674s, 1520.21/s  (0.677s, 1511.76/s)  LR: 9.550e-04  Data: 0.013 (0.015)
Train: 40 [1200/1251 ( 96%)]  Loss: 3.882 (4.03)  Time: 0.687s, 1490.73/s  (0.677s, 1511.79/s)  LR: 9.549e-04  Data: 0.013 (0.015)
Train: 40 [1250/1251 (100%)]  Loss: 3.957 (4.03)  Time: 0.664s, 1541.87/s  (0.677s, 1512.06/s)  LR: 9.548e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.064 (3.064)  Loss:  0.7051 (0.7051)  Acc@1: 85.4492 (85.4492)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.169 (0.324)  Loss:  0.7822 (1.3158)  Acc@1: 83.4906 (70.1160)  Acc@5: 95.2830 (90.1600)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-40.pth.tar', 70.11599993896485)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-39.pth.tar', 69.68800009521485)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-38.pth.tar', 69.65599999267579)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-37.pth.tar', 69.36599999023437)

Train: 41 [   0/1251 (  0%)]  Loss: 4.017 (4.02)  Time: 3.697s,  277.00/s  (3.697s,  277.00/s)  LR: 9.548e-04  Data: 1.833 (1.833)
Train: 41 [  50/1251 (  4%)]  Loss: 3.967 (3.99)  Time: 0.660s, 1551.37/s  (0.698s, 1467.73/s)  LR: 9.548e-04  Data: 0.013 (0.049)
Train: 41 [ 100/1251 (  8%)]  Loss: 3.985 (3.99)  Time: 0.672s, 1524.60/s  (0.683s, 1499.05/s)  LR: 9.547e-04  Data: 0.013 (0.032)
Train: 41 [ 150/1251 ( 12%)]  Loss: 3.999 (3.99)  Time: 0.678s, 1509.73/s  (0.680s, 1506.59/s)  LR: 9.546e-04  Data: 0.016 (0.026)
Train: 41 [ 200/1251 ( 16%)]  Loss: 3.835 (3.96)  Time: 0.674s, 1518.77/s  (0.679s, 1509.07/s)  LR: 9.545e-04  Data: 0.013 (0.023)
Train: 41 [ 250/1251 ( 20%)]  Loss: 3.560 (3.89)  Time: 0.676s, 1515.75/s  (0.678s, 1510.08/s)  LR: 9.544e-04  Data: 0.015 (0.021)
Train: 41 [ 300/1251 ( 24%)]  Loss: 4.148 (3.93)  Time: 0.686s, 1493.62/s  (0.678s, 1510.46/s)  LR: 9.543e-04  Data: 0.016 (0.020)
Train: 41 [ 350/1251 ( 28%)]  Loss: 4.072 (3.95)  Time: 0.672s, 1522.88/s  (0.678s, 1510.51/s)  LR: 9.542e-04  Data: 0.013 (0.019)
Train: 41 [ 400/1251 ( 32%)]  Loss: 4.257 (3.98)  Time: 0.667s, 1534.36/s  (0.678s, 1510.63/s)  LR: 9.541e-04  Data: 0.013 (0.018)
Train: 41 [ 450/1251 ( 36%)]  Loss: 4.018 (3.99)  Time: 0.676s, 1514.27/s  (0.678s, 1510.38/s)  LR: 9.541e-04  Data: 0.012 (0.018)
Train: 41 [ 500/1251 ( 40%)]  Loss: 3.863 (3.97)  Time: 0.672s, 1523.30/s  (0.678s, 1510.48/s)  LR: 9.540e-04  Data: 0.016 (0.017)
Train: 41 [ 550/1251 ( 44%)]  Loss: 3.806 (3.96)  Time: 0.684s, 1498.13/s  (0.678s, 1510.62/s)  LR: 9.539e-04  Data: 0.014 (0.017)
Train: 41 [ 600/1251 ( 48%)]  Loss: 3.787 (3.95)  Time: 0.675s, 1516.44/s  (0.678s, 1510.30/s)  LR: 9.538e-04  Data: 0.013 (0.017)
Train: 41 [ 650/1251 ( 52%)]  Loss: 4.236 (3.97)  Time: 0.690s, 1483.37/s  (0.678s, 1509.98/s)  LR: 9.537e-04  Data: 0.016 (0.016)
Train: 41 [ 700/1251 ( 56%)]  Loss: 3.630 (3.95)  Time: 0.672s, 1524.49/s  (0.678s, 1510.13/s)  LR: 9.536e-04  Data: 0.013 (0.016)
Train: 41 [ 750/1251 ( 60%)]  Loss: 3.677 (3.93)  Time: 0.677s, 1512.43/s  (0.678s, 1510.55/s)  LR: 9.535e-04  Data: 0.012 (0.016)
Train: 41 [ 800/1251 ( 64%)]  Loss: 4.064 (3.94)  Time: 0.677s, 1513.50/s  (0.678s, 1510.85/s)  LR: 9.534e-04  Data: 0.012 (0.016)
Train: 41 [ 850/1251 ( 68%)]  Loss: 3.969 (3.94)  Time: 0.681s, 1502.81/s  (0.678s, 1510.98/s)  LR: 9.534e-04  Data: 0.013 (0.016)
Train: 41 [ 900/1251 ( 72%)]  Loss: 3.882 (3.94)  Time: 0.672s, 1523.90/s  (0.677s, 1511.48/s)  LR: 9.533e-04  Data: 0.013 (0.016)
Train: 41 [ 950/1251 ( 76%)]  Loss: 3.977 (3.94)  Time: 0.678s, 1511.43/s  (0.677s, 1511.69/s)  LR: 9.532e-04  Data: 0.014 (0.016)
Train: 41 [1000/1251 ( 80%)]  Loss: 4.107 (3.95)  Time: 0.666s, 1537.64/s  (0.677s, 1512.19/s)  LR: 9.531e-04  Data: 0.013 (0.015)
Train: 41 [1050/1251 ( 84%)]  Loss: 4.152 (3.95)  Time: 0.686s, 1493.58/s  (0.677s, 1512.44/s)  LR: 9.530e-04  Data: 0.012 (0.015)
Train: 41 [1100/1251 ( 88%)]  Loss: 3.874 (3.95)  Time: 0.680s, 1505.66/s  (0.677s, 1512.75/s)  LR: 9.529e-04  Data: 0.014 (0.015)
Train: 41 [1150/1251 ( 92%)]  Loss: 4.156 (3.96)  Time: 0.672s, 1522.99/s  (0.677s, 1513.14/s)  LR: 9.528e-04  Data: 0.018 (0.015)
Train: 41 [1200/1251 ( 96%)]  Loss: 3.798 (3.95)  Time: 0.671s, 1526.73/s  (0.677s, 1513.26/s)  LR: 9.527e-04  Data: 0.013 (0.015)
Train: 41 [1250/1251 (100%)]  Loss: 4.025 (3.96)  Time: 0.672s, 1524.27/s  (0.677s, 1513.36/s)  LR: 9.527e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.089 (3.089)  Loss:  0.6680 (0.6680)  Acc@1: 86.5234 (86.5234)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.162 (0.332)  Loss:  0.8271 (1.3059)  Acc@1: 81.9575 (70.1240)  Acc@5: 94.8113 (90.0480)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-41.pth.tar', 70.1239999194336)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-40.pth.tar', 70.11599993896485)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-39.pth.tar', 69.68800009521485)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-38.pth.tar', 69.65599999267579)

Train: 42 [   0/1251 (  0%)]  Loss: 4.078 (4.08)  Time: 3.866s,  264.87/s  (3.866s,  264.87/s)  LR: 9.526e-04  Data: 1.903 (1.903)
Train: 42 [  50/1251 (  4%)]  Loss: 3.975 (4.03)  Time: 0.665s, 1540.33/s  (0.701s, 1460.87/s)  LR: 9.526e-04  Data: 0.013 (0.050)
Train: 42 [ 100/1251 (  8%)]  Loss: 4.077 (4.04)  Time: 0.669s, 1530.07/s  (0.685s, 1494.82/s)  LR: 9.525e-04  Data: 0.015 (0.032)
Train: 42 [ 150/1251 ( 12%)]  Loss: 3.996 (4.03)  Time: 0.675s, 1516.43/s  (0.682s, 1501.85/s)  LR: 9.524e-04  Data: 0.013 (0.026)
Train: 42 [ 200/1251 ( 16%)]  Loss: 3.624 (3.95)  Time: 0.680s, 1506.72/s  (0.680s, 1504.83/s)  LR: 9.523e-04  Data: 0.013 (0.023)
Train: 42 [ 250/1251 ( 20%)]  Loss: 4.336 (4.01)  Time: 0.670s, 1528.74/s  (0.680s, 1505.60/s)  LR: 9.522e-04  Data: 0.013 (0.021)
Train: 42 [ 300/1251 ( 24%)]  Loss: 4.175 (4.04)  Time: 0.691s, 1482.67/s  (0.680s, 1505.87/s)  LR: 9.521e-04  Data: 0.013 (0.020)
Train: 42 [ 350/1251 ( 28%)]  Loss: 3.911 (4.02)  Time: 0.676s, 1515.27/s  (0.680s, 1506.34/s)  LR: 9.520e-04  Data: 0.015 (0.019)
Train: 42 [ 400/1251 ( 32%)]  Loss: 3.928 (4.01)  Time: 0.683s, 1498.86/s  (0.680s, 1506.39/s)  LR: 9.519e-04  Data: 0.013 (0.018)
Train: 42 [ 450/1251 ( 36%)]  Loss: 4.137 (4.02)  Time: 0.673s, 1520.82/s  (0.680s, 1506.65/s)  LR: 9.518e-04  Data: 0.013 (0.018)
Train: 42 [ 500/1251 ( 40%)]  Loss: 4.130 (4.03)  Time: 0.679s, 1508.36/s  (0.680s, 1506.80/s)  LR: 9.518e-04  Data: 0.014 (0.017)
Train: 42 [ 550/1251 ( 44%)]  Loss: 3.697 (4.01)  Time: 0.682s, 1502.24/s  (0.680s, 1506.51/s)  LR: 9.517e-04  Data: 0.014 (0.017)
Train: 42 [ 600/1251 ( 48%)]  Loss: 4.163 (4.02)  Time: 0.686s, 1492.04/s  (0.680s, 1506.65/s)  LR: 9.516e-04  Data: 0.013 (0.017)
Train: 42 [ 650/1251 ( 52%)]  Loss: 3.880 (4.01)  Time: 0.672s, 1522.80/s  (0.680s, 1506.68/s)  LR: 9.515e-04  Data: 0.013 (0.016)
Train: 42 [ 700/1251 ( 56%)]  Loss: 3.960 (4.00)  Time: 0.686s, 1492.79/s  (0.680s, 1506.82/s)  LR: 9.514e-04  Data: 0.013 (0.016)
Train: 42 [ 750/1251 ( 60%)]  Loss: 3.861 (4.00)  Time: 0.669s, 1530.17/s  (0.680s, 1506.90/s)  LR: 9.513e-04  Data: 0.013 (0.016)
Train: 42 [ 800/1251 ( 64%)]  Loss: 3.802 (3.98)  Time: 0.682s, 1500.81/s  (0.680s, 1506.96/s)  LR: 9.512e-04  Data: 0.014 (0.016)
Train: 42 [ 850/1251 ( 68%)]  Loss: 4.281 (4.00)  Time: 0.679s, 1508.16/s  (0.679s, 1507.07/s)  LR: 9.511e-04  Data: 0.013 (0.016)
Train: 42 [ 900/1251 ( 72%)]  Loss: 3.823 (3.99)  Time: 0.688s, 1488.31/s  (0.680s, 1506.94/s)  LR: 9.510e-04  Data: 0.013 (0.016)
Train: 42 [ 950/1251 ( 76%)]  Loss: 3.696 (3.98)  Time: 0.666s, 1537.89/s  (0.680s, 1506.77/s)  LR: 9.510e-04  Data: 0.014 (0.015)
Train: 42 [1000/1251 ( 80%)]  Loss: 4.002 (3.98)  Time: 0.687s, 1491.39/s  (0.680s, 1506.74/s)  LR: 9.509e-04  Data: 0.016 (0.015)
Train: 42 [1050/1251 ( 84%)]  Loss: 3.920 (3.98)  Time: 0.669s, 1530.78/s  (0.679s, 1507.07/s)  LR: 9.508e-04  Data: 0.013 (0.015)
Train: 42 [1100/1251 ( 88%)]  Loss: 4.006 (3.98)  Time: 0.669s, 1530.39/s  (0.680s, 1506.88/s)  LR: 9.507e-04  Data: 0.013 (0.015)
Train: 42 [1150/1251 ( 92%)]  Loss: 3.967 (3.98)  Time: 0.680s, 1505.54/s  (0.680s, 1506.98/s)  LR: 9.506e-04  Data: 0.017 (0.015)
Train: 42 [1200/1251 ( 96%)]  Loss: 3.921 (3.97)  Time: 0.683s, 1499.55/s  (0.680s, 1506.87/s)  LR: 9.505e-04  Data: 0.013 (0.015)
Train: 42 [1250/1251 (100%)]  Loss: 3.766 (3.97)  Time: 0.667s, 1534.59/s  (0.679s, 1507.13/s)  LR: 9.504e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.073 (3.073)  Loss:  0.7114 (0.7114)  Acc@1: 84.4727 (84.4727)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.174 (0.324)  Loss:  0.7354 (1.3056)  Acc@1: 83.0189 (69.8440)  Acc@5: 95.1651 (90.0940)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-41.pth.tar', 70.1239999194336)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-40.pth.tar', 70.11599993896485)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-42.pth.tar', 69.84399999267578)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-39.pth.tar', 69.68800009521485)

Train: 43 [   0/1251 (  0%)]  Loss: 3.888 (3.89)  Time: 4.142s,  247.20/s  (4.142s,  247.20/s)  LR: 9.504e-04  Data: 1.652 (1.652)
Train: 43 [  50/1251 (  4%)]  Loss: 3.721 (3.80)  Time: 0.659s, 1553.02/s  (0.709s, 1444.75/s)  LR: 9.503e-04  Data: 0.012 (0.046)
Train: 43 [ 100/1251 (  8%)]  Loss: 4.187 (3.93)  Time: 0.671s, 1525.88/s  (0.689s, 1485.97/s)  LR: 9.502e-04  Data: 0.016 (0.030)
Train: 43 [ 150/1251 ( 12%)]  Loss: 4.069 (3.97)  Time: 0.679s, 1508.35/s  (0.684s, 1496.46/s)  LR: 9.501e-04  Data: 0.012 (0.025)
Train: 43 [ 200/1251 ( 16%)]  Loss: 3.742 (3.92)  Time: 0.676s, 1515.39/s  (0.683s, 1499.99/s)  LR: 9.500e-04  Data: 0.013 (0.022)
Train: 43 [ 250/1251 ( 20%)]  Loss: 3.789 (3.90)  Time: 0.681s, 1504.66/s  (0.681s, 1502.68/s)  LR: 9.500e-04  Data: 0.015 (0.020)
Train: 43 [ 300/1251 ( 24%)]  Loss: 3.896 (3.90)  Time: 0.684s, 1497.36/s  (0.681s, 1503.37/s)  LR: 9.499e-04  Data: 0.013 (0.019)
Train: 43 [ 350/1251 ( 28%)]  Loss: 3.933 (3.90)  Time: 0.678s, 1509.47/s  (0.681s, 1503.73/s)  LR: 9.498e-04  Data: 0.013 (0.018)
Train: 43 [ 400/1251 ( 32%)]  Loss: 3.856 (3.90)  Time: 0.676s, 1514.67/s  (0.681s, 1504.38/s)  LR: 9.497e-04  Data: 0.012 (0.018)
Train: 43 [ 450/1251 ( 36%)]  Loss: 4.088 (3.92)  Time: 0.682s, 1501.99/s  (0.681s, 1504.68/s)  LR: 9.496e-04  Data: 0.013 (0.017)
Train: 43 [ 500/1251 ( 40%)]  Loss: 3.891 (3.91)  Time: 0.681s, 1504.03/s  (0.680s, 1505.02/s)  LR: 9.495e-04  Data: 0.015 (0.017)
Train: 43 [ 550/1251 ( 44%)]  Loss: 3.725 (3.90)  Time: 0.674s, 1518.55/s  (0.680s, 1505.69/s)  LR: 9.494e-04  Data: 0.012 (0.017)
Train: 43 [ 600/1251 ( 48%)]  Loss: 3.885 (3.90)  Time: 0.673s, 1520.80/s  (0.680s, 1505.88/s)  LR: 9.493e-04  Data: 0.013 (0.016)
Train: 43 [ 650/1251 ( 52%)]  Loss: 4.149 (3.92)  Time: 0.675s, 1517.23/s  (0.680s, 1506.35/s)  LR: 9.492e-04  Data: 0.017 (0.016)
Train: 43 [ 700/1251 ( 56%)]  Loss: 4.040 (3.92)  Time: 0.678s, 1511.36/s  (0.680s, 1506.62/s)  LR: 9.491e-04  Data: 0.013 (0.016)
Train: 43 [ 750/1251 ( 60%)]  Loss: 3.898 (3.92)  Time: 0.683s, 1500.33/s  (0.680s, 1506.67/s)  LR: 9.490e-04  Data: 0.015 (0.016)
Train: 43 [ 800/1251 ( 64%)]  Loss: 3.786 (3.91)  Time: 0.674s, 1519.43/s  (0.679s, 1507.01/s)  LR: 9.489e-04  Data: 0.015 (0.016)
Train: 43 [ 850/1251 ( 68%)]  Loss: 3.956 (3.92)  Time: 0.674s, 1519.10/s  (0.680s, 1506.90/s)  LR: 9.489e-04  Data: 0.013 (0.015)
Train: 43 [ 900/1251 ( 72%)]  Loss: 3.706 (3.91)  Time: 0.682s, 1501.92/s  (0.679s, 1507.04/s)  LR: 9.488e-04  Data: 0.012 (0.015)
Train: 43 [ 950/1251 ( 76%)]  Loss: 4.291 (3.92)  Time: 0.682s, 1500.61/s  (0.679s, 1507.17/s)  LR: 9.487e-04  Data: 0.012 (0.015)
Train: 43 [1000/1251 ( 80%)]  Loss: 4.182 (3.94)  Time: 0.675s, 1516.79/s  (0.679s, 1507.67/s)  LR: 9.486e-04  Data: 0.013 (0.015)
Train: 43 [1050/1251 ( 84%)]  Loss: 3.830 (3.93)  Time: 0.681s, 1503.75/s  (0.679s, 1507.84/s)  LR: 9.485e-04  Data: 0.016 (0.015)
Train: 43 [1100/1251 ( 88%)]  Loss: 4.068 (3.94)  Time: 0.671s, 1526.08/s  (0.679s, 1508.17/s)  LR: 9.484e-04  Data: 0.017 (0.015)
Train: 43 [1150/1251 ( 92%)]  Loss: 4.069 (3.94)  Time: 0.666s, 1536.49/s  (0.679s, 1508.48/s)  LR: 9.483e-04  Data: 0.015 (0.015)
Train: 43 [1200/1251 ( 96%)]  Loss: 3.863 (3.94)  Time: 0.671s, 1525.39/s  (0.679s, 1508.82/s)  LR: 9.482e-04  Data: 0.013 (0.015)
Train: 43 [1250/1251 (100%)]  Loss: 4.117 (3.95)  Time: 0.672s, 1523.14/s  (0.678s, 1509.29/s)  LR: 9.481e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.032 (3.032)  Loss:  0.5986 (0.5986)  Acc@1: 86.9141 (86.9141)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.164 (0.328)  Loss:  0.7085 (1.2786)  Acc@1: 83.2547 (70.1300)  Acc@5: 95.5189 (90.1620)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-43.pth.tar', 70.12999996582032)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-41.pth.tar', 70.1239999194336)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-40.pth.tar', 70.11599993896485)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-42.pth.tar', 69.84399999267578)

Train: 44 [   0/1251 (  0%)]  Loss: 3.833 (3.83)  Time: 3.508s,  291.92/s  (3.508s,  291.92/s)  LR: 9.481e-04  Data: 1.740 (1.740)
Train: 44 [  50/1251 (  4%)]  Loss: 3.856 (3.84)  Time: 0.673s, 1521.84/s  (0.700s, 1463.24/s)  LR: 9.480e-04  Data: 0.014 (0.048)
Train: 44 [ 100/1251 (  8%)]  Loss: 3.853 (3.85)  Time: 0.666s, 1538.39/s  (0.685s, 1493.83/s)  LR: 9.479e-04  Data: 0.013 (0.031)
Train: 44 [ 150/1251 ( 12%)]  Loss: 3.948 (3.87)  Time: 0.677s, 1513.32/s  (0.682s, 1502.04/s)  LR: 9.478e-04  Data: 0.014 (0.025)
Train: 44 [ 200/1251 ( 16%)]  Loss: 3.938 (3.89)  Time: 0.671s, 1527.18/s  (0.681s, 1504.44/s)  LR: 9.477e-04  Data: 0.014 (0.022)
Train: 44 [ 250/1251 ( 20%)]  Loss: 3.870 (3.88)  Time: 0.668s, 1533.60/s  (0.679s, 1507.06/s)  LR: 9.477e-04  Data: 0.015 (0.021)
Train: 44 [ 300/1251 ( 24%)]  Loss: 3.958 (3.89)  Time: 0.678s, 1510.20/s  (0.679s, 1508.82/s)  LR: 9.476e-04  Data: 0.012 (0.019)
Train: 44 [ 350/1251 ( 28%)]  Loss: 3.826 (3.89)  Time: 0.676s, 1514.27/s  (0.678s, 1509.66/s)  LR: 9.475e-04  Data: 0.014 (0.019)
Train: 44 [ 400/1251 ( 32%)]  Loss: 3.929 (3.89)  Time: 0.677s, 1513.05/s  (0.678s, 1510.10/s)  LR: 9.474e-04  Data: 0.012 (0.018)
Train: 44 [ 450/1251 ( 36%)]  Loss: 3.722 (3.87)  Time: 0.676s, 1514.53/s  (0.678s, 1510.39/s)  LR: 9.473e-04  Data: 0.013 (0.017)
Train: 44 [ 500/1251 ( 40%)]  Loss: 3.754 (3.86)  Time: 0.673s, 1522.03/s  (0.678s, 1510.42/s)  LR: 9.472e-04  Data: 0.015 (0.017)
Train: 44 [ 550/1251 ( 44%)]  Loss: 4.287 (3.90)  Time: 0.679s, 1508.00/s  (0.678s, 1510.25/s)  LR: 9.471e-04  Data: 0.012 (0.017)
Train: 44 [ 600/1251 ( 48%)]  Loss: 4.023 (3.91)  Time: 0.677s, 1513.33/s  (0.678s, 1510.67/s)  LR: 9.470e-04  Data: 0.013 (0.016)
Train: 44 [ 650/1251 ( 52%)]  Loss: 3.796 (3.90)  Time: 0.675s, 1516.40/s  (0.678s, 1510.60/s)  LR: 9.469e-04  Data: 0.014 (0.016)
Train: 44 [ 700/1251 ( 56%)]  Loss: 4.074 (3.91)  Time: 0.683s, 1499.82/s  (0.678s, 1510.61/s)  LR: 9.468e-04  Data: 0.014 (0.016)
Train: 44 [ 750/1251 ( 60%)]  Loss: 3.856 (3.91)  Time: 0.681s, 1503.33/s  (0.678s, 1510.33/s)  LR: 9.467e-04  Data: 0.013 (0.016)
Train: 44 [ 800/1251 ( 64%)]  Loss: 4.021 (3.91)  Time: 0.689s, 1485.78/s  (0.678s, 1510.26/s)  LR: 9.466e-04  Data: 0.013 (0.016)
Train: 44 [ 850/1251 ( 68%)]  Loss: 3.938 (3.92)  Time: 0.683s, 1499.30/s  (0.678s, 1510.30/s)  LR: 9.465e-04  Data: 0.013 (0.016)
Train: 44 [ 900/1251 ( 72%)]  Loss: 3.729 (3.91)  Time: 0.673s, 1522.41/s  (0.678s, 1510.02/s)  LR: 9.464e-04  Data: 0.014 (0.015)
Train: 44 [ 950/1251 ( 76%)]  Loss: 3.997 (3.91)  Time: 0.689s, 1485.68/s  (0.678s, 1509.82/s)  LR: 9.463e-04  Data: 0.014 (0.015)
Train: 44 [1000/1251 ( 80%)]  Loss: 3.925 (3.91)  Time: 0.676s, 1515.83/s  (0.678s, 1509.76/s)  LR: 9.462e-04  Data: 0.013 (0.015)
Train: 44 [1050/1251 ( 84%)]  Loss: 3.937 (3.91)  Time: 0.675s, 1516.54/s  (0.678s, 1509.82/s)  LR: 9.462e-04  Data: 0.013 (0.015)
Train: 44 [1100/1251 ( 88%)]  Loss: 3.999 (3.92)  Time: 0.686s, 1492.76/s  (0.678s, 1509.77/s)  LR: 9.461e-04  Data: 0.015 (0.015)
Train: 44 [1150/1251 ( 92%)]  Loss: 3.827 (3.91)  Time: 0.687s, 1489.51/s  (0.678s, 1509.67/s)  LR: 9.460e-04  Data: 0.012 (0.015)
Train: 44 [1200/1251 ( 96%)]  Loss: 3.733 (3.91)  Time: 0.664s, 1541.33/s  (0.678s, 1509.86/s)  LR: 9.459e-04  Data: 0.013 (0.015)
Train: 44 [1250/1251 (100%)]  Loss: 3.939 (3.91)  Time: 0.662s, 1547.45/s  (0.678s, 1510.20/s)  LR: 9.458e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.711 (2.711)  Loss:  0.6245 (0.6245)  Acc@1: 87.2070 (87.2070)  Acc@5: 97.0703 (97.0703)
Test: [  48/48]  Time: 0.169 (0.325)  Loss:  0.7471 (1.2773)  Acc@1: 83.1368 (70.3380)  Acc@5: 95.4009 (90.4140)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-44.pth.tar', 70.33799991455078)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-43.pth.tar', 70.12999996582032)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-41.pth.tar', 70.1239999194336)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-40.pth.tar', 70.11599993896485)

Train: 45 [   0/1251 (  0%)]  Loss: 4.002 (4.00)  Time: 3.879s,  263.96/s  (3.879s,  263.96/s)  LR: 9.458e-04  Data: 1.591 (1.591)
Train: 45 [  50/1251 (  4%)]  Loss: 4.039 (4.02)  Time: 0.659s, 1553.02/s  (0.709s, 1444.81/s)  LR: 9.457e-04  Data: 0.013 (0.045)
Train: 45 [ 100/1251 (  8%)]  Loss: 3.968 (4.00)  Time: 0.661s, 1549.20/s  (0.689s, 1486.05/s)  LR: 9.456e-04  Data: 0.014 (0.030)
Train: 45 [ 150/1251 ( 12%)]  Loss: 3.902 (3.98)  Time: 0.666s, 1536.63/s  (0.683s, 1498.78/s)  LR: 9.455e-04  Data: 0.012 (0.024)
Train: 45 [ 200/1251 ( 16%)]  Loss: 4.226 (4.03)  Time: 0.672s, 1524.25/s  (0.681s, 1503.67/s)  LR: 9.454e-04  Data: 0.013 (0.022)
Train: 45 [ 250/1251 ( 20%)]  Loss: 3.667 (3.97)  Time: 0.683s, 1500.29/s  (0.680s, 1506.14/s)  LR: 9.453e-04  Data: 0.015 (0.020)
Train: 45 [ 300/1251 ( 24%)]  Loss: 4.061 (3.98)  Time: 0.673s, 1520.97/s  (0.679s, 1508.59/s)  LR: 9.452e-04  Data: 0.012 (0.019)
Train: 45 [ 350/1251 ( 28%)]  Loss: 3.918 (3.97)  Time: 0.683s, 1498.99/s  (0.678s, 1509.63/s)  LR: 9.451e-04  Data: 0.011 (0.018)
Train: 45 [ 400/1251 ( 32%)]  Loss: 3.639 (3.94)  Time: 0.678s, 1510.94/s  (0.678s, 1510.38/s)  LR: 9.450e-04  Data: 0.014 (0.018)
Train: 45 [ 450/1251 ( 36%)]  Loss: 4.228 (3.96)  Time: 0.673s, 1522.37/s  (0.678s, 1510.57/s)  LR: 9.449e-04  Data: 0.013 (0.017)
Train: 45 [ 500/1251 ( 40%)]  Loss: 4.010 (3.97)  Time: 0.674s, 1518.53/s  (0.678s, 1511.11/s)  LR: 9.448e-04  Data: 0.013 (0.017)
Train: 45 [ 550/1251 ( 44%)]  Loss: 4.033 (3.97)  Time: 0.675s, 1517.58/s  (0.678s, 1511.00/s)  LR: 9.447e-04  Data: 0.014 (0.017)
Train: 45 [ 600/1251 ( 48%)]  Loss: 3.812 (3.96)  Time: 0.668s, 1533.78/s  (0.677s, 1511.55/s)  LR: 9.446e-04  Data: 0.014 (0.016)
Train: 45 [ 650/1251 ( 52%)]  Loss: 3.622 (3.94)  Time: 0.678s, 1511.24/s  (0.677s, 1511.78/s)  LR: 9.445e-04  Data: 0.014 (0.016)
Train: 45 [ 700/1251 ( 56%)]  Loss: 3.892 (3.93)  Time: 0.667s, 1536.18/s  (0.677s, 1512.40/s)  LR: 9.444e-04  Data: 0.013 (0.016)
Train: 45 [ 750/1251 ( 60%)]  Loss: 4.113 (3.95)  Time: 0.671s, 1525.64/s  (0.677s, 1512.78/s)  LR: 9.443e-04  Data: 0.017 (0.016)
Train: 45 [ 800/1251 ( 64%)]  Loss: 4.030 (3.95)  Time: 0.685s, 1495.50/s  (0.677s, 1512.84/s)  LR: 9.443e-04  Data: 0.015 (0.016)
Train: 45 [ 850/1251 ( 68%)]  Loss: 4.233 (3.97)  Time: 0.684s, 1497.97/s  (0.677s, 1512.75/s)  LR: 9.442e-04  Data: 0.013 (0.015)
Train: 45 [ 900/1251 ( 72%)]  Loss: 3.707 (3.95)  Time: 0.675s, 1516.10/s  (0.677s, 1512.68/s)  LR: 9.441e-04  Data: 0.013 (0.015)
Train: 45 [ 950/1251 ( 76%)]  Loss: 4.007 (3.96)  Time: 0.673s, 1522.23/s  (0.677s, 1512.44/s)  LR: 9.440e-04  Data: 0.012 (0.015)
Train: 45 [1000/1251 ( 80%)]  Loss: 4.102 (3.96)  Time: 0.677s, 1511.48/s  (0.677s, 1512.29/s)  LR: 9.439e-04  Data: 0.013 (0.015)
Train: 45 [1050/1251 ( 84%)]  Loss: 3.974 (3.96)  Time: 0.676s, 1514.17/s  (0.677s, 1512.17/s)  LR: 9.438e-04  Data: 0.013 (0.015)
Train: 45 [1100/1251 ( 88%)]  Loss: 3.758 (3.95)  Time: 0.671s, 1525.91/s  (0.677s, 1512.01/s)  LR: 9.437e-04  Data: 0.013 (0.015)
Train: 45 [1150/1251 ( 92%)]  Loss: 3.833 (3.95)  Time: 0.678s, 1509.59/s  (0.677s, 1512.05/s)  LR: 9.436e-04  Data: 0.014 (0.015)
Train: 45 [1200/1251 ( 96%)]  Loss: 4.214 (3.96)  Time: 0.670s, 1529.34/s  (0.677s, 1512.18/s)  LR: 9.435e-04  Data: 0.015 (0.015)
Train: 45 [1250/1251 (100%)]  Loss: 3.941 (3.96)  Time: 0.668s, 1532.02/s  (0.677s, 1512.31/s)  LR: 9.434e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.727 (2.727)  Loss:  0.6357 (0.6357)  Acc@1: 86.1328 (86.1328)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.169 (0.324)  Loss:  0.7788 (1.2711)  Acc@1: 82.5472 (70.7500)  Acc@5: 95.7547 (90.4880)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-45.pth.tar', 70.75000004638672)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-44.pth.tar', 70.33799991455078)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-43.pth.tar', 70.12999996582032)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-41.pth.tar', 70.1239999194336)

Train: 46 [   0/1251 (  0%)]  Loss: 3.744 (3.74)  Time: 3.577s,  286.31/s  (3.577s,  286.31/s)  LR: 9.434e-04  Data: 1.749 (1.749)
Train: 46 [  50/1251 (  4%)]  Loss: 3.806 (3.78)  Time: 0.662s, 1546.59/s  (0.700s, 1462.22/s)  LR: 9.433e-04  Data: 0.014 (0.048)
Train: 46 [ 100/1251 (  8%)]  Loss: 3.908 (3.82)  Time: 0.675s, 1517.12/s  (0.685s, 1494.71/s)  LR: 9.432e-04  Data: 0.013 (0.031)
Train: 46 [ 150/1251 ( 12%)]  Loss: 3.607 (3.77)  Time: 0.675s, 1516.17/s  (0.681s, 1504.60/s)  LR: 9.431e-04  Data: 0.012 (0.025)
Train: 46 [ 200/1251 ( 16%)]  Loss: 3.846 (3.78)  Time: 0.671s, 1526.54/s  (0.679s, 1508.26/s)  LR: 9.430e-04  Data: 0.014 (0.022)
Train: 46 [ 250/1251 ( 20%)]  Loss: 3.927 (3.81)  Time: 0.662s, 1545.69/s  (0.678s, 1511.05/s)  LR: 9.429e-04  Data: 0.013 (0.020)
Train: 46 [ 300/1251 ( 24%)]  Loss: 4.121 (3.85)  Time: 0.677s, 1512.82/s  (0.678s, 1511.19/s)  LR: 9.428e-04  Data: 0.012 (0.019)
Train: 46 [ 350/1251 ( 28%)]  Loss: 3.833 (3.85)  Time: 0.665s, 1538.94/s  (0.677s, 1511.63/s)  LR: 9.427e-04  Data: 0.012 (0.018)
Train: 46 [ 400/1251 ( 32%)]  Loss: 4.155 (3.88)  Time: 0.675s, 1517.21/s  (0.677s, 1512.40/s)  LR: 9.426e-04  Data: 0.013 (0.018)
Train: 46 [ 450/1251 ( 36%)]  Loss: 3.804 (3.88)  Time: 0.671s, 1526.04/s  (0.677s, 1512.69/s)  LR: 9.425e-04  Data: 0.013 (0.017)
Train: 46 [ 500/1251 ( 40%)]  Loss: 3.677 (3.86)  Time: 0.683s, 1499.94/s  (0.677s, 1513.21/s)  LR: 9.424e-04  Data: 0.013 (0.017)
Train: 46 [ 550/1251 ( 44%)]  Loss: 4.112 (3.88)  Time: 0.672s, 1524.59/s  (0.677s, 1513.33/s)  LR: 9.423e-04  Data: 0.014 (0.017)
Train: 46 [ 600/1251 ( 48%)]  Loss: 3.630 (3.86)  Time: 0.676s, 1515.66/s  (0.677s, 1513.57/s)  LR: 9.422e-04  Data: 0.013 (0.016)
Train: 46 [ 650/1251 ( 52%)]  Loss: 3.839 (3.86)  Time: 0.672s, 1522.98/s  (0.677s, 1513.55/s)  LR: 9.421e-04  Data: 0.013 (0.016)
Train: 46 [ 700/1251 ( 56%)]  Loss: 3.901 (3.86)  Time: 0.668s, 1532.16/s  (0.677s, 1513.67/s)  LR: 9.420e-04  Data: 0.016 (0.016)
Train: 46 [ 750/1251 ( 60%)]  Loss: 3.778 (3.86)  Time: 0.673s, 1521.14/s  (0.677s, 1513.58/s)  LR: 9.419e-04  Data: 0.014 (0.016)
Train: 46 [ 800/1251 ( 64%)]  Loss: 3.877 (3.86)  Time: 0.674s, 1518.64/s  (0.676s, 1513.72/s)  LR: 9.418e-04  Data: 0.019 (0.016)
Train: 46 [ 850/1251 ( 68%)]  Loss: 4.061 (3.87)  Time: 0.680s, 1505.07/s  (0.676s, 1514.02/s)  LR: 9.417e-04  Data: 0.012 (0.016)
Train: 46 [ 900/1251 ( 72%)]  Loss: 3.774 (3.86)  Time: 0.675s, 1517.42/s  (0.676s, 1514.10/s)  LR: 9.416e-04  Data: 0.017 (0.016)
Train: 46 [ 950/1251 ( 76%)]  Loss: 3.777 (3.86)  Time: 0.678s, 1511.25/s  (0.676s, 1514.13/s)  LR: 9.415e-04  Data: 0.017 (0.015)
Train: 46 [1000/1251 ( 80%)]  Loss: 3.929 (3.86)  Time: 0.678s, 1510.18/s  (0.676s, 1514.12/s)  LR: 9.414e-04  Data: 0.013 (0.015)
Train: 46 [1050/1251 ( 84%)]  Loss: 4.168 (3.88)  Time: 0.667s, 1536.11/s  (0.676s, 1513.97/s)  LR: 9.413e-04  Data: 0.015 (0.015)
Train: 46 [1100/1251 ( 88%)]  Loss: 3.981 (3.88)  Time: 0.676s, 1515.45/s  (0.676s, 1513.83/s)  LR: 9.412e-04  Data: 0.012 (0.015)
Train: 46 [1150/1251 ( 92%)]  Loss: 3.979 (3.88)  Time: 0.687s, 1490.69/s  (0.677s, 1513.64/s)  LR: 9.411e-04  Data: 0.018 (0.015)
Train: 46 [1200/1251 ( 96%)]  Loss: 4.150 (3.90)  Time: 0.679s, 1508.97/s  (0.677s, 1513.37/s)  LR: 9.410e-04  Data: 0.013 (0.015)
Train: 46 [1250/1251 (100%)]  Loss: 4.074 (3.90)  Time: 0.677s, 1512.52/s  (0.677s, 1513.11/s)  LR: 9.409e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.732 (2.732)  Loss:  0.5874 (0.5874)  Acc@1: 87.5000 (87.5000)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.167 (0.325)  Loss:  0.7466 (1.2473)  Acc@1: 82.9009 (70.9740)  Acc@5: 95.6368 (90.5760)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-46.pth.tar', 70.97399994140625)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-45.pth.tar', 70.75000004638672)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-44.pth.tar', 70.33799991455078)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-43.pth.tar', 70.12999996582032)

Train: 47 [   0/1251 (  0%)]  Loss: 4.054 (4.05)  Time: 3.277s,  312.51/s  (3.277s,  312.51/s)  LR: 9.409e-04  Data: 1.553 (1.553)
Train: 47 [  50/1251 (  4%)]  Loss: 4.224 (4.14)  Time: 0.664s, 1541.05/s  (0.694s, 1475.56/s)  LR: 9.408e-04  Data: 0.014 (0.044)
Train: 47 [ 100/1251 (  8%)]  Loss: 4.095 (4.12)  Time: 0.673s, 1520.59/s  (0.682s, 1501.13/s)  LR: 9.407e-04  Data: 0.014 (0.029)
Train: 47 [ 150/1251 ( 12%)]  Loss: 3.830 (4.05)  Time: 0.685s, 1495.05/s  (0.680s, 1506.70/s)  LR: 9.407e-04  Data: 0.013 (0.024)
Train: 47 [ 200/1251 ( 16%)]  Loss: 3.713 (3.98)  Time: 0.689s, 1485.34/s  (0.679s, 1508.37/s)  LR: 9.406e-04  Data: 0.013 (0.021)
Train: 47 [ 250/1251 ( 20%)]  Loss: 4.131 (4.01)  Time: 0.677s, 1513.00/s  (0.678s, 1509.90/s)  LR: 9.405e-04  Data: 0.016 (0.020)
Train: 47 [ 300/1251 ( 24%)]  Loss: 3.954 (4.00)  Time: 0.674s, 1519.72/s  (0.678s, 1510.85/s)  LR: 9.404e-04  Data: 0.014 (0.019)
Train: 47 [ 350/1251 ( 28%)]  Loss: 3.776 (3.97)  Time: 0.684s, 1497.30/s  (0.678s, 1510.88/s)  LR: 9.403e-04  Data: 0.012 (0.018)
Train: 47 [ 400/1251 ( 32%)]  Loss: 4.122 (3.99)  Time: 0.689s, 1487.14/s  (0.678s, 1511.03/s)  LR: 9.402e-04  Data: 0.013 (0.017)
Train: 47 [ 450/1251 ( 36%)]  Loss: 3.934 (3.98)  Time: 0.670s, 1527.98/s  (0.677s, 1511.50/s)  LR: 9.401e-04  Data: 0.013 (0.017)
Train: 47 [ 500/1251 ( 40%)]  Loss: 4.378 (4.02)  Time: 0.676s, 1513.97/s  (0.677s, 1511.58/s)  LR: 9.400e-04  Data: 0.015 (0.017)
Train: 47 [ 550/1251 ( 44%)]  Loss: 3.848 (4.00)  Time: 0.674s, 1518.17/s  (0.677s, 1511.74/s)  LR: 9.399e-04  Data: 0.015 (0.016)
Train: 47 [ 600/1251 ( 48%)]  Loss: 3.675 (3.98)  Time: 0.675s, 1516.86/s  (0.677s, 1511.71/s)  LR: 9.398e-04  Data: 0.014 (0.016)
Train: 47 [ 650/1251 ( 52%)]  Loss: 3.846 (3.97)  Time: 0.676s, 1515.19/s  (0.677s, 1511.93/s)  LR: 9.397e-04  Data: 0.016 (0.016)
Train: 47 [ 700/1251 ( 56%)]  Loss: 3.996 (3.97)  Time: 0.671s, 1525.96/s  (0.677s, 1512.59/s)  LR: 9.396e-04  Data: 0.013 (0.016)
Train: 47 [ 750/1251 ( 60%)]  Loss: 3.886 (3.97)  Time: 0.673s, 1520.93/s  (0.677s, 1513.07/s)  LR: 9.395e-04  Data: 0.019 (0.016)
Train: 47 [ 800/1251 ( 64%)]  Loss: 3.966 (3.97)  Time: 0.673s, 1522.00/s  (0.677s, 1513.62/s)  LR: 9.394e-04  Data: 0.013 (0.016)
Train: 47 [ 850/1251 ( 68%)]  Loss: 3.809 (3.96)  Time: 0.674s, 1519.30/s  (0.676s, 1513.93/s)  LR: 9.393e-04  Data: 0.012 (0.015)
Train: 47 [ 900/1251 ( 72%)]  Loss: 4.027 (3.96)  Time: 0.677s, 1512.83/s  (0.676s, 1514.20/s)  LR: 9.392e-04  Data: 0.013 (0.015)
Train: 47 [ 950/1251 ( 76%)]  Loss: 4.290 (3.98)  Time: 0.681s, 1502.86/s  (0.676s, 1514.23/s)  LR: 9.391e-04  Data: 0.013 (0.015)
Train: 47 [1000/1251 ( 80%)]  Loss: 4.386 (4.00)  Time: 0.680s, 1505.98/s  (0.676s, 1514.46/s)  LR: 9.390e-04  Data: 0.014 (0.015)
Train: 47 [1050/1251 ( 84%)]  Loss: 3.820 (3.99)  Time: 0.673s, 1522.50/s  (0.676s, 1514.57/s)  LR: 9.389e-04  Data: 0.012 (0.015)
Train: 47 [1100/1251 ( 88%)]  Loss: 3.778 (3.98)  Time: 0.683s, 1499.65/s  (0.676s, 1514.69/s)  LR: 9.388e-04  Data: 0.014 (0.015)
Train: 47 [1150/1251 ( 92%)]  Loss: 4.040 (3.98)  Time: 0.673s, 1520.66/s  (0.676s, 1514.83/s)  LR: 9.387e-04  Data: 0.013 (0.015)
Train: 47 [1200/1251 ( 96%)]  Loss: 3.732 (3.97)  Time: 0.679s, 1507.58/s  (0.676s, 1514.89/s)  LR: 9.386e-04  Data: 0.012 (0.015)
Train: 47 [1250/1251 (100%)]  Loss: 4.053 (3.98)  Time: 0.660s, 1551.72/s  (0.676s, 1515.04/s)  LR: 9.385e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.729 (2.729)  Loss:  0.6968 (0.6968)  Acc@1: 84.8633 (84.8633)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.169 (0.332)  Loss:  0.7056 (1.2479)  Acc@1: 83.3726 (70.9840)  Acc@5: 95.7547 (90.6920)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-47.pth.tar', 70.98399988769532)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-46.pth.tar', 70.97399994140625)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-45.pth.tar', 70.75000004638672)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-44.pth.tar', 70.33799991455078)

Train: 48 [   0/1251 (  0%)]  Loss: 3.734 (3.73)  Time: 4.019s,  254.80/s  (4.019s,  254.80/s)  LR: 9.385e-04  Data: 1.706 (1.706)
Train: 48 [  50/1251 (  4%)]  Loss: 3.524 (3.63)  Time: 0.674s, 1520.21/s  (0.706s, 1449.61/s)  LR: 9.384e-04  Data: 0.013 (0.047)
Train: 48 [ 100/1251 (  8%)]  Loss: 3.957 (3.74)  Time: 0.681s, 1504.65/s  (0.690s, 1483.27/s)  LR: 9.383e-04  Data: 0.013 (0.030)
Train: 48 [ 150/1251 ( 12%)]  Loss: 3.902 (3.78)  Time: 0.678s, 1510.78/s  (0.685s, 1493.82/s)  LR: 9.382e-04  Data: 0.014 (0.025)
Train: 48 [ 200/1251 ( 16%)]  Loss: 3.748 (3.77)  Time: 0.683s, 1499.25/s  (0.684s, 1497.70/s)  LR: 9.381e-04  Data: 0.015 (0.022)
Train: 48 [ 250/1251 ( 20%)]  Loss: 4.311 (3.86)  Time: 0.682s, 1501.24/s  (0.683s, 1499.62/s)  LR: 9.380e-04  Data: 0.013 (0.020)
Train: 48 [ 300/1251 ( 24%)]  Loss: 3.715 (3.84)  Time: 0.684s, 1496.70/s  (0.682s, 1500.85/s)  LR: 9.379e-04  Data: 0.013 (0.019)
Train: 48 [ 350/1251 ( 28%)]  Loss: 3.827 (3.84)  Time: 0.681s, 1504.05/s  (0.682s, 1502.25/s)  LR: 9.378e-04  Data: 0.015 (0.018)
Train: 48 [ 400/1251 ( 32%)]  Loss: 4.049 (3.86)  Time: 0.685s, 1495.95/s  (0.681s, 1503.46/s)  LR: 9.377e-04  Data: 0.013 (0.018)
Train: 48 [ 450/1251 ( 36%)]  Loss: 4.097 (3.89)  Time: 0.686s, 1493.20/s  (0.681s, 1504.76/s)  LR: 9.376e-04  Data: 0.012 (0.017)
Train: 48 [ 500/1251 ( 40%)]  Loss: 3.830 (3.88)  Time: 0.679s, 1507.96/s  (0.680s, 1505.52/s)  LR: 9.375e-04  Data: 0.013 (0.017)
Train: 48 [ 550/1251 ( 44%)]  Loss: 3.792 (3.87)  Time: 0.679s, 1508.73/s  (0.680s, 1506.11/s)  LR: 9.374e-04  Data: 0.014 (0.017)
Train: 48 [ 600/1251 ( 48%)]  Loss: 3.601 (3.85)  Time: 0.677s, 1512.26/s  (0.680s, 1506.61/s)  LR: 9.373e-04  Data: 0.014 (0.016)
Train: 48 [ 650/1251 ( 52%)]  Loss: 3.988 (3.86)  Time: 0.672s, 1523.73/s  (0.679s, 1507.23/s)  LR: 9.372e-04  Data: 0.016 (0.016)
Train: 48 [ 700/1251 ( 56%)]  Loss: 3.821 (3.86)  Time: 0.679s, 1508.70/s  (0.679s, 1507.74/s)  LR: 9.370e-04  Data: 0.015 (0.016)
Train: 48 [ 750/1251 ( 60%)]  Loss: 3.813 (3.86)  Time: 0.676s, 1515.45/s  (0.679s, 1508.19/s)  LR: 9.369e-04  Data: 0.012 (0.016)
Train: 48 [ 800/1251 ( 64%)]  Loss: 3.416 (3.83)  Time: 0.683s, 1499.28/s  (0.679s, 1508.74/s)  LR: 9.368e-04  Data: 0.013 (0.016)
Train: 48 [ 850/1251 ( 68%)]  Loss: 4.048 (3.84)  Time: 0.673s, 1521.71/s  (0.678s, 1509.30/s)  LR: 9.367e-04  Data: 0.013 (0.016)
Train: 48 [ 900/1251 ( 72%)]  Loss: 4.085 (3.86)  Time: 0.683s, 1498.76/s  (0.678s, 1509.68/s)  LR: 9.366e-04  Data: 0.014 (0.016)
Train: 48 [ 950/1251 ( 76%)]  Loss: 3.582 (3.84)  Time: 0.676s, 1514.30/s  (0.678s, 1510.01/s)  LR: 9.365e-04  Data: 0.015 (0.015)
Train: 48 [1000/1251 ( 80%)]  Loss: 3.462 (3.82)  Time: 0.670s, 1527.80/s  (0.678s, 1510.61/s)  LR: 9.364e-04  Data: 0.013 (0.015)
Train: 48 [1050/1251 ( 84%)]  Loss: 3.964 (3.83)  Time: 0.691s, 1482.11/s  (0.678s, 1510.80/s)  LR: 9.363e-04  Data: 0.013 (0.015)
Train: 48 [1100/1251 ( 88%)]  Loss: 3.852 (3.83)  Time: 0.674s, 1520.36/s  (0.678s, 1511.21/s)  LR: 9.362e-04  Data: 0.014 (0.015)
Train: 48 [1150/1251 ( 92%)]  Loss: 4.068 (3.84)  Time: 0.669s, 1530.36/s  (0.677s, 1511.56/s)  LR: 9.361e-04  Data: 0.017 (0.015)
Train: 48 [1200/1251 ( 96%)]  Loss: 3.632 (3.83)  Time: 0.672s, 1522.88/s  (0.677s, 1511.67/s)  LR: 9.360e-04  Data: 0.013 (0.015)
Train: 48 [1250/1251 (100%)]  Loss: 4.005 (3.84)  Time: 0.661s, 1549.53/s  (0.677s, 1511.80/s)  LR: 9.359e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.870 (2.870)  Loss:  0.6133 (0.6133)  Acc@1: 87.4023 (87.4023)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.167 (0.323)  Loss:  0.6685 (1.2413)  Acc@1: 84.5519 (70.9060)  Acc@5: 96.5802 (90.5580)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-47.pth.tar', 70.98399988769532)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-46.pth.tar', 70.97399994140625)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-48.pth.tar', 70.90600001220703)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-45.pth.tar', 70.75000004638672)

Train: 49 [   0/1251 (  0%)]  Loss: 3.993 (3.99)  Time: 3.393s,  301.78/s  (3.393s,  301.78/s)  LR: 9.359e-04  Data: 1.704 (1.704)
Train: 49 [  50/1251 (  4%)]  Loss: 3.921 (3.96)  Time: 0.669s, 1531.77/s  (0.703s, 1457.22/s)  LR: 9.358e-04  Data: 0.016 (0.047)
Train: 49 [ 100/1251 (  8%)]  Loss: 4.066 (3.99)  Time: 0.667s, 1534.41/s  (0.686s, 1493.67/s)  LR: 9.357e-04  Data: 0.013 (0.031)
Train: 49 [ 150/1251 ( 12%)]  Loss: 3.826 (3.95)  Time: 0.661s, 1548.46/s  (0.681s, 1502.84/s)  LR: 9.356e-04  Data: 0.015 (0.025)
Train: 49 [ 200/1251 ( 16%)]  Loss: 3.520 (3.87)  Time: 0.675s, 1517.34/s  (0.679s, 1507.41/s)  LR: 9.355e-04  Data: 0.013 (0.022)
Train: 49 [ 250/1251 ( 20%)]  Loss: 3.968 (3.88)  Time: 0.678s, 1509.57/s  (0.678s, 1509.47/s)  LR: 9.354e-04  Data: 0.012 (0.020)
Train: 49 [ 300/1251 ( 24%)]  Loss: 4.139 (3.92)  Time: 0.681s, 1504.63/s  (0.678s, 1510.75/s)  LR: 9.353e-04  Data: 0.013 (0.019)
Train: 49 [ 350/1251 ( 28%)]  Loss: 3.715 (3.89)  Time: 0.680s, 1506.89/s  (0.678s, 1511.32/s)  LR: 9.352e-04  Data: 0.019 (0.019)
Train: 49 [ 400/1251 ( 32%)]  Loss: 3.923 (3.90)  Time: 0.673s, 1521.99/s  (0.677s, 1511.93/s)  LR: 9.351e-04  Data: 0.013 (0.018)
Train: 49 [ 450/1251 ( 36%)]  Loss: 3.906 (3.90)  Time: 0.670s, 1528.34/s  (0.677s, 1512.09/s)  LR: 9.350e-04  Data: 0.016 (0.017)
Train: 49 [ 500/1251 ( 40%)]  Loss: 3.740 (3.88)  Time: 0.695s, 1473.41/s  (0.677s, 1511.77/s)  LR: 9.349e-04  Data: 0.013 (0.017)
Train: 49 [ 550/1251 ( 44%)]  Loss: 4.103 (3.90)  Time: 0.682s, 1502.40/s  (0.677s, 1511.84/s)  LR: 9.348e-04  Data: 0.017 (0.017)
Train: 49 [ 600/1251 ( 48%)]  Loss: 4.088 (3.92)  Time: 0.681s, 1503.11/s  (0.677s, 1511.50/s)  LR: 9.347e-04  Data: 0.013 (0.016)
Train: 49 [ 650/1251 ( 52%)]  Loss: 3.999 (3.92)  Time: 0.663s, 1544.74/s  (0.677s, 1511.67/s)  LR: 9.346e-04  Data: 0.013 (0.016)
Train: 49 [ 700/1251 ( 56%)]  Loss: 3.701 (3.91)  Time: 0.676s, 1513.89/s  (0.677s, 1511.86/s)  LR: 9.345e-04  Data: 0.013 (0.016)
Train: 49 [ 750/1251 ( 60%)]  Loss: 3.941 (3.91)  Time: 0.681s, 1502.66/s  (0.677s, 1512.19/s)  LR: 9.344e-04  Data: 0.013 (0.016)
Train: 49 [ 800/1251 ( 64%)]  Loss: 4.124 (3.92)  Time: 0.671s, 1525.61/s  (0.677s, 1512.79/s)  LR: 9.343e-04  Data: 0.013 (0.016)
Train: 49 [ 850/1251 ( 68%)]  Loss: 3.919 (3.92)  Time: 0.673s, 1522.55/s  (0.677s, 1513.03/s)  LR: 9.342e-04  Data: 0.015 (0.016)
Train: 49 [ 900/1251 ( 72%)]  Loss: 4.187 (3.94)  Time: 0.673s, 1520.65/s  (0.676s, 1513.73/s)  LR: 9.341e-04  Data: 0.013 (0.015)
Train: 49 [ 950/1251 ( 76%)]  Loss: 3.814 (3.93)  Time: 0.677s, 1512.69/s  (0.676s, 1513.83/s)  LR: 9.340e-04  Data: 0.014 (0.015)
Train: 49 [1000/1251 ( 80%)]  Loss: 4.064 (3.94)  Time: 0.673s, 1522.06/s  (0.676s, 1514.07/s)  LR: 9.339e-04  Data: 0.013 (0.015)
Train: 49 [1050/1251 ( 84%)]  Loss: 3.840 (3.93)  Time: 0.668s, 1533.39/s  (0.676s, 1514.33/s)  LR: 9.338e-04  Data: 0.016 (0.015)
Train: 49 [1100/1251 ( 88%)]  Loss: 3.935 (3.93)  Time: 0.675s, 1516.74/s  (0.676s, 1514.46/s)  LR: 9.337e-04  Data: 0.013 (0.015)
Train: 49 [1150/1251 ( 92%)]  Loss: 3.834 (3.93)  Time: 0.669s, 1529.82/s  (0.676s, 1514.65/s)  LR: 9.336e-04  Data: 0.013 (0.015)
Train: 49 [1200/1251 ( 96%)]  Loss: 3.491 (3.91)  Time: 0.672s, 1523.45/s  (0.676s, 1514.69/s)  LR: 9.335e-04  Data: 0.016 (0.015)
Train: 49 [1250/1251 (100%)]  Loss: 3.990 (3.91)  Time: 0.663s, 1543.60/s  (0.676s, 1514.75/s)  LR: 9.333e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.687 (2.687)  Loss:  0.6226 (0.6226)  Acc@1: 86.6211 (86.6211)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.165 (0.330)  Loss:  0.7319 (1.2504)  Acc@1: 82.6651 (71.2020)  Acc@5: 96.1085 (90.7220)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-49.pth.tar', 71.20199996826172)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-47.pth.tar', 70.98399988769532)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-46.pth.tar', 70.97399994140625)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-48.pth.tar', 70.90600001220703)

Train: 50 [   0/1251 (  0%)]  Loss: 3.449 (3.45)  Time: 3.806s,  269.03/s  (3.806s,  269.03/s)  LR: 9.333e-04  Data: 1.645 (1.645)
Train: 50 [  50/1251 (  4%)]  Loss: 3.556 (3.50)  Time: 0.671s, 1525.34/s  (0.703s, 1455.90/s)  LR: 9.332e-04  Data: 0.014 (0.046)
Train: 50 [ 100/1251 (  8%)]  Loss: 3.742 (3.58)  Time: 0.676s, 1515.45/s  (0.688s, 1488.98/s)  LR: 9.331e-04  Data: 0.013 (0.030)
Train: 50 [ 150/1251 ( 12%)]  Loss: 3.563 (3.58)  Time: 0.676s, 1514.68/s  (0.683s, 1499.32/s)  LR: 9.330e-04  Data: 0.014 (0.024)
Train: 50 [ 200/1251 ( 16%)]  Loss: 3.497 (3.56)  Time: 0.669s, 1529.69/s  (0.681s, 1503.83/s)  LR: 9.329e-04  Data: 0.013 (0.022)
Train: 50 [ 250/1251 ( 20%)]  Loss: 3.963 (3.63)  Time: 0.676s, 1513.92/s  (0.680s, 1505.39/s)  LR: 9.328e-04  Data: 0.016 (0.020)
Train: 50 [ 300/1251 ( 24%)]  Loss: 3.899 (3.67)  Time: 0.681s, 1502.57/s  (0.680s, 1506.15/s)  LR: 9.327e-04  Data: 0.013 (0.019)
Train: 50 [ 350/1251 ( 28%)]  Loss: 4.040 (3.71)  Time: 0.686s, 1492.96/s  (0.680s, 1506.30/s)  LR: 9.326e-04  Data: 0.013 (0.018)
Train: 50 [ 400/1251 ( 32%)]  Loss: 3.767 (3.72)  Time: 0.681s, 1503.75/s  (0.680s, 1506.36/s)  LR: 9.325e-04  Data: 0.014 (0.018)
Train: 50 [ 450/1251 ( 36%)]  Loss: 4.035 (3.75)  Time: 0.677s, 1511.73/s  (0.680s, 1506.11/s)  LR: 9.324e-04  Data: 0.013 (0.017)
Train: 50 [ 500/1251 ( 40%)]  Loss: 4.070 (3.78)  Time: 0.679s, 1507.36/s  (0.680s, 1505.98/s)  LR: 9.323e-04  Data: 0.013 (0.017)
Train: 50 [ 550/1251 ( 44%)]  Loss: 3.681 (3.77)  Time: 0.684s, 1496.30/s  (0.680s, 1505.39/s)  LR: 9.322e-04  Data: 0.013 (0.017)
Train: 50 [ 600/1251 ( 48%)]  Loss: 4.146 (3.80)  Time: 0.670s, 1527.28/s  (0.680s, 1505.20/s)  LR: 9.321e-04  Data: 0.014 (0.016)
Train: 50 [ 650/1251 ( 52%)]  Loss: 3.931 (3.81)  Time: 0.675s, 1516.88/s  (0.680s, 1505.07/s)  LR: 9.320e-04  Data: 0.016 (0.016)
Train: 50 [ 700/1251 ( 56%)]  Loss: 4.041 (3.83)  Time: 0.679s, 1507.58/s  (0.680s, 1504.85/s)  LR: 9.319e-04  Data: 0.015 (0.016)
Train: 50 [ 750/1251 ( 60%)]  Loss: 4.139 (3.84)  Time: 0.681s, 1503.60/s  (0.681s, 1504.78/s)  LR: 9.318e-04  Data: 0.012 (0.016)
Train: 50 [ 800/1251 ( 64%)]  Loss: 4.129 (3.86)  Time: 0.677s, 1513.22/s  (0.680s, 1504.94/s)  LR: 9.317e-04  Data: 0.012 (0.016)
Train: 50 [ 850/1251 ( 68%)]  Loss: 3.803 (3.86)  Time: 0.678s, 1511.32/s  (0.680s, 1505.06/s)  LR: 9.316e-04  Data: 0.013 (0.015)
Train: 50 [ 900/1251 ( 72%)]  Loss: 3.955 (3.86)  Time: 0.676s, 1514.02/s  (0.680s, 1505.22/s)  LR: 9.315e-04  Data: 0.012 (0.015)
Train: 50 [ 950/1251 ( 76%)]  Loss: 3.650 (3.85)  Time: 0.680s, 1505.91/s  (0.680s, 1505.33/s)  LR: 9.314e-04  Data: 0.013 (0.015)
Train: 50 [1000/1251 ( 80%)]  Loss: 3.939 (3.86)  Time: 0.678s, 1509.81/s  (0.680s, 1505.64/s)  LR: 9.312e-04  Data: 0.013 (0.015)
Train: 50 [1050/1251 ( 84%)]  Loss: 3.892 (3.86)  Time: 0.675s, 1517.35/s  (0.680s, 1505.71/s)  LR: 9.311e-04  Data: 0.014 (0.015)
Train: 50 [1100/1251 ( 88%)]  Loss: 3.941 (3.86)  Time: 0.679s, 1509.00/s  (0.680s, 1505.61/s)  LR: 9.310e-04  Data: 0.012 (0.015)
Train: 50 [1150/1251 ( 92%)]  Loss: 3.942 (3.87)  Time: 0.675s, 1517.36/s  (0.680s, 1505.63/s)  LR: 9.309e-04  Data: 0.015 (0.015)
Train: 50 [1200/1251 ( 96%)]  Loss: 3.919 (3.87)  Time: 0.681s, 1503.02/s  (0.680s, 1505.68/s)  LR: 9.308e-04  Data: 0.012 (0.015)
Train: 50 [1250/1251 (100%)]  Loss: 4.100 (3.88)  Time: 0.679s, 1507.29/s  (0.680s, 1505.97/s)  LR: 9.307e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.773 (2.773)  Loss:  0.6191 (0.6191)  Acc@1: 87.4023 (87.4023)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.171 (0.330)  Loss:  0.7480 (1.2446)  Acc@1: 82.7830 (71.4280)  Acc@5: 95.4009 (90.7860)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-50.pth.tar', 71.42800001953125)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-49.pth.tar', 71.20199996826172)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-47.pth.tar', 70.98399988769532)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-46.pth.tar', 70.97399994140625)

Train: 51 [   0/1251 (  0%)]  Loss: 4.032 (4.03)  Time: 3.769s,  271.68/s  (3.769s,  271.68/s)  LR: 9.307e-04  Data: 1.902 (1.902)
Train: 51 [  50/1251 (  4%)]  Loss: 3.560 (3.80)  Time: 0.663s, 1543.54/s  (0.704s, 1454.63/s)  LR: 9.306e-04  Data: 0.014 (0.051)
Train: 51 [ 100/1251 (  8%)]  Loss: 3.888 (3.83)  Time: 0.675s, 1517.52/s  (0.687s, 1489.55/s)  LR: 9.305e-04  Data: 0.014 (0.033)
Train: 51 [ 150/1251 ( 12%)]  Loss: 3.401 (3.72)  Time: 0.662s, 1545.90/s  (0.682s, 1500.52/s)  LR: 9.304e-04  Data: 0.013 (0.027)
Train: 51 [ 200/1251 ( 16%)]  Loss: 3.749 (3.73)  Time: 0.674s, 1518.43/s  (0.681s, 1504.32/s)  LR: 9.303e-04  Data: 0.017 (0.023)
Train: 51 [ 250/1251 ( 20%)]  Loss: 3.889 (3.75)  Time: 0.684s, 1497.67/s  (0.680s, 1506.94/s)  LR: 9.302e-04  Data: 0.016 (0.022)
Train: 51 [ 300/1251 ( 24%)]  Loss: 3.746 (3.75)  Time: 0.661s, 1548.11/s  (0.679s, 1507.90/s)  LR: 9.301e-04  Data: 0.013 (0.020)
Train: 51 [ 350/1251 ( 28%)]  Loss: 3.918 (3.77)  Time: 0.677s, 1512.74/s  (0.679s, 1508.60/s)  LR: 9.300e-04  Data: 0.013 (0.019)
Train: 51 [ 400/1251 ( 32%)]  Loss: 3.933 (3.79)  Time: 0.683s, 1499.47/s  (0.679s, 1509.20/s)  LR: 9.299e-04  Data: 0.013 (0.019)
Train: 51 [ 450/1251 ( 36%)]  Loss: 3.773 (3.79)  Time: 0.676s, 1515.43/s  (0.678s, 1509.57/s)  LR: 9.298e-04  Data: 0.013 (0.018)
Train: 51 [ 500/1251 ( 40%)]  Loss: 4.032 (3.81)  Time: 0.683s, 1499.18/s  (0.678s, 1510.05/s)  LR: 9.297e-04  Data: 0.016 (0.018)
Train: 51 [ 550/1251 ( 44%)]  Loss: 3.983 (3.83)  Time: 0.682s, 1501.90/s  (0.678s, 1510.72/s)  LR: 9.295e-04  Data: 0.013 (0.017)
Train: 51 [ 600/1251 ( 48%)]  Loss: 3.553 (3.80)  Time: 0.680s, 1506.57/s  (0.678s, 1510.99/s)  LR: 9.294e-04  Data: 0.013 (0.017)
Train: 51 [ 650/1251 ( 52%)]  Loss: 3.885 (3.81)  Time: 0.670s, 1528.83/s  (0.677s, 1511.49/s)  LR: 9.293e-04  Data: 0.012 (0.017)
Train: 51 [ 700/1251 ( 56%)]  Loss: 3.815 (3.81)  Time: 0.674s, 1518.94/s  (0.677s, 1511.63/s)  LR: 9.292e-04  Data: 0.022 (0.016)
Train: 51 [ 750/1251 ( 60%)]  Loss: 4.228 (3.84)  Time: 0.670s, 1528.37/s  (0.677s, 1511.81/s)  LR: 9.291e-04  Data: 0.013 (0.016)
Train: 51 [ 800/1251 ( 64%)]  Loss: 3.417 (3.81)  Time: 0.678s, 1510.60/s  (0.677s, 1511.49/s)  LR: 9.290e-04  Data: 0.013 (0.016)
Train: 51 [ 850/1251 ( 68%)]  Loss: 3.699 (3.81)  Time: 0.683s, 1499.24/s  (0.678s, 1511.36/s)  LR: 9.289e-04  Data: 0.013 (0.016)
Train: 51 [ 900/1251 ( 72%)]  Loss: 3.757 (3.80)  Time: 0.677s, 1512.15/s  (0.677s, 1511.47/s)  LR: 9.288e-04  Data: 0.013 (0.016)
Train: 51 [ 950/1251 ( 76%)]  Loss: 4.297 (3.83)  Time: 0.667s, 1535.57/s  (0.677s, 1511.80/s)  LR: 9.287e-04  Data: 0.013 (0.016)
Train: 51 [1000/1251 ( 80%)]  Loss: 3.485 (3.81)  Time: 0.679s, 1508.74/s  (0.677s, 1512.02/s)  LR: 9.286e-04  Data: 0.013 (0.016)
Train: 51 [1050/1251 ( 84%)]  Loss: 4.337 (3.84)  Time: 0.669s, 1530.56/s  (0.677s, 1512.12/s)  LR: 9.285e-04  Data: 0.013 (0.015)
Train: 51 [1100/1251 ( 88%)]  Loss: 3.586 (3.82)  Time: 0.677s, 1511.52/s  (0.677s, 1512.31/s)  LR: 9.284e-04  Data: 0.013 (0.015)
Train: 51 [1150/1251 ( 92%)]  Loss: 3.776 (3.82)  Time: 0.683s, 1499.61/s  (0.677s, 1512.28/s)  LR: 9.283e-04  Data: 0.013 (0.015)
Train: 51 [1200/1251 ( 96%)]  Loss: 4.312 (3.84)  Time: 0.675s, 1516.09/s  (0.677s, 1512.49/s)  LR: 9.282e-04  Data: 0.013 (0.015)
Train: 51 [1250/1251 (100%)]  Loss: 3.718 (3.84)  Time: 0.663s, 1544.04/s  (0.677s, 1512.83/s)  LR: 9.280e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.710 (2.710)  Loss:  0.6260 (0.6260)  Acc@1: 86.9141 (86.9141)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.164 (0.325)  Loss:  0.7314 (1.2401)  Acc@1: 82.3113 (71.2720)  Acc@5: 95.8726 (90.6280)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-50.pth.tar', 71.42800001953125)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-51.pth.tar', 71.27200007324218)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-49.pth.tar', 71.20199996826172)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-47.pth.tar', 70.98399988769532)

Train: 52 [   0/1251 (  0%)]  Loss: 3.925 (3.92)  Time: 3.328s,  307.65/s  (3.328s,  307.65/s)  LR: 9.280e-04  Data: 1.796 (1.796)
Train: 52 [  50/1251 (  4%)]  Loss: 3.976 (3.95)  Time: 0.666s, 1537.47/s  (0.692s, 1480.64/s)  LR: 9.279e-04  Data: 0.013 (0.049)
Train: 52 [ 100/1251 (  8%)]  Loss: 4.038 (3.98)  Time: 0.664s, 1541.04/s  (0.680s, 1506.21/s)  LR: 9.278e-04  Data: 0.014 (0.031)
Train: 52 [ 150/1251 ( 12%)]  Loss: 4.051 (4.00)  Time: 0.663s, 1543.73/s  (0.677s, 1512.61/s)  LR: 9.277e-04  Data: 0.013 (0.026)
Train: 52 [ 200/1251 ( 16%)]  Loss: 3.700 (3.94)  Time: 0.673s, 1521.33/s  (0.676s, 1514.63/s)  LR: 9.276e-04  Data: 0.016 (0.023)
Train: 52 [ 250/1251 ( 20%)]  Loss: 3.960 (3.94)  Time: 0.683s, 1498.81/s  (0.676s, 1514.68/s)  LR: 9.275e-04  Data: 0.014 (0.021)
Train: 52 [ 300/1251 ( 24%)]  Loss: 3.929 (3.94)  Time: 0.678s, 1510.66/s  (0.676s, 1514.73/s)  LR: 9.274e-04  Data: 0.013 (0.020)
Train: 52 [ 350/1251 ( 28%)]  Loss: 3.499 (3.88)  Time: 0.675s, 1517.49/s  (0.676s, 1514.37/s)  LR: 9.273e-04  Data: 0.014 (0.019)
Train: 52 [ 400/1251 ( 32%)]  Loss: 3.891 (3.89)  Time: 0.677s, 1513.54/s  (0.676s, 1513.99/s)  LR: 9.272e-04  Data: 0.014 (0.018)
Train: 52 [ 450/1251 ( 36%)]  Loss: 3.832 (3.88)  Time: 0.678s, 1509.56/s  (0.677s, 1513.66/s)  LR: 9.271e-04  Data: 0.013 (0.018)
Train: 52 [ 500/1251 ( 40%)]  Loss: 4.076 (3.90)  Time: 0.673s, 1521.19/s  (0.677s, 1513.35/s)  LR: 9.270e-04  Data: 0.013 (0.017)
Train: 52 [ 550/1251 ( 44%)]  Loss: 3.333 (3.85)  Time: 0.683s, 1500.31/s  (0.677s, 1512.97/s)  LR: 9.269e-04  Data: 0.016 (0.017)
Train: 52 [ 600/1251 ( 48%)]  Loss: 4.080 (3.87)  Time: 0.680s, 1506.88/s  (0.677s, 1512.76/s)  LR: 9.267e-04  Data: 0.017 (0.017)
Train: 52 [ 650/1251 ( 52%)]  Loss: 3.871 (3.87)  Time: 0.674s, 1520.35/s  (0.677s, 1512.63/s)  LR: 9.266e-04  Data: 0.013 (0.016)
Train: 52 [ 700/1251 ( 56%)]  Loss: 4.052 (3.88)  Time: 0.664s, 1542.98/s  (0.677s, 1512.80/s)  LR: 9.265e-04  Data: 0.012 (0.016)
Train: 52 [ 750/1251 ( 60%)]  Loss: 3.735 (3.87)  Time: 0.675s, 1516.96/s  (0.677s, 1513.16/s)  LR: 9.264e-04  Data: 0.014 (0.016)
Train: 52 [ 800/1251 ( 64%)]  Loss: 4.079 (3.88)  Time: 0.672s, 1524.08/s  (0.677s, 1513.53/s)  LR: 9.263e-04  Data: 0.014 (0.016)
Train: 52 [ 850/1251 ( 68%)]  Loss: 4.274 (3.91)  Time: 0.683s, 1499.03/s  (0.677s, 1513.60/s)  LR: 9.262e-04  Data: 0.013 (0.016)
Train: 52 [ 900/1251 ( 72%)]  Loss: 3.606 (3.89)  Time: 0.689s, 1486.34/s  (0.677s, 1513.51/s)  LR: 9.261e-04  Data: 0.012 (0.016)
Train: 52 [ 950/1251 ( 76%)]  Loss: 3.833 (3.89)  Time: 0.671s, 1525.48/s  (0.677s, 1513.42/s)  LR: 9.260e-04  Data: 0.012 (0.016)
Train: 52 [1000/1251 ( 80%)]  Loss: 3.887 (3.89)  Time: 0.676s, 1515.32/s  (0.677s, 1513.19/s)  LR: 9.259e-04  Data: 0.013 (0.015)
Train: 52 [1050/1251 ( 84%)]  Loss: 4.152 (3.90)  Time: 0.666s, 1536.44/s  (0.677s, 1513.16/s)  LR: 9.258e-04  Data: 0.012 (0.015)
Train: 52 [1100/1251 ( 88%)]  Loss: 3.859 (3.90)  Time: 0.682s, 1501.89/s  (0.677s, 1513.07/s)  LR: 9.257e-04  Data: 0.014 (0.015)
Train: 52 [1150/1251 ( 92%)]  Loss: 3.732 (3.89)  Time: 0.676s, 1514.20/s  (0.677s, 1512.98/s)  LR: 9.255e-04  Data: 0.013 (0.015)
Train: 52 [1200/1251 ( 96%)]  Loss: 3.724 (3.88)  Time: 0.679s, 1509.19/s  (0.677s, 1512.82/s)  LR: 9.254e-04  Data: 0.013 (0.015)
Train: 52 [1250/1251 (100%)]  Loss: 3.749 (3.88)  Time: 0.663s, 1544.82/s  (0.677s, 1512.92/s)  LR: 9.253e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.777 (2.777)  Loss:  0.5928 (0.5928)  Acc@1: 86.9141 (86.9141)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.172 (0.325)  Loss:  0.6914 (1.2262)  Acc@1: 83.6085 (71.6460)  Acc@5: 96.2264 (90.8760)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-52.pth.tar', 71.64599999023437)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-50.pth.tar', 71.42800001953125)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-51.pth.tar', 71.27200007324218)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-49.pth.tar', 71.20199996826172)

Train: 53 [   0/1251 (  0%)]  Loss: 3.706 (3.71)  Time: 3.465s,  295.56/s  (3.465s,  295.56/s)  LR: 9.253e-04  Data: 2.057 (2.057)
Train: 53 [  50/1251 (  4%)]  Loss: 3.464 (3.58)  Time: 0.664s, 1543.22/s  (0.698s, 1466.22/s)  LR: 9.252e-04  Data: 0.012 (0.054)
Train: 53 [ 100/1251 (  8%)]  Loss: 3.562 (3.58)  Time: 0.664s, 1542.05/s  (0.684s, 1497.47/s)  LR: 9.251e-04  Data: 0.016 (0.034)
Train: 53 [ 150/1251 ( 12%)]  Loss: 3.876 (3.65)  Time: 0.669s, 1530.22/s  (0.680s, 1506.53/s)  LR: 9.250e-04  Data: 0.015 (0.028)
Train: 53 [ 200/1251 ( 16%)]  Loss: 3.927 (3.71)  Time: 0.679s, 1507.83/s  (0.679s, 1508.99/s)  LR: 9.249e-04  Data: 0.013 (0.024)
Train: 53 [ 250/1251 ( 20%)]  Loss: 3.964 (3.75)  Time: 0.684s, 1497.25/s  (0.678s, 1510.04/s)  LR: 9.248e-04  Data: 0.013 (0.022)
Train: 53 [ 300/1251 ( 24%)]  Loss: 3.810 (3.76)  Time: 0.676s, 1514.39/s  (0.678s, 1510.61/s)  LR: 9.247e-04  Data: 0.013 (0.021)
Train: 53 [ 350/1251 ( 28%)]  Loss: 4.014 (3.79)  Time: 0.672s, 1523.18/s  (0.678s, 1511.32/s)  LR: 9.245e-04  Data: 0.012 (0.020)
Train: 53 [ 400/1251 ( 32%)]  Loss: 3.650 (3.77)  Time: 0.677s, 1511.48/s  (0.677s, 1511.83/s)  LR: 9.244e-04  Data: 0.012 (0.019)
Train: 53 [ 450/1251 ( 36%)]  Loss: 3.977 (3.80)  Time: 0.677s, 1513.25/s  (0.677s, 1512.16/s)  LR: 9.243e-04  Data: 0.015 (0.019)
Train: 53 [ 500/1251 ( 40%)]  Loss: 3.893 (3.80)  Time: 0.677s, 1513.51/s  (0.677s, 1512.65/s)  LR: 9.242e-04  Data: 0.015 (0.018)
Train: 53 [ 550/1251 ( 44%)]  Loss: 4.032 (3.82)  Time: 0.684s, 1496.82/s  (0.677s, 1512.93/s)  LR: 9.241e-04  Data: 0.014 (0.018)
Train: 53 [ 600/1251 ( 48%)]  Loss: 3.986 (3.84)  Time: 0.672s, 1523.40/s  (0.677s, 1512.65/s)  LR: 9.240e-04  Data: 0.015 (0.017)
Train: 53 [ 650/1251 ( 52%)]  Loss: 3.554 (3.82)  Time: 0.683s, 1498.55/s  (0.677s, 1512.69/s)  LR: 9.239e-04  Data: 0.014 (0.017)
Train: 53 [ 700/1251 ( 56%)]  Loss: 3.681 (3.81)  Time: 0.676s, 1515.47/s  (0.677s, 1512.54/s)  LR: 9.238e-04  Data: 0.013 (0.017)
Train: 53 [ 750/1251 ( 60%)]  Loss: 3.698 (3.80)  Time: 0.675s, 1517.84/s  (0.677s, 1512.60/s)  LR: 9.237e-04  Data: 0.012 (0.017)
Train: 53 [ 800/1251 ( 64%)]  Loss: 3.954 (3.81)  Time: 0.678s, 1510.67/s  (0.677s, 1512.41/s)  LR: 9.236e-04  Data: 0.013 (0.016)
Train: 53 [ 850/1251 ( 68%)]  Loss: 4.143 (3.83)  Time: 0.694s, 1474.81/s  (0.677s, 1512.44/s)  LR: 9.234e-04  Data: 0.013 (0.016)
Train: 53 [ 900/1251 ( 72%)]  Loss: 3.634 (3.82)  Time: 0.687s, 1489.91/s  (0.677s, 1511.98/s)  LR: 9.233e-04  Data: 0.014 (0.016)
Train: 53 [ 950/1251 ( 76%)]  Loss: 3.586 (3.81)  Time: 0.683s, 1499.57/s  (0.677s, 1511.67/s)  LR: 9.232e-04  Data: 0.012 (0.016)
Train: 53 [1000/1251 ( 80%)]  Loss: 3.821 (3.81)  Time: 0.675s, 1516.28/s  (0.678s, 1511.07/s)  LR: 9.231e-04  Data: 0.012 (0.016)
Train: 53 [1050/1251 ( 84%)]  Loss: 3.673 (3.80)  Time: 0.683s, 1498.87/s  (0.678s, 1510.91/s)  LR: 9.230e-04  Data: 0.013 (0.016)
Train: 53 [1100/1251 ( 88%)]  Loss: 3.863 (3.80)  Time: 0.672s, 1524.28/s  (0.678s, 1510.71/s)  LR: 9.229e-04  Data: 0.014 (0.016)
Train: 53 [1150/1251 ( 92%)]  Loss: 3.860 (3.81)  Time: 0.677s, 1511.49/s  (0.678s, 1510.38/s)  LR: 9.228e-04  Data: 0.014 (0.016)
Train: 53 [1200/1251 ( 96%)]  Loss: 3.851 (3.81)  Time: 0.682s, 1502.29/s  (0.678s, 1510.12/s)  LR: 9.227e-04  Data: 0.017 (0.016)
Train: 53 [1250/1251 (100%)]  Loss: 4.034 (3.82)  Time: 0.658s, 1556.14/s  (0.678s, 1510.03/s)  LR: 9.226e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.974 (2.974)  Loss:  0.6094 (0.6094)  Acc@1: 87.8906 (87.8906)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.177 (0.328)  Loss:  0.7085 (1.2254)  Acc@1: 83.7264 (71.4320)  Acc@5: 95.5189 (90.8880)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-52.pth.tar', 71.64599999023437)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-53.pth.tar', 71.43200004150391)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-50.pth.tar', 71.42800001953125)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-51.pth.tar', 71.27200007324218)

Train: 54 [   0/1251 (  0%)]  Loss: 3.673 (3.67)  Time: 3.234s,  316.65/s  (3.234s,  316.65/s)  LR: 9.226e-04  Data: 1.870 (1.870)
Train: 54 [  50/1251 (  4%)]  Loss: 3.501 (3.59)  Time: 0.666s, 1537.50/s  (0.697s, 1469.57/s)  LR: 9.224e-04  Data: 0.018 (0.050)
Train: 54 [ 100/1251 (  8%)]  Loss: 3.935 (3.70)  Time: 0.673s, 1521.17/s  (0.686s, 1493.78/s)  LR: 9.223e-04  Data: 0.017 (0.032)
Train: 54 [ 150/1251 ( 12%)]  Loss: 4.013 (3.78)  Time: 0.682s, 1500.96/s  (0.683s, 1498.87/s)  LR: 9.222e-04  Data: 0.014 (0.026)
Train: 54 [ 200/1251 ( 16%)]  Loss: 3.446 (3.71)  Time: 0.674s, 1519.89/s  (0.682s, 1501.43/s)  LR: 9.221e-04  Data: 0.013 (0.023)
Train: 54 [ 250/1251 ( 20%)]  Loss: 3.886 (3.74)  Time: 0.685s, 1494.18/s  (0.681s, 1502.77/s)  LR: 9.220e-04  Data: 0.015 (0.021)
Train: 54 [ 300/1251 ( 24%)]  Loss: 3.716 (3.74)  Time: 0.679s, 1507.54/s  (0.681s, 1503.26/s)  LR: 9.219e-04  Data: 0.014 (0.020)
Train: 54 [ 350/1251 ( 28%)]  Loss: 3.578 (3.72)  Time: 0.684s, 1497.55/s  (0.681s, 1504.53/s)  LR: 9.218e-04  Data: 0.012 (0.019)
Train: 54 [ 400/1251 ( 32%)]  Loss: 4.068 (3.76)  Time: 0.680s, 1505.28/s  (0.680s, 1504.97/s)  LR: 9.217e-04  Data: 0.017 (0.018)
Train: 54 [ 450/1251 ( 36%)]  Loss: 3.836 (3.77)  Time: 0.675s, 1517.18/s  (0.680s, 1505.38/s)  LR: 9.215e-04  Data: 0.013 (0.018)
Train: 54 [ 500/1251 ( 40%)]  Loss: 3.310 (3.72)  Time: 0.682s, 1500.68/s  (0.680s, 1506.01/s)  LR: 9.214e-04  Data: 0.013 (0.017)
Train: 54 [ 550/1251 ( 44%)]  Loss: 3.926 (3.74)  Time: 0.684s, 1497.34/s  (0.680s, 1506.73/s)  LR: 9.213e-04  Data: 0.012 (0.017)
Train: 54 [ 600/1251 ( 48%)]  Loss: 4.136 (3.77)  Time: 0.679s, 1507.16/s  (0.680s, 1506.63/s)  LR: 9.212e-04  Data: 0.012 (0.017)
Train: 54 [ 650/1251 ( 52%)]  Loss: 3.926 (3.78)  Time: 0.672s, 1523.28/s  (0.680s, 1506.49/s)  LR: 9.211e-04  Data: 0.013 (0.017)
Train: 54 [ 700/1251 ( 56%)]  Loss: 3.786 (3.78)  Time: 0.674s, 1520.20/s  (0.680s, 1506.59/s)  LR: 9.210e-04  Data: 0.012 (0.016)
Train: 54 [ 750/1251 ( 60%)]  Loss: 3.554 (3.77)  Time: 0.682s, 1501.81/s  (0.680s, 1506.56/s)  LR: 9.209e-04  Data: 0.013 (0.016)
Train: 54 [ 800/1251 ( 64%)]  Loss: 3.867 (3.77)  Time: 0.677s, 1513.41/s  (0.680s, 1506.64/s)  LR: 9.208e-04  Data: 0.013 (0.016)
Train: 54 [ 850/1251 ( 68%)]  Loss: 3.703 (3.77)  Time: 0.684s, 1496.48/s  (0.680s, 1506.63/s)  LR: 9.206e-04  Data: 0.013 (0.016)
Train: 54 [ 900/1251 ( 72%)]  Loss: 3.737 (3.77)  Time: 0.682s, 1501.56/s  (0.679s, 1507.08/s)  LR: 9.205e-04  Data: 0.013 (0.016)
Train: 54 [ 950/1251 ( 76%)]  Loss: 3.873 (3.77)  Time: 0.676s, 1514.68/s  (0.680s, 1506.94/s)  LR: 9.204e-04  Data: 0.013 (0.016)
Train: 54 [1000/1251 ( 80%)]  Loss: 3.935 (3.78)  Time: 0.675s, 1517.46/s  (0.679s, 1507.20/s)  LR: 9.203e-04  Data: 0.017 (0.016)
Train: 54 [1050/1251 ( 84%)]  Loss: 3.695 (3.78)  Time: 0.676s, 1514.88/s  (0.679s, 1507.35/s)  LR: 9.202e-04  Data: 0.015 (0.016)
Train: 54 [1100/1251 ( 88%)]  Loss: 3.617 (3.77)  Time: 0.673s, 1520.65/s  (0.679s, 1507.39/s)  LR: 9.201e-04  Data: 0.013 (0.016)
Train: 54 [1150/1251 ( 92%)]  Loss: 3.919 (3.78)  Time: 0.679s, 1507.32/s  (0.679s, 1507.47/s)  LR: 9.200e-04  Data: 0.012 (0.015)
Train: 54 [1200/1251 ( 96%)]  Loss: 3.686 (3.77)  Time: 0.678s, 1509.27/s  (0.679s, 1507.40/s)  LR: 9.199e-04  Data: 0.013 (0.015)
Train: 54 [1250/1251 (100%)]  Loss: 3.631 (3.77)  Time: 0.667s, 1534.93/s  (0.679s, 1507.58/s)  LR: 9.197e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.954 (2.954)  Loss:  0.5664 (0.5664)  Acc@1: 87.9883 (87.9883)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.170 (0.329)  Loss:  0.6743 (1.1953)  Acc@1: 82.5472 (71.7360)  Acc@5: 95.8726 (91.1000)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-54.pth.tar', 71.73600004638672)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-52.pth.tar', 71.64599999023437)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-53.pth.tar', 71.43200004150391)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-50.pth.tar', 71.42800001953125)

Train: 55 [   0/1251 (  0%)]  Loss: 3.700 (3.70)  Time: 3.130s,  327.13/s  (3.130s,  327.13/s)  LR: 9.197e-04  Data: 1.707 (1.707)
Train: 55 [  50/1251 (  4%)]  Loss: 4.092 (3.90)  Time: 0.670s, 1527.59/s  (0.697s, 1469.62/s)  LR: 9.196e-04  Data: 0.013 (0.047)
Train: 55 [ 100/1251 (  8%)]  Loss: 3.615 (3.80)  Time: 0.668s, 1531.82/s  (0.683s, 1498.63/s)  LR: 9.195e-04  Data: 0.014 (0.031)
Train: 55 [ 150/1251 ( 12%)]  Loss: 3.734 (3.79)  Time: 0.673s, 1520.46/s  (0.680s, 1506.39/s)  LR: 9.194e-04  Data: 0.013 (0.025)
Train: 55 [ 200/1251 ( 16%)]  Loss: 3.359 (3.70)  Time: 0.683s, 1500.26/s  (0.679s, 1508.55/s)  LR: 9.193e-04  Data: 0.013 (0.022)
Train: 55 [ 250/1251 ( 20%)]  Loss: 3.917 (3.74)  Time: 0.681s, 1503.39/s  (0.678s, 1509.38/s)  LR: 9.192e-04  Data: 0.013 (0.021)
Train: 55 [ 300/1251 ( 24%)]  Loss: 3.687 (3.73)  Time: 0.675s, 1516.50/s  (0.678s, 1510.45/s)  LR: 9.191e-04  Data: 0.014 (0.020)
Train: 55 [ 350/1251 ( 28%)]  Loss: 3.813 (3.74)  Time: 0.680s, 1506.95/s  (0.677s, 1511.89/s)  LR: 9.189e-04  Data: 0.012 (0.019)
Train: 55 [ 400/1251 ( 32%)]  Loss: 3.894 (3.76)  Time: 0.689s, 1486.95/s  (0.677s, 1511.87/s)  LR: 9.188e-04  Data: 0.012 (0.018)
Train: 55 [ 450/1251 ( 36%)]  Loss: 3.824 (3.76)  Time: 0.689s, 1486.95/s  (0.678s, 1511.37/s)  LR: 9.187e-04  Data: 0.012 (0.018)
Train: 55 [ 500/1251 ( 40%)]  Loss: 4.008 (3.79)  Time: 0.673s, 1522.66/s  (0.678s, 1511.28/s)  LR: 9.186e-04  Data: 0.014 (0.017)
Train: 55 [ 550/1251 ( 44%)]  Loss: 3.838 (3.79)  Time: 0.680s, 1505.48/s  (0.678s, 1510.83/s)  LR: 9.185e-04  Data: 0.016 (0.017)
Train: 55 [ 600/1251 ( 48%)]  Loss: 3.667 (3.78)  Time: 0.678s, 1511.05/s  (0.678s, 1510.51/s)  LR: 9.184e-04  Data: 0.013 (0.017)
Train: 55 [ 650/1251 ( 52%)]  Loss: 3.628 (3.77)  Time: 0.678s, 1510.00/s  (0.678s, 1510.37/s)  LR: 9.183e-04  Data: 0.011 (0.016)
Train: 55 [ 700/1251 ( 56%)]  Loss: 4.067 (3.79)  Time: 0.679s, 1507.61/s  (0.678s, 1510.19/s)  LR: 9.181e-04  Data: 0.014 (0.016)
Train: 55 [ 750/1251 ( 60%)]  Loss: 3.714 (3.78)  Time: 0.678s, 1511.01/s  (0.678s, 1510.01/s)  LR: 9.180e-04  Data: 0.012 (0.016)
Train: 55 [ 800/1251 ( 64%)]  Loss: 3.833 (3.79)  Time: 0.674s, 1518.57/s  (0.678s, 1510.05/s)  LR: 9.179e-04  Data: 0.013 (0.016)
Train: 55 [ 850/1251 ( 68%)]  Loss: 3.722 (3.78)  Time: 0.683s, 1500.17/s  (0.678s, 1510.18/s)  LR: 9.178e-04  Data: 0.017 (0.016)
Train: 55 [ 900/1251 ( 72%)]  Loss: 4.316 (3.81)  Time: 0.693s, 1477.82/s  (0.678s, 1510.18/s)  LR: 9.177e-04  Data: 0.015 (0.016)
Train: 55 [ 950/1251 ( 76%)]  Loss: 3.624 (3.80)  Time: 0.680s, 1505.09/s  (0.678s, 1510.14/s)  LR: 9.176e-04  Data: 0.013 (0.016)
Train: 55 [1000/1251 ( 80%)]  Loss: 4.350 (3.83)  Time: 0.684s, 1497.17/s  (0.678s, 1510.26/s)  LR: 9.175e-04  Data: 0.013 (0.015)
Train: 55 [1050/1251 ( 84%)]  Loss: 3.832 (3.83)  Time: 0.678s, 1510.38/s  (0.678s, 1510.40/s)  LR: 9.173e-04  Data: 0.013 (0.015)
Train: 55 [1100/1251 ( 88%)]  Loss: 3.867 (3.83)  Time: 0.676s, 1514.04/s  (0.678s, 1510.36/s)  LR: 9.172e-04  Data: 0.013 (0.015)
Train: 55 [1150/1251 ( 92%)]  Loss: 3.724 (3.83)  Time: 0.679s, 1508.91/s  (0.678s, 1510.48/s)  LR: 9.171e-04  Data: 0.013 (0.015)
Train: 55 [1200/1251 ( 96%)]  Loss: 3.638 (3.82)  Time: 0.680s, 1505.08/s  (0.678s, 1510.72/s)  LR: 9.170e-04  Data: 0.013 (0.015)
Train: 55 [1250/1251 (100%)]  Loss: 3.786 (3.82)  Time: 0.654s, 1566.16/s  (0.678s, 1511.09/s)  LR: 9.169e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.708 (2.708)  Loss:  0.6489 (0.6489)  Acc@1: 86.9141 (86.9141)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.166 (0.338)  Loss:  0.6426 (1.2368)  Acc@1: 85.9670 (71.6640)  Acc@5: 96.8160 (91.1680)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-54.pth.tar', 71.73600004638672)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-55.pth.tar', 71.66399998046874)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-52.pth.tar', 71.64599999023437)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-53.pth.tar', 71.43200004150391)

Train: 56 [   0/1251 (  0%)]  Loss: 3.808 (3.81)  Time: 3.327s,  307.77/s  (3.327s,  307.77/s)  LR: 9.169e-04  Data: 1.647 (1.647)
Train: 56 [  50/1251 (  4%)]  Loss: 3.756 (3.78)  Time: 0.661s, 1549.95/s  (0.697s, 1468.81/s)  LR: 9.168e-04  Data: 0.013 (0.046)
Train: 56 [ 100/1251 (  8%)]  Loss: 3.785 (3.78)  Time: 0.685s, 1495.05/s  (0.684s, 1496.41/s)  LR: 9.166e-04  Data: 0.014 (0.030)
Train: 56 [ 150/1251 ( 12%)]  Loss: 3.614 (3.74)  Time: 0.683s, 1498.45/s  (0.681s, 1504.24/s)  LR: 9.165e-04  Data: 0.013 (0.025)
Train: 56 [ 200/1251 ( 16%)]  Loss: 3.745 (3.74)  Time: 0.673s, 1520.76/s  (0.680s, 1506.87/s)  LR: 9.164e-04  Data: 0.014 (0.022)
Train: 56 [ 250/1251 ( 20%)]  Loss: 4.000 (3.78)  Time: 0.678s, 1510.71/s  (0.679s, 1508.18/s)  LR: 9.163e-04  Data: 0.012 (0.021)
Train: 56 [ 300/1251 ( 24%)]  Loss: 3.669 (3.77)  Time: 0.672s, 1523.71/s  (0.679s, 1508.73/s)  LR: 9.162e-04  Data: 0.013 (0.019)
Train: 56 [ 350/1251 ( 28%)]  Loss: 4.027 (3.80)  Time: 0.673s, 1522.12/s  (0.679s, 1509.14/s)  LR: 9.161e-04  Data: 0.014 (0.019)
Train: 56 [ 400/1251 ( 32%)]  Loss: 3.420 (3.76)  Time: 0.679s, 1509.08/s  (0.678s, 1509.87/s)  LR: 9.160e-04  Data: 0.013 (0.018)
Train: 56 [ 450/1251 ( 36%)]  Loss: 3.807 (3.76)  Time: 0.666s, 1537.83/s  (0.678s, 1509.98/s)  LR: 9.158e-04  Data: 0.012 (0.018)
Train: 56 [ 500/1251 ( 40%)]  Loss: 3.614 (3.75)  Time: 0.684s, 1497.61/s  (0.678s, 1510.24/s)  LR: 9.157e-04  Data: 0.013 (0.017)
Train: 56 [ 550/1251 ( 44%)]  Loss: 4.079 (3.78)  Time: 0.692s, 1479.39/s  (0.678s, 1510.35/s)  LR: 9.156e-04  Data: 0.013 (0.017)
Train: 56 [ 600/1251 ( 48%)]  Loss: 3.888 (3.79)  Time: 0.673s, 1521.16/s  (0.678s, 1510.27/s)  LR: 9.155e-04  Data: 0.013 (0.017)
Train: 56 [ 650/1251 ( 52%)]  Loss: 3.578 (3.77)  Time: 0.679s, 1508.85/s  (0.678s, 1510.62/s)  LR: 9.154e-04  Data: 0.013 (0.016)
Train: 56 [ 700/1251 ( 56%)]  Loss: 3.641 (3.76)  Time: 0.672s, 1524.91/s  (0.678s, 1510.88/s)  LR: 9.153e-04  Data: 0.014 (0.016)
Train: 56 [ 750/1251 ( 60%)]  Loss: 3.799 (3.76)  Time: 0.678s, 1510.02/s  (0.678s, 1510.93/s)  LR: 9.151e-04  Data: 0.013 (0.016)
Train: 56 [ 800/1251 ( 64%)]  Loss: 3.298 (3.74)  Time: 0.674s, 1518.35/s  (0.678s, 1511.12/s)  LR: 9.150e-04  Data: 0.013 (0.016)
Train: 56 [ 850/1251 ( 68%)]  Loss: 3.991 (3.75)  Time: 0.674s, 1518.58/s  (0.677s, 1511.44/s)  LR: 9.149e-04  Data: 0.013 (0.016)
Train: 56 [ 900/1251 ( 72%)]  Loss: 3.997 (3.76)  Time: 0.675s, 1517.18/s  (0.677s, 1511.76/s)  LR: 9.148e-04  Data: 0.013 (0.016)
Train: 56 [ 950/1251 ( 76%)]  Loss: 3.898 (3.77)  Time: 0.675s, 1517.44/s  (0.677s, 1512.13/s)  LR: 9.147e-04  Data: 0.013 (0.016)
Train: 56 [1000/1251 ( 80%)]  Loss: 3.670 (3.77)  Time: 0.677s, 1513.00/s  (0.677s, 1512.09/s)  LR: 9.146e-04  Data: 0.012 (0.016)
Train: 56 [1050/1251 ( 84%)]  Loss: 3.866 (3.77)  Time: 0.692s, 1480.63/s  (0.677s, 1511.89/s)  LR: 9.144e-04  Data: 0.012 (0.015)
Train: 56 [1100/1251 ( 88%)]  Loss: 3.935 (3.78)  Time: 0.663s, 1544.91/s  (0.677s, 1512.05/s)  LR: 9.143e-04  Data: 0.012 (0.015)
Train: 56 [1150/1251 ( 92%)]  Loss: 3.828 (3.78)  Time: 0.678s, 1510.93/s  (0.677s, 1512.07/s)  LR: 9.142e-04  Data: 0.012 (0.015)
Train: 56 [1200/1251 ( 96%)]  Loss: 3.890 (3.78)  Time: 0.673s, 1522.40/s  (0.677s, 1512.11/s)  LR: 9.141e-04  Data: 0.013 (0.015)
Train: 56 [1250/1251 (100%)]  Loss: 3.814 (3.79)  Time: 0.677s, 1512.98/s  (0.677s, 1512.11/s)  LR: 9.140e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.904 (2.904)  Loss:  0.5918 (0.5918)  Acc@1: 87.7930 (87.7930)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.169 (0.323)  Loss:  0.7222 (1.2359)  Acc@1: 84.3160 (71.7000)  Acc@5: 96.3443 (91.1100)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-54.pth.tar', 71.73600004638672)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-56.pth.tar', 71.7000000390625)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-55.pth.tar', 71.66399998046874)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-52.pth.tar', 71.64599999023437)

Train: 57 [   0/1251 (  0%)]  Loss: 3.783 (3.78)  Time: 3.768s,  271.73/s  (3.768s,  271.73/s)  LR: 9.140e-04  Data: 1.629 (1.629)
Train: 57 [  50/1251 (  4%)]  Loss: 3.855 (3.82)  Time: 0.664s, 1542.55/s  (0.701s, 1461.73/s)  LR: 9.139e-04  Data: 0.012 (0.045)
Train: 57 [ 100/1251 (  8%)]  Loss: 3.958 (3.87)  Time: 0.668s, 1533.29/s  (0.686s, 1492.58/s)  LR: 9.137e-04  Data: 0.012 (0.030)
Train: 57 [ 150/1251 ( 12%)]  Loss: 3.495 (3.77)  Time: 0.676s, 1515.00/s  (0.683s, 1499.95/s)  LR: 9.136e-04  Data: 0.013 (0.024)
Train: 57 [ 200/1251 ( 16%)]  Loss: 3.724 (3.76)  Time: 0.673s, 1522.25/s  (0.681s, 1504.30/s)  LR: 9.135e-04  Data: 0.013 (0.022)
Train: 57 [ 250/1251 ( 20%)]  Loss: 3.707 (3.75)  Time: 0.677s, 1513.04/s  (0.680s, 1506.62/s)  LR: 9.134e-04  Data: 0.013 (0.020)
Train: 57 [ 300/1251 ( 24%)]  Loss: 3.886 (3.77)  Time: 0.683s, 1498.49/s  (0.679s, 1507.30/s)  LR: 9.133e-04  Data: 0.013 (0.019)
Train: 57 [ 350/1251 ( 28%)]  Loss: 3.750 (3.77)  Time: 0.673s, 1520.66/s  (0.679s, 1507.27/s)  LR: 9.131e-04  Data: 0.011 (0.018)
Train: 57 [ 400/1251 ( 32%)]  Loss: 3.984 (3.79)  Time: 0.682s, 1502.13/s  (0.679s, 1507.36/s)  LR: 9.130e-04  Data: 0.013 (0.018)
Train: 57 [ 450/1251 ( 36%)]  Loss: 3.884 (3.80)  Time: 0.688s, 1489.11/s  (0.679s, 1507.59/s)  LR: 9.129e-04  Data: 0.014 (0.017)
Train: 57 [ 500/1251 ( 40%)]  Loss: 3.654 (3.79)  Time: 0.665s, 1541.00/s  (0.679s, 1507.24/s)  LR: 9.128e-04  Data: 0.013 (0.017)
Train: 57 [ 550/1251 ( 44%)]  Loss: 3.816 (3.79)  Time: 0.680s, 1506.12/s  (0.679s, 1507.36/s)  LR: 9.127e-04  Data: 0.015 (0.017)
Train: 57 [ 600/1251 ( 48%)]  Loss: 3.625 (3.78)  Time: 0.673s, 1522.62/s  (0.679s, 1507.31/s)  LR: 9.126e-04  Data: 0.013 (0.016)
Train: 57 [ 650/1251 ( 52%)]  Loss: 3.529 (3.76)  Time: 0.671s, 1526.19/s  (0.679s, 1507.52/s)  LR: 9.124e-04  Data: 0.013 (0.016)
Train: 57 [ 700/1251 ( 56%)]  Loss: 3.840 (3.77)  Time: 0.672s, 1524.18/s  (0.679s, 1508.02/s)  LR: 9.123e-04  Data: 0.016 (0.016)
Train: 57 [ 750/1251 ( 60%)]  Loss: 3.722 (3.76)  Time: 0.675s, 1517.90/s  (0.679s, 1508.53/s)  LR: 9.122e-04  Data: 0.012 (0.016)
Train: 57 [ 800/1251 ( 64%)]  Loss: 3.704 (3.76)  Time: 0.670s, 1529.02/s  (0.679s, 1508.92/s)  LR: 9.121e-04  Data: 0.014 (0.016)
Train: 57 [ 850/1251 ( 68%)]  Loss: 3.725 (3.76)  Time: 0.675s, 1517.05/s  (0.678s, 1509.48/s)  LR: 9.120e-04  Data: 0.013 (0.015)
Train: 57 [ 900/1251 ( 72%)]  Loss: 3.758 (3.76)  Time: 0.676s, 1514.05/s  (0.678s, 1509.99/s)  LR: 9.119e-04  Data: 0.016 (0.015)
Train: 57 [ 950/1251 ( 76%)]  Loss: 4.081 (3.77)  Time: 0.681s, 1503.36/s  (0.678s, 1510.38/s)  LR: 9.117e-04  Data: 0.013 (0.015)
Train: 57 [1000/1251 ( 80%)]  Loss: 3.794 (3.78)  Time: 0.683s, 1498.43/s  (0.678s, 1510.81/s)  LR: 9.116e-04  Data: 0.013 (0.015)
Train: 57 [1050/1251 ( 84%)]  Loss: 4.033 (3.79)  Time: 0.675s, 1516.90/s  (0.678s, 1511.11/s)  LR: 9.115e-04  Data: 0.013 (0.015)
Train: 57 [1100/1251 ( 88%)]  Loss: 3.930 (3.79)  Time: 0.672s, 1524.87/s  (0.677s, 1511.50/s)  LR: 9.114e-04  Data: 0.013 (0.015)
Train: 57 [1150/1251 ( 92%)]  Loss: 4.149 (3.81)  Time: 0.672s, 1524.01/s  (0.677s, 1511.82/s)  LR: 9.113e-04  Data: 0.014 (0.015)
Train: 57 [1200/1251 ( 96%)]  Loss: 3.656 (3.80)  Time: 0.672s, 1522.88/s  (0.677s, 1511.77/s)  LR: 9.111e-04  Data: 0.012 (0.015)
Train: 57 [1250/1251 (100%)]  Loss: 4.067 (3.81)  Time: 0.674s, 1519.80/s  (0.677s, 1511.89/s)  LR: 9.110e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.753 (2.753)  Loss:  0.6299 (0.6299)  Acc@1: 86.4258 (86.4258)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.170 (0.326)  Loss:  0.7437 (1.2168)  Acc@1: 83.6085 (72.0000)  Acc@5: 95.9906 (91.1400)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-57.pth.tar', 71.99999999023437)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-54.pth.tar', 71.73600004638672)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-56.pth.tar', 71.7000000390625)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-55.pth.tar', 71.66399998046874)

Train: 58 [   0/1251 (  0%)]  Loss: 3.740 (3.74)  Time: 3.560s,  287.67/s  (3.560s,  287.67/s)  LR: 9.110e-04  Data: 1.823 (1.823)
Train: 58 [  50/1251 (  4%)]  Loss: 3.548 (3.64)  Time: 0.661s, 1549.19/s  (0.696s, 1471.71/s)  LR: 9.109e-04  Data: 0.013 (0.050)
Train: 58 [ 100/1251 (  8%)]  Loss: 3.861 (3.72)  Time: 0.667s, 1535.35/s  (0.683s, 1498.75/s)  LR: 9.108e-04  Data: 0.013 (0.032)
Train: 58 [ 150/1251 ( 12%)]  Loss: 3.977 (3.78)  Time: 0.674s, 1518.86/s  (0.680s, 1505.26/s)  LR: 9.107e-04  Data: 0.013 (0.026)
Train: 58 [ 200/1251 ( 16%)]  Loss: 3.830 (3.79)  Time: 0.677s, 1513.62/s  (0.679s, 1509.06/s)  LR: 9.105e-04  Data: 0.013 (0.023)
Train: 58 [ 250/1251 ( 20%)]  Loss: 3.918 (3.81)  Time: 0.671s, 1525.56/s  (0.678s, 1509.98/s)  LR: 9.104e-04  Data: 0.013 (0.021)
Train: 58 [ 300/1251 ( 24%)]  Loss: 3.706 (3.80)  Time: 0.683s, 1498.84/s  (0.678s, 1509.80/s)  LR: 9.103e-04  Data: 0.014 (0.020)
Train: 58 [ 350/1251 ( 28%)]  Loss: 4.054 (3.83)  Time: 0.674s, 1519.07/s  (0.678s, 1510.15/s)  LR: 9.102e-04  Data: 0.017 (0.019)
Train: 58 [ 400/1251 ( 32%)]  Loss: 3.824 (3.83)  Time: 0.674s, 1518.35/s  (0.678s, 1509.67/s)  LR: 9.101e-04  Data: 0.012 (0.018)
Train: 58 [ 450/1251 ( 36%)]  Loss: 3.911 (3.84)  Time: 0.691s, 1481.04/s  (0.678s, 1509.34/s)  LR: 9.099e-04  Data: 0.014 (0.018)
Train: 58 [ 500/1251 ( 40%)]  Loss: 3.834 (3.84)  Time: 0.696s, 1470.85/s  (0.678s, 1509.32/s)  LR: 9.098e-04  Data: 0.016 (0.017)
Train: 58 [ 550/1251 ( 44%)]  Loss: 3.580 (3.82)  Time: 0.673s, 1520.66/s  (0.678s, 1509.79/s)  LR: 9.097e-04  Data: 0.013 (0.017)
Train: 58 [ 600/1251 ( 48%)]  Loss: 3.363 (3.78)  Time: 0.673s, 1522.55/s  (0.678s, 1510.16/s)  LR: 9.096e-04  Data: 0.013 (0.017)
Train: 58 [ 650/1251 ( 52%)]  Loss: 3.942 (3.79)  Time: 0.670s, 1528.40/s  (0.678s, 1510.50/s)  LR: 9.095e-04  Data: 0.013 (0.016)
Train: 58 [ 700/1251 ( 56%)]  Loss: 3.898 (3.80)  Time: 0.674s, 1518.88/s  (0.678s, 1510.75/s)  LR: 9.093e-04  Data: 0.012 (0.016)
Train: 58 [ 750/1251 ( 60%)]  Loss: 4.253 (3.83)  Time: 0.677s, 1513.05/s  (0.678s, 1511.10/s)  LR: 9.092e-04  Data: 0.012 (0.016)
Train: 58 [ 800/1251 ( 64%)]  Loss: 3.580 (3.81)  Time: 0.677s, 1513.31/s  (0.677s, 1511.44/s)  LR: 9.091e-04  Data: 0.014 (0.016)
Train: 58 [ 850/1251 ( 68%)]  Loss: 4.059 (3.83)  Time: 0.668s, 1533.17/s  (0.677s, 1511.79/s)  LR: 9.090e-04  Data: 0.013 (0.016)
Train: 58 [ 900/1251 ( 72%)]  Loss: 3.611 (3.82)  Time: 0.679s, 1507.23/s  (0.677s, 1511.89/s)  LR: 9.089e-04  Data: 0.013 (0.016)
Train: 58 [ 950/1251 ( 76%)]  Loss: 4.025 (3.83)  Time: 0.677s, 1512.13/s  (0.677s, 1512.21/s)  LR: 9.087e-04  Data: 0.013 (0.016)
Train: 58 [1000/1251 ( 80%)]  Loss: 3.854 (3.83)  Time: 0.664s, 1541.97/s  (0.677s, 1512.31/s)  LR: 9.086e-04  Data: 0.012 (0.015)
Train: 58 [1050/1251 ( 84%)]  Loss: 3.814 (3.83)  Time: 0.678s, 1511.34/s  (0.677s, 1512.45/s)  LR: 9.085e-04  Data: 0.016 (0.015)
Train: 58 [1100/1251 ( 88%)]  Loss: 4.129 (3.84)  Time: 0.680s, 1506.02/s  (0.677s, 1512.64/s)  LR: 9.084e-04  Data: 0.013 (0.015)
Train: 58 [1150/1251 ( 92%)]  Loss: 3.631 (3.83)  Time: 0.681s, 1504.02/s  (0.677s, 1512.82/s)  LR: 9.083e-04  Data: 0.014 (0.015)
Train: 58 [1200/1251 ( 96%)]  Loss: 3.685 (3.82)  Time: 0.686s, 1492.58/s  (0.677s, 1513.10/s)  LR: 9.081e-04  Data: 0.013 (0.015)
Train: 58 [1250/1251 (100%)]  Loss: 3.730 (3.82)  Time: 0.671s, 1525.86/s  (0.677s, 1513.20/s)  LR: 9.080e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.702 (2.702)  Loss:  0.5952 (0.5952)  Acc@1: 87.0117 (87.0117)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.166 (0.326)  Loss:  0.7378 (1.1902)  Acc@1: 83.6085 (72.1320)  Acc@5: 95.9906 (91.2900)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-58.pth.tar', 72.13199999023438)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-57.pth.tar', 71.99999999023437)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-54.pth.tar', 71.73600004638672)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-56.pth.tar', 71.7000000390625)

Train: 59 [   0/1251 (  0%)]  Loss: 3.324 (3.32)  Time: 3.823s,  267.86/s  (3.823s,  267.86/s)  LR: 9.080e-04  Data: 2.003 (2.003)
Train: 59 [  50/1251 (  4%)]  Loss: 3.869 (3.60)  Time: 0.664s, 1541.06/s  (0.708s, 1446.85/s)  LR: 9.079e-04  Data: 0.013 (0.053)
Train: 59 [ 100/1251 (  8%)]  Loss: 3.845 (3.68)  Time: 0.659s, 1553.27/s  (0.689s, 1486.32/s)  LR: 9.078e-04  Data: 0.016 (0.033)
Train: 59 [ 150/1251 ( 12%)]  Loss: 3.687 (3.68)  Time: 0.667s, 1535.00/s  (0.684s, 1497.85/s)  LR: 9.077e-04  Data: 0.011 (0.027)
Train: 59 [ 200/1251 ( 16%)]  Loss: 3.733 (3.69)  Time: 0.683s, 1499.86/s  (0.682s, 1502.56/s)  LR: 9.075e-04  Data: 0.013 (0.023)
Train: 59 [ 250/1251 ( 20%)]  Loss: 3.794 (3.71)  Time: 0.681s, 1503.56/s  (0.681s, 1504.66/s)  LR: 9.074e-04  Data: 0.012 (0.021)
Train: 59 [ 300/1251 ( 24%)]  Loss: 3.964 (3.75)  Time: 0.676s, 1515.30/s  (0.680s, 1505.71/s)  LR: 9.073e-04  Data: 0.015 (0.020)
Train: 59 [ 350/1251 ( 28%)]  Loss: 3.565 (3.72)  Time: 0.686s, 1492.10/s  (0.680s, 1505.76/s)  LR: 9.072e-04  Data: 0.014 (0.019)
Train: 59 [ 400/1251 ( 32%)]  Loss: 3.954 (3.75)  Time: 0.682s, 1501.01/s  (0.680s, 1505.86/s)  LR: 9.071e-04  Data: 0.014 (0.018)
Train: 59 [ 450/1251 ( 36%)]  Loss: 3.737 (3.75)  Time: 0.676s, 1515.17/s  (0.680s, 1505.94/s)  LR: 9.069e-04  Data: 0.013 (0.018)
Train: 59 [ 500/1251 ( 40%)]  Loss: 4.003 (3.77)  Time: 0.672s, 1523.56/s  (0.680s, 1505.84/s)  LR: 9.068e-04  Data: 0.013 (0.017)
Train: 59 [ 550/1251 ( 44%)]  Loss: 3.443 (3.74)  Time: 0.682s, 1500.57/s  (0.680s, 1505.67/s)  LR: 9.067e-04  Data: 0.013 (0.017)
Train: 59 [ 600/1251 ( 48%)]  Loss: 3.426 (3.72)  Time: 0.679s, 1507.33/s  (0.680s, 1505.91/s)  LR: 9.066e-04  Data: 0.014 (0.017)
Train: 59 [ 650/1251 ( 52%)]  Loss: 3.678 (3.72)  Time: 0.693s, 1477.09/s  (0.680s, 1505.92/s)  LR: 9.064e-04  Data: 0.014 (0.017)
Train: 59 [ 700/1251 ( 56%)]  Loss: 3.815 (3.72)  Time: 0.670s, 1527.35/s  (0.680s, 1505.92/s)  LR: 9.063e-04  Data: 0.012 (0.016)
Train: 59 [ 750/1251 ( 60%)]  Loss: 3.851 (3.73)  Time: 0.675s, 1517.19/s  (0.680s, 1506.14/s)  LR: 9.062e-04  Data: 0.013 (0.016)
Train: 59 [ 800/1251 ( 64%)]  Loss: 4.054 (3.75)  Time: 0.679s, 1507.19/s  (0.680s, 1506.35/s)  LR: 9.061e-04  Data: 0.015 (0.016)
Train: 59 [ 850/1251 ( 68%)]  Loss: 3.721 (3.75)  Time: 0.672s, 1524.73/s  (0.680s, 1506.66/s)  LR: 9.060e-04  Data: 0.013 (0.016)
Train: 59 [ 900/1251 ( 72%)]  Loss: 3.801 (3.75)  Time: 0.671s, 1525.63/s  (0.679s, 1507.06/s)  LR: 9.058e-04  Data: 0.013 (0.016)
Train: 59 [ 950/1251 ( 76%)]  Loss: 3.595 (3.74)  Time: 0.661s, 1548.78/s  (0.679s, 1507.55/s)  LR: 9.057e-04  Data: 0.014 (0.016)
Train: 59 [1000/1251 ( 80%)]  Loss: 3.653 (3.74)  Time: 0.678s, 1510.11/s  (0.679s, 1507.87/s)  LR: 9.056e-04  Data: 0.015 (0.016)
Train: 59 [1050/1251 ( 84%)]  Loss: 3.892 (3.75)  Time: 0.686s, 1493.44/s  (0.679s, 1508.19/s)  LR: 9.055e-04  Data: 0.014 (0.015)
Train: 59 [1100/1251 ( 88%)]  Loss: 3.655 (3.74)  Time: 0.680s, 1505.88/s  (0.679s, 1508.45/s)  LR: 9.054e-04  Data: 0.013 (0.015)
Train: 59 [1150/1251 ( 92%)]  Loss: 3.778 (3.74)  Time: 0.670s, 1528.10/s  (0.679s, 1508.77/s)  LR: 9.052e-04  Data: 0.012 (0.015)
Train: 59 [1200/1251 ( 96%)]  Loss: 3.731 (3.74)  Time: 0.681s, 1504.65/s  (0.679s, 1509.02/s)  LR: 9.051e-04  Data: 0.014 (0.015)
Train: 59 [1250/1251 (100%)]  Loss: 3.419 (3.73)  Time: 0.658s, 1556.78/s  (0.678s, 1509.36/s)  LR: 9.050e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.789 (2.789)  Loss:  0.5737 (0.5737)  Acc@1: 86.9141 (86.9141)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.166 (0.325)  Loss:  0.6890 (1.2005)  Acc@1: 83.4906 (72.1940)  Acc@5: 95.9906 (91.1900)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-59.pth.tar', 72.19400006835937)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-58.pth.tar', 72.13199999023438)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-57.pth.tar', 71.99999999023437)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-54.pth.tar', 71.73600004638672)

Train: 60 [   0/1251 (  0%)]  Loss: 3.980 (3.98)  Time: 3.873s,  264.38/s  (3.873s,  264.38/s)  LR: 9.050e-04  Data: 1.654 (1.654)
Train: 60 [  50/1251 (  4%)]  Loss: 4.121 (4.05)  Time: 0.673s, 1521.60/s  (0.700s, 1462.62/s)  LR: 9.049e-04  Data: 0.014 (0.046)
Train: 60 [ 100/1251 (  8%)]  Loss: 3.820 (3.97)  Time: 0.668s, 1531.97/s  (0.685s, 1494.27/s)  LR: 9.047e-04  Data: 0.013 (0.030)
Train: 60 [ 150/1251 ( 12%)]  Loss: 4.022 (3.99)  Time: 0.670s, 1527.30/s  (0.681s, 1502.97/s)  LR: 9.046e-04  Data: 0.012 (0.024)
Train: 60 [ 200/1251 ( 16%)]  Loss: 3.467 (3.88)  Time: 0.674s, 1518.67/s  (0.680s, 1506.31/s)  LR: 9.045e-04  Data: 0.013 (0.022)
Train: 60 [ 250/1251 ( 20%)]  Loss: 3.551 (3.83)  Time: 0.669s, 1530.02/s  (0.679s, 1508.06/s)  LR: 9.044e-04  Data: 0.013 (0.020)
Train: 60 [ 300/1251 ( 24%)]  Loss: 3.887 (3.84)  Time: 0.689s, 1486.11/s  (0.678s, 1509.52/s)  LR: 9.042e-04  Data: 0.013 (0.019)
Train: 60 [ 350/1251 ( 28%)]  Loss: 3.968 (3.85)  Time: 0.690s, 1484.63/s  (0.678s, 1509.96/s)  LR: 9.041e-04  Data: 0.013 (0.018)
Train: 60 [ 400/1251 ( 32%)]  Loss: 4.134 (3.88)  Time: 0.671s, 1526.76/s  (0.678s, 1510.06/s)  LR: 9.040e-04  Data: 0.013 (0.018)
Train: 60 [ 450/1251 ( 36%)]  Loss: 3.718 (3.87)  Time: 0.679s, 1508.11/s  (0.678s, 1510.65/s)  LR: 9.039e-04  Data: 0.013 (0.017)
Train: 60 [ 500/1251 ( 40%)]  Loss: 3.964 (3.88)  Time: 0.664s, 1542.00/s  (0.677s, 1511.49/s)  LR: 9.038e-04  Data: 0.013 (0.017)
Train: 60 [ 550/1251 ( 44%)]  Loss: 4.221 (3.90)  Time: 0.682s, 1501.44/s  (0.677s, 1511.62/s)  LR: 9.036e-04  Data: 0.014 (0.016)
Train: 60 [ 600/1251 ( 48%)]  Loss: 3.561 (3.88)  Time: 0.683s, 1499.94/s  (0.678s, 1511.39/s)  LR: 9.035e-04  Data: 0.012 (0.016)
Train: 60 [ 650/1251 ( 52%)]  Loss: 3.689 (3.86)  Time: 0.679s, 1507.61/s  (0.677s, 1511.66/s)  LR: 9.034e-04  Data: 0.013 (0.016)
Train: 60 [ 700/1251 ( 56%)]  Loss: 3.866 (3.86)  Time: 0.682s, 1501.92/s  (0.678s, 1511.32/s)  LR: 9.033e-04  Data: 0.017 (0.016)
Train: 60 [ 750/1251 ( 60%)]  Loss: 4.081 (3.88)  Time: 0.673s, 1520.59/s  (0.678s, 1511.03/s)  LR: 9.031e-04  Data: 0.012 (0.016)
Train: 60 [ 800/1251 ( 64%)]  Loss: 3.697 (3.87)  Time: 0.683s, 1498.78/s  (0.678s, 1510.91/s)  LR: 9.030e-04  Data: 0.013 (0.015)
Train: 60 [ 850/1251 ( 68%)]  Loss: 3.704 (3.86)  Time: 0.682s, 1501.94/s  (0.678s, 1510.50/s)  LR: 9.029e-04  Data: 0.012 (0.015)
Train: 60 [ 900/1251 ( 72%)]  Loss: 3.648 (3.85)  Time: 0.680s, 1506.77/s  (0.678s, 1510.19/s)  LR: 9.028e-04  Data: 0.013 (0.015)
Train: 60 [ 950/1251 ( 76%)]  Loss: 3.861 (3.85)  Time: 0.681s, 1504.63/s  (0.678s, 1509.87/s)  LR: 9.026e-04  Data: 0.012 (0.015)
Train: 60 [1000/1251 ( 80%)]  Loss: 3.908 (3.85)  Time: 0.681s, 1504.62/s  (0.678s, 1509.83/s)  LR: 9.025e-04  Data: 0.013 (0.015)
Train: 60 [1050/1251 ( 84%)]  Loss: 3.445 (3.83)  Time: 0.680s, 1505.98/s  (0.678s, 1509.90/s)  LR: 9.024e-04  Data: 0.013 (0.015)
Train: 60 [1100/1251 ( 88%)]  Loss: 3.686 (3.83)  Time: 0.689s, 1485.27/s  (0.678s, 1509.85/s)  LR: 9.023e-04  Data: 0.016 (0.015)
Train: 60 [1150/1251 ( 92%)]  Loss: 3.680 (3.82)  Time: 0.685s, 1494.51/s  (0.678s, 1509.95/s)  LR: 9.021e-04  Data: 0.014 (0.015)
Train: 60 [1200/1251 ( 96%)]  Loss: 3.925 (3.82)  Time: 0.685s, 1494.60/s  (0.678s, 1509.94/s)  LR: 9.020e-04  Data: 0.012 (0.015)
Train: 60 [1250/1251 (100%)]  Loss: 3.974 (3.83)  Time: 0.670s, 1528.42/s  (0.678s, 1510.12/s)  LR: 9.019e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.773 (2.773)  Loss:  0.6230 (0.6230)  Acc@1: 87.0117 (87.0117)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.167 (0.327)  Loss:  0.6406 (1.1916)  Acc@1: 84.6698 (72.2980)  Acc@5: 97.0519 (91.4300)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-60.pth.tar', 72.29800006347656)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-59.pth.tar', 72.19400006835937)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-58.pth.tar', 72.13199999023438)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-57.pth.tar', 71.99999999023437)

Train: 61 [   0/1251 (  0%)]  Loss: 3.732 (3.73)  Time: 3.630s,  282.12/s  (3.630s,  282.12/s)  LR: 9.019e-04  Data: 1.852 (1.852)
Train: 61 [  50/1251 (  4%)]  Loss: 3.998 (3.87)  Time: 0.658s, 1555.23/s  (0.694s, 1474.70/s)  LR: 9.018e-04  Data: 0.013 (0.050)
Train: 61 [ 100/1251 (  8%)]  Loss: 4.009 (3.91)  Time: 0.671s, 1526.39/s  (0.682s, 1501.79/s)  LR: 9.017e-04  Data: 0.012 (0.032)
Train: 61 [ 150/1251 ( 12%)]  Loss: 3.795 (3.88)  Time: 0.674s, 1518.42/s  (0.678s, 1509.30/s)  LR: 9.015e-04  Data: 0.014 (0.026)
Train: 61 [ 200/1251 ( 16%)]  Loss: 3.657 (3.84)  Time: 0.684s, 1496.93/s  (0.678s, 1511.11/s)  LR: 9.014e-04  Data: 0.013 (0.023)
Train: 61 [ 250/1251 ( 20%)]  Loss: 3.925 (3.85)  Time: 0.675s, 1517.27/s  (0.678s, 1511.36/s)  LR: 9.013e-04  Data: 0.011 (0.021)
Train: 61 [ 300/1251 ( 24%)]  Loss: 4.026 (3.88)  Time: 0.673s, 1522.49/s  (0.678s, 1510.73/s)  LR: 9.012e-04  Data: 0.013 (0.020)
Train: 61 [ 350/1251 ( 28%)]  Loss: 3.887 (3.88)  Time: 0.678s, 1510.24/s  (0.678s, 1511.00/s)  LR: 9.010e-04  Data: 0.014 (0.019)
Train: 61 [ 400/1251 ( 32%)]  Loss: 3.875 (3.88)  Time: 0.676s, 1515.36/s  (0.678s, 1510.88/s)  LR: 9.009e-04  Data: 0.014 (0.019)
Train: 61 [ 450/1251 ( 36%)]  Loss: 3.973 (3.89)  Time: 0.680s, 1505.19/s  (0.678s, 1511.34/s)  LR: 9.008e-04  Data: 0.013 (0.018)
Train: 61 [ 500/1251 ( 40%)]  Loss: 4.007 (3.90)  Time: 0.670s, 1529.33/s  (0.677s, 1511.85/s)  LR: 9.007e-04  Data: 0.012 (0.018)
Train: 61 [ 550/1251 ( 44%)]  Loss: 3.926 (3.90)  Time: 0.675s, 1515.92/s  (0.677s, 1511.97/s)  LR: 9.005e-04  Data: 0.014 (0.017)
Train: 61 [ 600/1251 ( 48%)]  Loss: 3.826 (3.90)  Time: 0.664s, 1541.15/s  (0.677s, 1511.99/s)  LR: 9.004e-04  Data: 0.011 (0.017)
Train: 61 [ 650/1251 ( 52%)]  Loss: 3.621 (3.88)  Time: 0.687s, 1490.38/s  (0.677s, 1511.96/s)  LR: 9.003e-04  Data: 0.013 (0.017)
Train: 61 [ 700/1251 ( 56%)]  Loss: 3.844 (3.87)  Time: 0.677s, 1512.60/s  (0.677s, 1511.67/s)  LR: 9.002e-04  Data: 0.013 (0.017)
Train: 61 [ 750/1251 ( 60%)]  Loss: 3.741 (3.87)  Time: 0.676s, 1514.79/s  (0.677s, 1511.46/s)  LR: 9.000e-04  Data: 0.012 (0.016)
Train: 61 [ 800/1251 ( 64%)]  Loss: 4.011 (3.87)  Time: 0.686s, 1491.71/s  (0.678s, 1511.20/s)  LR: 8.999e-04  Data: 0.015 (0.016)
Train: 61 [ 850/1251 ( 68%)]  Loss: 3.819 (3.87)  Time: 0.674s, 1519.18/s  (0.678s, 1511.11/s)  LR: 8.998e-04  Data: 0.014 (0.016)
Train: 61 [ 900/1251 ( 72%)]  Loss: 3.901 (3.87)  Time: 0.678s, 1509.91/s  (0.678s, 1510.85/s)  LR: 8.997e-04  Data: 0.012 (0.016)
Train: 61 [ 950/1251 ( 76%)]  Loss: 3.875 (3.87)  Time: 0.681s, 1503.30/s  (0.678s, 1510.61/s)  LR: 8.995e-04  Data: 0.015 (0.016)
Train: 61 [1000/1251 ( 80%)]  Loss: 4.022 (3.88)  Time: 0.683s, 1500.31/s  (0.678s, 1510.42/s)  LR: 8.994e-04  Data: 0.014 (0.016)
Train: 61 [1050/1251 ( 84%)]  Loss: 3.893 (3.88)  Time: 0.684s, 1497.54/s  (0.678s, 1510.49/s)  LR: 8.993e-04  Data: 0.012 (0.016)
Train: 61 [1100/1251 ( 88%)]  Loss: 3.993 (3.89)  Time: 0.671s, 1526.23/s  (0.678s, 1510.15/s)  LR: 8.992e-04  Data: 0.018 (0.016)
Train: 61 [1150/1251 ( 92%)]  Loss: 3.693 (3.88)  Time: 0.690s, 1484.76/s  (0.678s, 1510.14/s)  LR: 8.990e-04  Data: 0.012 (0.015)
Train: 61 [1200/1251 ( 96%)]  Loss: 3.694 (3.87)  Time: 0.673s, 1522.55/s  (0.678s, 1509.96/s)  LR: 8.989e-04  Data: 0.013 (0.015)
Train: 61 [1250/1251 (100%)]  Loss: 3.851 (3.87)  Time: 0.669s, 1531.76/s  (0.678s, 1509.95/s)  LR: 8.988e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.717 (2.717)  Loss:  0.5498 (0.5498)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.165 (0.326)  Loss:  0.6509 (1.1838)  Acc@1: 86.2028 (72.1120)  Acc@5: 96.3443 (91.3640)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-60.pth.tar', 72.29800006347656)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-59.pth.tar', 72.19400006835937)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-58.pth.tar', 72.13199999023438)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-61.pth.tar', 72.11199995361328)

Train: 62 [   0/1251 (  0%)]  Loss: 3.727 (3.73)  Time: 3.600s,  284.48/s  (3.600s,  284.48/s)  LR: 8.988e-04  Data: 2.243 (2.243)
Train: 62 [  50/1251 (  4%)]  Loss: 4.208 (3.97)  Time: 0.671s, 1526.75/s  (0.704s, 1454.16/s)  LR: 8.986e-04  Data: 0.013 (0.057)
Train: 62 [ 100/1251 (  8%)]  Loss: 3.822 (3.92)  Time: 0.670s, 1528.64/s  (0.689s, 1485.30/s)  LR: 8.985e-04  Data: 0.012 (0.036)
Train: 62 [ 150/1251 ( 12%)]  Loss: 3.815 (3.89)  Time: 0.684s, 1497.55/s  (0.685s, 1494.88/s)  LR: 8.984e-04  Data: 0.014 (0.028)
Train: 62 [ 200/1251 ( 16%)]  Loss: 3.865 (3.89)  Time: 0.677s, 1513.41/s  (0.683s, 1499.92/s)  LR: 8.983e-04  Data: 0.018 (0.025)
Train: 62 [ 250/1251 ( 20%)]  Loss: 3.894 (3.89)  Time: 0.677s, 1513.31/s  (0.682s, 1502.53/s)  LR: 8.981e-04  Data: 0.013 (0.023)
Train: 62 [ 300/1251 ( 24%)]  Loss: 3.774 (3.87)  Time: 0.676s, 1515.22/s  (0.681s, 1503.85/s)  LR: 8.980e-04  Data: 0.014 (0.021)
Train: 62 [ 350/1251 ( 28%)]  Loss: 3.567 (3.83)  Time: 0.670s, 1528.64/s  (0.680s, 1505.71/s)  LR: 8.979e-04  Data: 0.013 (0.020)
Train: 62 [ 400/1251 ( 32%)]  Loss: 3.937 (3.85)  Time: 0.678s, 1511.42/s  (0.679s, 1507.73/s)  LR: 8.978e-04  Data: 0.014 (0.019)
Train: 62 [ 450/1251 ( 36%)]  Loss: 3.665 (3.83)  Time: 0.670s, 1528.07/s  (0.679s, 1509.10/s)  LR: 8.976e-04  Data: 0.013 (0.019)
Train: 62 [ 500/1251 ( 40%)]  Loss: 3.796 (3.82)  Time: 0.680s, 1506.13/s  (0.678s, 1510.10/s)  LR: 8.975e-04  Data: 0.014 (0.018)
Train: 62 [ 550/1251 ( 44%)]  Loss: 4.024 (3.84)  Time: 0.669s, 1529.69/s  (0.678s, 1510.73/s)  LR: 8.974e-04  Data: 0.013 (0.018)
Train: 62 [ 600/1251 ( 48%)]  Loss: 3.918 (3.85)  Time: 0.677s, 1512.21/s  (0.677s, 1511.57/s)  LR: 8.973e-04  Data: 0.013 (0.017)
Train: 62 [ 650/1251 ( 52%)]  Loss: 3.752 (3.84)  Time: 0.682s, 1501.11/s  (0.677s, 1512.02/s)  LR: 8.971e-04  Data: 0.014 (0.017)
Train: 62 [ 700/1251 ( 56%)]  Loss: 3.746 (3.83)  Time: 0.671s, 1525.78/s  (0.677s, 1512.35/s)  LR: 8.970e-04  Data: 0.013 (0.017)
Train: 62 [ 750/1251 ( 60%)]  Loss: 3.782 (3.83)  Time: 0.675s, 1517.12/s  (0.677s, 1512.64/s)  LR: 8.969e-04  Data: 0.014 (0.017)
Train: 62 [ 800/1251 ( 64%)]  Loss: 3.909 (3.84)  Time: 0.677s, 1512.42/s  (0.677s, 1513.03/s)  LR: 8.967e-04  Data: 0.016 (0.016)
Train: 62 [ 850/1251 ( 68%)]  Loss: 3.998 (3.84)  Time: 0.682s, 1500.99/s  (0.677s, 1513.60/s)  LR: 8.966e-04  Data: 0.014 (0.016)
Train: 62 [ 900/1251 ( 72%)]  Loss: 3.991 (3.85)  Time: 0.671s, 1526.56/s  (0.676s, 1513.67/s)  LR: 8.965e-04  Data: 0.014 (0.016)
Train: 62 [ 950/1251 ( 76%)]  Loss: 3.758 (3.85)  Time: 0.683s, 1498.73/s  (0.676s, 1513.76/s)  LR: 8.964e-04  Data: 0.016 (0.016)
Train: 62 [1000/1251 ( 80%)]  Loss: 3.416 (3.83)  Time: 0.682s, 1501.25/s  (0.676s, 1513.92/s)  LR: 8.962e-04  Data: 0.012 (0.016)
Train: 62 [1050/1251 ( 84%)]  Loss: 3.555 (3.81)  Time: 0.677s, 1512.43/s  (0.676s, 1513.89/s)  LR: 8.961e-04  Data: 0.013 (0.016)
Train: 62 [1100/1251 ( 88%)]  Loss: 3.840 (3.82)  Time: 0.693s, 1477.00/s  (0.676s, 1513.73/s)  LR: 8.960e-04  Data: 0.013 (0.016)
Train: 62 [1150/1251 ( 92%)]  Loss: 4.164 (3.83)  Time: 0.683s, 1499.34/s  (0.677s, 1513.54/s)  LR: 8.959e-04  Data: 0.013 (0.016)
Train: 62 [1200/1251 ( 96%)]  Loss: 3.411 (3.81)  Time: 0.662s, 1547.26/s  (0.677s, 1513.27/s)  LR: 8.957e-04  Data: 0.012 (0.016)
Train: 62 [1250/1251 (100%)]  Loss: 3.994 (3.82)  Time: 0.667s, 1535.98/s  (0.677s, 1513.20/s)  LR: 8.956e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.734 (2.734)  Loss:  0.5649 (0.5649)  Acc@1: 87.9883 (87.9883)  Acc@5: 97.3633 (97.3633)
Test: [  48/48]  Time: 0.167 (0.320)  Loss:  0.6431 (1.1653)  Acc@1: 84.3160 (72.4640)  Acc@5: 95.9906 (91.5020)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-62.pth.tar', 72.4640000390625)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-60.pth.tar', 72.29800006347656)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-59.pth.tar', 72.19400006835937)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-58.pth.tar', 72.13199999023438)

Train: 63 [   0/1251 (  0%)]  Loss: 3.734 (3.73)  Time: 3.292s,  311.07/s  (3.292s,  311.07/s)  LR: 8.956e-04  Data: 1.710 (1.710)
Train: 63 [  50/1251 (  4%)]  Loss: 3.971 (3.85)  Time: 0.674s, 1520.16/s  (0.699s, 1464.76/s)  LR: 8.955e-04  Data: 0.016 (0.048)
Train: 63 [ 100/1251 (  8%)]  Loss: 3.824 (3.84)  Time: 0.683s, 1498.20/s  (0.686s, 1492.35/s)  LR: 8.953e-04  Data: 0.014 (0.031)
Train: 63 [ 150/1251 ( 12%)]  Loss: 3.828 (3.84)  Time: 0.677s, 1513.40/s  (0.683s, 1499.18/s)  LR: 8.952e-04  Data: 0.013 (0.025)
Train: 63 [ 200/1251 ( 16%)]  Loss: 3.787 (3.83)  Time: 0.680s, 1505.80/s  (0.682s, 1501.99/s)  LR: 8.951e-04  Data: 0.015 (0.022)
Train: 63 [ 250/1251 ( 20%)]  Loss: 3.859 (3.83)  Time: 0.674s, 1518.89/s  (0.681s, 1503.39/s)  LR: 8.950e-04  Data: 0.013 (0.021)
Train: 63 [ 300/1251 ( 24%)]  Loss: 3.728 (3.82)  Time: 0.662s, 1546.90/s  (0.681s, 1504.64/s)  LR: 8.948e-04  Data: 0.013 (0.020)
Train: 63 [ 350/1251 ( 28%)]  Loss: 3.926 (3.83)  Time: 0.671s, 1524.96/s  (0.680s, 1505.30/s)  LR: 8.947e-04  Data: 0.014 (0.019)
Train: 63 [ 400/1251 ( 32%)]  Loss: 3.863 (3.84)  Time: 0.674s, 1520.34/s  (0.680s, 1506.76/s)  LR: 8.946e-04  Data: 0.017 (0.018)
Train: 63 [ 450/1251 ( 36%)]  Loss: 3.504 (3.80)  Time: 0.672s, 1522.94/s  (0.679s, 1507.56/s)  LR: 8.944e-04  Data: 0.012 (0.018)
Train: 63 [ 500/1251 ( 40%)]  Loss: 3.715 (3.79)  Time: 0.674s, 1519.21/s  (0.679s, 1508.20/s)  LR: 8.943e-04  Data: 0.012 (0.017)
Train: 63 [ 550/1251 ( 44%)]  Loss: 3.955 (3.81)  Time: 0.682s, 1502.20/s  (0.679s, 1508.63/s)  LR: 8.942e-04  Data: 0.012 (0.017)
Train: 63 [ 600/1251 ( 48%)]  Loss: 4.005 (3.82)  Time: 0.683s, 1498.53/s  (0.679s, 1508.91/s)  LR: 8.941e-04  Data: 0.013 (0.017)
Train: 63 [ 650/1251 ( 52%)]  Loss: 3.935 (3.83)  Time: 0.675s, 1517.49/s  (0.679s, 1509.19/s)  LR: 8.939e-04  Data: 0.014 (0.016)
Train: 63 [ 700/1251 ( 56%)]  Loss: 3.861 (3.83)  Time: 0.675s, 1516.62/s  (0.678s, 1509.71/s)  LR: 8.938e-04  Data: 0.014 (0.016)
Train: 63 [ 750/1251 ( 60%)]  Loss: 3.958 (3.84)  Time: 0.677s, 1512.03/s  (0.678s, 1510.01/s)  LR: 8.937e-04  Data: 0.013 (0.016)
Train: 63 [ 800/1251 ( 64%)]  Loss: 3.666 (3.83)  Time: 0.679s, 1507.74/s  (0.678s, 1510.48/s)  LR: 8.935e-04  Data: 0.014 (0.016)
Train: 63 [ 850/1251 ( 68%)]  Loss: 3.958 (3.84)  Time: 0.672s, 1523.46/s  (0.678s, 1510.57/s)  LR: 8.934e-04  Data: 0.013 (0.016)
Train: 63 [ 900/1251 ( 72%)]  Loss: 3.851 (3.84)  Time: 0.678s, 1510.35/s  (0.678s, 1510.56/s)  LR: 8.933e-04  Data: 0.012 (0.016)
Train: 63 [ 950/1251 ( 76%)]  Loss: 3.613 (3.83)  Time: 0.680s, 1505.72/s  (0.678s, 1510.57/s)  LR: 8.932e-04  Data: 0.011 (0.016)
Train: 63 [1000/1251 ( 80%)]  Loss: 3.693 (3.82)  Time: 0.681s, 1504.21/s  (0.678s, 1510.78/s)  LR: 8.930e-04  Data: 0.015 (0.016)
Train: 63 [1050/1251 ( 84%)]  Loss: 3.672 (3.81)  Time: 0.681s, 1503.10/s  (0.678s, 1511.00/s)  LR: 8.929e-04  Data: 0.014 (0.015)
Train: 63 [1100/1251 ( 88%)]  Loss: 3.561 (3.80)  Time: 0.686s, 1492.66/s  (0.678s, 1510.90/s)  LR: 8.928e-04  Data: 0.014 (0.015)
Train: 63 [1150/1251 ( 92%)]  Loss: 3.650 (3.80)  Time: 0.688s, 1487.79/s  (0.678s, 1510.94/s)  LR: 8.926e-04  Data: 0.015 (0.015)
Train: 63 [1200/1251 ( 96%)]  Loss: 3.608 (3.79)  Time: 0.690s, 1483.74/s  (0.678s, 1510.93/s)  LR: 8.925e-04  Data: 0.014 (0.015)
Train: 63 [1250/1251 (100%)]  Loss: 4.070 (3.80)  Time: 0.667s, 1536.38/s  (0.678s, 1510.93/s)  LR: 8.924e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.054 (3.054)  Loss:  0.5889 (0.5889)  Acc@1: 87.2070 (87.2070)  Acc@5: 97.1680 (97.1680)
Test: [  48/48]  Time: 0.165 (0.334)  Loss:  0.6553 (1.1731)  Acc@1: 84.1981 (72.3120)  Acc@5: 95.7547 (91.3440)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-62.pth.tar', 72.4640000390625)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-63.pth.tar', 72.3120001171875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-60.pth.tar', 72.29800006347656)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-59.pth.tar', 72.19400006835937)

Train: 64 [   0/1251 (  0%)]  Loss: 3.976 (3.98)  Time: 3.287s,  311.56/s  (3.287s,  311.56/s)  LR: 8.924e-04  Data: 1.896 (1.896)
Train: 64 [  50/1251 (  4%)]  Loss: 3.722 (3.85)  Time: 0.679s, 1508.98/s  (0.694s, 1475.37/s)  LR: 8.923e-04  Data: 0.014 (0.051)
Train: 64 [ 100/1251 (  8%)]  Loss: 3.720 (3.81)  Time: 0.674s, 1520.14/s  (0.683s, 1499.23/s)  LR: 8.921e-04  Data: 0.014 (0.033)
Train: 64 [ 150/1251 ( 12%)]  Loss: 3.801 (3.80)  Time: 0.674s, 1520.21/s  (0.680s, 1504.97/s)  LR: 8.920e-04  Data: 0.014 (0.027)
Train: 64 [ 200/1251 ( 16%)]  Loss: 4.044 (3.85)  Time: 0.679s, 1507.16/s  (0.680s, 1506.71/s)  LR: 8.919e-04  Data: 0.016 (0.024)
Train: 64 [ 250/1251 ( 20%)]  Loss: 3.701 (3.83)  Time: 0.683s, 1498.92/s  (0.679s, 1507.75/s)  LR: 8.917e-04  Data: 0.014 (0.022)
Train: 64 [ 300/1251 ( 24%)]  Loss: 4.091 (3.86)  Time: 0.666s, 1536.64/s  (0.679s, 1508.98/s)  LR: 8.916e-04  Data: 0.013 (0.020)
Train: 64 [ 350/1251 ( 28%)]  Loss: 3.922 (3.87)  Time: 0.684s, 1496.47/s  (0.678s, 1509.86/s)  LR: 8.915e-04  Data: 0.015 (0.019)
Train: 64 [ 400/1251 ( 32%)]  Loss: 3.837 (3.87)  Time: 0.686s, 1491.63/s  (0.678s, 1510.15/s)  LR: 8.913e-04  Data: 0.013 (0.019)
Train: 64 [ 450/1251 ( 36%)]  Loss: 4.074 (3.89)  Time: 0.662s, 1546.33/s  (0.678s, 1510.78/s)  LR: 8.912e-04  Data: 0.013 (0.018)
Train: 64 [ 500/1251 ( 40%)]  Loss: 3.550 (3.86)  Time: 0.681s, 1503.86/s  (0.678s, 1510.93/s)  LR: 8.911e-04  Data: 0.016 (0.018)
Train: 64 [ 550/1251 ( 44%)]  Loss: 4.193 (3.89)  Time: 0.687s, 1490.51/s  (0.678s, 1511.22/s)  LR: 8.910e-04  Data: 0.015 (0.017)
Train: 64 [ 600/1251 ( 48%)]  Loss: 3.883 (3.89)  Time: 0.675s, 1516.13/s  (0.678s, 1510.78/s)  LR: 8.908e-04  Data: 0.013 (0.017)
Train: 64 [ 650/1251 ( 52%)]  Loss: 3.837 (3.88)  Time: 0.671s, 1527.11/s  (0.678s, 1510.85/s)  LR: 8.907e-04  Data: 0.014 (0.017)
Train: 64 [ 700/1251 ( 56%)]  Loss: 4.003 (3.89)  Time: 0.679s, 1509.16/s  (0.678s, 1511.09/s)  LR: 8.906e-04  Data: 0.013 (0.017)
Train: 64 [ 750/1251 ( 60%)]  Loss: 3.655 (3.88)  Time: 0.673s, 1521.16/s  (0.677s, 1511.53/s)  LR: 8.904e-04  Data: 0.013 (0.016)
Train: 64 [ 800/1251 ( 64%)]  Loss: 3.615 (3.86)  Time: 0.676s, 1513.70/s  (0.677s, 1511.86/s)  LR: 8.903e-04  Data: 0.015 (0.016)
Train: 64 [ 850/1251 ( 68%)]  Loss: 4.019 (3.87)  Time: 0.677s, 1512.58/s  (0.677s, 1512.00/s)  LR: 8.902e-04  Data: 0.013 (0.016)
Train: 64 [ 900/1251 ( 72%)]  Loss: 3.748 (3.86)  Time: 0.675s, 1517.68/s  (0.677s, 1512.32/s)  LR: 8.900e-04  Data: 0.014 (0.016)
Train: 64 [ 950/1251 ( 76%)]  Loss: 3.830 (3.86)  Time: 0.670s, 1528.06/s  (0.677s, 1512.56/s)  LR: 8.899e-04  Data: 0.013 (0.016)
Train: 64 [1000/1251 ( 80%)]  Loss: 3.781 (3.86)  Time: 0.683s, 1498.90/s  (0.677s, 1512.86/s)  LR: 8.898e-04  Data: 0.013 (0.016)
Train: 64 [1050/1251 ( 84%)]  Loss: 3.914 (3.86)  Time: 0.668s, 1532.46/s  (0.677s, 1513.06/s)  LR: 8.897e-04  Data: 0.013 (0.016)
Train: 64 [1100/1251 ( 88%)]  Loss: 3.637 (3.85)  Time: 0.674s, 1520.06/s  (0.677s, 1513.22/s)  LR: 8.895e-04  Data: 0.015 (0.016)
Train: 64 [1150/1251 ( 92%)]  Loss: 3.874 (3.85)  Time: 0.678s, 1509.64/s  (0.677s, 1513.28/s)  LR: 8.894e-04  Data: 0.012 (0.015)
Train: 64 [1200/1251 ( 96%)]  Loss: 3.945 (3.85)  Time: 0.675s, 1517.95/s  (0.677s, 1513.47/s)  LR: 8.893e-04  Data: 0.017 (0.015)
Train: 64 [1250/1251 (100%)]  Loss: 3.985 (3.86)  Time: 0.664s, 1542.23/s  (0.677s, 1513.64/s)  LR: 8.891e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.716 (2.716)  Loss:  0.6133 (0.6133)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.170 (0.318)  Loss:  0.7119 (1.1991)  Acc@1: 83.9623 (72.6360)  Acc@5: 95.7547 (91.4880)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-64.pth.tar', 72.63600001464843)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-62.pth.tar', 72.4640000390625)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-63.pth.tar', 72.3120001171875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-60.pth.tar', 72.29800006347656)

Train: 65 [   0/1251 (  0%)]  Loss: 4.208 (4.21)  Time: 3.907s,  262.12/s  (3.907s,  262.12/s)  LR: 8.891e-04  Data: 1.982 (1.982)
Train: 65 [  50/1251 (  4%)]  Loss: 3.782 (3.99)  Time: 0.660s, 1551.19/s  (0.701s, 1461.35/s)  LR: 8.890e-04  Data: 0.014 (0.052)
Train: 65 [ 100/1251 (  8%)]  Loss: 3.724 (3.90)  Time: 0.672s, 1523.84/s  (0.685s, 1494.37/s)  LR: 8.889e-04  Data: 0.013 (0.034)
Train: 65 [ 150/1251 ( 12%)]  Loss: 4.001 (3.93)  Time: 0.672s, 1524.59/s  (0.681s, 1503.19/s)  LR: 8.887e-04  Data: 0.014 (0.027)
Train: 65 [ 200/1251 ( 16%)]  Loss: 3.849 (3.91)  Time: 0.676s, 1515.03/s  (0.679s, 1507.30/s)  LR: 8.886e-04  Data: 0.013 (0.024)
Train: 65 [ 250/1251 ( 20%)]  Loss: 3.852 (3.90)  Time: 0.670s, 1529.42/s  (0.678s, 1510.16/s)  LR: 8.885e-04  Data: 0.013 (0.022)
Train: 65 [ 300/1251 ( 24%)]  Loss: 4.017 (3.92)  Time: 0.670s, 1527.37/s  (0.678s, 1511.30/s)  LR: 8.883e-04  Data: 0.014 (0.020)
Train: 65 [ 350/1251 ( 28%)]  Loss: 3.678 (3.89)  Time: 0.673s, 1520.51/s  (0.677s, 1512.60/s)  LR: 8.882e-04  Data: 0.015 (0.019)
Train: 65 [ 400/1251 ( 32%)]  Loss: 4.056 (3.91)  Time: 0.670s, 1528.02/s  (0.677s, 1512.44/s)  LR: 8.881e-04  Data: 0.015 (0.019)
Train: 65 [ 450/1251 ( 36%)]  Loss: 3.570 (3.87)  Time: 0.688s, 1489.05/s  (0.677s, 1512.90/s)  LR: 8.879e-04  Data: 0.015 (0.018)
Train: 65 [ 500/1251 ( 40%)]  Loss: 3.418 (3.83)  Time: 0.676s, 1515.30/s  (0.677s, 1513.29/s)  LR: 8.878e-04  Data: 0.014 (0.018)
Train: 65 [ 550/1251 ( 44%)]  Loss: 3.666 (3.82)  Time: 0.683s, 1499.09/s  (0.677s, 1513.16/s)  LR: 8.877e-04  Data: 0.012 (0.017)
Train: 65 [ 600/1251 ( 48%)]  Loss: 3.806 (3.82)  Time: 0.678s, 1509.42/s  (0.677s, 1513.08/s)  LR: 8.876e-04  Data: 0.015 (0.017)
Train: 65 [ 650/1251 ( 52%)]  Loss: 3.939 (3.83)  Time: 0.671s, 1525.17/s  (0.677s, 1513.11/s)  LR: 8.874e-04  Data: 0.013 (0.017)
Train: 65 [ 700/1251 ( 56%)]  Loss: 3.853 (3.83)  Time: 0.674s, 1518.21/s  (0.677s, 1512.95/s)  LR: 8.873e-04  Data: 0.012 (0.017)
Train: 65 [ 750/1251 ( 60%)]  Loss: 3.529 (3.81)  Time: 0.673s, 1521.79/s  (0.677s, 1512.99/s)  LR: 8.872e-04  Data: 0.017 (0.016)
Train: 65 [ 800/1251 ( 64%)]  Loss: 4.257 (3.84)  Time: 0.677s, 1512.47/s  (0.677s, 1513.11/s)  LR: 8.870e-04  Data: 0.013 (0.016)
Train: 65 [ 850/1251 ( 68%)]  Loss: 3.807 (3.83)  Time: 0.675s, 1515.92/s  (0.677s, 1513.33/s)  LR: 8.869e-04  Data: 0.013 (0.016)
Train: 65 [ 900/1251 ( 72%)]  Loss: 4.196 (3.85)  Time: 0.668s, 1532.52/s  (0.677s, 1513.45/s)  LR: 8.868e-04  Data: 0.013 (0.016)
Train: 65 [ 950/1251 ( 76%)]  Loss: 4.032 (3.86)  Time: 0.681s, 1504.33/s  (0.676s, 1513.70/s)  LR: 8.866e-04  Data: 0.012 (0.016)
Train: 65 [1000/1251 ( 80%)]  Loss: 3.921 (3.86)  Time: 0.673s, 1521.78/s  (0.676s, 1514.13/s)  LR: 8.865e-04  Data: 0.014 (0.016)
Train: 65 [1050/1251 ( 84%)]  Loss: 3.553 (3.85)  Time: 0.676s, 1515.34/s  (0.676s, 1514.47/s)  LR: 8.864e-04  Data: 0.013 (0.016)
Train: 65 [1100/1251 ( 88%)]  Loss: 3.631 (3.84)  Time: 0.680s, 1505.78/s  (0.676s, 1514.65/s)  LR: 8.862e-04  Data: 0.014 (0.016)
Train: 65 [1150/1251 ( 92%)]  Loss: 3.898 (3.84)  Time: 0.672s, 1524.46/s  (0.676s, 1514.80/s)  LR: 8.861e-04  Data: 0.013 (0.015)
Train: 65 [1200/1251 ( 96%)]  Loss: 3.739 (3.84)  Time: 0.676s, 1515.29/s  (0.676s, 1514.90/s)  LR: 8.860e-04  Data: 0.013 (0.015)
Train: 65 [1250/1251 (100%)]  Loss: 3.863 (3.84)  Time: 0.660s, 1550.87/s  (0.676s, 1515.29/s)  LR: 8.858e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.925 (2.925)  Loss:  0.5527 (0.5527)  Acc@1: 88.3789 (88.3789)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.168 (0.326)  Loss:  0.6348 (1.1599)  Acc@1: 84.7877 (72.6080)  Acc@5: 96.8160 (91.4740)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-64.pth.tar', 72.63600001464843)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-65.pth.tar', 72.60800011474609)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-62.pth.tar', 72.4640000390625)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-63.pth.tar', 72.3120001171875)

Train: 66 [   0/1251 (  0%)]  Loss: 3.755 (3.75)  Time: 3.277s,  312.47/s  (3.277s,  312.47/s)  LR: 8.858e-04  Data: 1.806 (1.806)
Train: 66 [  50/1251 (  4%)]  Loss: 3.836 (3.80)  Time: 0.660s, 1551.41/s  (0.689s, 1486.08/s)  LR: 8.857e-04  Data: 0.016 (0.049)
Train: 66 [ 100/1251 (  8%)]  Loss: 3.668 (3.75)  Time: 0.669s, 1529.99/s  (0.679s, 1508.63/s)  LR: 8.856e-04  Data: 0.016 (0.032)
Train: 66 [ 150/1251 ( 12%)]  Loss: 3.821 (3.77)  Time: 0.671s, 1525.09/s  (0.677s, 1512.90/s)  LR: 8.854e-04  Data: 0.012 (0.026)
Train: 66 [ 200/1251 ( 16%)]  Loss: 3.759 (3.77)  Time: 0.682s, 1502.23/s  (0.677s, 1513.56/s)  LR: 8.853e-04  Data: 0.015 (0.023)
Train: 66 [ 250/1251 ( 20%)]  Loss: 3.654 (3.75)  Time: 0.676s, 1514.75/s  (0.677s, 1513.53/s)  LR: 8.852e-04  Data: 0.016 (0.021)
Train: 66 [ 300/1251 ( 24%)]  Loss: 3.959 (3.78)  Time: 0.677s, 1512.60/s  (0.676s, 1513.68/s)  LR: 8.850e-04  Data: 0.013 (0.020)
Train: 66 [ 350/1251 ( 28%)]  Loss: 3.974 (3.80)  Time: 0.669s, 1531.15/s  (0.677s, 1513.54/s)  LR: 8.849e-04  Data: 0.012 (0.019)
Train: 66 [ 400/1251 ( 32%)]  Loss: 3.778 (3.80)  Time: 0.680s, 1505.84/s  (0.677s, 1512.81/s)  LR: 8.848e-04  Data: 0.014 (0.018)
Train: 66 [ 450/1251 ( 36%)]  Loss: 3.970 (3.82)  Time: 0.685s, 1494.09/s  (0.677s, 1512.15/s)  LR: 8.846e-04  Data: 0.014 (0.018)
Train: 66 [ 500/1251 ( 40%)]  Loss: 3.411 (3.78)  Time: 0.671s, 1525.30/s  (0.677s, 1511.84/s)  LR: 8.845e-04  Data: 0.013 (0.017)
Train: 66 [ 550/1251 ( 44%)]  Loss: 3.416 (3.75)  Time: 0.675s, 1516.46/s  (0.677s, 1511.48/s)  LR: 8.844e-04  Data: 0.012 (0.017)
Train: 66 [ 600/1251 ( 48%)]  Loss: 3.646 (3.74)  Time: 0.673s, 1520.80/s  (0.678s, 1511.17/s)  LR: 8.842e-04  Data: 0.018 (0.017)
Train: 66 [ 650/1251 ( 52%)]  Loss: 3.729 (3.74)  Time: 0.681s, 1504.15/s  (0.678s, 1511.15/s)  LR: 8.841e-04  Data: 0.016 (0.017)
Train: 66 [ 700/1251 ( 56%)]  Loss: 3.681 (3.74)  Time: 0.667s, 1534.72/s  (0.677s, 1511.44/s)  LR: 8.840e-04  Data: 0.013 (0.016)
Train: 66 [ 750/1251 ( 60%)]  Loss: 4.117 (3.76)  Time: 0.682s, 1502.32/s  (0.678s, 1511.40/s)  LR: 8.838e-04  Data: 0.013 (0.016)
Train: 66 [ 800/1251 ( 64%)]  Loss: 3.375 (3.74)  Time: 0.679s, 1507.50/s  (0.677s, 1511.45/s)  LR: 8.837e-04  Data: 0.013 (0.016)
Train: 66 [ 850/1251 ( 68%)]  Loss: 3.941 (3.75)  Time: 0.681s, 1503.91/s  (0.677s, 1511.93/s)  LR: 8.836e-04  Data: 0.014 (0.016)
Train: 66 [ 900/1251 ( 72%)]  Loss: 3.869 (3.76)  Time: 0.681s, 1503.57/s  (0.677s, 1512.31/s)  LR: 8.834e-04  Data: 0.013 (0.016)
Train: 66 [ 950/1251 ( 76%)]  Loss: 3.991 (3.77)  Time: 0.673s, 1521.62/s  (0.677s, 1512.79/s)  LR: 8.833e-04  Data: 0.014 (0.016)
Train: 66 [1000/1251 ( 80%)]  Loss: 4.120 (3.78)  Time: 0.689s, 1487.26/s  (0.677s, 1512.86/s)  LR: 8.832e-04  Data: 0.013 (0.016)
Train: 66 [1050/1251 ( 84%)]  Loss: 3.514 (3.77)  Time: 0.672s, 1524.47/s  (0.677s, 1513.22/s)  LR: 8.830e-04  Data: 0.018 (0.016)
Train: 66 [1100/1251 ( 88%)]  Loss: 3.849 (3.78)  Time: 0.674s, 1519.57/s  (0.677s, 1513.48/s)  LR: 8.829e-04  Data: 0.013 (0.016)
Train: 66 [1150/1251 ( 92%)]  Loss: 4.080 (3.79)  Time: 0.671s, 1525.95/s  (0.676s, 1513.93/s)  LR: 8.828e-04  Data: 0.016 (0.015)
Train: 66 [1200/1251 ( 96%)]  Loss: 3.872 (3.79)  Time: 0.683s, 1500.36/s  (0.676s, 1514.43/s)  LR: 8.826e-04  Data: 0.017 (0.015)
Train: 66 [1250/1251 (100%)]  Loss: 3.831 (3.79)  Time: 0.660s, 1550.83/s  (0.676s, 1514.93/s)  LR: 8.825e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.769 (2.769)  Loss:  0.5747 (0.5747)  Acc@1: 87.4023 (87.4023)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.170 (0.323)  Loss:  0.6973 (1.1782)  Acc@1: 83.1368 (72.7040)  Acc@5: 95.6368 (91.7200)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-66.pth.tar', 72.70400017333985)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-64.pth.tar', 72.63600001464843)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-65.pth.tar', 72.60800011474609)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-62.pth.tar', 72.4640000390625)

Train: 67 [   0/1251 (  0%)]  Loss: 3.933 (3.93)  Time: 3.227s,  317.31/s  (3.227s,  317.31/s)  LR: 8.825e-04  Data: 1.822 (1.822)
Train: 67 [  50/1251 (  4%)]  Loss: 4.221 (4.08)  Time: 0.654s, 1564.61/s  (0.690s, 1484.85/s)  LR: 8.824e-04  Data: 0.013 (0.049)
Train: 67 [ 100/1251 (  8%)]  Loss: 4.049 (4.07)  Time: 0.669s, 1530.70/s  (0.677s, 1511.90/s)  LR: 8.822e-04  Data: 0.013 (0.032)
Train: 67 [ 150/1251 ( 12%)]  Loss: 3.899 (4.03)  Time: 0.665s, 1538.97/s  (0.675s, 1517.32/s)  LR: 8.821e-04  Data: 0.013 (0.026)
Train: 67 [ 200/1251 ( 16%)]  Loss: 3.684 (3.96)  Time: 0.674s, 1518.22/s  (0.674s, 1518.57/s)  LR: 8.819e-04  Data: 0.013 (0.023)
Train: 67 [ 250/1251 ( 20%)]  Loss: 3.916 (3.95)  Time: 0.672s, 1523.31/s  (0.674s, 1518.68/s)  LR: 8.818e-04  Data: 0.013 (0.021)
Train: 67 [ 300/1251 ( 24%)]  Loss: 3.533 (3.89)  Time: 0.671s, 1526.63/s  (0.674s, 1518.63/s)  LR: 8.817e-04  Data: 0.014 (0.020)
Train: 67 [ 350/1251 ( 28%)]  Loss: 3.740 (3.87)  Time: 0.663s, 1543.89/s  (0.674s, 1518.99/s)  LR: 8.815e-04  Data: 0.013 (0.019)
Train: 67 [ 400/1251 ( 32%)]  Loss: 3.799 (3.86)  Time: 0.676s, 1514.72/s  (0.674s, 1518.49/s)  LR: 8.814e-04  Data: 0.012 (0.018)
Train: 67 [ 450/1251 ( 36%)]  Loss: 3.803 (3.86)  Time: 0.677s, 1512.11/s  (0.674s, 1518.42/s)  LR: 8.813e-04  Data: 0.015 (0.018)
Train: 67 [ 500/1251 ( 40%)]  Loss: 3.376 (3.81)  Time: 0.672s, 1522.89/s  (0.675s, 1517.88/s)  LR: 8.811e-04  Data: 0.012 (0.018)
Train: 67 [ 550/1251 ( 44%)]  Loss: 3.824 (3.81)  Time: 0.680s, 1506.17/s  (0.675s, 1517.19/s)  LR: 8.810e-04  Data: 0.017 (0.017)
Train: 67 [ 600/1251 ( 48%)]  Loss: 4.161 (3.84)  Time: 0.674s, 1518.23/s  (0.675s, 1516.91/s)  LR: 8.809e-04  Data: 0.014 (0.017)
Train: 67 [ 650/1251 ( 52%)]  Loss: 3.510 (3.82)  Time: 0.673s, 1522.63/s  (0.675s, 1516.91/s)  LR: 8.807e-04  Data: 0.017 (0.017)
Train: 67 [ 700/1251 ( 56%)]  Loss: 3.949 (3.83)  Time: 0.680s, 1505.34/s  (0.675s, 1516.79/s)  LR: 8.806e-04  Data: 0.013 (0.017)
Train: 67 [ 750/1251 ( 60%)]  Loss: 3.513 (3.81)  Time: 0.674s, 1518.66/s  (0.675s, 1516.52/s)  LR: 8.805e-04  Data: 0.014 (0.016)
Train: 67 [ 800/1251 ( 64%)]  Loss: 3.743 (3.80)  Time: 0.666s, 1537.83/s  (0.675s, 1516.57/s)  LR: 8.803e-04  Data: 0.013 (0.016)
Train: 67 [ 850/1251 ( 68%)]  Loss: 3.887 (3.81)  Time: 0.680s, 1505.65/s  (0.675s, 1516.50/s)  LR: 8.802e-04  Data: 0.014 (0.016)
Train: 67 [ 900/1251 ( 72%)]  Loss: 3.718 (3.80)  Time: 0.682s, 1501.68/s  (0.675s, 1516.31/s)  LR: 8.801e-04  Data: 0.013 (0.016)
Train: 67 [ 950/1251 ( 76%)]  Loss: 3.657 (3.80)  Time: 0.672s, 1523.85/s  (0.675s, 1516.45/s)  LR: 8.799e-04  Data: 0.017 (0.016)
Train: 67 [1000/1251 ( 80%)]  Loss: 3.996 (3.81)  Time: 0.676s, 1514.07/s  (0.675s, 1516.73/s)  LR: 8.798e-04  Data: 0.013 (0.016)
Train: 67 [1050/1251 ( 84%)]  Loss: 3.680 (3.80)  Time: 0.681s, 1503.00/s  (0.675s, 1516.71/s)  LR: 8.796e-04  Data: 0.013 (0.016)
Train: 67 [1100/1251 ( 88%)]  Loss: 3.959 (3.81)  Time: 0.666s, 1537.07/s  (0.675s, 1516.87/s)  LR: 8.795e-04  Data: 0.014 (0.016)
Train: 67 [1150/1251 ( 92%)]  Loss: 3.800 (3.81)  Time: 0.671s, 1525.19/s  (0.675s, 1516.82/s)  LR: 8.794e-04  Data: 0.012 (0.015)
Train: 67 [1200/1251 ( 96%)]  Loss: 3.756 (3.80)  Time: 0.681s, 1504.72/s  (0.675s, 1516.71/s)  LR: 8.792e-04  Data: 0.013 (0.015)
Train: 67 [1250/1251 (100%)]  Loss: 3.776 (3.80)  Time: 0.658s, 1555.13/s  (0.675s, 1516.64/s)  LR: 8.791e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.805 (2.805)  Loss:  0.5801 (0.5801)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.171 (0.327)  Loss:  0.6641 (1.1882)  Acc@1: 83.4906 (72.7100)  Acc@5: 96.3443 (91.6500)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-67.pth.tar', 72.71000006835938)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-66.pth.tar', 72.70400017333985)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-64.pth.tar', 72.63600001464843)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-65.pth.tar', 72.60800011474609)

Train: 68 [   0/1251 (  0%)]  Loss: 3.825 (3.83)  Time: 3.405s,  300.70/s  (3.405s,  300.70/s)  LR: 8.791e-04  Data: 1.776 (1.776)
Train: 68 [  50/1251 (  4%)]  Loss: 3.914 (3.87)  Time: 0.661s, 1550.24/s  (0.698s, 1466.40/s)  LR: 8.790e-04  Data: 0.013 (0.048)
Train: 68 [ 100/1251 (  8%)]  Loss: 3.959 (3.90)  Time: 0.678s, 1511.25/s  (0.684s, 1497.04/s)  LR: 8.788e-04  Data: 0.013 (0.031)
Train: 68 [ 150/1251 ( 12%)]  Loss: 3.893 (3.90)  Time: 0.674s, 1518.25/s  (0.681s, 1504.50/s)  LR: 8.787e-04  Data: 0.013 (0.026)
Train: 68 [ 200/1251 ( 16%)]  Loss: 3.636 (3.85)  Time: 0.675s, 1517.30/s  (0.679s, 1508.76/s)  LR: 8.786e-04  Data: 0.013 (0.023)
Train: 68 [ 250/1251 ( 20%)]  Loss: 3.854 (3.85)  Time: 0.661s, 1549.11/s  (0.678s, 1511.04/s)  LR: 8.784e-04  Data: 0.013 (0.021)
Train: 68 [ 300/1251 ( 24%)]  Loss: 3.689 (3.82)  Time: 0.669s, 1531.56/s  (0.676s, 1513.84/s)  LR: 8.783e-04  Data: 0.013 (0.020)
Train: 68 [ 350/1251 ( 28%)]  Loss: 3.732 (3.81)  Time: 0.669s, 1530.27/s  (0.676s, 1515.60/s)  LR: 8.781e-04  Data: 0.012 (0.019)
Train: 68 [ 400/1251 ( 32%)]  Loss: 3.819 (3.81)  Time: 0.673s, 1522.27/s  (0.675s, 1516.82/s)  LR: 8.780e-04  Data: 0.017 (0.018)
Train: 68 [ 450/1251 ( 36%)]  Loss: 4.102 (3.84)  Time: 0.676s, 1513.89/s  (0.675s, 1517.73/s)  LR: 8.779e-04  Data: 0.012 (0.018)
Train: 68 [ 500/1251 ( 40%)]  Loss: 3.676 (3.83)  Time: 0.686s, 1492.53/s  (0.675s, 1517.90/s)  LR: 8.777e-04  Data: 0.013 (0.017)
Train: 68 [ 550/1251 ( 44%)]  Loss: 3.674 (3.81)  Time: 0.671s, 1527.15/s  (0.674s, 1518.19/s)  LR: 8.776e-04  Data: 0.014 (0.017)
Train: 68 [ 600/1251 ( 48%)]  Loss: 3.645 (3.80)  Time: 0.675s, 1517.85/s  (0.674s, 1518.61/s)  LR: 8.775e-04  Data: 0.018 (0.017)
Train: 68 [ 650/1251 ( 52%)]  Loss: 3.578 (3.79)  Time: 0.666s, 1537.82/s  (0.674s, 1518.37/s)  LR: 8.773e-04  Data: 0.012 (0.016)
Train: 68 [ 700/1251 ( 56%)]  Loss: 3.728 (3.78)  Time: 0.678s, 1509.78/s  (0.675s, 1518.04/s)  LR: 8.772e-04  Data: 0.014 (0.016)
Train: 68 [ 750/1251 ( 60%)]  Loss: 3.802 (3.78)  Time: 0.669s, 1530.51/s  (0.675s, 1517.86/s)  LR: 8.771e-04  Data: 0.015 (0.016)
Train: 68 [ 800/1251 ( 64%)]  Loss: 3.700 (3.78)  Time: 0.680s, 1504.98/s  (0.675s, 1517.77/s)  LR: 8.769e-04  Data: 0.014 (0.016)
Train: 68 [ 850/1251 ( 68%)]  Loss: 4.139 (3.80)  Time: 0.671s, 1527.11/s  (0.675s, 1518.09/s)  LR: 8.768e-04  Data: 0.013 (0.016)
Train: 68 [ 900/1251 ( 72%)]  Loss: 3.842 (3.80)  Time: 0.666s, 1537.66/s  (0.675s, 1518.02/s)  LR: 8.766e-04  Data: 0.016 (0.016)
Train: 68 [ 950/1251 ( 76%)]  Loss: 3.564 (3.79)  Time: 0.666s, 1537.20/s  (0.674s, 1518.26/s)  LR: 8.765e-04  Data: 0.015 (0.016)
Train: 68 [1000/1251 ( 80%)]  Loss: 3.970 (3.80)  Time: 0.673s, 1521.66/s  (0.674s, 1518.42/s)  LR: 8.764e-04  Data: 0.014 (0.015)
Train: 68 [1050/1251 ( 84%)]  Loss: 3.414 (3.78)  Time: 0.667s, 1535.68/s  (0.674s, 1518.43/s)  LR: 8.762e-04  Data: 0.016 (0.015)
Train: 68 [1100/1251 ( 88%)]  Loss: 3.706 (3.78)  Time: 0.684s, 1496.88/s  (0.674s, 1518.20/s)  LR: 8.761e-04  Data: 0.013 (0.015)
Train: 68 [1150/1251 ( 92%)]  Loss: 3.914 (3.78)  Time: 0.673s, 1521.50/s  (0.675s, 1518.14/s)  LR: 8.760e-04  Data: 0.015 (0.015)
Train: 68 [1200/1251 ( 96%)]  Loss: 4.048 (3.79)  Time: 0.663s, 1544.46/s  (0.675s, 1518.01/s)  LR: 8.758e-04  Data: 0.014 (0.015)
Train: 68 [1250/1251 (100%)]  Loss: 3.931 (3.80)  Time: 0.656s, 1561.58/s  (0.675s, 1518.10/s)  LR: 8.757e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.832 (2.832)  Loss:  0.6206 (0.6206)  Acc@1: 88.3789 (88.3789)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.169 (0.324)  Loss:  0.6602 (1.1854)  Acc@1: 85.3774 (72.7820)  Acc@5: 96.3443 (91.6720)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-68.pth.tar', 72.78199998291015)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-67.pth.tar', 72.71000006835938)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-66.pth.tar', 72.70400017333985)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-64.pth.tar', 72.63600001464843)

Train: 69 [   0/1251 (  0%)]  Loss: 3.790 (3.79)  Time: 3.461s,  295.83/s  (3.461s,  295.83/s)  LR: 8.757e-04  Data: 1.839 (1.839)
Train: 69 [  50/1251 (  4%)]  Loss: 3.553 (3.67)  Time: 0.662s, 1547.82/s  (0.694s, 1476.32/s)  LR: 8.755e-04  Data: 0.013 (0.050)
Train: 69 [ 100/1251 (  8%)]  Loss: 3.497 (3.61)  Time: 0.667s, 1534.11/s  (0.681s, 1503.64/s)  LR: 8.754e-04  Data: 0.013 (0.032)
Train: 69 [ 150/1251 ( 12%)]  Loss: 3.711 (3.64)  Time: 0.669s, 1529.65/s  (0.678s, 1510.45/s)  LR: 8.753e-04  Data: 0.015 (0.026)
Train: 69 [ 200/1251 ( 16%)]  Loss: 3.961 (3.70)  Time: 0.674s, 1520.28/s  (0.677s, 1513.01/s)  LR: 8.751e-04  Data: 0.013 (0.023)
Train: 69 [ 250/1251 ( 20%)]  Loss: 4.008 (3.75)  Time: 0.685s, 1495.64/s  (0.676s, 1514.46/s)  LR: 8.750e-04  Data: 0.013 (0.021)
Train: 69 [ 300/1251 ( 24%)]  Loss: 3.762 (3.75)  Time: 0.669s, 1529.85/s  (0.676s, 1515.35/s)  LR: 8.749e-04  Data: 0.012 (0.020)
Train: 69 [ 350/1251 ( 28%)]  Loss: 3.999 (3.79)  Time: 0.674s, 1520.12/s  (0.675s, 1516.25/s)  LR: 8.747e-04  Data: 0.014 (0.019)
Train: 69 [ 400/1251 ( 32%)]  Loss: 3.947 (3.80)  Time: 0.667s, 1534.20/s  (0.675s, 1517.29/s)  LR: 8.746e-04  Data: 0.013 (0.018)
Train: 69 [ 450/1251 ( 36%)]  Loss: 3.826 (3.81)  Time: 0.672s, 1523.98/s  (0.675s, 1517.96/s)  LR: 8.744e-04  Data: 0.012 (0.018)
Train: 69 [ 500/1251 ( 40%)]  Loss: 3.735 (3.80)  Time: 0.668s, 1533.09/s  (0.674s, 1518.29/s)  LR: 8.743e-04  Data: 0.013 (0.017)
Train: 69 [ 550/1251 ( 44%)]  Loss: 3.780 (3.80)  Time: 0.671s, 1525.65/s  (0.674s, 1518.24/s)  LR: 8.742e-04  Data: 0.014 (0.017)
Train: 69 [ 600/1251 ( 48%)]  Loss: 3.976 (3.81)  Time: 0.677s, 1512.62/s  (0.674s, 1518.20/s)  LR: 8.740e-04  Data: 0.014 (0.017)
Train: 69 [ 650/1251 ( 52%)]  Loss: 3.652 (3.80)  Time: 0.674s, 1518.34/s  (0.674s, 1518.35/s)  LR: 8.739e-04  Data: 0.017 (0.017)
Train: 69 [ 700/1251 ( 56%)]  Loss: 3.860 (3.80)  Time: 0.676s, 1515.30/s  (0.674s, 1518.20/s)  LR: 8.737e-04  Data: 0.013 (0.016)
Train: 69 [ 750/1251 ( 60%)]  Loss: 3.706 (3.80)  Time: 0.680s, 1506.24/s  (0.674s, 1518.53/s)  LR: 8.736e-04  Data: 0.012 (0.016)
Train: 69 [ 800/1251 ( 64%)]  Loss: 3.457 (3.78)  Time: 0.674s, 1520.32/s  (0.674s, 1518.55/s)  LR: 8.735e-04  Data: 0.014 (0.016)
Train: 69 [ 850/1251 ( 68%)]  Loss: 3.798 (3.78)  Time: 0.685s, 1494.94/s  (0.674s, 1518.38/s)  LR: 8.733e-04  Data: 0.012 (0.016)
Train: 69 [ 900/1251 ( 72%)]  Loss: 4.049 (3.79)  Time: 0.678s, 1509.62/s  (0.674s, 1518.34/s)  LR: 8.732e-04  Data: 0.012 (0.016)
Train: 69 [ 950/1251 ( 76%)]  Loss: 3.653 (3.79)  Time: 0.671s, 1525.28/s  (0.675s, 1518.08/s)  LR: 8.730e-04  Data: 0.013 (0.016)
Train: 69 [1000/1251 ( 80%)]  Loss: 3.989 (3.80)  Time: 0.677s, 1513.05/s  (0.674s, 1518.26/s)  LR: 8.729e-04  Data: 0.013 (0.015)
Train: 69 [1050/1251 ( 84%)]  Loss: 3.703 (3.79)  Time: 0.670s, 1527.24/s  (0.675s, 1518.16/s)  LR: 8.728e-04  Data: 0.014 (0.015)
Train: 69 [1100/1251 ( 88%)]  Loss: 4.048 (3.80)  Time: 0.673s, 1522.39/s  (0.675s, 1518.00/s)  LR: 8.726e-04  Data: 0.013 (0.015)
Train: 69 [1150/1251 ( 92%)]  Loss: 4.149 (3.82)  Time: 0.689s, 1487.21/s  (0.675s, 1517.81/s)  LR: 8.725e-04  Data: 0.013 (0.015)
Train: 69 [1200/1251 ( 96%)]  Loss: 3.554 (3.81)  Time: 0.676s, 1513.87/s  (0.675s, 1517.59/s)  LR: 8.724e-04  Data: 0.015 (0.015)
Train: 69 [1250/1251 (100%)]  Loss: 3.812 (3.81)  Time: 0.658s, 1556.47/s  (0.675s, 1517.61/s)  LR: 8.722e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.745 (2.745)  Loss:  0.6230 (0.6230)  Acc@1: 87.0117 (87.0117)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.167 (0.331)  Loss:  0.6533 (1.1476)  Acc@1: 83.6085 (72.9520)  Acc@5: 95.9906 (91.8380)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-69.pth.tar', 72.95199999023437)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-68.pth.tar', 72.78199998291015)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-67.pth.tar', 72.71000006835938)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-66.pth.tar', 72.70400017333985)

Train: 70 [   0/1251 (  0%)]  Loss: 3.737 (3.74)  Time: 3.427s,  298.83/s  (3.427s,  298.83/s)  LR: 8.722e-04  Data: 1.701 (1.701)
Train: 70 [  50/1251 (  4%)]  Loss: 3.680 (3.71)  Time: 0.665s, 1540.16/s  (0.702s, 1458.18/s)  LR: 8.721e-04  Data: 0.013 (0.047)
Train: 70 [ 100/1251 (  8%)]  Loss: 3.738 (3.72)  Time: 0.672s, 1523.41/s  (0.686s, 1492.34/s)  LR: 8.719e-04  Data: 0.013 (0.031)
Train: 70 [ 150/1251 ( 12%)]  Loss: 3.681 (3.71)  Time: 0.689s, 1487.03/s  (0.683s, 1500.34/s)  LR: 8.718e-04  Data: 0.013 (0.025)
Train: 70 [ 200/1251 ( 16%)]  Loss: 3.647 (3.70)  Time: 0.680s, 1505.88/s  (0.681s, 1503.90/s)  LR: 8.717e-04  Data: 0.013 (0.022)
Train: 70 [ 250/1251 ( 20%)]  Loss: 3.844 (3.72)  Time: 0.688s, 1488.44/s  (0.680s, 1505.85/s)  LR: 8.715e-04  Data: 0.018 (0.021)
Train: 70 [ 300/1251 ( 24%)]  Loss: 3.596 (3.70)  Time: 0.674s, 1519.71/s  (0.680s, 1506.96/s)  LR: 8.714e-04  Data: 0.014 (0.020)
Train: 70 [ 350/1251 ( 28%)]  Loss: 3.991 (3.74)  Time: 0.675s, 1517.67/s  (0.679s, 1507.10/s)  LR: 8.712e-04  Data: 0.012 (0.019)
Train: 70 [ 400/1251 ( 32%)]  Loss: 3.792 (3.75)  Time: 0.675s, 1517.39/s  (0.679s, 1507.71/s)  LR: 8.711e-04  Data: 0.014 (0.018)
Train: 70 [ 450/1251 ( 36%)]  Loss: 3.661 (3.74)  Time: 0.675s, 1516.01/s  (0.679s, 1508.69/s)  LR: 8.710e-04  Data: 0.013 (0.018)
Train: 70 [ 500/1251 ( 40%)]  Loss: 3.958 (3.76)  Time: 0.678s, 1509.78/s  (0.678s, 1509.27/s)  LR: 8.708e-04  Data: 0.012 (0.017)
Train: 70 [ 550/1251 ( 44%)]  Loss: 3.800 (3.76)  Time: 0.681s, 1502.62/s  (0.678s, 1509.64/s)  LR: 8.707e-04  Data: 0.013 (0.017)
Train: 70 [ 600/1251 ( 48%)]  Loss: 3.806 (3.76)  Time: 0.676s, 1515.53/s  (0.678s, 1509.64/s)  LR: 8.705e-04  Data: 0.013 (0.017)
Train: 70 [ 650/1251 ( 52%)]  Loss: 4.143 (3.79)  Time: 0.671s, 1526.88/s  (0.678s, 1509.87/s)  LR: 8.704e-04  Data: 0.013 (0.016)
Train: 70 [ 700/1251 ( 56%)]  Loss: 3.964 (3.80)  Time: 0.671s, 1525.24/s  (0.678s, 1509.78/s)  LR: 8.703e-04  Data: 0.013 (0.016)
Train: 70 [ 750/1251 ( 60%)]  Loss: 3.868 (3.81)  Time: 0.685s, 1494.44/s  (0.678s, 1509.94/s)  LR: 8.701e-04  Data: 0.014 (0.016)
Train: 70 [ 800/1251 ( 64%)]  Loss: 3.582 (3.79)  Time: 0.684s, 1497.61/s  (0.678s, 1509.89/s)  LR: 8.700e-04  Data: 0.016 (0.016)
Train: 70 [ 850/1251 ( 68%)]  Loss: 3.539 (3.78)  Time: 0.672s, 1522.84/s  (0.678s, 1509.61/s)  LR: 8.698e-04  Data: 0.012 (0.016)
Train: 70 [ 900/1251 ( 72%)]  Loss: 4.009 (3.79)  Time: 0.674s, 1519.11/s  (0.678s, 1509.48/s)  LR: 8.697e-04  Data: 0.013 (0.016)
Train: 70 [ 950/1251 ( 76%)]  Loss: 3.836 (3.79)  Time: 0.675s, 1516.07/s  (0.678s, 1509.36/s)  LR: 8.696e-04  Data: 0.015 (0.016)
Train: 70 [1000/1251 ( 80%)]  Loss: 4.018 (3.80)  Time: 0.680s, 1505.67/s  (0.679s, 1509.00/s)  LR: 8.694e-04  Data: 0.015 (0.016)
Train: 70 [1050/1251 ( 84%)]  Loss: 3.552 (3.79)  Time: 0.684s, 1496.44/s  (0.679s, 1508.75/s)  LR: 8.693e-04  Data: 0.013 (0.015)
Train: 70 [1100/1251 ( 88%)]  Loss: 3.517 (3.78)  Time: 0.676s, 1513.72/s  (0.679s, 1508.38/s)  LR: 8.691e-04  Data: 0.013 (0.015)
Train: 70 [1150/1251 ( 92%)]  Loss: 3.834 (3.78)  Time: 0.678s, 1510.43/s  (0.679s, 1508.22/s)  LR: 8.690e-04  Data: 0.012 (0.015)
Train: 70 [1200/1251 ( 96%)]  Loss: 3.943 (3.79)  Time: 0.679s, 1508.80/s  (0.679s, 1508.18/s)  LR: 8.688e-04  Data: 0.014 (0.015)
Train: 70 [1250/1251 (100%)]  Loss: 3.995 (3.80)  Time: 0.663s, 1543.60/s  (0.679s, 1508.16/s)  LR: 8.687e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.845 (2.845)  Loss:  0.5322 (0.5322)  Acc@1: 89.1602 (89.1602)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.171 (0.326)  Loss:  0.6426 (1.1586)  Acc@1: 85.1415 (72.9560)  Acc@5: 96.6981 (91.7500)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-70.pth.tar', 72.95600013916015)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-69.pth.tar', 72.95199999023437)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-68.pth.tar', 72.78199998291015)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-67.pth.tar', 72.71000006835938)

Train: 71 [   0/1251 (  0%)]  Loss: 3.672 (3.67)  Time: 3.378s,  303.10/s  (3.378s,  303.10/s)  LR: 8.687e-04  Data: 1.685 (1.685)
Train: 71 [  50/1251 (  4%)]  Loss: 3.748 (3.71)  Time: 0.663s, 1543.67/s  (0.699s, 1464.65/s)  LR: 8.686e-04  Data: 0.013 (0.047)
Train: 71 [ 100/1251 (  8%)]  Loss: 3.848 (3.76)  Time: 0.681s, 1502.93/s  (0.685s, 1494.31/s)  LR: 8.684e-04  Data: 0.012 (0.030)
Train: 71 [ 150/1251 ( 12%)]  Loss: 3.636 (3.73)  Time: 0.674s, 1520.39/s  (0.682s, 1501.15/s)  LR: 8.683e-04  Data: 0.013 (0.025)
Train: 71 [ 200/1251 ( 16%)]  Loss: 3.748 (3.73)  Time: 0.687s, 1490.27/s  (0.681s, 1503.00/s)  LR: 8.681e-04  Data: 0.013 (0.022)
Train: 71 [ 250/1251 ( 20%)]  Loss: 4.041 (3.78)  Time: 0.674s, 1519.12/s  (0.681s, 1504.10/s)  LR: 8.680e-04  Data: 0.014 (0.021)
Train: 71 [ 300/1251 ( 24%)]  Loss: 4.041 (3.82)  Time: 0.679s, 1508.00/s  (0.681s, 1504.22/s)  LR: 8.679e-04  Data: 0.013 (0.019)
Train: 71 [ 350/1251 ( 28%)]  Loss: 3.732 (3.81)  Time: 0.679s, 1507.11/s  (0.681s, 1504.12/s)  LR: 8.677e-04  Data: 0.013 (0.019)
Train: 71 [ 400/1251 ( 32%)]  Loss: 3.739 (3.80)  Time: 0.690s, 1484.17/s  (0.681s, 1503.58/s)  LR: 8.676e-04  Data: 0.013 (0.018)
Train: 71 [ 450/1251 ( 36%)]  Loss: 4.093 (3.83)  Time: 0.687s, 1489.69/s  (0.681s, 1503.56/s)  LR: 8.674e-04  Data: 0.013 (0.017)
Train: 71 [ 500/1251 ( 40%)]  Loss: 4.003 (3.85)  Time: 0.676s, 1514.86/s  (0.681s, 1504.01/s)  LR: 8.673e-04  Data: 0.013 (0.017)
Train: 71 [ 550/1251 ( 44%)]  Loss: 4.129 (3.87)  Time: 0.687s, 1490.90/s  (0.681s, 1504.09/s)  LR: 8.672e-04  Data: 0.012 (0.017)
Train: 71 [ 600/1251 ( 48%)]  Loss: 3.763 (3.86)  Time: 0.684s, 1497.82/s  (0.681s, 1503.89/s)  LR: 8.670e-04  Data: 0.013 (0.016)
Train: 71 [ 650/1251 ( 52%)]  Loss: 4.144 (3.88)  Time: 0.668s, 1532.71/s  (0.681s, 1504.07/s)  LR: 8.669e-04  Data: 0.013 (0.016)
Train: 71 [ 700/1251 ( 56%)]  Loss: 3.690 (3.87)  Time: 0.681s, 1504.13/s  (0.681s, 1503.68/s)  LR: 8.667e-04  Data: 0.013 (0.016)
Train: 71 [ 750/1251 ( 60%)]  Loss: 3.740 (3.86)  Time: 0.681s, 1502.85/s  (0.681s, 1503.57/s)  LR: 8.666e-04  Data: 0.012 (0.016)
Train: 71 [ 800/1251 ( 64%)]  Loss: 3.660 (3.85)  Time: 0.684s, 1498.07/s  (0.681s, 1503.53/s)  LR: 8.664e-04  Data: 0.013 (0.016)
Train: 71 [ 850/1251 ( 68%)]  Loss: 3.477 (3.83)  Time: 0.680s, 1505.44/s  (0.681s, 1503.24/s)  LR: 8.663e-04  Data: 0.013 (0.015)
Train: 71 [ 900/1251 ( 72%)]  Loss: 4.080 (3.84)  Time: 0.676s, 1514.30/s  (0.681s, 1502.91/s)  LR: 8.662e-04  Data: 0.015 (0.015)
Train: 71 [ 950/1251 ( 76%)]  Loss: 3.676 (3.83)  Time: 0.678s, 1510.58/s  (0.681s, 1502.74/s)  LR: 8.660e-04  Data: 0.012 (0.015)
Train: 71 [1000/1251 ( 80%)]  Loss: 4.269 (3.85)  Time: 0.675s, 1517.86/s  (0.682s, 1502.49/s)  LR: 8.659e-04  Data: 0.015 (0.015)
Train: 71 [1050/1251 ( 84%)]  Loss: 3.962 (3.86)  Time: 0.686s, 1493.50/s  (0.682s, 1502.24/s)  LR: 8.657e-04  Data: 0.013 (0.015)
Train: 71 [1100/1251 ( 88%)]  Loss: 3.799 (3.86)  Time: 0.690s, 1485.02/s  (0.682s, 1501.96/s)  LR: 8.656e-04  Data: 0.013 (0.015)
Train: 71 [1150/1251 ( 92%)]  Loss: 4.034 (3.86)  Time: 0.684s, 1497.30/s  (0.682s, 1501.73/s)  LR: 8.654e-04  Data: 0.012 (0.015)
Train: 71 [1200/1251 ( 96%)]  Loss: 3.799 (3.86)  Time: 0.678s, 1511.27/s  (0.682s, 1501.45/s)  LR: 8.653e-04  Data: 0.013 (0.015)
Train: 71 [1250/1251 (100%)]  Loss: 3.735 (3.86)  Time: 0.678s, 1511.31/s  (0.682s, 1501.42/s)  LR: 8.652e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.653 (2.653)  Loss:  0.5674 (0.5674)  Acc@1: 87.9883 (87.9883)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.175 (0.326)  Loss:  0.6558 (1.1547)  Acc@1: 83.9623 (72.9880)  Acc@5: 95.9906 (91.7900)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-71.pth.tar', 72.98800001464843)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-70.pth.tar', 72.95600013916015)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-69.pth.tar', 72.95199999023437)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-68.pth.tar', 72.78199998291015)

Train: 72 [   0/1251 (  0%)]  Loss: 3.666 (3.67)  Time: 3.845s,  266.29/s  (3.845s,  266.29/s)  LR: 8.652e-04  Data: 1.668 (1.668)
Train: 72 [  50/1251 (  4%)]  Loss: 4.053 (3.86)  Time: 0.672s, 1522.83/s  (0.711s, 1441.09/s)  LR: 8.650e-04  Data: 0.013 (0.046)
Train: 72 [ 100/1251 (  8%)]  Loss: 4.004 (3.91)  Time: 0.673s, 1521.58/s  (0.694s, 1476.09/s)  LR: 8.649e-04  Data: 0.013 (0.030)
Train: 72 [ 150/1251 ( 12%)]  Loss: 3.944 (3.92)  Time: 0.687s, 1491.60/s  (0.689s, 1487.03/s)  LR: 8.647e-04  Data: 0.013 (0.024)
Train: 72 [ 200/1251 ( 16%)]  Loss: 3.616 (3.86)  Time: 0.682s, 1501.96/s  (0.687s, 1490.94/s)  LR: 8.646e-04  Data: 0.013 (0.022)
Train: 72 [ 250/1251 ( 20%)]  Loss: 3.985 (3.88)  Time: 0.687s, 1491.15/s  (0.686s, 1492.14/s)  LR: 8.644e-04  Data: 0.013 (0.020)
Train: 72 [ 300/1251 ( 24%)]  Loss: 3.696 (3.85)  Time: 0.668s, 1532.09/s  (0.685s, 1494.29/s)  LR: 8.643e-04  Data: 0.013 (0.019)
Train: 72 [ 350/1251 ( 28%)]  Loss: 3.753 (3.84)  Time: 0.689s, 1486.61/s  (0.685s, 1495.39/s)  LR: 8.642e-04  Data: 0.013 (0.018)
Train: 72 [ 400/1251 ( 32%)]  Loss: 3.882 (3.84)  Time: 0.683s, 1499.59/s  (0.684s, 1496.50/s)  LR: 8.640e-04  Data: 0.013 (0.018)
Train: 72 [ 450/1251 ( 36%)]  Loss: 3.880 (3.85)  Time: 0.669s, 1530.77/s  (0.684s, 1497.08/s)  LR: 8.639e-04  Data: 0.013 (0.017)
Train: 72 [ 500/1251 ( 40%)]  Loss: 3.850 (3.85)  Time: 0.679s, 1507.07/s  (0.684s, 1497.81/s)  LR: 8.637e-04  Data: 0.016 (0.017)
Train: 72 [ 550/1251 ( 44%)]  Loss: 3.641 (3.83)  Time: 0.681s, 1503.64/s  (0.683s, 1498.51/s)  LR: 8.636e-04  Data: 0.014 (0.016)
Train: 72 [ 600/1251 ( 48%)]  Loss: 3.639 (3.82)  Time: 0.679s, 1507.92/s  (0.683s, 1498.87/s)  LR: 8.634e-04  Data: 0.014 (0.016)
Train: 72 [ 650/1251 ( 52%)]  Loss: 4.106 (3.84)  Time: 0.684s, 1497.16/s  (0.683s, 1499.26/s)  LR: 8.633e-04  Data: 0.014 (0.016)
Train: 72 [ 700/1251 ( 56%)]  Loss: 3.770 (3.83)  Time: 0.682s, 1500.68/s  (0.683s, 1499.73/s)  LR: 8.632e-04  Data: 0.013 (0.016)
Train: 72 [ 750/1251 ( 60%)]  Loss: 3.647 (3.82)  Time: 0.684s, 1496.11/s  (0.683s, 1500.01/s)  LR: 8.630e-04  Data: 0.011 (0.016)
Train: 72 [ 800/1251 ( 64%)]  Loss: 4.055 (3.83)  Time: 0.688s, 1487.35/s  (0.683s, 1500.21/s)  LR: 8.629e-04  Data: 0.014 (0.015)
Train: 72 [ 850/1251 ( 68%)]  Loss: 4.100 (3.85)  Time: 0.684s, 1496.80/s  (0.682s, 1500.43/s)  LR: 8.627e-04  Data: 0.013 (0.015)
Train: 72 [ 900/1251 ( 72%)]  Loss: 3.893 (3.85)  Time: 0.680s, 1506.14/s  (0.682s, 1500.77/s)  LR: 8.626e-04  Data: 0.012 (0.015)
Train: 72 [ 950/1251 ( 76%)]  Loss: 3.709 (3.84)  Time: 0.675s, 1517.22/s  (0.682s, 1500.99/s)  LR: 8.624e-04  Data: 0.013 (0.015)
Train: 72 [1000/1251 ( 80%)]  Loss: 4.029 (3.85)  Time: 0.679s, 1509.04/s  (0.682s, 1501.25/s)  LR: 8.623e-04  Data: 0.013 (0.015)
Train: 72 [1050/1251 ( 84%)]  Loss: 3.405 (3.83)  Time: 0.681s, 1503.42/s  (0.682s, 1501.56/s)  LR: 8.622e-04  Data: 0.014 (0.015)
Train: 72 [1100/1251 ( 88%)]  Loss: 3.527 (3.82)  Time: 0.680s, 1505.52/s  (0.682s, 1501.68/s)  LR: 8.620e-04  Data: 0.013 (0.015)
Train: 72 [1150/1251 ( 92%)]  Loss: 3.782 (3.82)  Time: 0.673s, 1521.21/s  (0.682s, 1501.88/s)  LR: 8.619e-04  Data: 0.013 (0.015)
Train: 72 [1200/1251 ( 96%)]  Loss: 3.875 (3.82)  Time: 0.669s, 1531.62/s  (0.682s, 1502.34/s)  LR: 8.617e-04  Data: 0.013 (0.015)
Train: 72 [1250/1251 (100%)]  Loss: 3.796 (3.82)  Time: 0.670s, 1528.17/s  (0.681s, 1502.63/s)  LR: 8.616e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.827 (2.827)  Loss:  0.5654 (0.5654)  Acc@1: 87.8906 (87.8906)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.173 (0.323)  Loss:  0.6865 (1.1576)  Acc@1: 84.1981 (73.0260)  Acc@5: 96.5802 (91.9100)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-72.pth.tar', 73.02599998779297)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-71.pth.tar', 72.98800001464843)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-70.pth.tar', 72.95600013916015)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-69.pth.tar', 72.95199999023437)

Train: 73 [   0/1251 (  0%)]  Loss: 3.682 (3.68)  Time: 3.590s,  285.21/s  (3.590s,  285.21/s)  LR: 8.616e-04  Data: 1.714 (1.714)
Train: 73 [  50/1251 (  4%)]  Loss: 3.596 (3.64)  Time: 0.667s, 1534.75/s  (0.708s, 1447.22/s)  LR: 8.614e-04  Data: 0.013 (0.047)
Train: 73 [ 100/1251 (  8%)]  Loss: 3.566 (3.61)  Time: 0.672s, 1523.25/s  (0.690s, 1484.46/s)  LR: 8.613e-04  Data: 0.013 (0.031)
Train: 73 [ 150/1251 ( 12%)]  Loss: 3.508 (3.59)  Time: 0.675s, 1518.15/s  (0.685s, 1494.55/s)  LR: 8.611e-04  Data: 0.012 (0.025)
Train: 73 [ 200/1251 ( 16%)]  Loss: 3.826 (3.64)  Time: 0.679s, 1507.40/s  (0.683s, 1498.80/s)  LR: 8.610e-04  Data: 0.012 (0.022)
Train: 73 [ 250/1251 ( 20%)]  Loss: 3.707 (3.65)  Time: 0.690s, 1483.51/s  (0.682s, 1501.43/s)  LR: 8.609e-04  Data: 0.018 (0.020)
Train: 73 [ 300/1251 ( 24%)]  Loss: 4.082 (3.71)  Time: 0.674s, 1519.39/s  (0.682s, 1502.39/s)  LR: 8.607e-04  Data: 0.013 (0.019)
Train: 73 [ 350/1251 ( 28%)]  Loss: 4.013 (3.75)  Time: 0.680s, 1506.80/s  (0.681s, 1502.57/s)  LR: 8.606e-04  Data: 0.018 (0.018)
Train: 73 [ 400/1251 ( 32%)]  Loss: 3.868 (3.76)  Time: 0.673s, 1520.49/s  (0.681s, 1503.15/s)  LR: 8.604e-04  Data: 0.013 (0.018)
Train: 73 [ 450/1251 ( 36%)]  Loss: 3.656 (3.75)  Time: 0.678s, 1509.38/s  (0.681s, 1503.56/s)  LR: 8.603e-04  Data: 0.017 (0.017)
Train: 73 [ 500/1251 ( 40%)]  Loss: 3.766 (3.75)  Time: 0.682s, 1501.16/s  (0.681s, 1503.57/s)  LR: 8.601e-04  Data: 0.013 (0.017)
Train: 73 [ 550/1251 ( 44%)]  Loss: 3.764 (3.75)  Time: 0.688s, 1487.67/s  (0.681s, 1503.75/s)  LR: 8.600e-04  Data: 0.013 (0.017)
Train: 73 [ 600/1251 ( 48%)]  Loss: 3.649 (3.74)  Time: 0.684s, 1497.34/s  (0.681s, 1503.32/s)  LR: 8.598e-04  Data: 0.013 (0.016)
Train: 73 [ 650/1251 ( 52%)]  Loss: 3.756 (3.75)  Time: 0.679s, 1507.26/s  (0.681s, 1503.41/s)  LR: 8.597e-04  Data: 0.014 (0.016)
Train: 73 [ 700/1251 ( 56%)]  Loss: 3.543 (3.73)  Time: 0.690s, 1483.37/s  (0.681s, 1503.39/s)  LR: 8.595e-04  Data: 0.013 (0.016)
Train: 73 [ 750/1251 ( 60%)]  Loss: 3.794 (3.74)  Time: 0.695s, 1472.74/s  (0.681s, 1503.24/s)  LR: 8.594e-04  Data: 0.012 (0.016)
Train: 73 [ 800/1251 ( 64%)]  Loss: 3.835 (3.74)  Time: 0.684s, 1496.70/s  (0.681s, 1503.11/s)  LR: 8.593e-04  Data: 0.014 (0.016)
Train: 73 [ 850/1251 ( 68%)]  Loss: 3.652 (3.74)  Time: 0.685s, 1494.07/s  (0.681s, 1503.33/s)  LR: 8.591e-04  Data: 0.017 (0.016)
Train: 73 [ 900/1251 ( 72%)]  Loss: 3.952 (3.75)  Time: 0.684s, 1497.13/s  (0.681s, 1503.17/s)  LR: 8.590e-04  Data: 0.015 (0.015)
Train: 73 [ 950/1251 ( 76%)]  Loss: 3.908 (3.76)  Time: 0.677s, 1511.96/s  (0.681s, 1503.10/s)  LR: 8.588e-04  Data: 0.012 (0.015)
Train: 73 [1000/1251 ( 80%)]  Loss: 3.478 (3.74)  Time: 0.680s, 1505.68/s  (0.681s, 1503.31/s)  LR: 8.587e-04  Data: 0.014 (0.015)
Train: 73 [1050/1251 ( 84%)]  Loss: 3.852 (3.75)  Time: 0.681s, 1504.56/s  (0.681s, 1503.55/s)  LR: 8.585e-04  Data: 0.012 (0.015)
Train: 73 [1100/1251 ( 88%)]  Loss: 3.599 (3.74)  Time: 0.682s, 1500.92/s  (0.681s, 1503.72/s)  LR: 8.584e-04  Data: 0.012 (0.015)
Train: 73 [1150/1251 ( 92%)]  Loss: 3.616 (3.74)  Time: 0.673s, 1521.57/s  (0.681s, 1503.93/s)  LR: 8.582e-04  Data: 0.015 (0.015)
Train: 73 [1200/1251 ( 96%)]  Loss: 3.662 (3.73)  Time: 0.685s, 1495.93/s  (0.681s, 1504.08/s)  LR: 8.581e-04  Data: 0.013 (0.015)
Train: 73 [1250/1251 (100%)]  Loss: 3.847 (3.74)  Time: 0.669s, 1529.83/s  (0.681s, 1504.39/s)  LR: 8.580e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.728 (2.728)  Loss:  0.5347 (0.5347)  Acc@1: 88.0859 (88.0859)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.171 (0.332)  Loss:  0.6606 (1.1451)  Acc@1: 83.7264 (72.9460)  Acc@5: 96.5802 (91.7020)
Train: 74 [   0/1251 (  0%)]  Loss: 3.979 (3.98)  Time: 3.498s,  292.78/s  (3.498s,  292.78/s)  LR: 8.579e-04  Data: 1.926 (1.926)
Train: 74 [  50/1251 (  4%)]  Loss: 3.798 (3.89)  Time: 0.668s, 1531.88/s  (0.698s, 1466.27/s)  LR: 8.578e-04  Data: 0.016 (0.051)
Train: 74 [ 100/1251 (  8%)]  Loss: 3.482 (3.75)  Time: 0.671s, 1526.43/s  (0.685s, 1494.85/s)  LR: 8.577e-04  Data: 0.012 (0.033)
Train: 74 [ 150/1251 ( 12%)]  Loss: 3.612 (3.72)  Time: 0.675s, 1516.60/s  (0.682s, 1502.01/s)  LR: 8.575e-04  Data: 0.013 (0.026)
Train: 74 [ 200/1251 ( 16%)]  Loss: 3.699 (3.71)  Time: 0.680s, 1505.53/s  (0.681s, 1503.69/s)  LR: 8.574e-04  Data: 0.019 (0.023)
Train: 74 [ 250/1251 ( 20%)]  Loss: 3.947 (3.75)  Time: 0.688s, 1488.76/s  (0.681s, 1504.12/s)  LR: 8.572e-04  Data: 0.015 (0.021)
Train: 74 [ 300/1251 ( 24%)]  Loss: 3.679 (3.74)  Time: 0.688s, 1488.57/s  (0.681s, 1504.20/s)  LR: 8.571e-04  Data: 0.012 (0.020)
Train: 74 [ 350/1251 ( 28%)]  Loss: 3.554 (3.72)  Time: 0.682s, 1501.38/s  (0.680s, 1504.90/s)  LR: 8.569e-04  Data: 0.012 (0.019)
Train: 74 [ 400/1251 ( 32%)]  Loss: 3.518 (3.70)  Time: 0.673s, 1520.78/s  (0.680s, 1505.06/s)  LR: 8.568e-04  Data: 0.013 (0.018)
Train: 74 [ 450/1251 ( 36%)]  Loss: 3.748 (3.70)  Time: 0.678s, 1510.87/s  (0.680s, 1505.59/s)  LR: 8.566e-04  Data: 0.015 (0.018)
Train: 74 [ 500/1251 ( 40%)]  Loss: 4.043 (3.73)  Time: 0.676s, 1514.79/s  (0.680s, 1506.05/s)  LR: 8.565e-04  Data: 0.013 (0.017)
Train: 74 [ 550/1251 ( 44%)]  Loss: 3.922 (3.75)  Time: 0.674s, 1518.26/s  (0.680s, 1506.15/s)  LR: 8.563e-04  Data: 0.013 (0.017)
Train: 74 [ 600/1251 ( 48%)]  Loss: 3.668 (3.74)  Time: 0.681s, 1503.17/s  (0.680s, 1506.42/s)  LR: 8.562e-04  Data: 0.013 (0.017)
Train: 74 [ 650/1251 ( 52%)]  Loss: 3.816 (3.75)  Time: 0.684s, 1497.66/s  (0.680s, 1506.22/s)  LR: 8.560e-04  Data: 0.013 (0.017)
Train: 74 [ 700/1251 ( 56%)]  Loss: 3.853 (3.75)  Time: 0.683s, 1499.52/s  (0.680s, 1505.73/s)  LR: 8.559e-04  Data: 0.013 (0.016)
Train: 74 [ 750/1251 ( 60%)]  Loss: 3.671 (3.75)  Time: 0.687s, 1490.40/s  (0.680s, 1505.22/s)  LR: 8.558e-04  Data: 0.015 (0.016)
Train: 74 [ 800/1251 ( 64%)]  Loss: 3.673 (3.74)  Time: 0.678s, 1510.25/s  (0.681s, 1504.77/s)  LR: 8.556e-04  Data: 0.013 (0.016)
Train: 74 [ 850/1251 ( 68%)]  Loss: 3.770 (3.75)  Time: 0.682s, 1501.62/s  (0.681s, 1504.62/s)  LR: 8.555e-04  Data: 0.013 (0.016)
Train: 74 [ 900/1251 ( 72%)]  Loss: 3.566 (3.74)  Time: 0.679s, 1507.50/s  (0.681s, 1504.68/s)  LR: 8.553e-04  Data: 0.012 (0.016)
Train: 74 [ 950/1251 ( 76%)]  Loss: 3.678 (3.73)  Time: 0.689s, 1486.52/s  (0.681s, 1504.60/s)  LR: 8.552e-04  Data: 0.012 (0.016)
Train: 74 [1000/1251 ( 80%)]  Loss: 3.729 (3.73)  Time: 0.678s, 1510.14/s  (0.681s, 1504.57/s)  LR: 8.550e-04  Data: 0.015 (0.015)
Train: 74 [1050/1251 ( 84%)]  Loss: 3.874 (3.74)  Time: 0.687s, 1490.62/s  (0.681s, 1504.67/s)  LR: 8.549e-04  Data: 0.012 (0.015)
Train: 74 [1100/1251 ( 88%)]  Loss: 3.689 (3.74)  Time: 0.677s, 1513.33/s  (0.681s, 1504.59/s)  LR: 8.547e-04  Data: 0.013 (0.015)
Train: 74 [1150/1251 ( 92%)]  Loss: 3.714 (3.74)  Time: 0.688s, 1488.92/s  (0.681s, 1504.38/s)  LR: 8.546e-04  Data: 0.012 (0.015)
Train: 74 [1200/1251 ( 96%)]  Loss: 3.684 (3.73)  Time: 0.672s, 1524.34/s  (0.681s, 1504.51/s)  LR: 8.544e-04  Data: 0.015 (0.015)
Train: 74 [1250/1251 (100%)]  Loss: 3.780 (3.74)  Time: 0.679s, 1507.60/s  (0.681s, 1504.53/s)  LR: 8.543e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.777 (2.777)  Loss:  0.5981 (0.5981)  Acc@1: 87.5977 (87.5977)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.170 (0.320)  Loss:  0.6660 (1.1593)  Acc@1: 83.9623 (73.0460)  Acc@5: 96.5802 (91.9660)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-74.pth.tar', 73.04600001464844)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-72.pth.tar', 73.02599998779297)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-71.pth.tar', 72.98800001464843)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-70.pth.tar', 72.95600013916015)

Train: 75 [   0/1251 (  0%)]  Loss: 3.872 (3.87)  Time: 3.718s,  275.41/s  (3.718s,  275.41/s)  LR: 8.543e-04  Data: 1.903 (1.903)
Train: 75 [  50/1251 (  4%)]  Loss: 3.885 (3.88)  Time: 0.672s, 1524.44/s  (0.709s, 1444.90/s)  LR: 8.541e-04  Data: 0.013 (0.051)
Train: 75 [ 100/1251 (  8%)]  Loss: 3.754 (3.84)  Time: 0.678s, 1510.83/s  (0.691s, 1481.39/s)  LR: 8.540e-04  Data: 0.013 (0.033)
Train: 75 [ 150/1251 ( 12%)]  Loss: 3.514 (3.76)  Time: 0.690s, 1484.67/s  (0.686s, 1491.81/s)  LR: 8.538e-04  Data: 0.012 (0.027)
Train: 75 [ 200/1251 ( 16%)]  Loss: 3.642 (3.73)  Time: 0.679s, 1507.04/s  (0.685s, 1495.76/s)  LR: 8.537e-04  Data: 0.012 (0.023)
Train: 75 [ 250/1251 ( 20%)]  Loss: 3.973 (3.77)  Time: 0.677s, 1511.73/s  (0.683s, 1499.12/s)  LR: 8.535e-04  Data: 0.013 (0.021)
Train: 75 [ 300/1251 ( 24%)]  Loss: 3.610 (3.75)  Time: 0.681s, 1502.59/s  (0.682s, 1501.20/s)  LR: 8.534e-04  Data: 0.014 (0.020)
Train: 75 [ 350/1251 ( 28%)]  Loss: 3.743 (3.75)  Time: 0.672s, 1524.64/s  (0.681s, 1502.89/s)  LR: 8.533e-04  Data: 0.014 (0.019)
Train: 75 [ 400/1251 ( 32%)]  Loss: 3.283 (3.70)  Time: 0.682s, 1502.46/s  (0.681s, 1503.85/s)  LR: 8.531e-04  Data: 0.013 (0.018)
Train: 75 [ 450/1251 ( 36%)]  Loss: 3.847 (3.71)  Time: 0.669s, 1530.48/s  (0.681s, 1504.51/s)  LR: 8.530e-04  Data: 0.013 (0.018)
Train: 75 [ 500/1251 ( 40%)]  Loss: 3.849 (3.72)  Time: 0.684s, 1496.81/s  (0.680s, 1505.41/s)  LR: 8.528e-04  Data: 0.014 (0.018)
Train: 75 [ 550/1251 ( 44%)]  Loss: 3.501 (3.71)  Time: 0.678s, 1509.39/s  (0.680s, 1505.78/s)  LR: 8.527e-04  Data: 0.014 (0.017)
Train: 75 [ 600/1251 ( 48%)]  Loss: 3.473 (3.69)  Time: 0.681s, 1504.50/s  (0.680s, 1506.69/s)  LR: 8.525e-04  Data: 0.015 (0.017)
Train: 75 [ 650/1251 ( 52%)]  Loss: 3.499 (3.67)  Time: 0.670s, 1527.65/s  (0.679s, 1507.37/s)  LR: 8.524e-04  Data: 0.013 (0.017)
Train: 75 [ 700/1251 ( 56%)]  Loss: 3.772 (3.68)  Time: 0.679s, 1509.16/s  (0.679s, 1507.74/s)  LR: 8.522e-04  Data: 0.014 (0.016)
Train: 75 [ 750/1251 ( 60%)]  Loss: 3.851 (3.69)  Time: 0.671s, 1525.88/s  (0.679s, 1508.10/s)  LR: 8.521e-04  Data: 0.013 (0.016)
Train: 75 [ 800/1251 ( 64%)]  Loss: 3.384 (3.67)  Time: 0.680s, 1505.73/s  (0.679s, 1508.58/s)  LR: 8.519e-04  Data: 0.013 (0.016)
Train: 75 [ 850/1251 ( 68%)]  Loss: 3.786 (3.68)  Time: 0.675s, 1517.86/s  (0.679s, 1508.85/s)  LR: 8.518e-04  Data: 0.013 (0.016)
Train: 75 [ 900/1251 ( 72%)]  Loss: 3.888 (3.69)  Time: 0.678s, 1510.77/s  (0.679s, 1509.06/s)  LR: 8.516e-04  Data: 0.012 (0.016)
Train: 75 [ 950/1251 ( 76%)]  Loss: 3.711 (3.69)  Time: 0.681s, 1503.69/s  (0.678s, 1509.41/s)  LR: 8.515e-04  Data: 0.012 (0.016)
Train: 75 [1000/1251 ( 80%)]  Loss: 3.907 (3.70)  Time: 0.685s, 1494.85/s  (0.678s, 1509.51/s)  LR: 8.513e-04  Data: 0.017 (0.016)
Train: 75 [1050/1251 ( 84%)]  Loss: 3.500 (3.69)  Time: 0.685s, 1494.40/s  (0.678s, 1509.65/s)  LR: 8.512e-04  Data: 0.013 (0.015)
Train: 75 [1100/1251 ( 88%)]  Loss: 3.522 (3.69)  Time: 0.680s, 1504.83/s  (0.678s, 1509.82/s)  LR: 8.510e-04  Data: 0.013 (0.015)
Train: 75 [1150/1251 ( 92%)]  Loss: 3.926 (3.70)  Time: 0.668s, 1532.28/s  (0.678s, 1509.90/s)  LR: 8.509e-04  Data: 0.014 (0.015)
Train: 75 [1200/1251 ( 96%)]  Loss: 3.788 (3.70)  Time: 0.682s, 1501.95/s  (0.678s, 1510.03/s)  LR: 8.507e-04  Data: 0.015 (0.015)
Train: 75 [1250/1251 (100%)]  Loss: 3.667 (3.70)  Time: 0.658s, 1555.80/s  (0.678s, 1510.06/s)  LR: 8.506e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.974 (2.974)  Loss:  0.5840 (0.5840)  Acc@1: 88.9648 (88.9648)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.171 (0.319)  Loss:  0.6973 (1.1654)  Acc@1: 84.0802 (73.1160)  Acc@5: 96.2264 (91.8820)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-75.pth.tar', 73.11600006591797)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-74.pth.tar', 73.04600001464844)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-72.pth.tar', 73.02599998779297)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-71.pth.tar', 72.98800001464843)

Train: 76 [   0/1251 (  0%)]  Loss: 3.457 (3.46)  Time: 3.476s,  294.62/s  (3.476s,  294.62/s)  LR: 8.506e-04  Data: 1.570 (1.570)
Train: 76 [  50/1251 (  4%)]  Loss: 3.750 (3.60)  Time: 0.662s, 1547.60/s  (0.699s, 1464.04/s)  LR: 8.504e-04  Data: 0.013 (0.044)
Train: 76 [ 100/1251 (  8%)]  Loss: 3.088 (3.43)  Time: 0.684s, 1497.01/s  (0.686s, 1492.94/s)  LR: 8.503e-04  Data: 0.012 (0.029)
Train: 76 [ 150/1251 ( 12%)]  Loss: 3.798 (3.52)  Time: 0.689s, 1487.06/s  (0.683s, 1499.62/s)  LR: 8.501e-04  Data: 0.012 (0.024)
Train: 76 [ 200/1251 ( 16%)]  Loss: 3.643 (3.55)  Time: 0.668s, 1533.07/s  (0.681s, 1503.32/s)  LR: 8.500e-04  Data: 0.013 (0.021)
Train: 76 [ 250/1251 ( 20%)]  Loss: 3.670 (3.57)  Time: 0.674s, 1519.95/s  (0.680s, 1505.87/s)  LR: 8.498e-04  Data: 0.013 (0.020)
Train: 76 [ 300/1251 ( 24%)]  Loss: 3.949 (3.62)  Time: 0.675s, 1517.29/s  (0.679s, 1507.70/s)  LR: 8.497e-04  Data: 0.014 (0.019)
Train: 76 [ 350/1251 ( 28%)]  Loss: 3.852 (3.65)  Time: 0.674s, 1520.19/s  (0.679s, 1508.38/s)  LR: 8.495e-04  Data: 0.017 (0.018)
Train: 76 [ 400/1251 ( 32%)]  Loss: 3.590 (3.64)  Time: 0.672s, 1522.92/s  (0.679s, 1508.88/s)  LR: 8.494e-04  Data: 0.013 (0.018)
Train: 76 [ 450/1251 ( 36%)]  Loss: 3.732 (3.65)  Time: 0.680s, 1506.38/s  (0.679s, 1508.91/s)  LR: 8.492e-04  Data: 0.012 (0.017)
Train: 76 [ 500/1251 ( 40%)]  Loss: 3.712 (3.66)  Time: 0.685s, 1494.63/s  (0.679s, 1508.89/s)  LR: 8.491e-04  Data: 0.013 (0.017)
Train: 76 [ 550/1251 ( 44%)]  Loss: 3.402 (3.64)  Time: 0.682s, 1501.80/s  (0.679s, 1508.77/s)  LR: 8.489e-04  Data: 0.014 (0.017)
Train: 76 [ 600/1251 ( 48%)]  Loss: 3.981 (3.66)  Time: 0.673s, 1521.46/s  (0.679s, 1508.88/s)  LR: 8.488e-04  Data: 0.013 (0.016)
Train: 76 [ 650/1251 ( 52%)]  Loss: 3.589 (3.66)  Time: 0.681s, 1502.94/s  (0.679s, 1508.54/s)  LR: 8.486e-04  Data: 0.013 (0.016)
Train: 76 [ 700/1251 ( 56%)]  Loss: 3.799 (3.67)  Time: 0.678s, 1510.12/s  (0.679s, 1508.52/s)  LR: 8.485e-04  Data: 0.012 (0.016)
Train: 76 [ 750/1251 ( 60%)]  Loss: 3.898 (3.68)  Time: 0.675s, 1516.58/s  (0.679s, 1508.75/s)  LR: 8.483e-04  Data: 0.014 (0.016)
Train: 76 [ 800/1251 ( 64%)]  Loss: 3.581 (3.68)  Time: 0.682s, 1501.17/s  (0.679s, 1508.82/s)  LR: 8.482e-04  Data: 0.013 (0.016)
Train: 76 [ 850/1251 ( 68%)]  Loss: 3.830 (3.68)  Time: 0.687s, 1489.52/s  (0.679s, 1509.01/s)  LR: 8.480e-04  Data: 0.013 (0.016)
Train: 76 [ 900/1251 ( 72%)]  Loss: 3.780 (3.69)  Time: 0.670s, 1528.63/s  (0.679s, 1508.93/s)  LR: 8.479e-04  Data: 0.014 (0.016)
Train: 76 [ 950/1251 ( 76%)]  Loss: 3.744 (3.69)  Time: 0.677s, 1513.07/s  (0.679s, 1509.20/s)  LR: 8.477e-04  Data: 0.014 (0.015)
Train: 76 [1000/1251 ( 80%)]  Loss: 3.682 (3.69)  Time: 0.679s, 1507.32/s  (0.678s, 1509.36/s)  LR: 8.476e-04  Data: 0.020 (0.015)
Train: 76 [1050/1251 ( 84%)]  Loss: 4.021 (3.71)  Time: 0.685s, 1494.09/s  (0.678s, 1509.50/s)  LR: 8.474e-04  Data: 0.016 (0.015)
Train: 76 [1100/1251 ( 88%)]  Loss: 3.562 (3.70)  Time: 0.681s, 1504.41/s  (0.678s, 1509.69/s)  LR: 8.473e-04  Data: 0.014 (0.015)
Train: 76 [1150/1251 ( 92%)]  Loss: 3.735 (3.70)  Time: 0.682s, 1501.87/s  (0.678s, 1510.05/s)  LR: 8.471e-04  Data: 0.013 (0.015)
Train: 76 [1200/1251 ( 96%)]  Loss: 3.874 (3.71)  Time: 0.676s, 1515.66/s  (0.678s, 1510.19/s)  LR: 8.470e-04  Data: 0.014 (0.015)
Train: 76 [1250/1251 (100%)]  Loss: 3.726 (3.71)  Time: 0.674s, 1518.91/s  (0.678s, 1510.35/s)  LR: 8.468e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.780 (2.780)  Loss:  0.5723 (0.5723)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.172 (0.319)  Loss:  0.6831 (1.1513)  Acc@1: 85.1415 (73.0620)  Acc@5: 96.4623 (91.9640)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-75.pth.tar', 73.11600006591797)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-76.pth.tar', 73.06200013916016)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-74.pth.tar', 73.04600001464844)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-72.pth.tar', 73.02599998779297)

Train: 77 [   0/1251 (  0%)]  Loss: 3.620 (3.62)  Time: 3.850s,  265.99/s  (3.850s,  265.99/s)  LR: 8.468e-04  Data: 1.723 (1.723)
Train: 77 [  50/1251 (  4%)]  Loss: 3.702 (3.66)  Time: 0.662s, 1546.98/s  (0.702s, 1459.23/s)  LR: 8.467e-04  Data: 0.013 (0.047)
Train: 77 [ 100/1251 (  8%)]  Loss: 3.430 (3.58)  Time: 0.675s, 1517.93/s  (0.687s, 1491.24/s)  LR: 8.465e-04  Data: 0.013 (0.031)
Train: 77 [ 150/1251 ( 12%)]  Loss: 3.790 (3.64)  Time: 0.677s, 1511.99/s  (0.684s, 1498.12/s)  LR: 8.464e-04  Data: 0.012 (0.025)
Train: 77 [ 200/1251 ( 16%)]  Loss: 3.452 (3.60)  Time: 0.684s, 1496.45/s  (0.682s, 1500.78/s)  LR: 8.462e-04  Data: 0.013 (0.022)
Train: 77 [ 250/1251 ( 20%)]  Loss: 3.898 (3.65)  Time: 0.674s, 1518.73/s  (0.682s, 1502.12/s)  LR: 8.461e-04  Data: 0.013 (0.020)
Train: 77 [ 300/1251 ( 24%)]  Loss: 3.365 (3.61)  Time: 0.670s, 1528.42/s  (0.681s, 1503.83/s)  LR: 8.459e-04  Data: 0.013 (0.019)
Train: 77 [ 350/1251 ( 28%)]  Loss: 3.973 (3.65)  Time: 0.681s, 1502.84/s  (0.680s, 1505.32/s)  LR: 8.458e-04  Data: 0.012 (0.019)
Train: 77 [ 400/1251 ( 32%)]  Loss: 3.590 (3.65)  Time: 0.679s, 1508.13/s  (0.680s, 1506.12/s)  LR: 8.456e-04  Data: 0.012 (0.018)
Train: 77 [ 450/1251 ( 36%)]  Loss: 3.773 (3.66)  Time: 0.679s, 1507.25/s  (0.680s, 1506.77/s)  LR: 8.455e-04  Data: 0.014 (0.017)
Train: 77 [ 500/1251 ( 40%)]  Loss: 3.875 (3.68)  Time: 0.670s, 1527.40/s  (0.679s, 1507.49/s)  LR: 8.453e-04  Data: 0.015 (0.017)
Train: 77 [ 550/1251 ( 44%)]  Loss: 3.900 (3.70)  Time: 0.673s, 1520.98/s  (0.679s, 1508.01/s)  LR: 8.452e-04  Data: 0.017 (0.017)
Train: 77 [ 600/1251 ( 48%)]  Loss: 3.557 (3.69)  Time: 0.671s, 1525.52/s  (0.679s, 1508.70/s)  LR: 8.450e-04  Data: 0.012 (0.016)
Train: 77 [ 650/1251 ( 52%)]  Loss: 3.839 (3.70)  Time: 0.672s, 1523.53/s  (0.679s, 1509.15/s)  LR: 8.449e-04  Data: 0.013 (0.016)
Train: 77 [ 700/1251 ( 56%)]  Loss: 3.435 (3.68)  Time: 0.684s, 1496.08/s  (0.679s, 1509.19/s)  LR: 8.447e-04  Data: 0.012 (0.016)
Train: 77 [ 750/1251 ( 60%)]  Loss: 3.757 (3.68)  Time: 0.683s, 1500.31/s  (0.679s, 1509.05/s)  LR: 8.446e-04  Data: 0.014 (0.016)
Train: 77 [ 800/1251 ( 64%)]  Loss: 3.610 (3.68)  Time: 0.683s, 1499.39/s  (0.679s, 1508.84/s)  LR: 8.444e-04  Data: 0.014 (0.016)
Train: 77 [ 850/1251 ( 68%)]  Loss: 3.749 (3.68)  Time: 0.682s, 1501.56/s  (0.679s, 1508.53/s)  LR: 8.443e-04  Data: 0.013 (0.016)
Train: 77 [ 900/1251 ( 72%)]  Loss: 3.780 (3.69)  Time: 0.673s, 1521.57/s  (0.679s, 1508.65/s)  LR: 8.441e-04  Data: 0.016 (0.015)
Train: 77 [ 950/1251 ( 76%)]  Loss: 3.290 (3.67)  Time: 0.687s, 1489.54/s  (0.679s, 1508.60/s)  LR: 8.440e-04  Data: 0.012 (0.015)
Train: 77 [1000/1251 ( 80%)]  Loss: 4.039 (3.69)  Time: 0.671s, 1526.53/s  (0.679s, 1508.49/s)  LR: 8.438e-04  Data: 0.014 (0.015)
Train: 77 [1050/1251 ( 84%)]  Loss: 3.850 (3.69)  Time: 0.677s, 1513.37/s  (0.679s, 1508.43/s)  LR: 8.437e-04  Data: 0.013 (0.015)
Train: 77 [1100/1251 ( 88%)]  Loss: 3.839 (3.70)  Time: 0.683s, 1498.41/s  (0.679s, 1508.34/s)  LR: 8.435e-04  Data: 0.013 (0.015)
Train: 77 [1150/1251 ( 92%)]  Loss: 3.661 (3.70)  Time: 0.679s, 1508.12/s  (0.679s, 1508.11/s)  LR: 8.434e-04  Data: 0.018 (0.015)
Train: 77 [1200/1251 ( 96%)]  Loss: 3.318 (3.68)  Time: 0.686s, 1493.61/s  (0.679s, 1507.73/s)  LR: 8.432e-04  Data: 0.014 (0.015)
Train: 77 [1250/1251 (100%)]  Loss: 3.744 (3.69)  Time: 0.667s, 1534.17/s  (0.679s, 1507.62/s)  LR: 8.431e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.789 (2.789)  Loss:  0.5923 (0.5923)  Acc@1: 87.1094 (87.1094)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.165 (0.327)  Loss:  0.6196 (1.1443)  Acc@1: 85.1415 (73.1140)  Acc@5: 96.4623 (91.9120)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-75.pth.tar', 73.11600006591797)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-77.pth.tar', 73.11400000976562)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-76.pth.tar', 73.06200013916016)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-74.pth.tar', 73.04600001464844)

Train: 78 [   0/1251 (  0%)]  Loss: 3.933 (3.93)  Time: 3.640s,  281.29/s  (3.640s,  281.29/s)  LR: 8.431e-04  Data: 1.650 (1.650)
Train: 78 [  50/1251 (  4%)]  Loss: 3.938 (3.94)  Time: 0.667s, 1534.59/s  (0.703s, 1455.78/s)  LR: 8.429e-04  Data: 0.015 (0.046)
Train: 78 [ 100/1251 (  8%)]  Loss: 3.768 (3.88)  Time: 0.672s, 1524.48/s  (0.688s, 1488.04/s)  LR: 8.428e-04  Data: 0.013 (0.030)
Train: 78 [ 150/1251 ( 12%)]  Loss: 3.553 (3.80)  Time: 0.676s, 1515.35/s  (0.684s, 1496.55/s)  LR: 8.426e-04  Data: 0.012 (0.025)
Train: 78 [ 200/1251 ( 16%)]  Loss: 3.751 (3.79)  Time: 0.681s, 1503.36/s  (0.683s, 1500.16/s)  LR: 8.425e-04  Data: 0.013 (0.022)
Train: 78 [ 250/1251 ( 20%)]  Loss: 3.536 (3.75)  Time: 0.678s, 1510.01/s  (0.682s, 1501.64/s)  LR: 8.423e-04  Data: 0.013 (0.020)
Train: 78 [ 300/1251 ( 24%)]  Loss: 4.082 (3.79)  Time: 0.681s, 1504.05/s  (0.681s, 1503.81/s)  LR: 8.421e-04  Data: 0.014 (0.019)
Train: 78 [ 350/1251 ( 28%)]  Loss: 4.037 (3.82)  Time: 0.678s, 1510.29/s  (0.681s, 1504.65/s)  LR: 8.420e-04  Data: 0.013 (0.018)
Train: 78 [ 400/1251 ( 32%)]  Loss: 3.694 (3.81)  Time: 0.682s, 1501.26/s  (0.680s, 1504.87/s)  LR: 8.418e-04  Data: 0.012 (0.018)
Train: 78 [ 450/1251 ( 36%)]  Loss: 3.675 (3.80)  Time: 0.687s, 1490.67/s  (0.681s, 1504.75/s)  LR: 8.417e-04  Data: 0.016 (0.017)
Train: 78 [ 500/1251 ( 40%)]  Loss: 3.381 (3.76)  Time: 0.679s, 1507.04/s  (0.680s, 1505.11/s)  LR: 8.415e-04  Data: 0.012 (0.017)
Train: 78 [ 550/1251 ( 44%)]  Loss: 3.667 (3.75)  Time: 0.680s, 1505.81/s  (0.680s, 1505.23/s)  LR: 8.414e-04  Data: 0.012 (0.016)
Train: 78 [ 600/1251 ( 48%)]  Loss: 4.047 (3.77)  Time: 0.684s, 1496.51/s  (0.680s, 1505.27/s)  LR: 8.412e-04  Data: 0.013 (0.016)
Train: 78 [ 650/1251 ( 52%)]  Loss: 3.771 (3.77)  Time: 0.696s, 1470.76/s  (0.680s, 1505.41/s)  LR: 8.411e-04  Data: 0.013 (0.016)
Train: 78 [ 700/1251 ( 56%)]  Loss: 3.541 (3.76)  Time: 0.688s, 1487.68/s  (0.680s, 1505.33/s)  LR: 8.409e-04  Data: 0.015 (0.016)
Train: 78 [ 750/1251 ( 60%)]  Loss: 3.314 (3.73)  Time: 0.681s, 1503.77/s  (0.680s, 1505.03/s)  LR: 8.408e-04  Data: 0.015 (0.016)
Train: 78 [ 800/1251 ( 64%)]  Loss: 3.734 (3.73)  Time: 0.694s, 1475.25/s  (0.680s, 1504.95/s)  LR: 8.406e-04  Data: 0.012 (0.016)
Train: 78 [ 850/1251 ( 68%)]  Loss: 3.618 (3.72)  Time: 0.685s, 1494.09/s  (0.680s, 1504.93/s)  LR: 8.405e-04  Data: 0.013 (0.015)
Train: 78 [ 900/1251 ( 72%)]  Loss: 3.710 (3.72)  Time: 0.681s, 1504.03/s  (0.680s, 1504.82/s)  LR: 8.403e-04  Data: 0.013 (0.015)
Train: 78 [ 950/1251 ( 76%)]  Loss: 3.771 (3.73)  Time: 0.678s, 1511.00/s  (0.681s, 1504.63/s)  LR: 8.402e-04  Data: 0.013 (0.015)
Train: 78 [1000/1251 ( 80%)]  Loss: 3.906 (3.73)  Time: 0.679s, 1509.08/s  (0.681s, 1504.47/s)  LR: 8.400e-04  Data: 0.013 (0.015)
Train: 78 [1050/1251 ( 84%)]  Loss: 3.760 (3.74)  Time: 0.681s, 1502.95/s  (0.681s, 1504.23/s)  LR: 8.399e-04  Data: 0.016 (0.015)
Train: 78 [1100/1251 ( 88%)]  Loss: 3.472 (3.72)  Time: 0.674s, 1520.17/s  (0.681s, 1504.12/s)  LR: 8.397e-04  Data: 0.012 (0.015)
Train: 78 [1150/1251 ( 92%)]  Loss: 3.508 (3.72)  Time: 0.681s, 1504.00/s  (0.681s, 1504.04/s)  LR: 8.396e-04  Data: 0.013 (0.015)
Train: 78 [1200/1251 ( 96%)]  Loss: 3.660 (3.71)  Time: 0.677s, 1513.06/s  (0.681s, 1503.69/s)  LR: 8.394e-04  Data: 0.017 (0.015)
Train: 78 [1250/1251 (100%)]  Loss: 3.884 (3.72)  Time: 0.672s, 1524.91/s  (0.681s, 1503.51/s)  LR: 8.392e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.976 (2.976)  Loss:  0.5791 (0.5791)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.164 (0.319)  Loss:  0.6567 (1.1345)  Acc@1: 85.4953 (73.5180)  Acc@5: 96.3443 (92.1360)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-78.pth.tar', 73.51800003417969)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-75.pth.tar', 73.11600006591797)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-77.pth.tar', 73.11400000976562)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-76.pth.tar', 73.06200013916016)

Train: 79 [   0/1251 (  0%)]  Loss: 3.701 (3.70)  Time: 3.422s,  299.21/s  (3.422s,  299.21/s)  LR: 8.392e-04  Data: 1.568 (1.568)
Train: 79 [  50/1251 (  4%)]  Loss: 3.721 (3.71)  Time: 0.677s, 1513.41/s  (0.708s, 1446.78/s)  LR: 8.391e-04  Data: 0.016 (0.044)
Train: 79 [ 100/1251 (  8%)]  Loss: 3.912 (3.78)  Time: 0.688s, 1488.40/s  (0.693s, 1477.76/s)  LR: 8.389e-04  Data: 0.013 (0.029)
Train: 79 [ 150/1251 ( 12%)]  Loss: 4.021 (3.84)  Time: 0.688s, 1487.32/s  (0.689s, 1486.16/s)  LR: 8.388e-04  Data: 0.017 (0.024)
Train: 79 [ 200/1251 ( 16%)]  Loss: 3.831 (3.84)  Time: 0.676s, 1515.30/s  (0.687s, 1490.27/s)  LR: 8.386e-04  Data: 0.013 (0.021)
Train: 79 [ 250/1251 ( 20%)]  Loss: 3.556 (3.79)  Time: 0.682s, 1501.89/s  (0.686s, 1492.50/s)  LR: 8.385e-04  Data: 0.013 (0.020)
Train: 79 [ 300/1251 ( 24%)]  Loss: 3.783 (3.79)  Time: 0.683s, 1499.12/s  (0.686s, 1493.36/s)  LR: 8.383e-04  Data: 0.014 (0.019)
Train: 79 [ 350/1251 ( 28%)]  Loss: 3.283 (3.73)  Time: 0.675s, 1518.14/s  (0.685s, 1494.52/s)  LR: 8.382e-04  Data: 0.014 (0.018)
Train: 79 [ 400/1251 ( 32%)]  Loss: 3.504 (3.70)  Time: 0.675s, 1517.27/s  (0.685s, 1495.28/s)  LR: 8.380e-04  Data: 0.014 (0.017)
Train: 79 [ 450/1251 ( 36%)]  Loss: 3.805 (3.71)  Time: 0.681s, 1503.25/s  (0.684s, 1496.00/s)  LR: 8.379e-04  Data: 0.013 (0.017)
Train: 79 [ 500/1251 ( 40%)]  Loss: 3.848 (3.72)  Time: 0.697s, 1468.68/s  (0.684s, 1496.85/s)  LR: 8.377e-04  Data: 0.013 (0.017)
Train: 79 [ 550/1251 ( 44%)]  Loss: 3.653 (3.72)  Time: 0.676s, 1514.06/s  (0.684s, 1497.45/s)  LR: 8.376e-04  Data: 0.013 (0.016)
Train: 79 [ 600/1251 ( 48%)]  Loss: 3.765 (3.72)  Time: 0.686s, 1492.56/s  (0.684s, 1498.06/s)  LR: 8.374e-04  Data: 0.013 (0.016)
Train: 79 [ 650/1251 ( 52%)]  Loss: 3.813 (3.73)  Time: 0.682s, 1501.73/s  (0.683s, 1498.67/s)  LR: 8.372e-04  Data: 0.013 (0.016)
Train: 79 [ 700/1251 ( 56%)]  Loss: 3.511 (3.71)  Time: 0.678s, 1511.22/s  (0.683s, 1499.26/s)  LR: 8.371e-04  Data: 0.013 (0.016)
Train: 79 [ 750/1251 ( 60%)]  Loss: 3.779 (3.72)  Time: 0.682s, 1500.55/s  (0.683s, 1499.61/s)  LR: 8.369e-04  Data: 0.012 (0.016)
Train: 79 [ 800/1251 ( 64%)]  Loss: 3.369 (3.70)  Time: 0.686s, 1493.58/s  (0.683s, 1500.24/s)  LR: 8.368e-04  Data: 0.013 (0.016)
Train: 79 [ 850/1251 ( 68%)]  Loss: 3.606 (3.69)  Time: 0.677s, 1512.68/s  (0.683s, 1500.02/s)  LR: 8.366e-04  Data: 0.013 (0.015)
Train: 79 [ 900/1251 ( 72%)]  Loss: 3.480 (3.68)  Time: 0.688s, 1488.15/s  (0.683s, 1499.96/s)  LR: 8.365e-04  Data: 0.012 (0.015)
Train: 79 [ 950/1251 ( 76%)]  Loss: 4.075 (3.70)  Time: 0.692s, 1479.56/s  (0.683s, 1499.95/s)  LR: 8.363e-04  Data: 0.013 (0.015)
Train: 79 [1000/1251 ( 80%)]  Loss: 3.611 (3.70)  Time: 0.691s, 1482.21/s  (0.683s, 1499.77/s)  LR: 8.362e-04  Data: 0.016 (0.015)
Train: 79 [1050/1251 ( 84%)]  Loss: 3.906 (3.71)  Time: 0.681s, 1503.33/s  (0.683s, 1499.74/s)  LR: 8.360e-04  Data: 0.013 (0.015)
Train: 79 [1100/1251 ( 88%)]  Loss: 3.827 (3.71)  Time: 0.687s, 1490.92/s  (0.683s, 1499.61/s)  LR: 8.359e-04  Data: 0.013 (0.015)
Train: 79 [1150/1251 ( 92%)]  Loss: 3.839 (3.72)  Time: 0.688s, 1489.28/s  (0.683s, 1499.53/s)  LR: 8.357e-04  Data: 0.013 (0.015)
Train: 79 [1200/1251 ( 96%)]  Loss: 4.004 (3.73)  Time: 0.673s, 1521.32/s  (0.683s, 1499.45/s)  LR: 8.355e-04  Data: 0.014 (0.015)
Train: 79 [1250/1251 (100%)]  Loss: 3.860 (3.73)  Time: 0.663s, 1544.45/s  (0.683s, 1499.49/s)  LR: 8.354e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.906 (2.906)  Loss:  0.5415 (0.5415)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.171 (0.325)  Loss:  0.6729 (1.1386)  Acc@1: 84.0802 (73.4720)  Acc@5: 96.5802 (91.9580)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-78.pth.tar', 73.51800003417969)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-79.pth.tar', 73.47199993652343)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-75.pth.tar', 73.11600006591797)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-77.pth.tar', 73.11400000976562)

Train: 80 [   0/1251 (  0%)]  Loss: 3.641 (3.64)  Time: 3.438s,  297.86/s  (3.438s,  297.86/s)  LR: 8.354e-04  Data: 1.757 (1.757)
Train: 80 [  50/1251 (  4%)]  Loss: 3.954 (3.80)  Time: 0.671s, 1526.72/s  (0.704s, 1453.91/s)  LR: 8.352e-04  Data: 0.013 (0.048)
Train: 80 [ 100/1251 (  8%)]  Loss: 3.733 (3.78)  Time: 0.677s, 1512.00/s  (0.691s, 1482.30/s)  LR: 8.351e-04  Data: 0.013 (0.031)
Train: 80 [ 150/1251 ( 12%)]  Loss: 4.021 (3.84)  Time: 0.676s, 1514.30/s  (0.687s, 1491.52/s)  LR: 8.349e-04  Data: 0.014 (0.025)
Train: 80 [ 200/1251 ( 16%)]  Loss: 3.655 (3.80)  Time: 0.673s, 1520.82/s  (0.685s, 1494.75/s)  LR: 8.348e-04  Data: 0.015 (0.022)
Train: 80 [ 250/1251 ( 20%)]  Loss: 3.578 (3.76)  Time: 0.687s, 1490.86/s  (0.685s, 1494.78/s)  LR: 8.346e-04  Data: 0.013 (0.021)
Train: 80 [ 300/1251 ( 24%)]  Loss: 3.441 (3.72)  Time: 0.675s, 1516.58/s  (0.684s, 1496.23/s)  LR: 8.345e-04  Data: 0.013 (0.020)
Train: 80 [ 350/1251 ( 28%)]  Loss: 3.777 (3.73)  Time: 0.689s, 1485.24/s  (0.684s, 1497.38/s)  LR: 8.343e-04  Data: 0.015 (0.019)
Train: 80 [ 400/1251 ( 32%)]  Loss: 3.756 (3.73)  Time: 0.678s, 1510.29/s  (0.683s, 1498.50/s)  LR: 8.341e-04  Data: 0.013 (0.018)
Train: 80 [ 450/1251 ( 36%)]  Loss: 3.930 (3.75)  Time: 0.682s, 1500.62/s  (0.683s, 1498.99/s)  LR: 8.340e-04  Data: 0.013 (0.018)
Train: 80 [ 500/1251 ( 40%)]  Loss: 3.667 (3.74)  Time: 0.686s, 1492.87/s  (0.683s, 1499.10/s)  LR: 8.338e-04  Data: 0.015 (0.017)
Train: 80 [ 550/1251 ( 44%)]  Loss: 3.875 (3.75)  Time: 0.684s, 1497.06/s  (0.683s, 1499.15/s)  LR: 8.337e-04  Data: 0.014 (0.017)
Train: 80 [ 600/1251 ( 48%)]  Loss: 3.791 (3.76)  Time: 0.677s, 1513.57/s  (0.683s, 1499.25/s)  LR: 8.335e-04  Data: 0.012 (0.016)
Train: 80 [ 650/1251 ( 52%)]  Loss: 3.683 (3.75)  Time: 0.683s, 1498.57/s  (0.683s, 1499.56/s)  LR: 8.334e-04  Data: 0.014 (0.016)
Train: 80 [ 700/1251 ( 56%)]  Loss: 3.870 (3.76)  Time: 0.677s, 1513.55/s  (0.683s, 1499.91/s)  LR: 8.332e-04  Data: 0.013 (0.016)
Train: 80 [ 750/1251 ( 60%)]  Loss: 3.766 (3.76)  Time: 0.686s, 1493.50/s  (0.683s, 1500.18/s)  LR: 8.331e-04  Data: 0.016 (0.016)
Train: 80 [ 800/1251 ( 64%)]  Loss: 3.569 (3.75)  Time: 0.681s, 1503.78/s  (0.682s, 1500.41/s)  LR: 8.329e-04  Data: 0.018 (0.016)
Train: 80 [ 850/1251 ( 68%)]  Loss: 3.677 (3.74)  Time: 0.688s, 1488.87/s  (0.682s, 1500.59/s)  LR: 8.328e-04  Data: 0.015 (0.016)
Train: 80 [ 900/1251 ( 72%)]  Loss: 4.095 (3.76)  Time: 0.688s, 1488.67/s  (0.682s, 1500.84/s)  LR: 8.326e-04  Data: 0.015 (0.015)
Train: 80 [ 950/1251 ( 76%)]  Loss: 3.974 (3.77)  Time: 0.681s, 1504.09/s  (0.682s, 1501.09/s)  LR: 8.324e-04  Data: 0.013 (0.015)
Train: 80 [1000/1251 ( 80%)]  Loss: 3.748 (3.77)  Time: 0.670s, 1527.32/s  (0.682s, 1501.38/s)  LR: 8.323e-04  Data: 0.013 (0.015)
Train: 80 [1050/1251 ( 84%)]  Loss: 3.639 (3.77)  Time: 0.690s, 1483.77/s  (0.682s, 1501.41/s)  LR: 8.321e-04  Data: 0.014 (0.015)
Train: 80 [1100/1251 ( 88%)]  Loss: 3.757 (3.77)  Time: 0.682s, 1502.40/s  (0.682s, 1501.43/s)  LR: 8.320e-04  Data: 0.012 (0.015)
Train: 80 [1150/1251 ( 92%)]  Loss: 4.227 (3.78)  Time: 0.676s, 1515.76/s  (0.682s, 1501.36/s)  LR: 8.318e-04  Data: 0.013 (0.015)
Train: 80 [1200/1251 ( 96%)]  Loss: 3.675 (3.78)  Time: 0.677s, 1513.07/s  (0.682s, 1501.41/s)  LR: 8.317e-04  Data: 0.013 (0.015)
Train: 80 [1250/1251 (100%)]  Loss: 3.538 (3.77)  Time: 0.670s, 1527.51/s  (0.682s, 1501.62/s)  LR: 8.315e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.853 (2.853)  Loss:  0.5146 (0.5146)  Acc@1: 87.8906 (87.8906)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.172 (0.329)  Loss:  0.6416 (1.1291)  Acc@1: 83.9623 (73.2240)  Acc@5: 96.3443 (91.9040)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-78.pth.tar', 73.51800003417969)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-79.pth.tar', 73.47199993652343)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-80.pth.tar', 73.22400001464844)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-75.pth.tar', 73.11600006591797)

Train: 81 [   0/1251 (  0%)]  Loss: 3.793 (3.79)  Time: 3.824s,  267.79/s  (3.824s,  267.79/s)  LR: 8.315e-04  Data: 1.621 (1.621)
Train: 81 [  50/1251 (  4%)]  Loss: 3.668 (3.73)  Time: 0.663s, 1543.50/s  (0.710s, 1442.08/s)  LR: 8.313e-04  Data: 0.014 (0.045)
Train: 81 [ 100/1251 (  8%)]  Loss: 3.551 (3.67)  Time: 0.666s, 1537.25/s  (0.690s, 1483.41/s)  LR: 8.312e-04  Data: 0.012 (0.030)
Train: 81 [ 150/1251 ( 12%)]  Loss: 3.516 (3.63)  Time: 0.676s, 1513.71/s  (0.685s, 1494.81/s)  LR: 8.310e-04  Data: 0.012 (0.024)
Train: 81 [ 200/1251 ( 16%)]  Loss: 3.597 (3.63)  Time: 0.678s, 1509.49/s  (0.683s, 1499.09/s)  LR: 8.309e-04  Data: 0.013 (0.022)
Train: 81 [ 250/1251 ( 20%)]  Loss: 3.928 (3.68)  Time: 0.682s, 1500.76/s  (0.682s, 1502.19/s)  LR: 8.307e-04  Data: 0.013 (0.020)
Train: 81 [ 300/1251 ( 24%)]  Loss: 3.535 (3.66)  Time: 0.668s, 1533.98/s  (0.681s, 1504.28/s)  LR: 8.306e-04  Data: 0.017 (0.019)
Train: 81 [ 350/1251 ( 28%)]  Loss: 3.991 (3.70)  Time: 0.667s, 1534.48/s  (0.680s, 1505.41/s)  LR: 8.304e-04  Data: 0.013 (0.018)
Train: 81 [ 400/1251 ( 32%)]  Loss: 3.810 (3.71)  Time: 0.687s, 1490.02/s  (0.680s, 1505.59/s)  LR: 8.302e-04  Data: 0.013 (0.018)
Train: 81 [ 450/1251 ( 36%)]  Loss: 3.497 (3.69)  Time: 0.676s, 1514.56/s  (0.680s, 1506.11/s)  LR: 8.301e-04  Data: 0.013 (0.017)
Train: 81 [ 500/1251 ( 40%)]  Loss: 3.938 (3.71)  Time: 0.682s, 1501.67/s  (0.680s, 1506.26/s)  LR: 8.299e-04  Data: 0.013 (0.017)
Train: 81 [ 550/1251 ( 44%)]  Loss: 3.679 (3.71)  Time: 0.681s, 1503.01/s  (0.680s, 1506.54/s)  LR: 8.298e-04  Data: 0.013 (0.017)
Train: 81 [ 600/1251 ( 48%)]  Loss: 3.589 (3.70)  Time: 0.678s, 1510.21/s  (0.679s, 1507.22/s)  LR: 8.296e-04  Data: 0.015 (0.016)
Train: 81 [ 650/1251 ( 52%)]  Loss: 3.143 (3.66)  Time: 0.680s, 1505.35/s  (0.679s, 1507.46/s)  LR: 8.295e-04  Data: 0.013 (0.016)
Train: 81 [ 700/1251 ( 56%)]  Loss: 3.514 (3.65)  Time: 0.668s, 1533.28/s  (0.679s, 1507.40/s)  LR: 8.293e-04  Data: 0.013 (0.016)
Train: 81 [ 750/1251 ( 60%)]  Loss: 3.664 (3.65)  Time: 0.682s, 1500.82/s  (0.679s, 1507.39/s)  LR: 8.292e-04  Data: 0.012 (0.016)
Train: 81 [ 800/1251 ( 64%)]  Loss: 3.720 (3.65)  Time: 0.680s, 1504.95/s  (0.679s, 1507.31/s)  LR: 8.290e-04  Data: 0.013 (0.016)
Train: 81 [ 850/1251 ( 68%)]  Loss: 3.883 (3.67)  Time: 0.686s, 1492.53/s  (0.679s, 1507.17/s)  LR: 8.288e-04  Data: 0.013 (0.016)
Train: 81 [ 900/1251 ( 72%)]  Loss: 3.637 (3.67)  Time: 0.669s, 1531.15/s  (0.679s, 1507.40/s)  LR: 8.287e-04  Data: 0.013 (0.015)
Train: 81 [ 950/1251 ( 76%)]  Loss: 3.990 (3.68)  Time: 0.680s, 1506.80/s  (0.679s, 1507.39/s)  LR: 8.285e-04  Data: 0.013 (0.015)
Train: 81 [1000/1251 ( 80%)]  Loss: 3.645 (3.68)  Time: 0.675s, 1516.11/s  (0.679s, 1507.50/s)  LR: 8.284e-04  Data: 0.013 (0.015)
Train: 81 [1050/1251 ( 84%)]  Loss: 3.725 (3.68)  Time: 0.676s, 1515.09/s  (0.679s, 1507.68/s)  LR: 8.282e-04  Data: 0.013 (0.015)
Train: 81 [1100/1251 ( 88%)]  Loss: 3.941 (3.69)  Time: 0.690s, 1484.53/s  (0.679s, 1507.73/s)  LR: 8.280e-04  Data: 0.013 (0.015)
Train: 81 [1150/1251 ( 92%)]  Loss: 3.598 (3.69)  Time: 0.683s, 1498.50/s  (0.679s, 1507.78/s)  LR: 8.279e-04  Data: 0.014 (0.015)
Train: 81 [1200/1251 ( 96%)]  Loss: 3.624 (3.69)  Time: 0.683s, 1499.68/s  (0.679s, 1507.80/s)  LR: 8.277e-04  Data: 0.013 (0.015)
Train: 81 [1250/1251 (100%)]  Loss: 3.593 (3.68)  Time: 0.664s, 1542.47/s  (0.679s, 1508.08/s)  LR: 8.276e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.823 (2.823)  Loss:  0.5342 (0.5342)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.171 (0.325)  Loss:  0.6934 (1.1295)  Acc@1: 84.9057 (73.4500)  Acc@5: 96.4623 (91.9600)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-78.pth.tar', 73.51800003417969)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-79.pth.tar', 73.47199993652343)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-81.pth.tar', 73.45000003662109)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-80.pth.tar', 73.22400001464844)

Train: 82 [   0/1251 (  0%)]  Loss: 3.898 (3.90)  Time: 3.611s,  283.60/s  (3.611s,  283.60/s)  LR: 8.276e-04  Data: 1.798 (1.798)
Train: 82 [  50/1251 (  4%)]  Loss: 3.798 (3.85)  Time: 0.656s, 1560.31/s  (0.706s, 1450.59/s)  LR: 8.274e-04  Data: 0.013 (0.049)
Train: 82 [ 100/1251 (  8%)]  Loss: 3.452 (3.72)  Time: 0.678s, 1509.95/s  (0.689s, 1486.81/s)  LR: 8.273e-04  Data: 0.013 (0.032)
Train: 82 [ 150/1251 ( 12%)]  Loss: 3.732 (3.72)  Time: 0.683s, 1500.16/s  (0.685s, 1495.24/s)  LR: 8.271e-04  Data: 0.013 (0.026)
Train: 82 [ 200/1251 ( 16%)]  Loss: 3.748 (3.73)  Time: 0.675s, 1517.74/s  (0.683s, 1499.75/s)  LR: 8.269e-04  Data: 0.012 (0.023)
Train: 82 [ 250/1251 ( 20%)]  Loss: 4.051 (3.78)  Time: 0.673s, 1520.53/s  (0.682s, 1502.28/s)  LR: 8.268e-04  Data: 0.017 (0.021)
Train: 82 [ 300/1251 ( 24%)]  Loss: 4.111 (3.83)  Time: 0.672s, 1524.74/s  (0.680s, 1505.21/s)  LR: 8.266e-04  Data: 0.013 (0.020)
Train: 82 [ 350/1251 ( 28%)]  Loss: 3.734 (3.82)  Time: 0.678s, 1509.52/s  (0.680s, 1506.76/s)  LR: 8.265e-04  Data: 0.013 (0.019)
Train: 82 [ 400/1251 ( 32%)]  Loss: 3.745 (3.81)  Time: 0.678s, 1510.41/s  (0.679s, 1508.39/s)  LR: 8.263e-04  Data: 0.013 (0.018)
Train: 82 [ 450/1251 ( 36%)]  Loss: 3.196 (3.75)  Time: 0.677s, 1513.37/s  (0.679s, 1508.92/s)  LR: 8.262e-04  Data: 0.013 (0.018)
Train: 82 [ 500/1251 ( 40%)]  Loss: 3.957 (3.77)  Time: 0.683s, 1499.52/s  (0.678s, 1509.83/s)  LR: 8.260e-04  Data: 0.013 (0.017)
Train: 82 [ 550/1251 ( 44%)]  Loss: 3.794 (3.77)  Time: 0.678s, 1509.59/s  (0.678s, 1510.37/s)  LR: 8.258e-04  Data: 0.019 (0.017)
Train: 82 [ 600/1251 ( 48%)]  Loss: 3.907 (3.78)  Time: 0.680s, 1505.46/s  (0.678s, 1510.71/s)  LR: 8.257e-04  Data: 0.013 (0.017)
Train: 82 [ 650/1251 ( 52%)]  Loss: 3.458 (3.76)  Time: 0.680s, 1506.40/s  (0.678s, 1511.27/s)  LR: 8.255e-04  Data: 0.014 (0.016)
Train: 82 [ 700/1251 ( 56%)]  Loss: 3.714 (3.75)  Time: 0.671s, 1525.72/s  (0.678s, 1511.31/s)  LR: 8.254e-04  Data: 0.014 (0.016)
Train: 82 [ 750/1251 ( 60%)]  Loss: 3.451 (3.73)  Time: 0.681s, 1504.53/s  (0.678s, 1511.32/s)  LR: 8.252e-04  Data: 0.017 (0.016)
Train: 82 [ 800/1251 ( 64%)]  Loss: 3.472 (3.72)  Time: 0.666s, 1538.62/s  (0.678s, 1511.43/s)  LR: 8.250e-04  Data: 0.013 (0.016)
Train: 82 [ 850/1251 ( 68%)]  Loss: 3.440 (3.70)  Time: 0.670s, 1528.33/s  (0.677s, 1511.52/s)  LR: 8.249e-04  Data: 0.013 (0.016)
Train: 82 [ 900/1251 ( 72%)]  Loss: 3.861 (3.71)  Time: 0.674s, 1518.63/s  (0.677s, 1511.49/s)  LR: 8.247e-04  Data: 0.013 (0.016)
Train: 82 [ 950/1251 ( 76%)]  Loss: 3.974 (3.72)  Time: 0.682s, 1502.34/s  (0.678s, 1511.41/s)  LR: 8.246e-04  Data: 0.013 (0.016)
Train: 82 [1000/1251 ( 80%)]  Loss: 3.824 (3.73)  Time: 0.671s, 1526.04/s  (0.678s, 1511.38/s)  LR: 8.244e-04  Data: 0.016 (0.015)
Train: 82 [1050/1251 ( 84%)]  Loss: 3.648 (3.73)  Time: 0.680s, 1505.93/s  (0.677s, 1511.62/s)  LR: 8.243e-04  Data: 0.013 (0.015)
Train: 82 [1100/1251 ( 88%)]  Loss: 4.050 (3.74)  Time: 0.678s, 1511.39/s  (0.678s, 1511.41/s)  LR: 8.241e-04  Data: 0.012 (0.015)
Train: 82 [1150/1251 ( 92%)]  Loss: 3.717 (3.74)  Time: 0.683s, 1499.65/s  (0.678s, 1511.22/s)  LR: 8.239e-04  Data: 0.015 (0.015)
Train: 82 [1200/1251 ( 96%)]  Loss: 3.788 (3.74)  Time: 0.678s, 1509.72/s  (0.678s, 1511.21/s)  LR: 8.238e-04  Data: 0.013 (0.015)
Train: 82 [1250/1251 (100%)]  Loss: 3.608 (3.74)  Time: 0.659s, 1553.14/s  (0.678s, 1511.16/s)  LR: 8.236e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.845 (2.845)  Loss:  0.5596 (0.5596)  Acc@1: 88.2812 (88.2812)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.168 (0.329)  Loss:  0.6934 (1.1372)  Acc@1: 84.3160 (73.3540)  Acc@5: 96.1085 (92.0440)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-78.pth.tar', 73.51800003417969)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-79.pth.tar', 73.47199993652343)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-81.pth.tar', 73.45000003662109)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-82.pth.tar', 73.35399990966796)

Train: 83 [   0/1251 (  0%)]  Loss: 3.834 (3.83)  Time: 3.954s,  258.99/s  (3.954s,  258.99/s)  LR: 8.236e-04  Data: 1.730 (1.730)
Train: 83 [  50/1251 (  4%)]  Loss: 3.768 (3.80)  Time: 0.673s, 1520.92/s  (0.710s, 1441.32/s)  LR: 8.235e-04  Data: 0.016 (0.048)
Train: 83 [ 100/1251 (  8%)]  Loss: 3.227 (3.61)  Time: 0.670s, 1527.84/s  (0.691s, 1481.53/s)  LR: 8.233e-04  Data: 0.013 (0.031)
Train: 83 [ 150/1251 ( 12%)]  Loss: 4.121 (3.74)  Time: 0.692s, 1480.09/s  (0.687s, 1491.62/s)  LR: 8.231e-04  Data: 0.014 (0.025)
Train: 83 [ 200/1251 ( 16%)]  Loss: 3.675 (3.73)  Time: 0.685s, 1494.68/s  (0.684s, 1496.75/s)  LR: 8.230e-04  Data: 0.013 (0.022)
Train: 83 [ 250/1251 ( 20%)]  Loss: 3.912 (3.76)  Time: 0.687s, 1490.73/s  (0.684s, 1497.66/s)  LR: 8.228e-04  Data: 0.012 (0.021)
Train: 83 [ 300/1251 ( 24%)]  Loss: 3.308 (3.69)  Time: 0.684s, 1496.79/s  (0.684s, 1497.25/s)  LR: 8.227e-04  Data: 0.013 (0.019)
Train: 83 [ 350/1251 ( 28%)]  Loss: 3.595 (3.68)  Time: 0.683s, 1498.27/s  (0.684s, 1497.25/s)  LR: 8.225e-04  Data: 0.015 (0.019)
Train: 83 [ 400/1251 ( 32%)]  Loss: 3.700 (3.68)  Time: 0.684s, 1497.43/s  (0.684s, 1497.59/s)  LR: 8.223e-04  Data: 0.014 (0.018)
Train: 83 [ 450/1251 ( 36%)]  Loss: 3.822 (3.70)  Time: 0.677s, 1512.33/s  (0.683s, 1498.37/s)  LR: 8.222e-04  Data: 0.012 (0.018)
Train: 83 [ 500/1251 ( 40%)]  Loss: 3.811 (3.71)  Time: 0.677s, 1512.63/s  (0.683s, 1499.08/s)  LR: 8.220e-04  Data: 0.013 (0.017)
Train: 83 [ 550/1251 ( 44%)]  Loss: 3.750 (3.71)  Time: 0.689s, 1486.95/s  (0.683s, 1499.21/s)  LR: 8.219e-04  Data: 0.014 (0.017)
Train: 83 [ 600/1251 ( 48%)]  Loss: 3.513 (3.70)  Time: 0.675s, 1518.11/s  (0.683s, 1499.29/s)  LR: 8.217e-04  Data: 0.014 (0.016)
Train: 83 [ 650/1251 ( 52%)]  Loss: 3.411 (3.67)  Time: 0.680s, 1505.09/s  (0.683s, 1499.73/s)  LR: 8.215e-04  Data: 0.013 (0.016)
Train: 83 [ 700/1251 ( 56%)]  Loss: 3.567 (3.67)  Time: 0.681s, 1504.41/s  (0.683s, 1500.14/s)  LR: 8.214e-04  Data: 0.013 (0.016)
Train: 83 [ 750/1251 ( 60%)]  Loss: 3.551 (3.66)  Time: 0.682s, 1500.94/s  (0.682s, 1500.98/s)  LR: 8.212e-04  Data: 0.012 (0.016)
Train: 83 [ 800/1251 ( 64%)]  Loss: 3.901 (3.67)  Time: 0.675s, 1517.95/s  (0.682s, 1501.77/s)  LR: 8.211e-04  Data: 0.013 (0.016)
Train: 83 [ 850/1251 ( 68%)]  Loss: 3.900 (3.69)  Time: 0.673s, 1521.51/s  (0.682s, 1502.42/s)  LR: 8.209e-04  Data: 0.014 (0.016)
Train: 83 [ 900/1251 ( 72%)]  Loss: 3.661 (3.69)  Time: 0.682s, 1500.96/s  (0.681s, 1502.71/s)  LR: 8.207e-04  Data: 0.014 (0.016)
Train: 83 [ 950/1251 ( 76%)]  Loss: 3.676 (3.69)  Time: 0.687s, 1491.30/s  (0.681s, 1503.01/s)  LR: 8.206e-04  Data: 0.013 (0.015)
Train: 83 [1000/1251 ( 80%)]  Loss: 3.499 (3.68)  Time: 0.674s, 1519.69/s  (0.681s, 1502.97/s)  LR: 8.204e-04  Data: 0.013 (0.015)
Train: 83 [1050/1251 ( 84%)]  Loss: 3.611 (3.67)  Time: 0.676s, 1513.89/s  (0.681s, 1503.24/s)  LR: 8.203e-04  Data: 0.012 (0.015)
Train: 83 [1100/1251 ( 88%)]  Loss: 3.578 (3.67)  Time: 0.684s, 1498.01/s  (0.681s, 1503.30/s)  LR: 8.201e-04  Data: 0.013 (0.015)
Train: 83 [1150/1251 ( 92%)]  Loss: 3.436 (3.66)  Time: 0.676s, 1514.26/s  (0.681s, 1503.20/s)  LR: 8.199e-04  Data: 0.015 (0.015)
Train: 83 [1200/1251 ( 96%)]  Loss: 3.926 (3.67)  Time: 0.666s, 1537.20/s  (0.681s, 1503.35/s)  LR: 8.198e-04  Data: 0.013 (0.015)
Train: 83 [1250/1251 (100%)]  Loss: 3.693 (3.67)  Time: 0.667s, 1535.07/s  (0.681s, 1503.54/s)  LR: 8.196e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.255 (3.255)  Loss:  0.5620 (0.5620)  Acc@1: 87.7930 (87.7930)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.168 (0.327)  Loss:  0.6611 (1.1204)  Acc@1: 85.1415 (73.5720)  Acc@5: 96.1085 (92.0980)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-83.pth.tar', 73.57200000976563)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-78.pth.tar', 73.51800003417969)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-79.pth.tar', 73.47199993652343)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-81.pth.tar', 73.45000003662109)

Train: 84 [   0/1251 (  0%)]  Loss: 3.768 (3.77)  Time: 3.700s,  276.79/s  (3.700s,  276.79/s)  LR: 8.196e-04  Data: 1.842 (1.842)
Train: 84 [  50/1251 (  4%)]  Loss: 3.855 (3.81)  Time: 0.667s, 1535.90/s  (0.705s, 1453.07/s)  LR: 8.195e-04  Data: 0.013 (0.050)
Train: 84 [ 100/1251 (  8%)]  Loss: 3.808 (3.81)  Time: 0.669s, 1530.84/s  (0.689s, 1485.70/s)  LR: 8.193e-04  Data: 0.014 (0.032)
Train: 84 [ 150/1251 ( 12%)]  Loss: 3.420 (3.71)  Time: 0.674s, 1519.08/s  (0.685s, 1493.87/s)  LR: 8.191e-04  Data: 0.012 (0.025)
Train: 84 [ 200/1251 ( 16%)]  Loss: 3.654 (3.70)  Time: 0.675s, 1518.11/s  (0.684s, 1496.17/s)  LR: 8.190e-04  Data: 0.013 (0.022)
Train: 84 [ 250/1251 ( 20%)]  Loss: 3.801 (3.72)  Time: 0.683s, 1499.58/s  (0.684s, 1496.62/s)  LR: 8.188e-04  Data: 0.013 (0.020)
Train: 84 [ 300/1251 ( 24%)]  Loss: 3.714 (3.72)  Time: 0.688s, 1488.22/s  (0.684s, 1497.85/s)  LR: 8.187e-04  Data: 0.013 (0.019)
Train: 84 [ 350/1251 ( 28%)]  Loss: 3.719 (3.72)  Time: 0.685s, 1495.12/s  (0.683s, 1498.94/s)  LR: 8.185e-04  Data: 0.014 (0.019)
Train: 84 [ 400/1251 ( 32%)]  Loss: 3.700 (3.72)  Time: 0.677s, 1512.47/s  (0.683s, 1499.58/s)  LR: 8.183e-04  Data: 0.013 (0.018)
Train: 84 [ 450/1251 ( 36%)]  Loss: 3.747 (3.72)  Time: 0.674s, 1520.22/s  (0.683s, 1499.85/s)  LR: 8.182e-04  Data: 0.012 (0.017)
Train: 84 [ 500/1251 ( 40%)]  Loss: 3.691 (3.72)  Time: 0.678s, 1511.06/s  (0.683s, 1500.10/s)  LR: 8.180e-04  Data: 0.017 (0.017)
Train: 84 [ 550/1251 ( 44%)]  Loss: 3.698 (3.71)  Time: 0.676s, 1515.83/s  (0.682s, 1500.61/s)  LR: 8.178e-04  Data: 0.014 (0.017)
Train: 84 [ 600/1251 ( 48%)]  Loss: 3.422 (3.69)  Time: 0.685s, 1494.10/s  (0.682s, 1501.03/s)  LR: 8.177e-04  Data: 0.013 (0.016)
Train: 84 [ 650/1251 ( 52%)]  Loss: 3.500 (3.68)  Time: 0.682s, 1502.52/s  (0.682s, 1501.02/s)  LR: 8.175e-04  Data: 0.014 (0.016)
Train: 84 [ 700/1251 ( 56%)]  Loss: 3.837 (3.69)  Time: 0.675s, 1516.19/s  (0.682s, 1500.69/s)  LR: 8.174e-04  Data: 0.015 (0.016)
Train: 84 [ 750/1251 ( 60%)]  Loss: 3.808 (3.70)  Time: 0.675s, 1517.35/s  (0.682s, 1500.53/s)  LR: 8.172e-04  Data: 0.012 (0.016)
Train: 84 [ 800/1251 ( 64%)]  Loss: 3.525 (3.69)  Time: 0.682s, 1502.41/s  (0.682s, 1500.68/s)  LR: 8.170e-04  Data: 0.013 (0.016)
Train: 84 [ 850/1251 ( 68%)]  Loss: 3.784 (3.69)  Time: 0.678s, 1509.44/s  (0.682s, 1500.57/s)  LR: 8.169e-04  Data: 0.014 (0.016)
Train: 84 [ 900/1251 ( 72%)]  Loss: 3.912 (3.70)  Time: 0.689s, 1486.80/s  (0.682s, 1500.41/s)  LR: 8.167e-04  Data: 0.013 (0.015)
Train: 84 [ 950/1251 ( 76%)]  Loss: 3.807 (3.71)  Time: 0.684s, 1497.52/s  (0.683s, 1500.26/s)  LR: 8.166e-04  Data: 0.014 (0.015)
Train: 84 [1000/1251 ( 80%)]  Loss: 3.784 (3.71)  Time: 0.678s, 1509.99/s  (0.683s, 1500.35/s)  LR: 8.164e-04  Data: 0.013 (0.015)
Train: 84 [1050/1251 ( 84%)]  Loss: 3.802 (3.72)  Time: 0.682s, 1500.59/s  (0.683s, 1500.36/s)  LR: 8.162e-04  Data: 0.012 (0.015)
Train: 84 [1100/1251 ( 88%)]  Loss: 3.566 (3.71)  Time: 0.683s, 1498.20/s  (0.682s, 1500.58/s)  LR: 8.161e-04  Data: 0.016 (0.015)
Train: 84 [1150/1251 ( 92%)]  Loss: 3.984 (3.72)  Time: 0.682s, 1501.95/s  (0.682s, 1500.84/s)  LR: 8.159e-04  Data: 0.014 (0.015)
Train: 84 [1200/1251 ( 96%)]  Loss: 4.174 (3.74)  Time: 0.669s, 1530.52/s  (0.682s, 1500.82/s)  LR: 8.157e-04  Data: 0.018 (0.015)
Train: 84 [1250/1251 (100%)]  Loss: 3.845 (3.74)  Time: 0.674s, 1518.78/s  (0.682s, 1500.88/s)  LR: 8.156e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.791 (2.791)  Loss:  0.5122 (0.5122)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.171 (0.324)  Loss:  0.6685 (1.1148)  Acc@1: 84.4340 (73.6200)  Acc@5: 95.7547 (92.0860)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-84.pth.tar', 73.62000009033203)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-83.pth.tar', 73.57200000976563)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-78.pth.tar', 73.51800003417969)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-79.pth.tar', 73.47199993652343)

Train: 85 [   0/1251 (  0%)]  Loss: 3.962 (3.96)  Time: 3.884s,  263.66/s  (3.884s,  263.66/s)  LR: 8.156e-04  Data: 1.730 (1.730)
Train: 85 [  50/1251 (  4%)]  Loss: 3.582 (3.77)  Time: 0.670s, 1529.47/s  (0.710s, 1441.69/s)  LR: 8.154e-04  Data: 0.014 (0.048)
Train: 85 [ 100/1251 (  8%)]  Loss: 3.599 (3.71)  Time: 0.680s, 1505.13/s  (0.693s, 1477.05/s)  LR: 8.153e-04  Data: 0.013 (0.031)
Train: 85 [ 150/1251 ( 12%)]  Loss: 3.637 (3.70)  Time: 0.688s, 1487.71/s  (0.689s, 1486.13/s)  LR: 8.151e-04  Data: 0.013 (0.025)
Train: 85 [ 200/1251 ( 16%)]  Loss: 3.884 (3.73)  Time: 0.675s, 1517.55/s  (0.687s, 1490.85/s)  LR: 8.149e-04  Data: 0.013 (0.023)
Train: 85 [ 250/1251 ( 20%)]  Loss: 3.771 (3.74)  Time: 0.693s, 1478.39/s  (0.686s, 1493.04/s)  LR: 8.148e-04  Data: 0.014 (0.021)
Train: 85 [ 300/1251 ( 24%)]  Loss: 3.618 (3.72)  Time: 0.674s, 1518.81/s  (0.685s, 1494.08/s)  LR: 8.146e-04  Data: 0.016 (0.020)
Train: 85 [ 350/1251 ( 28%)]  Loss: 4.000 (3.76)  Time: 0.687s, 1490.74/s  (0.685s, 1494.54/s)  LR: 8.144e-04  Data: 0.014 (0.019)
Train: 85 [ 400/1251 ( 32%)]  Loss: 3.591 (3.74)  Time: 0.678s, 1510.59/s  (0.685s, 1495.81/s)  LR: 8.143e-04  Data: 0.012 (0.018)
Train: 85 [ 450/1251 ( 36%)]  Loss: 3.553 (3.72)  Time: 0.674s, 1519.23/s  (0.684s, 1496.84/s)  LR: 8.141e-04  Data: 0.015 (0.018)
Train: 85 [ 500/1251 ( 40%)]  Loss: 3.513 (3.70)  Time: 0.682s, 1502.23/s  (0.684s, 1497.19/s)  LR: 8.140e-04  Data: 0.016 (0.017)
Train: 85 [ 550/1251 ( 44%)]  Loss: 3.444 (3.68)  Time: 0.678s, 1511.29/s  (0.684s, 1497.68/s)  LR: 8.138e-04  Data: 0.018 (0.017)
Train: 85 [ 600/1251 ( 48%)]  Loss: 3.765 (3.69)  Time: 0.680s, 1504.84/s  (0.684s, 1498.08/s)  LR: 8.136e-04  Data: 0.013 (0.017)
Train: 85 [ 650/1251 ( 52%)]  Loss: 3.567 (3.68)  Time: 0.680s, 1506.10/s  (0.683s, 1498.44/s)  LR: 8.135e-04  Data: 0.016 (0.016)
Train: 85 [ 700/1251 ( 56%)]  Loss: 3.486 (3.66)  Time: 0.680s, 1506.79/s  (0.683s, 1499.01/s)  LR: 8.133e-04  Data: 0.013 (0.016)
Train: 85 [ 750/1251 ( 60%)]  Loss: 3.875 (3.68)  Time: 0.670s, 1527.67/s  (0.683s, 1499.47/s)  LR: 8.132e-04  Data: 0.016 (0.016)
Train: 85 [ 800/1251 ( 64%)]  Loss: 3.837 (3.69)  Time: 0.682s, 1502.44/s  (0.683s, 1499.99/s)  LR: 8.130e-04  Data: 0.013 (0.016)
Train: 85 [ 850/1251 ( 68%)]  Loss: 3.626 (3.68)  Time: 0.673s, 1521.30/s  (0.683s, 1500.29/s)  LR: 8.128e-04  Data: 0.015 (0.016)
Train: 85 [ 900/1251 ( 72%)]  Loss: 3.780 (3.69)  Time: 0.677s, 1513.15/s  (0.682s, 1500.91/s)  LR: 8.127e-04  Data: 0.014 (0.016)
Train: 85 [ 950/1251 ( 76%)]  Loss: 3.481 (3.68)  Time: 0.678s, 1511.35/s  (0.682s, 1501.48/s)  LR: 8.125e-04  Data: 0.013 (0.016)
Train: 85 [1000/1251 ( 80%)]  Loss: 3.636 (3.68)  Time: 0.673s, 1522.59/s  (0.682s, 1502.18/s)  LR: 8.123e-04  Data: 0.013 (0.016)
Train: 85 [1050/1251 ( 84%)]  Loss: 3.724 (3.68)  Time: 0.679s, 1507.48/s  (0.681s, 1502.60/s)  LR: 8.122e-04  Data: 0.014 (0.015)
Train: 85 [1100/1251 ( 88%)]  Loss: 3.769 (3.68)  Time: 0.676s, 1514.66/s  (0.681s, 1502.89/s)  LR: 8.120e-04  Data: 0.014 (0.015)
Train: 85 [1150/1251 ( 92%)]  Loss: 3.690 (3.68)  Time: 0.693s, 1476.83/s  (0.681s, 1502.95/s)  LR: 8.118e-04  Data: 0.013 (0.015)
Train: 85 [1200/1251 ( 96%)]  Loss: 3.597 (3.68)  Time: 0.681s, 1504.33/s  (0.681s, 1503.20/s)  LR: 8.117e-04  Data: 0.013 (0.015)
Train: 85 [1250/1251 (100%)]  Loss: 3.558 (3.67)  Time: 0.675s, 1517.30/s  (0.681s, 1503.55/s)  LR: 8.115e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.743 (2.743)  Loss:  0.5063 (0.5063)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.172 (0.323)  Loss:  0.5947 (1.1078)  Acc@1: 85.6132 (73.8300)  Acc@5: 97.1698 (92.2140)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-85.pth.tar', 73.83000008544921)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-84.pth.tar', 73.62000009033203)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-83.pth.tar', 73.57200000976563)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-78.pth.tar', 73.51800003417969)

Train: 86 [   0/1251 (  0%)]  Loss: 3.870 (3.87)  Time: 3.534s,  289.74/s  (3.534s,  289.74/s)  LR: 8.115e-04  Data: 1.743 (1.743)
Train: 86 [  50/1251 (  4%)]  Loss: 3.368 (3.62)  Time: 0.673s, 1521.76/s  (0.707s, 1448.62/s)  LR: 8.114e-04  Data: 0.013 (0.048)
Train: 86 [ 100/1251 (  8%)]  Loss: 3.823 (3.69)  Time: 0.671s, 1525.34/s  (0.690s, 1484.28/s)  LR: 8.112e-04  Data: 0.014 (0.031)
Train: 86 [ 150/1251 ( 12%)]  Loss: 3.904 (3.74)  Time: 0.678s, 1510.84/s  (0.685s, 1495.15/s)  LR: 8.110e-04  Data: 0.013 (0.025)
Train: 86 [ 200/1251 ( 16%)]  Loss: 3.805 (3.75)  Time: 0.679s, 1507.68/s  (0.683s, 1499.66/s)  LR: 8.109e-04  Data: 0.013 (0.022)
Train: 86 [ 250/1251 ( 20%)]  Loss: 3.828 (3.77)  Time: 0.686s, 1492.46/s  (0.682s, 1502.19/s)  LR: 8.107e-04  Data: 0.014 (0.021)
Train: 86 [ 300/1251 ( 24%)]  Loss: 3.386 (3.71)  Time: 0.676s, 1515.77/s  (0.681s, 1503.91/s)  LR: 8.105e-04  Data: 0.013 (0.019)
Train: 86 [ 350/1251 ( 28%)]  Loss: 3.447 (3.68)  Time: 0.677s, 1513.14/s  (0.680s, 1505.19/s)  LR: 8.104e-04  Data: 0.012 (0.019)
Train: 86 [ 400/1251 ( 32%)]  Loss: 3.595 (3.67)  Time: 0.675s, 1517.52/s  (0.680s, 1506.17/s)  LR: 8.102e-04  Data: 0.016 (0.018)
Train: 86 [ 450/1251 ( 36%)]  Loss: 3.454 (3.65)  Time: 0.686s, 1492.50/s  (0.680s, 1506.91/s)  LR: 8.100e-04  Data: 0.012 (0.017)
Train: 86 [ 500/1251 ( 40%)]  Loss: 3.816 (3.66)  Time: 0.670s, 1528.27/s  (0.679s, 1507.51/s)  LR: 8.099e-04  Data: 0.013 (0.017)
Train: 86 [ 550/1251 ( 44%)]  Loss: 3.811 (3.68)  Time: 0.669s, 1531.20/s  (0.679s, 1508.27/s)  LR: 8.097e-04  Data: 0.013 (0.017)
Train: 86 [ 600/1251 ( 48%)]  Loss: 3.653 (3.67)  Time: 0.670s, 1528.31/s  (0.678s, 1509.29/s)  LR: 8.096e-04  Data: 0.013 (0.016)
Train: 86 [ 650/1251 ( 52%)]  Loss: 3.754 (3.68)  Time: 0.678s, 1510.02/s  (0.678s, 1509.94/s)  LR: 8.094e-04  Data: 0.015 (0.016)
Train: 86 [ 700/1251 ( 56%)]  Loss: 3.756 (3.68)  Time: 0.666s, 1537.88/s  (0.678s, 1510.46/s)  LR: 8.092e-04  Data: 0.013 (0.016)
Train: 86 [ 750/1251 ( 60%)]  Loss: 3.913 (3.70)  Time: 0.682s, 1502.35/s  (0.678s, 1510.84/s)  LR: 8.091e-04  Data: 0.017 (0.016)
Train: 86 [ 800/1251 ( 64%)]  Loss: 3.788 (3.70)  Time: 0.675s, 1516.68/s  (0.678s, 1510.84/s)  LR: 8.089e-04  Data: 0.012 (0.016)
Train: 86 [ 850/1251 ( 68%)]  Loss: 3.664 (3.70)  Time: 0.674s, 1518.28/s  (0.678s, 1510.85/s)  LR: 8.087e-04  Data: 0.013 (0.016)
Train: 86 [ 900/1251 ( 72%)]  Loss: 3.702 (3.70)  Time: 0.673s, 1521.50/s  (0.678s, 1510.89/s)  LR: 8.086e-04  Data: 0.012 (0.015)
Train: 86 [ 950/1251 ( 76%)]  Loss: 3.649 (3.70)  Time: 0.677s, 1513.50/s  (0.678s, 1510.83/s)  LR: 8.084e-04  Data: 0.013 (0.015)
Train: 86 [1000/1251 ( 80%)]  Loss: 3.740 (3.70)  Time: 0.679s, 1508.45/s  (0.678s, 1510.96/s)  LR: 8.082e-04  Data: 0.014 (0.015)
Train: 86 [1050/1251 ( 84%)]  Loss: 3.737 (3.70)  Time: 0.680s, 1505.58/s  (0.678s, 1511.02/s)  LR: 8.081e-04  Data: 0.012 (0.015)
Train: 86 [1100/1251 ( 88%)]  Loss: 3.816 (3.71)  Time: 0.672s, 1524.31/s  (0.678s, 1511.17/s)  LR: 8.079e-04  Data: 0.013 (0.015)
Train: 86 [1150/1251 ( 92%)]  Loss: 3.836 (3.71)  Time: 0.671s, 1526.45/s  (0.678s, 1511.31/s)  LR: 8.078e-04  Data: 0.013 (0.015)
Train: 86 [1200/1251 ( 96%)]  Loss: 3.795 (3.72)  Time: 0.684s, 1496.44/s  (0.678s, 1511.31/s)  LR: 8.076e-04  Data: 0.013 (0.015)
Train: 86 [1250/1251 (100%)]  Loss: 3.719 (3.72)  Time: 0.669s, 1530.37/s  (0.677s, 1511.49/s)  LR: 8.074e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.797 (2.797)  Loss:  0.5552 (0.5552)  Acc@1: 88.8672 (88.8672)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.168 (0.327)  Loss:  0.6992 (1.1371)  Acc@1: 84.7877 (74.1360)  Acc@5: 96.1085 (92.2620)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-86.pth.tar', 74.1360001147461)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-85.pth.tar', 73.83000008544921)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-84.pth.tar', 73.62000009033203)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-83.pth.tar', 73.57200000976563)

Train: 87 [   0/1251 (  0%)]  Loss: 4.029 (4.03)  Time: 3.262s,  313.89/s  (3.262s,  313.89/s)  LR: 8.074e-04  Data: 1.708 (1.708)
Train: 87 [  50/1251 (  4%)]  Loss: 3.746 (3.89)  Time: 0.662s, 1546.67/s  (0.698s, 1467.49/s)  LR: 8.073e-04  Data: 0.013 (0.047)
Train: 87 [ 100/1251 (  8%)]  Loss: 3.982 (3.92)  Time: 0.677s, 1511.80/s  (0.684s, 1497.99/s)  LR: 8.071e-04  Data: 0.013 (0.031)
Train: 87 [ 150/1251 ( 12%)]  Loss: 3.733 (3.87)  Time: 0.680s, 1505.19/s  (0.681s, 1502.90/s)  LR: 8.069e-04  Data: 0.011 (0.025)
Train: 87 [ 200/1251 ( 16%)]  Loss: 3.527 (3.80)  Time: 0.680s, 1504.84/s  (0.680s, 1505.13/s)  LR: 8.068e-04  Data: 0.013 (0.022)
Train: 87 [ 250/1251 ( 20%)]  Loss: 3.524 (3.76)  Time: 0.677s, 1512.57/s  (0.680s, 1506.59/s)  LR: 8.066e-04  Data: 0.017 (0.021)
Train: 87 [ 300/1251 ( 24%)]  Loss: 3.892 (3.78)  Time: 0.682s, 1501.76/s  (0.679s, 1507.80/s)  LR: 8.064e-04  Data: 0.013 (0.019)
Train: 87 [ 350/1251 ( 28%)]  Loss: 3.797 (3.78)  Time: 0.682s, 1500.90/s  (0.679s, 1508.21/s)  LR: 8.063e-04  Data: 0.014 (0.019)
Train: 87 [ 400/1251 ( 32%)]  Loss: 3.612 (3.76)  Time: 0.683s, 1498.87/s  (0.679s, 1508.23/s)  LR: 8.061e-04  Data: 0.013 (0.018)
Train: 87 [ 450/1251 ( 36%)]  Loss: 3.699 (3.75)  Time: 0.681s, 1503.41/s  (0.679s, 1508.70/s)  LR: 8.059e-04  Data: 0.013 (0.017)
Train: 87 [ 500/1251 ( 40%)]  Loss: 3.780 (3.76)  Time: 0.668s, 1534.05/s  (0.679s, 1508.56/s)  LR: 8.058e-04  Data: 0.013 (0.017)
Train: 87 [ 550/1251 ( 44%)]  Loss: 3.701 (3.75)  Time: 0.685s, 1494.12/s  (0.679s, 1508.41/s)  LR: 8.056e-04  Data: 0.012 (0.017)
Train: 87 [ 600/1251 ( 48%)]  Loss: 3.461 (3.73)  Time: 0.678s, 1510.84/s  (0.679s, 1508.23/s)  LR: 8.054e-04  Data: 0.013 (0.016)
Train: 87 [ 650/1251 ( 52%)]  Loss: 3.903 (3.74)  Time: 0.673s, 1522.48/s  (0.679s, 1508.44/s)  LR: 8.053e-04  Data: 0.013 (0.016)
Train: 87 [ 700/1251 ( 56%)]  Loss: 3.977 (3.76)  Time: 0.685s, 1495.88/s  (0.679s, 1508.42/s)  LR: 8.051e-04  Data: 0.014 (0.016)
Train: 87 [ 750/1251 ( 60%)]  Loss: 3.937 (3.77)  Time: 0.673s, 1521.22/s  (0.679s, 1508.72/s)  LR: 8.049e-04  Data: 0.013 (0.016)
Train: 87 [ 800/1251 ( 64%)]  Loss: 3.427 (3.75)  Time: 0.672s, 1523.62/s  (0.679s, 1509.13/s)  LR: 8.048e-04  Data: 0.013 (0.016)
Train: 87 [ 850/1251 ( 68%)]  Loss: 3.408 (3.73)  Time: 0.684s, 1497.48/s  (0.679s, 1509.05/s)  LR: 8.046e-04  Data: 0.011 (0.015)
Train: 87 [ 900/1251 ( 72%)]  Loss: 3.599 (3.72)  Time: 0.669s, 1530.30/s  (0.679s, 1508.93/s)  LR: 8.044e-04  Data: 0.012 (0.015)
Train: 87 [ 950/1251 ( 76%)]  Loss: 3.830 (3.73)  Time: 0.683s, 1498.21/s  (0.679s, 1509.00/s)  LR: 8.043e-04  Data: 0.012 (0.015)
Train: 87 [1000/1251 ( 80%)]  Loss: 3.622 (3.72)  Time: 0.682s, 1500.76/s  (0.679s, 1508.79/s)  LR: 8.041e-04  Data: 0.013 (0.015)
Train: 87 [1050/1251 ( 84%)]  Loss: 3.370 (3.71)  Time: 0.684s, 1497.13/s  (0.679s, 1508.64/s)  LR: 8.040e-04  Data: 0.014 (0.015)
Train: 87 [1100/1251 ( 88%)]  Loss: 3.329 (3.69)  Time: 0.681s, 1503.92/s  (0.679s, 1508.48/s)  LR: 8.038e-04  Data: 0.015 (0.015)
Train: 87 [1150/1251 ( 92%)]  Loss: 3.874 (3.70)  Time: 0.693s, 1477.23/s  (0.679s, 1508.64/s)  LR: 8.036e-04  Data: 0.015 (0.015)
Train: 87 [1200/1251 ( 96%)]  Loss: 3.429 (3.69)  Time: 0.680s, 1506.70/s  (0.679s, 1508.74/s)  LR: 8.035e-04  Data: 0.013 (0.015)
Train: 87 [1250/1251 (100%)]  Loss: 3.543 (3.68)  Time: 0.671s, 1526.12/s  (0.679s, 1508.79/s)  LR: 8.033e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.841 (2.841)  Loss:  0.5156 (0.5156)  Acc@1: 89.6484 (89.6484)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.171 (0.321)  Loss:  0.6299 (1.0937)  Acc@1: 85.3774 (74.0080)  Acc@5: 96.6981 (92.2760)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-86.pth.tar', 74.1360001147461)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-87.pth.tar', 74.00799998291015)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-85.pth.tar', 73.83000008544921)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-84.pth.tar', 73.62000009033203)

Train: 88 [   0/1251 (  0%)]  Loss: 3.817 (3.82)  Time: 3.733s,  274.33/s  (3.733s,  274.33/s)  LR: 8.033e-04  Data: 1.781 (1.781)
Train: 88 [  50/1251 (  4%)]  Loss: 3.292 (3.55)  Time: 0.672s, 1524.12/s  (0.706s, 1451.05/s)  LR: 8.031e-04  Data: 0.016 (0.048)
Train: 88 [ 100/1251 (  8%)]  Loss: 3.817 (3.64)  Time: 0.683s, 1499.91/s  (0.690s, 1484.98/s)  LR: 8.030e-04  Data: 0.012 (0.031)
Train: 88 [ 150/1251 ( 12%)]  Loss: 3.580 (3.63)  Time: 0.674s, 1519.36/s  (0.686s, 1491.64/s)  LR: 8.028e-04  Data: 0.012 (0.025)
Train: 88 [ 200/1251 ( 16%)]  Loss: 3.653 (3.63)  Time: 0.675s, 1516.78/s  (0.685s, 1495.44/s)  LR: 8.026e-04  Data: 0.018 (0.022)
Train: 88 [ 250/1251 ( 20%)]  Loss: 3.540 (3.62)  Time: 0.677s, 1512.28/s  (0.683s, 1498.33/s)  LR: 8.025e-04  Data: 0.013 (0.021)
Train: 88 [ 300/1251 ( 24%)]  Loss: 3.799 (3.64)  Time: 0.684s, 1496.83/s  (0.683s, 1499.08/s)  LR: 8.023e-04  Data: 0.013 (0.019)
Train: 88 [ 350/1251 ( 28%)]  Loss: 3.663 (3.64)  Time: 0.685s, 1494.01/s  (0.683s, 1500.02/s)  LR: 8.021e-04  Data: 0.013 (0.019)
Train: 88 [ 400/1251 ( 32%)]  Loss: 3.667 (3.65)  Time: 0.679s, 1508.18/s  (0.682s, 1500.79/s)  LR: 8.020e-04  Data: 0.013 (0.018)
Train: 88 [ 450/1251 ( 36%)]  Loss: 3.846 (3.67)  Time: 0.680s, 1505.99/s  (0.682s, 1501.72/s)  LR: 8.018e-04  Data: 0.015 (0.018)
Train: 88 [ 500/1251 ( 40%)]  Loss: 3.624 (3.66)  Time: 0.684s, 1497.75/s  (0.682s, 1502.42/s)  LR: 8.016e-04  Data: 0.014 (0.017)
Train: 88 [ 550/1251 ( 44%)]  Loss: 3.819 (3.68)  Time: 0.680s, 1506.73/s  (0.681s, 1503.12/s)  LR: 8.015e-04  Data: 0.017 (0.017)
Train: 88 [ 600/1251 ( 48%)]  Loss: 3.318 (3.65)  Time: 0.684s, 1496.11/s  (0.681s, 1503.69/s)  LR: 8.013e-04  Data: 0.013 (0.017)
Train: 88 [ 650/1251 ( 52%)]  Loss: 3.630 (3.65)  Time: 0.679s, 1507.32/s  (0.681s, 1504.09/s)  LR: 8.011e-04  Data: 0.014 (0.016)
Train: 88 [ 700/1251 ( 56%)]  Loss: 3.556 (3.64)  Time: 0.683s, 1500.11/s  (0.681s, 1503.89/s)  LR: 8.010e-04  Data: 0.013 (0.016)
Train: 88 [ 750/1251 ( 60%)]  Loss: 3.574 (3.64)  Time: 0.683s, 1499.95/s  (0.681s, 1503.91/s)  LR: 8.008e-04  Data: 0.014 (0.016)
Train: 88 [ 800/1251 ( 64%)]  Loss: 3.676 (3.64)  Time: 0.675s, 1517.56/s  (0.681s, 1504.02/s)  LR: 8.006e-04  Data: 0.014 (0.016)
Train: 88 [ 850/1251 ( 68%)]  Loss: 3.861 (3.65)  Time: 0.676s, 1515.80/s  (0.681s, 1503.84/s)  LR: 8.005e-04  Data: 0.014 (0.016)
Train: 88 [ 900/1251 ( 72%)]  Loss: 3.782 (3.66)  Time: 0.691s, 1482.85/s  (0.681s, 1503.60/s)  LR: 8.003e-04  Data: 0.013 (0.015)
Train: 88 [ 950/1251 ( 76%)]  Loss: 3.745 (3.66)  Time: 0.684s, 1496.52/s  (0.681s, 1503.60/s)  LR: 8.001e-04  Data: 0.013 (0.015)
Train: 88 [1000/1251 ( 80%)]  Loss: 3.565 (3.66)  Time: 0.677s, 1511.65/s  (0.681s, 1503.30/s)  LR: 8.000e-04  Data: 0.012 (0.015)
Train: 88 [1050/1251 ( 84%)]  Loss: 3.544 (3.65)  Time: 0.671s, 1525.78/s  (0.681s, 1503.02/s)  LR: 7.998e-04  Data: 0.013 (0.015)
Train: 88 [1100/1251 ( 88%)]  Loss: 3.779 (3.66)  Time: 0.683s, 1498.31/s  (0.681s, 1503.08/s)  LR: 7.996e-04  Data: 0.014 (0.015)
Train: 88 [1150/1251 ( 92%)]  Loss: 3.826 (3.67)  Time: 0.680s, 1505.23/s  (0.681s, 1503.08/s)  LR: 7.995e-04  Data: 0.015 (0.015)
Train: 88 [1200/1251 ( 96%)]  Loss: 3.656 (3.67)  Time: 0.684s, 1497.08/s  (0.681s, 1502.88/s)  LR: 7.993e-04  Data: 0.012 (0.015)
Train: 88 [1250/1251 (100%)]  Loss: 3.531 (3.66)  Time: 0.678s, 1510.17/s  (0.681s, 1502.82/s)  LR: 7.991e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.879 (2.879)  Loss:  0.5264 (0.5264)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.173 (0.324)  Loss:  0.6475 (1.1078)  Acc@1: 83.9623 (74.0340)  Acc@5: 96.6981 (92.2420)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-86.pth.tar', 74.1360001147461)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-88.pth.tar', 74.03400001464844)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-87.pth.tar', 74.00799998291015)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-85.pth.tar', 73.83000008544921)

Train: 89 [   0/1251 (  0%)]  Loss: 3.586 (3.59)  Time: 3.616s,  283.19/s  (3.616s,  283.19/s)  LR: 7.991e-04  Data: 2.166 (2.166)
Train: 89 [  50/1251 (  4%)]  Loss: 3.844 (3.72)  Time: 0.673s, 1521.36/s  (0.706s, 1449.70/s)  LR: 7.990e-04  Data: 0.014 (0.056)
Train: 89 [ 100/1251 (  8%)]  Loss: 3.813 (3.75)  Time: 0.680s, 1505.42/s  (0.692s, 1479.66/s)  LR: 7.988e-04  Data: 0.013 (0.035)
Train: 89 [ 150/1251 ( 12%)]  Loss: 3.330 (3.64)  Time: 0.682s, 1502.39/s  (0.688s, 1488.72/s)  LR: 7.986e-04  Data: 0.012 (0.028)
Train: 89 [ 200/1251 ( 16%)]  Loss: 3.695 (3.65)  Time: 0.688s, 1489.39/s  (0.686s, 1493.64/s)  LR: 7.984e-04  Data: 0.013 (0.024)
Train: 89 [ 250/1251 ( 20%)]  Loss: 3.461 (3.62)  Time: 0.670s, 1528.90/s  (0.685s, 1495.92/s)  LR: 7.983e-04  Data: 0.015 (0.022)
Train: 89 [ 300/1251 ( 24%)]  Loss: 3.496 (3.60)  Time: 0.681s, 1504.44/s  (0.684s, 1498.15/s)  LR: 7.981e-04  Data: 0.013 (0.021)
Train: 89 [ 350/1251 ( 28%)]  Loss: 3.637 (3.61)  Time: 0.681s, 1502.88/s  (0.683s, 1499.17/s)  LR: 7.979e-04  Data: 0.012 (0.020)
Train: 89 [ 400/1251 ( 32%)]  Loss: 3.601 (3.61)  Time: 0.677s, 1513.58/s  (0.682s, 1500.85/s)  LR: 7.978e-04  Data: 0.013 (0.019)
Train: 89 [ 450/1251 ( 36%)]  Loss: 3.841 (3.63)  Time: 0.677s, 1513.43/s  (0.682s, 1502.01/s)  LR: 7.976e-04  Data: 0.013 (0.018)
Train: 89 [ 500/1251 ( 40%)]  Loss: 3.661 (3.63)  Time: 0.679s, 1509.13/s  (0.682s, 1502.42/s)  LR: 7.974e-04  Data: 0.013 (0.018)
Train: 89 [ 550/1251 ( 44%)]  Loss: 3.767 (3.64)  Time: 0.691s, 1482.81/s  (0.681s, 1502.90/s)  LR: 7.973e-04  Data: 0.012 (0.017)
Train: 89 [ 600/1251 ( 48%)]  Loss: 3.732 (3.65)  Time: 0.688s, 1488.29/s  (0.681s, 1503.43/s)  LR: 7.971e-04  Data: 0.013 (0.017)
Train: 89 [ 650/1251 ( 52%)]  Loss: 3.805 (3.66)  Time: 0.673s, 1520.59/s  (0.681s, 1504.29/s)  LR: 7.969e-04  Data: 0.012 (0.017)
Train: 89 [ 700/1251 ( 56%)]  Loss: 3.571 (3.66)  Time: 0.668s, 1533.69/s  (0.681s, 1504.71/s)  LR: 7.968e-04  Data: 0.015 (0.016)
Train: 89 [ 750/1251 ( 60%)]  Loss: 3.898 (3.67)  Time: 0.682s, 1501.91/s  (0.680s, 1505.09/s)  LR: 7.966e-04  Data: 0.015 (0.016)
Train: 89 [ 800/1251 ( 64%)]  Loss: 3.918 (3.69)  Time: 0.680s, 1506.11/s  (0.680s, 1505.39/s)  LR: 7.964e-04  Data: 0.012 (0.016)
Train: 89 [ 850/1251 ( 68%)]  Loss: 3.604 (3.68)  Time: 0.680s, 1506.44/s  (0.680s, 1505.81/s)  LR: 7.963e-04  Data: 0.013 (0.016)
Train: 89 [ 900/1251 ( 72%)]  Loss: 3.815 (3.69)  Time: 0.677s, 1512.35/s  (0.680s, 1505.89/s)  LR: 7.961e-04  Data: 0.013 (0.016)
Train: 89 [ 950/1251 ( 76%)]  Loss: 3.962 (3.70)  Time: 0.679s, 1507.32/s  (0.680s, 1506.02/s)  LR: 7.959e-04  Data: 0.013 (0.016)
Train: 89 [1000/1251 ( 80%)]  Loss: 3.720 (3.70)  Time: 0.676s, 1515.64/s  (0.680s, 1505.92/s)  LR: 7.958e-04  Data: 0.013 (0.016)
Train: 89 [1050/1251 ( 84%)]  Loss: 3.423 (3.69)  Time: 0.684s, 1497.94/s  (0.680s, 1506.04/s)  LR: 7.956e-04  Data: 0.014 (0.015)
Train: 89 [1100/1251 ( 88%)]  Loss: 3.705 (3.69)  Time: 0.679s, 1507.21/s  (0.680s, 1506.06/s)  LR: 7.954e-04  Data: 0.017 (0.015)
Train: 89 [1150/1251 ( 92%)]  Loss: 3.654 (3.69)  Time: 0.676s, 1515.67/s  (0.680s, 1506.09/s)  LR: 7.953e-04  Data: 0.012 (0.015)
Train: 89 [1200/1251 ( 96%)]  Loss: 3.415 (3.68)  Time: 0.679s, 1508.48/s  (0.680s, 1505.88/s)  LR: 7.951e-04  Data: 0.012 (0.015)
Train: 89 [1250/1251 (100%)]  Loss: 3.729 (3.68)  Time: 0.679s, 1508.05/s  (0.680s, 1505.97/s)  LR: 7.949e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.778 (2.778)  Loss:  0.4963 (0.4963)  Acc@1: 90.0391 (90.0391)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.172 (0.325)  Loss:  0.6548 (1.1097)  Acc@1: 84.4340 (74.0920)  Acc@5: 96.5802 (92.3140)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-86.pth.tar', 74.1360001147461)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-89.pth.tar', 74.09200009033204)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-88.pth.tar', 74.03400001464844)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-87.pth.tar', 74.00799998291015)

Train: 90 [   0/1251 (  0%)]  Loss: 3.873 (3.87)  Time: 3.645s,  280.93/s  (3.645s,  280.93/s)  LR: 7.949e-04  Data: 1.580 (1.580)
Train: 90 [  50/1251 (  4%)]  Loss: 3.717 (3.80)  Time: 0.669s, 1529.65/s  (0.700s, 1463.65/s)  LR: 7.948e-04  Data: 0.013 (0.044)
Train: 90 [ 100/1251 (  8%)]  Loss: 3.881 (3.82)  Time: 0.667s, 1535.27/s  (0.686s, 1492.77/s)  LR: 7.946e-04  Data: 0.013 (0.029)
Train: 90 [ 150/1251 ( 12%)]  Loss: 3.831 (3.83)  Time: 0.686s, 1492.40/s  (0.682s, 1501.90/s)  LR: 7.944e-04  Data: 0.014 (0.024)
Train: 90 [ 200/1251 ( 16%)]  Loss: 3.781 (3.82)  Time: 0.673s, 1520.82/s  (0.680s, 1505.19/s)  LR: 7.942e-04  Data: 0.013 (0.021)
Train: 90 [ 250/1251 ( 20%)]  Loss: 3.664 (3.79)  Time: 0.670s, 1528.74/s  (0.680s, 1506.79/s)  LR: 7.941e-04  Data: 0.014 (0.020)
Train: 90 [ 300/1251 ( 24%)]  Loss: 3.606 (3.76)  Time: 0.680s, 1504.94/s  (0.679s, 1507.36/s)  LR: 7.939e-04  Data: 0.014 (0.019)
Train: 90 [ 350/1251 ( 28%)]  Loss: 3.708 (3.76)  Time: 0.681s, 1502.71/s  (0.679s, 1507.59/s)  LR: 7.937e-04  Data: 0.013 (0.018)
Train: 90 [ 400/1251 ( 32%)]  Loss: 3.435 (3.72)  Time: 0.676s, 1514.96/s  (0.679s, 1508.14/s)  LR: 7.936e-04  Data: 0.013 (0.018)
Train: 90 [ 450/1251 ( 36%)]  Loss: 3.730 (3.72)  Time: 0.677s, 1513.22/s  (0.679s, 1508.34/s)  LR: 7.934e-04  Data: 0.013 (0.017)
Train: 90 [ 500/1251 ( 40%)]  Loss: 3.766 (3.73)  Time: 0.669s, 1531.60/s  (0.679s, 1508.54/s)  LR: 7.932e-04  Data: 0.015 (0.017)
Train: 90 [ 550/1251 ( 44%)]  Loss: 3.343 (3.69)  Time: 0.681s, 1504.11/s  (0.679s, 1508.55/s)  LR: 7.931e-04  Data: 0.014 (0.016)
Train: 90 [ 600/1251 ( 48%)]  Loss: 3.938 (3.71)  Time: 0.684s, 1497.28/s  (0.678s, 1509.49/s)  LR: 7.929e-04  Data: 0.013 (0.016)
Train: 90 [ 650/1251 ( 52%)]  Loss: 3.680 (3.71)  Time: 0.677s, 1511.93/s  (0.678s, 1509.91/s)  LR: 7.927e-04  Data: 0.013 (0.016)
Train: 90 [ 700/1251 ( 56%)]  Loss: 3.642 (3.71)  Time: 0.683s, 1499.84/s  (0.678s, 1510.23/s)  LR: 7.926e-04  Data: 0.014 (0.016)
Train: 90 [ 750/1251 ( 60%)]  Loss: 3.726 (3.71)  Time: 0.672s, 1524.88/s  (0.678s, 1510.42/s)  LR: 7.924e-04  Data: 0.015 (0.016)
Train: 90 [ 800/1251 ( 64%)]  Loss: 3.800 (3.71)  Time: 0.677s, 1513.08/s  (0.678s, 1510.77/s)  LR: 7.922e-04  Data: 0.013 (0.016)
Train: 90 [ 850/1251 ( 68%)]  Loss: 3.814 (3.72)  Time: 0.668s, 1532.31/s  (0.678s, 1511.08/s)  LR: 7.920e-04  Data: 0.013 (0.015)
Train: 90 [ 900/1251 ( 72%)]  Loss: 3.573 (3.71)  Time: 0.678s, 1510.89/s  (0.678s, 1511.42/s)  LR: 7.919e-04  Data: 0.014 (0.015)
Train: 90 [ 950/1251 ( 76%)]  Loss: 3.611 (3.71)  Time: 0.675s, 1516.40/s  (0.678s, 1511.40/s)  LR: 7.917e-04  Data: 0.012 (0.015)
Train: 90 [1000/1251 ( 80%)]  Loss: 3.719 (3.71)  Time: 0.681s, 1504.17/s  (0.678s, 1511.35/s)  LR: 7.915e-04  Data: 0.016 (0.015)
Train: 90 [1050/1251 ( 84%)]  Loss: 3.905 (3.72)  Time: 0.673s, 1521.54/s  (0.678s, 1511.30/s)  LR: 7.914e-04  Data: 0.013 (0.015)
Train: 90 [1100/1251 ( 88%)]  Loss: 3.811 (3.72)  Time: 0.679s, 1508.99/s  (0.678s, 1511.35/s)  LR: 7.912e-04  Data: 0.014 (0.015)
Train: 90 [1150/1251 ( 92%)]  Loss: 3.823 (3.72)  Time: 0.670s, 1529.27/s  (0.678s, 1511.41/s)  LR: 7.910e-04  Data: 0.012 (0.015)
Train: 90 [1200/1251 ( 96%)]  Loss: 3.603 (3.72)  Time: 0.680s, 1506.26/s  (0.677s, 1511.78/s)  LR: 7.909e-04  Data: 0.015 (0.015)
Train: 90 [1250/1251 (100%)]  Loss: 3.892 (3.73)  Time: 0.655s, 1563.28/s  (0.677s, 1512.09/s)  LR: 7.907e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.754 (2.754)  Loss:  0.5249 (0.5249)  Acc@1: 89.1602 (89.1602)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.169 (0.325)  Loss:  0.6240 (1.0893)  Acc@1: 83.8443 (74.2880)  Acc@5: 96.4623 (92.4900)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-90.pth.tar', 74.28799996337891)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-86.pth.tar', 74.1360001147461)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-89.pth.tar', 74.09200009033204)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-88.pth.tar', 74.03400001464844)

Train: 91 [   0/1251 (  0%)]  Loss: 3.544 (3.54)  Time: 4.029s,  254.17/s  (4.029s,  254.17/s)  LR: 7.907e-04  Data: 1.798 (1.798)
Train: 91 [  50/1251 (  4%)]  Loss: 3.676 (3.61)  Time: 0.660s, 1552.10/s  (0.706s, 1449.57/s)  LR: 7.905e-04  Data: 0.014 (0.048)
Train: 91 [ 100/1251 (  8%)]  Loss: 3.889 (3.70)  Time: 0.675s, 1516.46/s  (0.687s, 1490.18/s)  LR: 7.903e-04  Data: 0.014 (0.031)
Train: 91 [ 150/1251 ( 12%)]  Loss: 3.774 (3.72)  Time: 0.682s, 1501.19/s  (0.681s, 1502.80/s)  LR: 7.902e-04  Data: 0.012 (0.025)
Train: 91 [ 200/1251 ( 16%)]  Loss: 3.871 (3.75)  Time: 0.680s, 1506.82/s  (0.679s, 1508.61/s)  LR: 7.900e-04  Data: 0.013 (0.022)
Train: 91 [ 250/1251 ( 20%)]  Loss: 3.571 (3.72)  Time: 0.668s, 1533.51/s  (0.677s, 1511.99/s)  LR: 7.898e-04  Data: 0.012 (0.021)
Train: 91 [ 300/1251 ( 24%)]  Loss: 3.442 (3.68)  Time: 0.676s, 1515.61/s  (0.677s, 1513.65/s)  LR: 7.897e-04  Data: 0.013 (0.019)
Train: 91 [ 350/1251 ( 28%)]  Loss: 3.508 (3.66)  Time: 0.671s, 1525.33/s  (0.676s, 1514.90/s)  LR: 7.895e-04  Data: 0.017 (0.018)
Train: 91 [ 400/1251 ( 32%)]  Loss: 3.951 (3.69)  Time: 0.667s, 1534.61/s  (0.675s, 1515.99/s)  LR: 7.893e-04  Data: 0.013 (0.018)
Train: 91 [ 450/1251 ( 36%)]  Loss: 3.826 (3.71)  Time: 0.671s, 1525.26/s  (0.675s, 1517.33/s)  LR: 7.892e-04  Data: 0.013 (0.017)
Train: 91 [ 500/1251 ( 40%)]  Loss: 3.739 (3.71)  Time: 0.675s, 1516.06/s  (0.675s, 1518.12/s)  LR: 7.890e-04  Data: 0.014 (0.017)
Train: 91 [ 550/1251 ( 44%)]  Loss: 3.586 (3.70)  Time: 0.671s, 1526.02/s  (0.674s, 1519.12/s)  LR: 7.888e-04  Data: 0.012 (0.017)
Train: 91 [ 600/1251 ( 48%)]  Loss: 3.641 (3.69)  Time: 0.671s, 1525.36/s  (0.674s, 1519.75/s)  LR: 7.886e-04  Data: 0.013 (0.016)
Train: 91 [ 650/1251 ( 52%)]  Loss: 3.601 (3.69)  Time: 0.672s, 1524.24/s  (0.674s, 1519.85/s)  LR: 7.885e-04  Data: 0.014 (0.016)
Train: 91 [ 700/1251 ( 56%)]  Loss: 3.815 (3.70)  Time: 0.679s, 1508.62/s  (0.674s, 1520.04/s)  LR: 7.883e-04  Data: 0.013 (0.016)
Train: 91 [ 750/1251 ( 60%)]  Loss: 3.557 (3.69)  Time: 0.665s, 1539.72/s  (0.674s, 1520.20/s)  LR: 7.881e-04  Data: 0.017 (0.016)
Train: 91 [ 800/1251 ( 64%)]  Loss: 3.950 (3.70)  Time: 0.684s, 1497.41/s  (0.673s, 1520.47/s)  LR: 7.880e-04  Data: 0.016 (0.016)
Train: 91 [ 850/1251 ( 68%)]  Loss: 3.566 (3.69)  Time: 0.676s, 1515.82/s  (0.673s, 1520.64/s)  LR: 7.878e-04  Data: 0.013 (0.015)
Train: 91 [ 900/1251 ( 72%)]  Loss: 3.566 (3.69)  Time: 0.672s, 1522.79/s  (0.673s, 1520.80/s)  LR: 7.876e-04  Data: 0.012 (0.015)
Train: 91 [ 950/1251 ( 76%)]  Loss: 3.911 (3.70)  Time: 0.682s, 1501.39/s  (0.673s, 1521.27/s)  LR: 7.875e-04  Data: 0.013 (0.015)
Train: 91 [1000/1251 ( 80%)]  Loss: 3.890 (3.71)  Time: 0.672s, 1524.27/s  (0.673s, 1521.81/s)  LR: 7.873e-04  Data: 0.014 (0.015)
Train: 91 [1050/1251 ( 84%)]  Loss: 3.668 (3.71)  Time: 0.673s, 1521.77/s  (0.673s, 1522.47/s)  LR: 7.871e-04  Data: 0.013 (0.015)
Train: 91 [1100/1251 ( 88%)]  Loss: 3.699 (3.71)  Time: 0.652s, 1569.36/s  (0.672s, 1523.12/s)  LR: 7.869e-04  Data: 0.016 (0.015)
Train: 91 [1150/1251 ( 92%)]  Loss: 3.741 (3.71)  Time: 0.669s, 1530.67/s  (0.672s, 1523.55/s)  LR: 7.868e-04  Data: 0.014 (0.015)
Train: 91 [1200/1251 ( 96%)]  Loss: 4.065 (3.72)  Time: 0.664s, 1542.88/s  (0.672s, 1524.05/s)  LR: 7.866e-04  Data: 0.013 (0.015)
Train: 91 [1250/1251 (100%)]  Loss: 3.984 (3.73)  Time: 0.653s, 1567.23/s  (0.672s, 1524.69/s)  LR: 7.864e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.863 (2.863)  Loss:  0.5303 (0.5303)  Acc@1: 88.4766 (88.4766)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.161 (0.325)  Loss:  0.6338 (1.1009)  Acc@1: 85.1415 (74.1720)  Acc@5: 96.6981 (92.3940)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-90.pth.tar', 74.28799996337891)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-91.pth.tar', 74.17200000976563)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-86.pth.tar', 74.1360001147461)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-89.pth.tar', 74.09200009033204)

Train: 92 [   0/1251 (  0%)]  Loss: 3.448 (3.45)  Time: 4.094s,  250.14/s  (4.094s,  250.14/s)  LR: 7.864e-04  Data: 1.984 (1.984)
Train: 92 [  50/1251 (  4%)]  Loss: 3.855 (3.65)  Time: 0.654s, 1565.16/s  (0.696s, 1472.04/s)  LR: 7.863e-04  Data: 0.013 (0.052)
Train: 92 [ 100/1251 (  8%)]  Loss: 3.879 (3.73)  Time: 0.660s, 1552.07/s  (0.678s, 1510.71/s)  LR: 7.861e-04  Data: 0.014 (0.033)
Train: 92 [ 150/1251 ( 12%)]  Loss: 3.853 (3.76)  Time: 0.663s, 1544.59/s  (0.673s, 1521.64/s)  LR: 7.859e-04  Data: 0.015 (0.027)
Train: 92 [ 200/1251 ( 16%)]  Loss: 3.568 (3.72)  Time: 0.663s, 1543.33/s  (0.671s, 1526.28/s)  LR: 7.857e-04  Data: 0.012 (0.024)
Train: 92 [ 250/1251 ( 20%)]  Loss: 3.168 (3.63)  Time: 0.661s, 1549.82/s  (0.670s, 1528.73/s)  LR: 7.856e-04  Data: 0.014 (0.022)
Train: 92 [ 300/1251 ( 24%)]  Loss: 3.430 (3.60)  Time: 0.667s, 1536.35/s  (0.669s, 1529.78/s)  LR: 7.854e-04  Data: 0.014 (0.020)
Train: 92 [ 350/1251 ( 28%)]  Loss: 3.965 (3.65)  Time: 0.664s, 1542.82/s  (0.669s, 1530.34/s)  LR: 7.852e-04  Data: 0.013 (0.019)
Train: 92 [ 400/1251 ( 32%)]  Loss: 3.623 (3.64)  Time: 0.669s, 1530.33/s  (0.669s, 1530.64/s)  LR: 7.851e-04  Data: 0.013 (0.019)
Train: 92 [ 450/1251 ( 36%)]  Loss: 3.414 (3.62)  Time: 0.675s, 1516.76/s  (0.669s, 1530.59/s)  LR: 7.849e-04  Data: 0.013 (0.018)
Train: 92 [ 500/1251 ( 40%)]  Loss: 3.758 (3.63)  Time: 0.670s, 1527.61/s  (0.669s, 1530.60/s)  LR: 7.847e-04  Data: 0.013 (0.018)
Train: 92 [ 550/1251 ( 44%)]  Loss: 4.017 (3.66)  Time: 0.673s, 1522.46/s  (0.669s, 1530.77/s)  LR: 7.845e-04  Data: 0.013 (0.017)
Train: 92 [ 600/1251 ( 48%)]  Loss: 3.622 (3.66)  Time: 0.669s, 1529.53/s  (0.669s, 1530.85/s)  LR: 7.844e-04  Data: 0.013 (0.017)
Train: 92 [ 650/1251 ( 52%)]  Loss: 3.784 (3.67)  Time: 0.665s, 1539.68/s  (0.669s, 1531.00/s)  LR: 7.842e-04  Data: 0.013 (0.017)
Train: 92 [ 700/1251 ( 56%)]  Loss: 3.778 (3.68)  Time: 0.661s, 1549.97/s  (0.669s, 1531.58/s)  LR: 7.840e-04  Data: 0.014 (0.016)
Train: 92 [ 750/1251 ( 60%)]  Loss: 3.784 (3.68)  Time: 0.659s, 1554.78/s  (0.669s, 1531.78/s)  LR: 7.839e-04  Data: 0.015 (0.016)
Train: 92 [ 800/1251 ( 64%)]  Loss: 3.544 (3.68)  Time: 0.666s, 1537.54/s  (0.668s, 1531.89/s)  LR: 7.837e-04  Data: 0.014 (0.016)
Train: 92 [ 850/1251 ( 68%)]  Loss: 3.826 (3.68)  Time: 0.673s, 1521.74/s  (0.668s, 1531.98/s)  LR: 7.835e-04  Data: 0.013 (0.016)
Train: 92 [ 900/1251 ( 72%)]  Loss: 3.408 (3.67)  Time: 0.671s, 1525.95/s  (0.668s, 1531.91/s)  LR: 7.833e-04  Data: 0.016 (0.016)
Train: 92 [ 950/1251 ( 76%)]  Loss: 3.449 (3.66)  Time: 0.672s, 1524.46/s  (0.668s, 1532.13/s)  LR: 7.832e-04  Data: 0.015 (0.016)
Train: 92 [1000/1251 ( 80%)]  Loss: 3.740 (3.66)  Time: 0.666s, 1537.08/s  (0.668s, 1532.21/s)  LR: 7.830e-04  Data: 0.013 (0.016)
Train: 92 [1050/1251 ( 84%)]  Loss: 3.807 (3.67)  Time: 0.670s, 1529.16/s  (0.668s, 1532.31/s)  LR: 7.828e-04  Data: 0.013 (0.016)
Train: 92 [1100/1251 ( 88%)]  Loss: 3.630 (3.67)  Time: 0.665s, 1539.01/s  (0.668s, 1532.28/s)  LR: 7.827e-04  Data: 0.013 (0.015)
Train: 92 [1150/1251 ( 92%)]  Loss: 3.645 (3.67)  Time: 0.673s, 1521.53/s  (0.668s, 1532.44/s)  LR: 7.825e-04  Data: 0.018 (0.015)
Train: 92 [1200/1251 ( 96%)]  Loss: 4.156 (3.69)  Time: 0.663s, 1544.85/s  (0.668s, 1532.45/s)  LR: 7.823e-04  Data: 0.013 (0.015)
Train: 92 [1250/1251 (100%)]  Loss: 3.622 (3.68)  Time: 0.663s, 1545.02/s  (0.668s, 1532.36/s)  LR: 7.821e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.843 (2.843)  Loss:  0.5420 (0.5420)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.166 (0.327)  Loss:  0.5991 (1.1085)  Acc@1: 85.6132 (74.1820)  Acc@5: 96.8160 (92.3060)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-90.pth.tar', 74.28799996337891)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-92.pth.tar', 74.18199995605468)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-91.pth.tar', 74.17200000976563)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-86.pth.tar', 74.1360001147461)

Train: 93 [   0/1251 (  0%)]  Loss: 3.709 (3.71)  Time: 3.832s,  267.24/s  (3.832s,  267.24/s)  LR: 7.821e-04  Data: 1.761 (1.761)
Train: 93 [  50/1251 (  4%)]  Loss: 3.590 (3.65)  Time: 0.651s, 1572.51/s  (0.697s, 1469.65/s)  LR: 7.820e-04  Data: 0.014 (0.048)
Train: 93 [ 100/1251 (  8%)]  Loss: 3.863 (3.72)  Time: 0.666s, 1536.42/s  (0.680s, 1505.82/s)  LR: 7.818e-04  Data: 0.013 (0.031)
Train: 93 [ 150/1251 ( 12%)]  Loss: 3.691 (3.71)  Time: 0.668s, 1532.10/s  (0.676s, 1514.47/s)  LR: 7.816e-04  Data: 0.016 (0.025)
Train: 93 [ 200/1251 ( 16%)]  Loss: 3.840 (3.74)  Time: 0.684s, 1496.34/s  (0.674s, 1518.34/s)  LR: 7.814e-04  Data: 0.013 (0.022)
Train: 93 [ 250/1251 ( 20%)]  Loss: 3.201 (3.65)  Time: 0.669s, 1529.68/s  (0.674s, 1519.07/s)  LR: 7.813e-04  Data: 0.012 (0.021)
Train: 93 [ 300/1251 ( 24%)]  Loss: 3.740 (3.66)  Time: 0.677s, 1512.93/s  (0.674s, 1520.35/s)  LR: 7.811e-04  Data: 0.016 (0.019)
Train: 93 [ 350/1251 ( 28%)]  Loss: 3.942 (3.70)  Time: 0.673s, 1521.81/s  (0.673s, 1521.13/s)  LR: 7.809e-04  Data: 0.012 (0.018)
Train: 93 [ 400/1251 ( 32%)]  Loss: 3.520 (3.68)  Time: 0.665s, 1540.95/s  (0.673s, 1522.04/s)  LR: 7.808e-04  Data: 0.012 (0.018)
Train: 93 [ 450/1251 ( 36%)]  Loss: 3.424 (3.65)  Time: 0.663s, 1545.63/s  (0.673s, 1522.21/s)  LR: 7.806e-04  Data: 0.013 (0.017)
Train: 93 [ 500/1251 ( 40%)]  Loss: 3.678 (3.65)  Time: 0.679s, 1508.60/s  (0.673s, 1522.44/s)  LR: 7.804e-04  Data: 0.016 (0.017)
Train: 93 [ 550/1251 ( 44%)]  Loss: 3.575 (3.65)  Time: 0.674s, 1518.65/s  (0.673s, 1522.52/s)  LR: 7.802e-04  Data: 0.012 (0.017)
Train: 93 [ 600/1251 ( 48%)]  Loss: 3.839 (3.66)  Time: 0.669s, 1530.28/s  (0.673s, 1522.67/s)  LR: 7.801e-04  Data: 0.013 (0.016)
Train: 93 [ 650/1251 ( 52%)]  Loss: 3.799 (3.67)  Time: 0.680s, 1505.31/s  (0.672s, 1522.81/s)  LR: 7.799e-04  Data: 0.014 (0.016)
Train: 93 [ 700/1251 ( 56%)]  Loss: 3.854 (3.68)  Time: 0.668s, 1532.10/s  (0.672s, 1522.68/s)  LR: 7.797e-04  Data: 0.013 (0.016)
Train: 93 [ 750/1251 ( 60%)]  Loss: 3.639 (3.68)  Time: 0.675s, 1517.00/s  (0.673s, 1522.66/s)  LR: 7.795e-04  Data: 0.020 (0.016)
Train: 93 [ 800/1251 ( 64%)]  Loss: 3.819 (3.69)  Time: 0.669s, 1530.50/s  (0.673s, 1522.39/s)  LR: 7.794e-04  Data: 0.016 (0.016)
Train: 93 [ 850/1251 ( 68%)]  Loss: 3.622 (3.69)  Time: 0.674s, 1520.28/s  (0.673s, 1522.04/s)  LR: 7.792e-04  Data: 0.015 (0.016)
Train: 93 [ 900/1251 ( 72%)]  Loss: 3.288 (3.66)  Time: 0.683s, 1499.77/s  (0.673s, 1521.66/s)  LR: 7.790e-04  Data: 0.015 (0.015)
Train: 93 [ 950/1251 ( 76%)]  Loss: 3.340 (3.65)  Time: 0.676s, 1514.79/s  (0.673s, 1521.64/s)  LR: 7.789e-04  Data: 0.013 (0.015)
Train: 93 [1000/1251 ( 80%)]  Loss: 3.806 (3.66)  Time: 0.673s, 1522.64/s  (0.673s, 1521.71/s)  LR: 7.787e-04  Data: 0.013 (0.015)
Train: 93 [1050/1251 ( 84%)]  Loss: 3.754 (3.66)  Time: 0.672s, 1524.93/s  (0.673s, 1521.63/s)  LR: 7.785e-04  Data: 0.014 (0.015)
Train: 93 [1100/1251 ( 88%)]  Loss: 3.336 (3.65)  Time: 0.675s, 1516.85/s  (0.673s, 1521.42/s)  LR: 7.783e-04  Data: 0.012 (0.015)
Train: 93 [1150/1251 ( 92%)]  Loss: 3.661 (3.65)  Time: 0.671s, 1524.96/s  (0.673s, 1521.21/s)  LR: 7.782e-04  Data: 0.017 (0.015)
Train: 93 [1200/1251 ( 96%)]  Loss: 3.425 (3.64)  Time: 0.672s, 1524.20/s  (0.673s, 1521.14/s)  LR: 7.780e-04  Data: 0.014 (0.015)
Train: 93 [1250/1251 (100%)]  Loss: 3.705 (3.64)  Time: 0.664s, 1541.86/s  (0.673s, 1521.14/s)  LR: 7.778e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.800 (2.800)  Loss:  0.5562 (0.5562)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.167 (0.331)  Loss:  0.6045 (1.1123)  Acc@1: 86.4387 (74.1820)  Acc@5: 97.0519 (92.4320)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-90.pth.tar', 74.28799996337891)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-93.pth.tar', 74.18200005615235)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-92.pth.tar', 74.18199995605468)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-91.pth.tar', 74.17200000976563)

Train: 94 [   0/1251 (  0%)]  Loss: 3.705 (3.70)  Time: 3.505s,  292.14/s  (3.505s,  292.14/s)  LR: 7.778e-04  Data: 1.889 (1.889)
Train: 94 [  50/1251 (  4%)]  Loss: 4.004 (3.85)  Time: 0.670s, 1527.25/s  (0.697s, 1469.76/s)  LR: 7.776e-04  Data: 0.017 (0.051)
Train: 94 [ 100/1251 (  8%)]  Loss: 3.924 (3.88)  Time: 0.662s, 1546.76/s  (0.683s, 1499.46/s)  LR: 7.775e-04  Data: 0.013 (0.032)
Train: 94 [ 150/1251 ( 12%)]  Loss: 3.366 (3.75)  Time: 0.664s, 1542.92/s  (0.679s, 1508.76/s)  LR: 7.773e-04  Data: 0.014 (0.026)
Train: 94 [ 200/1251 ( 16%)]  Loss: 4.117 (3.82)  Time: 0.663s, 1543.61/s  (0.677s, 1511.93/s)  LR: 7.771e-04  Data: 0.013 (0.023)
Train: 94 [ 250/1251 ( 20%)]  Loss: 3.933 (3.84)  Time: 0.668s, 1531.86/s  (0.677s, 1513.55/s)  LR: 7.769e-04  Data: 0.014 (0.021)
Train: 94 [ 300/1251 ( 24%)]  Loss: 4.037 (3.87)  Time: 0.676s, 1514.99/s  (0.676s, 1514.02/s)  LR: 7.768e-04  Data: 0.013 (0.020)
Train: 94 [ 350/1251 ( 28%)]  Loss: 3.547 (3.83)  Time: 0.679s, 1508.60/s  (0.676s, 1514.14/s)  LR: 7.766e-04  Data: 0.015 (0.019)
Train: 94 [ 400/1251 ( 32%)]  Loss: 3.790 (3.82)  Time: 0.675s, 1516.76/s  (0.676s, 1513.97/s)  LR: 7.764e-04  Data: 0.016 (0.018)
Train: 94 [ 450/1251 ( 36%)]  Loss: 3.572 (3.80)  Time: 0.685s, 1494.61/s  (0.676s, 1513.82/s)  LR: 7.762e-04  Data: 0.013 (0.018)
Train: 94 [ 500/1251 ( 40%)]  Loss: 3.814 (3.80)  Time: 0.681s, 1502.89/s  (0.676s, 1513.71/s)  LR: 7.761e-04  Data: 0.015 (0.017)
Train: 94 [ 550/1251 ( 44%)]  Loss: 3.894 (3.81)  Time: 0.675s, 1517.20/s  (0.677s, 1513.49/s)  LR: 7.759e-04  Data: 0.014 (0.017)
Train: 94 [ 600/1251 ( 48%)]  Loss: 3.708 (3.80)  Time: 0.672s, 1523.07/s  (0.677s, 1513.57/s)  LR: 7.757e-04  Data: 0.014 (0.017)
Train: 94 [ 650/1251 ( 52%)]  Loss: 3.903 (3.81)  Time: 0.681s, 1502.63/s  (0.676s, 1513.99/s)  LR: 7.756e-04  Data: 0.013 (0.016)
Train: 94 [ 700/1251 ( 56%)]  Loss: 3.631 (3.80)  Time: 0.685s, 1494.24/s  (0.676s, 1514.04/s)  LR: 7.754e-04  Data: 0.012 (0.016)
Train: 94 [ 750/1251 ( 60%)]  Loss: 3.736 (3.79)  Time: 0.689s, 1486.96/s  (0.676s, 1513.90/s)  LR: 7.752e-04  Data: 0.011 (0.016)
Train: 94 [ 800/1251 ( 64%)]  Loss: 3.779 (3.79)  Time: 0.681s, 1502.89/s  (0.676s, 1514.17/s)  LR: 7.750e-04  Data: 0.012 (0.016)
Train: 94 [ 850/1251 ( 68%)]  Loss: 3.451 (3.77)  Time: 0.682s, 1501.57/s  (0.676s, 1513.85/s)  LR: 7.749e-04  Data: 0.012 (0.016)
Train: 94 [ 900/1251 ( 72%)]  Loss: 3.958 (3.78)  Time: 0.680s, 1506.77/s  (0.677s, 1513.56/s)  LR: 7.747e-04  Data: 0.015 (0.015)
Train: 94 [ 950/1251 ( 76%)]  Loss: 3.534 (3.77)  Time: 0.677s, 1513.49/s  (0.677s, 1513.24/s)  LR: 7.745e-04  Data: 0.015 (0.015)
Train: 94 [1000/1251 ( 80%)]  Loss: 3.357 (3.75)  Time: 0.678s, 1509.32/s  (0.677s, 1512.94/s)  LR: 7.743e-04  Data: 0.013 (0.015)
Train: 94 [1050/1251 ( 84%)]  Loss: 3.410 (3.74)  Time: 0.678s, 1509.73/s  (0.677s, 1512.84/s)  LR: 7.742e-04  Data: 0.013 (0.015)
Train: 94 [1100/1251 ( 88%)]  Loss: 3.547 (3.73)  Time: 0.680s, 1506.87/s  (0.677s, 1512.79/s)  LR: 7.740e-04  Data: 0.013 (0.015)
Train: 94 [1150/1251 ( 92%)]  Loss: 3.695 (3.73)  Time: 0.688s, 1488.35/s  (0.677s, 1512.49/s)  LR: 7.738e-04  Data: 0.014 (0.015)
Train: 94 [1200/1251 ( 96%)]  Loss: 3.930 (3.73)  Time: 0.686s, 1493.72/s  (0.677s, 1512.08/s)  LR: 7.736e-04  Data: 0.013 (0.015)
Train: 94 [1250/1251 (100%)]  Loss: 3.769 (3.74)  Time: 0.678s, 1509.39/s  (0.677s, 1511.82/s)  LR: 7.735e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.915 (2.915)  Loss:  0.5317 (0.5317)  Acc@1: 89.3555 (89.3555)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.171 (0.325)  Loss:  0.6304 (1.1074)  Acc@1: 85.7311 (74.1920)  Acc@5: 96.4623 (92.2380)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-90.pth.tar', 74.28799996337891)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-94.pth.tar', 74.19200013671875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-93.pth.tar', 74.18200005615235)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-92.pth.tar', 74.18199995605468)

Train: 95 [   0/1251 (  0%)]  Loss: 3.550 (3.55)  Time: 3.991s,  256.61/s  (3.991s,  256.61/s)  LR: 7.735e-04  Data: 1.818 (1.818)
Train: 95 [  50/1251 (  4%)]  Loss: 3.774 (3.66)  Time: 0.667s, 1535.15/s  (0.715s, 1432.77/s)  LR: 7.733e-04  Data: 0.013 (0.049)
Train: 95 [ 100/1251 (  8%)]  Loss: 3.940 (3.75)  Time: 0.684s, 1497.62/s  (0.695s, 1473.84/s)  LR: 7.731e-04  Data: 0.015 (0.031)
Train: 95 [ 150/1251 ( 12%)]  Loss: 3.505 (3.69)  Time: 0.669s, 1531.08/s  (0.689s, 1486.28/s)  LR: 7.729e-04  Data: 0.018 (0.026)
Train: 95 [ 200/1251 ( 16%)]  Loss: 3.571 (3.67)  Time: 0.690s, 1483.96/s  (0.686s, 1492.59/s)  LR: 7.728e-04  Data: 0.014 (0.022)
Train: 95 [ 250/1251 ( 20%)]  Loss: 3.672 (3.67)  Time: 0.679s, 1508.16/s  (0.685s, 1494.99/s)  LR: 7.726e-04  Data: 0.014 (0.021)
Train: 95 [ 300/1251 ( 24%)]  Loss: 3.763 (3.68)  Time: 0.683s, 1498.74/s  (0.684s, 1497.63/s)  LR: 7.724e-04  Data: 0.014 (0.020)
Train: 95 [ 350/1251 ( 28%)]  Loss: 3.649 (3.68)  Time: 0.691s, 1481.29/s  (0.683s, 1498.69/s)  LR: 7.722e-04  Data: 0.015 (0.019)
Train: 95 [ 400/1251 ( 32%)]  Loss: 3.643 (3.67)  Time: 0.682s, 1500.46/s  (0.683s, 1499.04/s)  LR: 7.721e-04  Data: 0.013 (0.018)
Train: 95 [ 450/1251 ( 36%)]  Loss: 3.934 (3.70)  Time: 0.679s, 1507.38/s  (0.683s, 1498.95/s)  LR: 7.719e-04  Data: 0.013 (0.018)
Train: 95 [ 500/1251 ( 40%)]  Loss: 3.713 (3.70)  Time: 0.679s, 1507.00/s  (0.683s, 1499.61/s)  LR: 7.717e-04  Data: 0.012 (0.017)
Train: 95 [ 550/1251 ( 44%)]  Loss: 3.670 (3.70)  Time: 0.682s, 1500.99/s  (0.683s, 1499.99/s)  LR: 7.715e-04  Data: 0.013 (0.017)
Train: 95 [ 600/1251 ( 48%)]  Loss: 3.523 (3.69)  Time: 0.673s, 1520.89/s  (0.682s, 1500.47/s)  LR: 7.714e-04  Data: 0.013 (0.017)
Train: 95 [ 650/1251 ( 52%)]  Loss: 3.349 (3.66)  Time: 0.684s, 1497.99/s  (0.682s, 1500.65/s)  LR: 7.712e-04  Data: 0.013 (0.016)
Train: 95 [ 700/1251 ( 56%)]  Loss: 3.497 (3.65)  Time: 0.678s, 1511.26/s  (0.682s, 1501.04/s)  LR: 7.710e-04  Data: 0.013 (0.016)
Train: 95 [ 750/1251 ( 60%)]  Loss: 3.736 (3.66)  Time: 0.690s, 1484.00/s  (0.682s, 1501.17/s)  LR: 7.708e-04  Data: 0.018 (0.016)
Train: 95 [ 800/1251 ( 64%)]  Loss: 3.768 (3.66)  Time: 0.678s, 1511.03/s  (0.682s, 1501.59/s)  LR: 7.707e-04  Data: 0.014 (0.016)
Train: 95 [ 850/1251 ( 68%)]  Loss: 3.766 (3.67)  Time: 0.681s, 1503.51/s  (0.682s, 1501.98/s)  LR: 7.705e-04  Data: 0.013 (0.016)
Train: 95 [ 900/1251 ( 72%)]  Loss: 3.740 (3.67)  Time: 0.688s, 1488.58/s  (0.682s, 1502.31/s)  LR: 7.703e-04  Data: 0.014 (0.016)
Train: 95 [ 950/1251 ( 76%)]  Loss: 3.439 (3.66)  Time: 0.691s, 1482.91/s  (0.681s, 1502.87/s)  LR: 7.701e-04  Data: 0.016 (0.015)
Train: 95 [1000/1251 ( 80%)]  Loss: 3.614 (3.66)  Time: 0.673s, 1520.58/s  (0.681s, 1503.04/s)  LR: 7.700e-04  Data: 0.013 (0.015)
Train: 95 [1050/1251 ( 84%)]  Loss: 3.963 (3.67)  Time: 0.683s, 1498.28/s  (0.681s, 1503.29/s)  LR: 7.698e-04  Data: 0.014 (0.015)
Train: 95 [1100/1251 ( 88%)]  Loss: 3.732 (3.67)  Time: 0.685s, 1494.13/s  (0.681s, 1503.47/s)  LR: 7.696e-04  Data: 0.014 (0.015)
Train: 95 [1150/1251 ( 92%)]  Loss: 3.983 (3.69)  Time: 0.680s, 1506.77/s  (0.681s, 1503.61/s)  LR: 7.694e-04  Data: 0.015 (0.015)
Train: 95 [1200/1251 ( 96%)]  Loss: 3.650 (3.69)  Time: 0.684s, 1496.64/s  (0.681s, 1503.83/s)  LR: 7.692e-04  Data: 0.015 (0.015)
Train: 95 [1250/1251 (100%)]  Loss: 3.471 (3.68)  Time: 0.660s, 1552.30/s  (0.681s, 1503.86/s)  LR: 7.691e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.713 (2.713)  Loss:  0.4971 (0.4971)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.167 (0.332)  Loss:  0.6323 (1.0597)  Acc@1: 84.7877 (74.5820)  Acc@5: 95.8726 (92.6680)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-95.pth.tar', 74.58199998535156)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-90.pth.tar', 74.28799996337891)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-94.pth.tar', 74.19200013671875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-93.pth.tar', 74.18200005615235)

Train: 96 [   0/1251 (  0%)]  Loss: 3.659 (3.66)  Time: 3.804s,  269.21/s  (3.804s,  269.21/s)  LR: 7.691e-04  Data: 1.787 (1.787)
Train: 96 [  50/1251 (  4%)]  Loss: 3.849 (3.75)  Time: 0.667s, 1534.50/s  (0.705s, 1453.06/s)  LR: 7.689e-04  Data: 0.013 (0.048)
Train: 96 [ 100/1251 (  8%)]  Loss: 3.498 (3.67)  Time: 0.674s, 1518.24/s  (0.689s, 1485.99/s)  LR: 7.687e-04  Data: 0.013 (0.031)
Train: 96 [ 150/1251 ( 12%)]  Loss: 3.662 (3.67)  Time: 0.677s, 1513.29/s  (0.685s, 1493.81/s)  LR: 7.685e-04  Data: 0.018 (0.025)
Train: 96 [ 200/1251 ( 16%)]  Loss: 3.479 (3.63)  Time: 0.680s, 1504.81/s  (0.684s, 1497.14/s)  LR: 7.684e-04  Data: 0.016 (0.022)
Train: 96 [ 250/1251 ( 20%)]  Loss: 4.018 (3.69)  Time: 0.688s, 1488.39/s  (0.683s, 1498.85/s)  LR: 7.682e-04  Data: 0.016 (0.021)
Train: 96 [ 300/1251 ( 24%)]  Loss: 3.654 (3.69)  Time: 0.677s, 1512.69/s  (0.683s, 1499.85/s)  LR: 7.680e-04  Data: 0.013 (0.020)
Train: 96 [ 350/1251 ( 28%)]  Loss: 3.606 (3.68)  Time: 0.693s, 1477.73/s  (0.683s, 1500.20/s)  LR: 7.678e-04  Data: 0.015 (0.019)
Train: 96 [ 400/1251 ( 32%)]  Loss: 3.941 (3.71)  Time: 0.673s, 1520.81/s  (0.683s, 1499.89/s)  LR: 7.677e-04  Data: 0.016 (0.018)
Train: 96 [ 450/1251 ( 36%)]  Loss: 3.672 (3.70)  Time: 0.691s, 1481.66/s  (0.683s, 1500.03/s)  LR: 7.675e-04  Data: 0.012 (0.018)
Train: 96 [ 500/1251 ( 40%)]  Loss: 3.709 (3.70)  Time: 0.679s, 1508.70/s  (0.683s, 1500.30/s)  LR: 7.673e-04  Data: 0.014 (0.017)
Train: 96 [ 550/1251 ( 44%)]  Loss: 3.584 (3.69)  Time: 0.679s, 1508.77/s  (0.682s, 1500.81/s)  LR: 7.671e-04  Data: 0.012 (0.017)
Train: 96 [ 600/1251 ( 48%)]  Loss: 3.806 (3.70)  Time: 0.690s, 1483.82/s  (0.682s, 1500.75/s)  LR: 7.670e-04  Data: 0.013 (0.017)
Train: 96 [ 650/1251 ( 52%)]  Loss: 3.774 (3.71)  Time: 0.681s, 1504.40/s  (0.682s, 1500.40/s)  LR: 7.668e-04  Data: 0.016 (0.016)
Train: 96 [ 700/1251 ( 56%)]  Loss: 3.844 (3.72)  Time: 0.678s, 1509.26/s  (0.682s, 1500.52/s)  LR: 7.666e-04  Data: 0.016 (0.016)
Train: 96 [ 750/1251 ( 60%)]  Loss: 3.779 (3.72)  Time: 0.683s, 1499.42/s  (0.682s, 1500.71/s)  LR: 7.664e-04  Data: 0.014 (0.016)
Train: 96 [ 800/1251 ( 64%)]  Loss: 3.671 (3.72)  Time: 0.669s, 1529.95/s  (0.682s, 1500.80/s)  LR: 7.663e-04  Data: 0.013 (0.016)
Train: 96 [ 850/1251 ( 68%)]  Loss: 3.799 (3.72)  Time: 0.692s, 1479.44/s  (0.682s, 1500.90/s)  LR: 7.661e-04  Data: 0.013 (0.016)
Train: 96 [ 900/1251 ( 72%)]  Loss: 3.535 (3.71)  Time: 0.677s, 1512.33/s  (0.682s, 1501.30/s)  LR: 7.659e-04  Data: 0.013 (0.016)
Train: 96 [ 950/1251 ( 76%)]  Loss: 3.916 (3.72)  Time: 0.688s, 1489.17/s  (0.682s, 1501.27/s)  LR: 7.657e-04  Data: 0.014 (0.015)
Train: 96 [1000/1251 ( 80%)]  Loss: 3.780 (3.73)  Time: 0.682s, 1500.69/s  (0.682s, 1501.51/s)  LR: 7.655e-04  Data: 0.012 (0.015)
Train: 96 [1050/1251 ( 84%)]  Loss: 3.518 (3.72)  Time: 0.682s, 1502.21/s  (0.682s, 1501.75/s)  LR: 7.654e-04  Data: 0.013 (0.015)
Train: 96 [1100/1251 ( 88%)]  Loss: 3.926 (3.73)  Time: 0.667s, 1534.31/s  (0.682s, 1501.99/s)  LR: 7.652e-04  Data: 0.013 (0.015)
Train: 96 [1150/1251 ( 92%)]  Loss: 3.551 (3.72)  Time: 0.679s, 1508.17/s  (0.682s, 1502.36/s)  LR: 7.650e-04  Data: 0.014 (0.015)
Train: 96 [1200/1251 ( 96%)]  Loss: 3.677 (3.72)  Time: 0.679s, 1508.66/s  (0.681s, 1502.74/s)  LR: 7.648e-04  Data: 0.018 (0.015)
Train: 96 [1250/1251 (100%)]  Loss: 3.917 (3.72)  Time: 0.671s, 1525.20/s  (0.681s, 1503.07/s)  LR: 7.647e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.758 (2.758)  Loss:  0.5396 (0.5396)  Acc@1: 88.7695 (88.7695)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.170 (0.320)  Loss:  0.6240 (1.1145)  Acc@1: 85.3774 (74.0740)  Acc@5: 97.0519 (92.4580)
Train: 97 [   0/1251 (  0%)]  Loss: 3.743 (3.74)  Time: 3.814s,  268.51/s  (3.814s,  268.51/s)  LR: 7.647e-04  Data: 1.620 (1.620)
Train: 97 [  50/1251 (  4%)]  Loss: 3.407 (3.57)  Time: 0.669s, 1530.29/s  (0.701s, 1460.21/s)  LR: 7.645e-04  Data: 0.016 (0.045)
Train: 97 [ 100/1251 (  8%)]  Loss: 3.529 (3.56)  Time: 0.667s, 1535.29/s  (0.686s, 1493.47/s)  LR: 7.643e-04  Data: 0.013 (0.029)
Train: 97 [ 150/1251 ( 12%)]  Loss: 3.847 (3.63)  Time: 0.675s, 1518.13/s  (0.682s, 1501.16/s)  LR: 7.641e-04  Data: 0.012 (0.024)
Train: 97 [ 200/1251 ( 16%)]  Loss: 4.009 (3.71)  Time: 0.674s, 1519.48/s  (0.681s, 1504.53/s)  LR: 7.639e-04  Data: 0.016 (0.022)
Train: 97 [ 250/1251 ( 20%)]  Loss: 3.553 (3.68)  Time: 0.678s, 1509.67/s  (0.680s, 1506.04/s)  LR: 7.638e-04  Data: 0.013 (0.020)
Train: 97 [ 300/1251 ( 24%)]  Loss: 3.567 (3.66)  Time: 0.684s, 1496.05/s  (0.680s, 1506.84/s)  LR: 7.636e-04  Data: 0.013 (0.019)
Train: 97 [ 350/1251 ( 28%)]  Loss: 3.561 (3.65)  Time: 0.682s, 1501.84/s  (0.680s, 1506.62/s)  LR: 7.634e-04  Data: 0.013 (0.018)
Train: 97 [ 400/1251 ( 32%)]  Loss: 3.609 (3.65)  Time: 0.686s, 1492.38/s  (0.680s, 1506.25/s)  LR: 7.632e-04  Data: 0.013 (0.018)
Train: 97 [ 450/1251 ( 36%)]  Loss: 3.597 (3.64)  Time: 0.683s, 1499.90/s  (0.680s, 1506.45/s)  LR: 7.631e-04  Data: 0.014 (0.017)
Train: 97 [ 500/1251 ( 40%)]  Loss: 3.674 (3.64)  Time: 0.690s, 1484.32/s  (0.680s, 1505.97/s)  LR: 7.629e-04  Data: 0.013 (0.017)
Train: 97 [ 550/1251 ( 44%)]  Loss: 3.627 (3.64)  Time: 0.683s, 1499.80/s  (0.680s, 1505.70/s)  LR: 7.627e-04  Data: 0.015 (0.016)
Train: 97 [ 600/1251 ( 48%)]  Loss: 3.802 (3.66)  Time: 0.684s, 1496.12/s  (0.680s, 1505.41/s)  LR: 7.625e-04  Data: 0.016 (0.016)
Train: 97 [ 650/1251 ( 52%)]  Loss: 3.658 (3.66)  Time: 0.688s, 1488.03/s  (0.680s, 1505.53/s)  LR: 7.624e-04  Data: 0.014 (0.016)
Train: 97 [ 700/1251 ( 56%)]  Loss: 4.014 (3.68)  Time: 0.679s, 1507.28/s  (0.680s, 1505.68/s)  LR: 7.622e-04  Data: 0.013 (0.016)
Train: 97 [ 750/1251 ( 60%)]  Loss: 3.682 (3.68)  Time: 0.682s, 1501.67/s  (0.680s, 1505.75/s)  LR: 7.620e-04  Data: 0.013 (0.016)
Train: 97 [ 800/1251 ( 64%)]  Loss: 3.886 (3.69)  Time: 0.681s, 1504.00/s  (0.680s, 1506.12/s)  LR: 7.618e-04  Data: 0.014 (0.016)
Train: 97 [ 850/1251 ( 68%)]  Loss: 3.669 (3.69)  Time: 0.683s, 1498.56/s  (0.680s, 1506.03/s)  LR: 7.616e-04  Data: 0.013 (0.015)
Train: 97 [ 900/1251 ( 72%)]  Loss: 4.054 (3.71)  Time: 0.681s, 1504.06/s  (0.680s, 1506.14/s)  LR: 7.615e-04  Data: 0.013 (0.015)
Train: 97 [ 950/1251 ( 76%)]  Loss: 3.552 (3.70)  Time: 0.675s, 1516.60/s  (0.680s, 1506.43/s)  LR: 7.613e-04  Data: 0.012 (0.015)
Train: 97 [1000/1251 ( 80%)]  Loss: 3.628 (3.70)  Time: 0.688s, 1489.41/s  (0.680s, 1506.66/s)  LR: 7.611e-04  Data: 0.013 (0.015)
Train: 97 [1050/1251 ( 84%)]  Loss: 3.785 (3.70)  Time: 0.670s, 1528.16/s  (0.679s, 1507.02/s)  LR: 7.609e-04  Data: 0.013 (0.015)
Train: 97 [1100/1251 ( 88%)]  Loss: 3.472 (3.69)  Time: 0.678s, 1509.21/s  (0.679s, 1507.13/s)  LR: 7.608e-04  Data: 0.012 (0.015)
Train: 97 [1150/1251 ( 92%)]  Loss: 3.660 (3.69)  Time: 0.672s, 1523.34/s  (0.679s, 1507.37/s)  LR: 7.606e-04  Data: 0.012 (0.015)
Train: 97 [1200/1251 ( 96%)]  Loss: 3.623 (3.69)  Time: 0.681s, 1503.70/s  (0.679s, 1507.74/s)  LR: 7.604e-04  Data: 0.013 (0.015)
Train: 97 [1250/1251 (100%)]  Loss: 3.752 (3.69)  Time: 0.659s, 1553.79/s  (0.679s, 1508.31/s)  LR: 7.602e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.995 (2.995)  Loss:  0.4990 (0.4990)  Acc@1: 88.8672 (88.8672)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.164 (0.326)  Loss:  0.5776 (1.0692)  Acc@1: 86.3208 (74.6040)  Acc@5: 96.8160 (92.5980)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-97.pth.tar', 74.6040000048828)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-95.pth.tar', 74.58199998535156)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-90.pth.tar', 74.28799996337891)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-94.pth.tar', 74.19200013671875)

Train: 98 [   0/1251 (  0%)]  Loss: 3.932 (3.93)  Time: 3.552s,  288.26/s  (3.552s,  288.26/s)  LR: 7.602e-04  Data: 2.004 (2.004)
Train: 98 [  50/1251 (  4%)]  Loss: 3.781 (3.86)  Time: 0.662s, 1546.97/s  (0.690s, 1483.46/s)  LR: 7.600e-04  Data: 0.014 (0.053)
Train: 98 [ 100/1251 (  8%)]  Loss: 3.728 (3.81)  Time: 0.675s, 1517.64/s  (0.679s, 1508.46/s)  LR: 7.599e-04  Data: 0.015 (0.033)
Train: 98 [ 150/1251 ( 12%)]  Loss: 3.761 (3.80)  Time: 0.667s, 1536.14/s  (0.677s, 1512.81/s)  LR: 7.597e-04  Data: 0.015 (0.027)
Train: 98 [ 200/1251 ( 16%)]  Loss: 4.010 (3.84)  Time: 0.685s, 1495.72/s  (0.676s, 1514.17/s)  LR: 7.595e-04  Data: 0.016 (0.024)
Train: 98 [ 250/1251 ( 20%)]  Loss: 3.669 (3.81)  Time: 0.678s, 1510.44/s  (0.676s, 1515.18/s)  LR: 7.593e-04  Data: 0.012 (0.022)
Train: 98 [ 300/1251 ( 24%)]  Loss: 3.776 (3.81)  Time: 0.676s, 1514.82/s  (0.675s, 1516.13/s)  LR: 7.591e-04  Data: 0.013 (0.020)
Train: 98 [ 350/1251 ( 28%)]  Loss: 3.720 (3.80)  Time: 0.674s, 1518.27/s  (0.675s, 1516.95/s)  LR: 7.590e-04  Data: 0.012 (0.019)
Train: 98 [ 400/1251 ( 32%)]  Loss: 3.919 (3.81)  Time: 0.676s, 1513.89/s  (0.675s, 1517.71/s)  LR: 7.588e-04  Data: 0.013 (0.019)
Train: 98 [ 450/1251 ( 36%)]  Loss: 3.879 (3.82)  Time: 0.664s, 1541.76/s  (0.674s, 1519.24/s)  LR: 7.586e-04  Data: 0.013 (0.018)
Train: 98 [ 500/1251 ( 40%)]  Loss: 3.579 (3.80)  Time: 0.678s, 1510.32/s  (0.674s, 1520.04/s)  LR: 7.584e-04  Data: 0.014 (0.018)
Train: 98 [ 550/1251 ( 44%)]  Loss: 3.588 (3.78)  Time: 0.671s, 1525.70/s  (0.673s, 1520.46/s)  LR: 7.583e-04  Data: 0.013 (0.017)
Train: 98 [ 600/1251 ( 48%)]  Loss: 4.001 (3.80)  Time: 0.677s, 1511.69/s  (0.673s, 1520.42/s)  LR: 7.581e-04  Data: 0.012 (0.017)
Train: 98 [ 650/1251 ( 52%)]  Loss: 3.769 (3.79)  Time: 0.679s, 1507.04/s  (0.673s, 1520.62/s)  LR: 7.579e-04  Data: 0.014 (0.017)
Train: 98 [ 700/1251 ( 56%)]  Loss: 3.468 (3.77)  Time: 0.671s, 1526.39/s  (0.673s, 1520.59/s)  LR: 7.577e-04  Data: 0.013 (0.016)
Train: 98 [ 750/1251 ( 60%)]  Loss: 3.688 (3.77)  Time: 0.679s, 1508.85/s  (0.674s, 1520.39/s)  LR: 7.575e-04  Data: 0.012 (0.016)
Train: 98 [ 800/1251 ( 64%)]  Loss: 3.658 (3.76)  Time: 0.679s, 1507.50/s  (0.673s, 1520.51/s)  LR: 7.574e-04  Data: 0.016 (0.016)
Train: 98 [ 850/1251 ( 68%)]  Loss: 3.809 (3.76)  Time: 0.671s, 1526.43/s  (0.673s, 1520.56/s)  LR: 7.572e-04  Data: 0.013 (0.016)
Train: 98 [ 900/1251 ( 72%)]  Loss: 3.325 (3.74)  Time: 0.677s, 1512.25/s  (0.673s, 1520.50/s)  LR: 7.570e-04  Data: 0.014 (0.016)
Train: 98 [ 950/1251 ( 76%)]  Loss: 3.469 (3.73)  Time: 0.679s, 1507.29/s  (0.674s, 1520.28/s)  LR: 7.568e-04  Data: 0.015 (0.016)
Train: 98 [1000/1251 ( 80%)]  Loss: 3.722 (3.73)  Time: 0.679s, 1507.49/s  (0.674s, 1519.96/s)  LR: 7.566e-04  Data: 0.013 (0.016)
Train: 98 [1050/1251 ( 84%)]  Loss: 3.482 (3.72)  Time: 0.673s, 1522.59/s  (0.674s, 1519.83/s)  LR: 7.565e-04  Data: 0.013 (0.016)
Train: 98 [1100/1251 ( 88%)]  Loss: 3.872 (3.72)  Time: 0.677s, 1511.84/s  (0.674s, 1519.56/s)  LR: 7.563e-04  Data: 0.013 (0.015)
Train: 98 [1150/1251 ( 92%)]  Loss: 3.654 (3.72)  Time: 0.671s, 1525.68/s  (0.674s, 1519.48/s)  LR: 7.561e-04  Data: 0.014 (0.015)
Train: 98 [1200/1251 ( 96%)]  Loss: 3.673 (3.72)  Time: 0.683s, 1499.29/s  (0.674s, 1519.16/s)  LR: 7.559e-04  Data: 0.012 (0.015)
Train: 98 [1250/1251 (100%)]  Loss: 3.601 (3.71)  Time: 0.656s, 1559.84/s  (0.674s, 1519.20/s)  LR: 7.557e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.889 (2.889)  Loss:  0.4902 (0.4902)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.168 (0.326)  Loss:  0.6255 (1.0897)  Acc@1: 84.7877 (74.3640)  Acc@5: 97.0519 (92.5120)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-97.pth.tar', 74.6040000048828)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-95.pth.tar', 74.58199998535156)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-98.pth.tar', 74.36399998535157)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-90.pth.tar', 74.28799996337891)

Train: 99 [   0/1251 (  0%)]  Loss: 3.788 (3.79)  Time: 4.035s,  253.79/s  (4.035s,  253.79/s)  LR: 7.557e-04  Data: 1.812 (1.812)
Train: 99 [  50/1251 (  4%)]  Loss: 3.828 (3.81)  Time: 0.668s, 1533.52/s  (0.705s, 1453.04/s)  LR: 7.556e-04  Data: 0.012 (0.049)
Train: 99 [ 100/1251 (  8%)]  Loss: 3.517 (3.71)  Time: 0.665s, 1539.21/s  (0.688s, 1489.24/s)  LR: 7.554e-04  Data: 0.012 (0.032)
Train: 99 [ 150/1251 ( 12%)]  Loss: 3.754 (3.72)  Time: 0.673s, 1520.54/s  (0.683s, 1499.21/s)  LR: 7.552e-04  Data: 0.013 (0.026)
Train: 99 [ 200/1251 ( 16%)]  Loss: 3.618 (3.70)  Time: 0.661s, 1550.07/s  (0.681s, 1504.27/s)  LR: 7.550e-04  Data: 0.013 (0.023)
Train: 99 [ 250/1251 ( 20%)]  Loss: 3.778 (3.71)  Time: 0.684s, 1496.77/s  (0.679s, 1507.06/s)  LR: 7.548e-04  Data: 0.012 (0.021)
Train: 99 [ 300/1251 ( 24%)]  Loss: 3.660 (3.71)  Time: 0.679s, 1507.75/s  (0.679s, 1508.86/s)  LR: 7.547e-04  Data: 0.012 (0.020)
Train: 99 [ 350/1251 ( 28%)]  Loss: 3.412 (3.67)  Time: 0.684s, 1497.08/s  (0.678s, 1510.19/s)  LR: 7.545e-04  Data: 0.014 (0.019)
Train: 99 [ 400/1251 ( 32%)]  Loss: 3.705 (3.67)  Time: 0.674s, 1519.89/s  (0.678s, 1510.73/s)  LR: 7.543e-04  Data: 0.013 (0.018)
Train: 99 [ 450/1251 ( 36%)]  Loss: 4.062 (3.71)  Time: 0.683s, 1499.15/s  (0.678s, 1510.91/s)  LR: 7.541e-04  Data: 0.020 (0.018)
Train: 99 [ 500/1251 ( 40%)]  Loss: 3.749 (3.72)  Time: 0.681s, 1503.43/s  (0.678s, 1510.50/s)  LR: 7.540e-04  Data: 0.015 (0.017)
Train: 99 [ 550/1251 ( 44%)]  Loss: 3.611 (3.71)  Time: 0.679s, 1508.42/s  (0.678s, 1510.46/s)  LR: 7.538e-04  Data: 0.014 (0.017)
Train: 99 [ 600/1251 ( 48%)]  Loss: 3.769 (3.71)  Time: 0.679s, 1507.49/s  (0.678s, 1510.48/s)  LR: 7.536e-04  Data: 0.013 (0.017)
Train: 99 [ 650/1251 ( 52%)]  Loss: 3.901 (3.73)  Time: 0.676s, 1515.83/s  (0.678s, 1510.40/s)  LR: 7.534e-04  Data: 0.014 (0.016)
Train: 99 [ 700/1251 ( 56%)]  Loss: 4.001 (3.74)  Time: 0.689s, 1485.41/s  (0.678s, 1510.48/s)  LR: 7.532e-04  Data: 0.013 (0.016)
Train: 99 [ 750/1251 ( 60%)]  Loss: 3.773 (3.75)  Time: 0.671s, 1525.21/s  (0.678s, 1510.59/s)  LR: 7.531e-04  Data: 0.017 (0.016)
Train: 99 [ 800/1251 ( 64%)]  Loss: 3.596 (3.74)  Time: 0.677s, 1512.18/s  (0.678s, 1510.80/s)  LR: 7.529e-04  Data: 0.013 (0.016)
Train: 99 [ 850/1251 ( 68%)]  Loss: 3.316 (3.71)  Time: 0.670s, 1529.33/s  (0.678s, 1510.88/s)  LR: 7.527e-04  Data: 0.013 (0.016)
Train: 99 [ 900/1251 ( 72%)]  Loss: 3.944 (3.73)  Time: 0.670s, 1527.48/s  (0.678s, 1511.12/s)  LR: 7.525e-04  Data: 0.013 (0.016)
Train: 99 [ 950/1251 ( 76%)]  Loss: 3.286 (3.70)  Time: 0.669s, 1530.90/s  (0.678s, 1511.35/s)  LR: 7.523e-04  Data: 0.017 (0.016)
Train: 99 [1000/1251 ( 80%)]  Loss: 3.531 (3.70)  Time: 0.683s, 1499.28/s  (0.677s, 1511.81/s)  LR: 7.522e-04  Data: 0.013 (0.015)
Train: 99 [1050/1251 ( 84%)]  Loss: 3.799 (3.70)  Time: 0.680s, 1506.78/s  (0.677s, 1512.02/s)  LR: 7.520e-04  Data: 0.012 (0.015)
Train: 99 [1100/1251 ( 88%)]  Loss: 3.945 (3.71)  Time: 0.670s, 1527.73/s  (0.677s, 1512.38/s)  LR: 7.518e-04  Data: 0.013 (0.015)
Train: 99 [1150/1251 ( 92%)]  Loss: 3.444 (3.70)  Time: 0.669s, 1529.50/s  (0.677s, 1512.62/s)  LR: 7.516e-04  Data: 0.012 (0.015)
Train: 99 [1200/1251 ( 96%)]  Loss: 3.905 (3.71)  Time: 0.673s, 1520.79/s  (0.677s, 1512.94/s)  LR: 7.514e-04  Data: 0.014 (0.015)
Train: 99 [1250/1251 (100%)]  Loss: 3.659 (3.71)  Time: 0.658s, 1556.92/s  (0.677s, 1513.46/s)  LR: 7.512e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.070 (3.070)  Loss:  0.5298 (0.5298)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.167 (0.327)  Loss:  0.6299 (1.1098)  Acc@1: 85.1415 (74.3600)  Acc@5: 96.5802 (92.4520)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-97.pth.tar', 74.6040000048828)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-95.pth.tar', 74.58199998535156)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-98.pth.tar', 74.36399998535157)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-99.pth.tar', 74.36000013916015)

Train: 100 [   0/1251 (  0%)]  Loss: 3.308 (3.31)  Time: 3.663s,  279.51/s  (3.663s,  279.51/s)  LR: 7.512e-04  Data: 1.680 (1.680)
Train: 100 [  50/1251 (  4%)]  Loss: 3.864 (3.59)  Time: 0.654s, 1565.31/s  (0.691s, 1481.53/s)  LR: 7.511e-04  Data: 0.013 (0.046)
Train: 100 [ 100/1251 (  8%)]  Loss: 3.657 (3.61)  Time: 0.665s, 1539.23/s  (0.678s, 1510.38/s)  LR: 7.509e-04  Data: 0.013 (0.030)
Train: 100 [ 150/1251 ( 12%)]  Loss: 3.910 (3.68)  Time: 0.674s, 1518.84/s  (0.675s, 1516.30/s)  LR: 7.507e-04  Data: 0.014 (0.025)
Train: 100 [ 200/1251 ( 16%)]  Loss: 3.439 (3.64)  Time: 0.672s, 1523.03/s  (0.674s, 1518.68/s)  LR: 7.505e-04  Data: 0.016 (0.022)
Train: 100 [ 250/1251 ( 20%)]  Loss: 3.751 (3.65)  Time: 0.668s, 1532.87/s  (0.674s, 1519.71/s)  LR: 7.503e-04  Data: 0.013 (0.020)
Train: 100 [ 300/1251 ( 24%)]  Loss: 3.466 (3.63)  Time: 0.670s, 1529.26/s  (0.673s, 1520.43/s)  LR: 7.502e-04  Data: 0.017 (0.019)
Train: 100 [ 350/1251 ( 28%)]  Loss: 3.612 (3.63)  Time: 0.681s, 1504.36/s  (0.673s, 1520.46/s)  LR: 7.500e-04  Data: 0.015 (0.018)
Train: 100 [ 400/1251 ( 32%)]  Loss: 3.813 (3.65)  Time: 0.664s, 1541.18/s  (0.673s, 1520.91/s)  LR: 7.498e-04  Data: 0.013 (0.018)
Train: 100 [ 450/1251 ( 36%)]  Loss: 3.421 (3.62)  Time: 0.680s, 1506.85/s  (0.673s, 1520.90/s)  LR: 7.496e-04  Data: 0.013 (0.017)
Train: 100 [ 500/1251 ( 40%)]  Loss: 3.562 (3.62)  Time: 0.675s, 1517.80/s  (0.673s, 1521.46/s)  LR: 7.494e-04  Data: 0.013 (0.017)
Train: 100 [ 550/1251 ( 44%)]  Loss: 3.943 (3.65)  Time: 0.661s, 1548.07/s  (0.673s, 1521.67/s)  LR: 7.493e-04  Data: 0.014 (0.017)
Train: 100 [ 600/1251 ( 48%)]  Loss: 3.581 (3.64)  Time: 0.669s, 1530.09/s  (0.673s, 1521.81/s)  LR: 7.491e-04  Data: 0.014 (0.016)
Train: 100 [ 650/1251 ( 52%)]  Loss: 3.610 (3.64)  Time: 0.665s, 1540.56/s  (0.673s, 1521.88/s)  LR: 7.489e-04  Data: 0.016 (0.016)
Train: 100 [ 700/1251 ( 56%)]  Loss: 3.512 (3.63)  Time: 0.668s, 1532.39/s  (0.673s, 1521.77/s)  LR: 7.487e-04  Data: 0.013 (0.016)
Train: 100 [ 750/1251 ( 60%)]  Loss: 3.833 (3.64)  Time: 0.685s, 1494.62/s  (0.673s, 1521.63/s)  LR: 7.485e-04  Data: 0.012 (0.016)
Train: 100 [ 800/1251 ( 64%)]  Loss: 3.602 (3.64)  Time: 0.673s, 1520.91/s  (0.673s, 1521.69/s)  LR: 7.484e-04  Data: 0.014 (0.016)
Train: 100 [ 850/1251 ( 68%)]  Loss: 3.569 (3.64)  Time: 0.684s, 1497.96/s  (0.673s, 1521.56/s)  LR: 7.482e-04  Data: 0.012 (0.016)
Train: 100 [ 900/1251 ( 72%)]  Loss: 3.318 (3.62)  Time: 0.674s, 1520.02/s  (0.673s, 1521.18/s)  LR: 7.480e-04  Data: 0.016 (0.016)
Train: 100 [ 950/1251 ( 76%)]  Loss: 3.374 (3.61)  Time: 0.674s, 1519.25/s  (0.673s, 1521.13/s)  LR: 7.478e-04  Data: 0.014 (0.015)
Train: 100 [1000/1251 ( 80%)]  Loss: 4.096 (3.63)  Time: 0.683s, 1499.65/s  (0.673s, 1521.17/s)  LR: 7.476e-04  Data: 0.013 (0.015)
Train: 100 [1050/1251 ( 84%)]  Loss: 3.744 (3.64)  Time: 0.677s, 1512.34/s  (0.673s, 1520.92/s)  LR: 7.474e-04  Data: 0.013 (0.015)
Train: 100 [1100/1251 ( 88%)]  Loss: 3.590 (3.63)  Time: 0.670s, 1529.17/s  (0.673s, 1520.81/s)  LR: 7.473e-04  Data: 0.012 (0.015)
Train: 100 [1150/1251 ( 92%)]  Loss: 3.650 (3.63)  Time: 0.677s, 1513.62/s  (0.673s, 1520.55/s)  LR: 7.471e-04  Data: 0.016 (0.015)
Train: 100 [1200/1251 ( 96%)]  Loss: 3.491 (3.63)  Time: 0.683s, 1498.41/s  (0.673s, 1520.53/s)  LR: 7.469e-04  Data: 0.015 (0.015)
Train: 100 [1250/1251 (100%)]  Loss: 3.751 (3.63)  Time: 0.666s, 1538.09/s  (0.673s, 1520.49/s)  LR: 7.467e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.743 (2.743)  Loss:  0.4988 (0.4988)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.162 (0.322)  Loss:  0.6387 (1.0827)  Acc@1: 85.0236 (74.3980)  Acc@5: 96.8160 (92.5460)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-97.pth.tar', 74.6040000048828)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-95.pth.tar', 74.58199998535156)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-100.pth.tar', 74.39800008789062)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-98.pth.tar', 74.36399998535157)

Train: 101 [   0/1251 (  0%)]  Loss: 3.320 (3.32)  Time: 3.528s,  290.22/s  (3.528s,  290.22/s)  LR: 7.467e-04  Data: 1.752 (1.752)
Train: 101 [  50/1251 (  4%)]  Loss: 3.807 (3.56)  Time: 0.669s, 1530.83/s  (0.701s, 1460.81/s)  LR: 7.465e-04  Data: 0.013 (0.048)
Train: 101 [ 100/1251 (  8%)]  Loss: 3.562 (3.56)  Time: 0.674s, 1519.40/s  (0.685s, 1495.21/s)  LR: 7.464e-04  Data: 0.014 (0.031)
Train: 101 [ 150/1251 ( 12%)]  Loss: 3.582 (3.57)  Time: 0.665s, 1539.93/s  (0.680s, 1505.84/s)  LR: 7.462e-04  Data: 0.012 (0.026)
Train: 101 [ 200/1251 ( 16%)]  Loss: 3.404 (3.53)  Time: 0.673s, 1521.37/s  (0.678s, 1510.44/s)  LR: 7.460e-04  Data: 0.014 (0.023)
Train: 101 [ 250/1251 ( 20%)]  Loss: 3.814 (3.58)  Time: 0.675s, 1517.07/s  (0.677s, 1512.42/s)  LR: 7.458e-04  Data: 0.013 (0.021)
Train: 101 [ 300/1251 ( 24%)]  Loss: 3.518 (3.57)  Time: 0.677s, 1513.61/s  (0.676s, 1513.76/s)  LR: 7.456e-04  Data: 0.013 (0.020)
Train: 101 [ 350/1251 ( 28%)]  Loss: 3.574 (3.57)  Time: 0.673s, 1521.80/s  (0.676s, 1515.47/s)  LR: 7.455e-04  Data: 0.018 (0.019)
Train: 101 [ 400/1251 ( 32%)]  Loss: 3.677 (3.58)  Time: 0.679s, 1508.80/s  (0.675s, 1516.55/s)  LR: 7.453e-04  Data: 0.013 (0.018)
Train: 101 [ 450/1251 ( 36%)]  Loss: 3.559 (3.58)  Time: 0.670s, 1527.36/s  (0.675s, 1517.06/s)  LR: 7.451e-04  Data: 0.014 (0.018)
Train: 101 [ 500/1251 ( 40%)]  Loss: 3.433 (3.57)  Time: 0.674s, 1519.49/s  (0.675s, 1517.62/s)  LR: 7.449e-04  Data: 0.012 (0.017)
Train: 101 [ 550/1251 ( 44%)]  Loss: 3.792 (3.59)  Time: 0.674s, 1519.22/s  (0.674s, 1518.34/s)  LR: 7.447e-04  Data: 0.013 (0.017)
Train: 101 [ 600/1251 ( 48%)]  Loss: 3.872 (3.61)  Time: 0.667s, 1535.26/s  (0.674s, 1518.56/s)  LR: 7.445e-04  Data: 0.016 (0.017)
Train: 101 [ 650/1251 ( 52%)]  Loss: 3.720 (3.62)  Time: 0.666s, 1537.07/s  (0.674s, 1519.04/s)  LR: 7.444e-04  Data: 0.012 (0.016)
Train: 101 [ 700/1251 ( 56%)]  Loss: 3.458 (3.61)  Time: 0.666s, 1538.67/s  (0.674s, 1519.46/s)  LR: 7.442e-04  Data: 0.016 (0.016)
Train: 101 [ 750/1251 ( 60%)]  Loss: 3.948 (3.63)  Time: 0.667s, 1535.25/s  (0.674s, 1519.91/s)  LR: 7.440e-04  Data: 0.013 (0.016)
Train: 101 [ 800/1251 ( 64%)]  Loss: 3.512 (3.62)  Time: 0.673s, 1520.87/s  (0.674s, 1520.41/s)  LR: 7.438e-04  Data: 0.013 (0.016)
Train: 101 [ 850/1251 ( 68%)]  Loss: 3.365 (3.61)  Time: 0.664s, 1541.36/s  (0.673s, 1520.51/s)  LR: 7.436e-04  Data: 0.013 (0.016)
Train: 101 [ 900/1251 ( 72%)]  Loss: 3.712 (3.61)  Time: 0.670s, 1529.43/s  (0.673s, 1520.71/s)  LR: 7.434e-04  Data: 0.013 (0.016)
Train: 101 [ 950/1251 ( 76%)]  Loss: 3.680 (3.62)  Time: 0.676s, 1513.94/s  (0.673s, 1520.75/s)  LR: 7.433e-04  Data: 0.013 (0.015)
Train: 101 [1000/1251 ( 80%)]  Loss: 3.357 (3.60)  Time: 0.671s, 1525.41/s  (0.673s, 1520.97/s)  LR: 7.431e-04  Data: 0.014 (0.015)
Train: 101 [1050/1251 ( 84%)]  Loss: 3.577 (3.60)  Time: 0.671s, 1525.85/s  (0.673s, 1521.20/s)  LR: 7.429e-04  Data: 0.014 (0.015)
Train: 101 [1100/1251 ( 88%)]  Loss: 3.795 (3.61)  Time: 0.672s, 1523.43/s  (0.673s, 1521.24/s)  LR: 7.427e-04  Data: 0.013 (0.015)
Train: 101 [1150/1251 ( 92%)]  Loss: 3.661 (3.61)  Time: 0.664s, 1543.28/s  (0.673s, 1521.42/s)  LR: 7.425e-04  Data: 0.016 (0.015)
Train: 101 [1200/1251 ( 96%)]  Loss: 3.599 (3.61)  Time: 0.667s, 1536.37/s  (0.673s, 1521.55/s)  LR: 7.424e-04  Data: 0.013 (0.015)
Train: 101 [1250/1251 (100%)]  Loss: 3.708 (3.62)  Time: 0.653s, 1568.99/s  (0.673s, 1521.74/s)  LR: 7.422e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.835 (2.835)  Loss:  0.5366 (0.5366)  Acc@1: 89.4531 (89.4531)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.168 (0.331)  Loss:  0.7397 (1.0989)  Acc@1: 83.2547 (74.3460)  Acc@5: 95.4009 (92.6720)
Train: 102 [   0/1251 (  0%)]  Loss: 3.495 (3.50)  Time: 3.427s,  298.76/s  (3.427s,  298.76/s)  LR: 7.422e-04  Data: 2.176 (2.176)
Train: 102 [  50/1251 (  4%)]  Loss: 3.712 (3.60)  Time: 0.656s, 1561.47/s  (0.694s, 1475.79/s)  LR: 7.420e-04  Data: 0.013 (0.056)
Train: 102 [ 100/1251 (  8%)]  Loss: 3.313 (3.51)  Time: 0.669s, 1530.59/s  (0.680s, 1504.96/s)  LR: 7.418e-04  Data: 0.014 (0.035)
Train: 102 [ 150/1251 ( 12%)]  Loss: 3.684 (3.55)  Time: 0.680s, 1506.05/s  (0.677s, 1511.77/s)  LR: 7.416e-04  Data: 0.015 (0.028)
Train: 102 [ 200/1251 ( 16%)]  Loss: 3.708 (3.58)  Time: 0.667s, 1534.21/s  (0.675s, 1515.99/s)  LR: 7.414e-04  Data: 0.014 (0.024)
Train: 102 [ 250/1251 ( 20%)]  Loss: 3.769 (3.61)  Time: 0.669s, 1531.14/s  (0.674s, 1518.53/s)  LR: 7.413e-04  Data: 0.012 (0.022)
Train: 102 [ 300/1251 ( 24%)]  Loss: 3.789 (3.64)  Time: 0.673s, 1521.50/s  (0.674s, 1519.73/s)  LR: 7.411e-04  Data: 0.014 (0.021)
Train: 102 [ 350/1251 ( 28%)]  Loss: 3.587 (3.63)  Time: 0.670s, 1528.65/s  (0.673s, 1520.72/s)  LR: 7.409e-04  Data: 0.013 (0.020)
Train: 102 [ 400/1251 ( 32%)]  Loss: 3.847 (3.66)  Time: 0.667s, 1534.69/s  (0.673s, 1521.19/s)  LR: 7.407e-04  Data: 0.013 (0.019)
Train: 102 [ 450/1251 ( 36%)]  Loss: 3.601 (3.65)  Time: 0.664s, 1541.95/s  (0.673s, 1521.65/s)  LR: 7.405e-04  Data: 0.013 (0.018)
Train: 102 [ 500/1251 ( 40%)]  Loss: 3.434 (3.63)  Time: 0.666s, 1537.62/s  (0.673s, 1522.04/s)  LR: 7.403e-04  Data: 0.011 (0.018)
Train: 102 [ 550/1251 ( 44%)]  Loss: 3.631 (3.63)  Time: 0.675s, 1516.12/s  (0.673s, 1522.58/s)  LR: 7.402e-04  Data: 0.013 (0.017)
Train: 102 [ 600/1251 ( 48%)]  Loss: 3.491 (3.62)  Time: 0.669s, 1530.45/s  (0.673s, 1522.65/s)  LR: 7.400e-04  Data: 0.013 (0.017)
Train: 102 [ 650/1251 ( 52%)]  Loss: 3.659 (3.62)  Time: 0.678s, 1510.13/s  (0.673s, 1522.63/s)  LR: 7.398e-04  Data: 0.016 (0.017)
Train: 102 [ 700/1251 ( 56%)]  Loss: 3.720 (3.63)  Time: 0.678s, 1511.06/s  (0.673s, 1522.57/s)  LR: 7.396e-04  Data: 0.016 (0.017)
Train: 102 [ 750/1251 ( 60%)]  Loss: 3.790 (3.64)  Time: 0.668s, 1532.37/s  (0.673s, 1522.65/s)  LR: 7.394e-04  Data: 0.015 (0.016)
Train: 102 [ 800/1251 ( 64%)]  Loss: 3.483 (3.63)  Time: 0.671s, 1526.02/s  (0.673s, 1522.63/s)  LR: 7.392e-04  Data: 0.013 (0.016)
Train: 102 [ 850/1251 ( 68%)]  Loss: 3.420 (3.62)  Time: 0.667s, 1535.08/s  (0.673s, 1522.59/s)  LR: 7.391e-04  Data: 0.013 (0.016)
Train: 102 [ 900/1251 ( 72%)]  Loss: 3.980 (3.64)  Time: 0.669s, 1530.81/s  (0.672s, 1522.88/s)  LR: 7.389e-04  Data: 0.013 (0.016)
Train: 102 [ 950/1251 ( 76%)]  Loss: 3.601 (3.64)  Time: 0.672s, 1523.74/s  (0.672s, 1523.02/s)  LR: 7.387e-04  Data: 0.014 (0.016)
Train: 102 [1000/1251 ( 80%)]  Loss: 3.555 (3.63)  Time: 0.666s, 1538.28/s  (0.672s, 1523.23/s)  LR: 7.385e-04  Data: 0.013 (0.016)
Train: 102 [1050/1251 ( 84%)]  Loss: 3.643 (3.63)  Time: 0.674s, 1519.23/s  (0.672s, 1523.39/s)  LR: 7.383e-04  Data: 0.013 (0.016)
Train: 102 [1100/1251 ( 88%)]  Loss: 3.618 (3.63)  Time: 0.666s, 1537.99/s  (0.672s, 1523.60/s)  LR: 7.381e-04  Data: 0.013 (0.016)
Train: 102 [1150/1251 ( 92%)]  Loss: 3.857 (3.64)  Time: 0.673s, 1521.23/s  (0.672s, 1523.77/s)  LR: 7.380e-04  Data: 0.018 (0.015)
Train: 102 [1200/1251 ( 96%)]  Loss: 3.490 (3.64)  Time: 0.670s, 1527.42/s  (0.672s, 1523.80/s)  LR: 7.378e-04  Data: 0.012 (0.015)
Train: 102 [1250/1251 (100%)]  Loss: 3.678 (3.64)  Time: 0.656s, 1561.84/s  (0.672s, 1524.08/s)  LR: 7.376e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.881 (2.881)  Loss:  0.5557 (0.5557)  Acc@1: 87.6953 (87.6953)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.171 (0.325)  Loss:  0.5864 (1.0712)  Acc@1: 85.1415 (74.5320)  Acc@5: 97.1698 (92.7200)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-97.pth.tar', 74.6040000048828)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-95.pth.tar', 74.58199998535156)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-102.pth.tar', 74.53200013916016)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-100.pth.tar', 74.39800008789062)

Train: 103 [   0/1251 (  0%)]  Loss: 3.755 (3.75)  Time: 3.966s,  258.19/s  (3.966s,  258.19/s)  LR: 7.376e-04  Data: 1.826 (1.826)
Train: 103 [  50/1251 (  4%)]  Loss: 3.622 (3.69)  Time: 0.649s, 1577.31/s  (0.695s, 1472.53/s)  LR: 7.374e-04  Data: 0.016 (0.049)
Train: 103 [ 100/1251 (  8%)]  Loss: 3.562 (3.65)  Time: 0.670s, 1527.81/s  (0.680s, 1506.77/s)  LR: 7.372e-04  Data: 0.013 (0.032)
Train: 103 [ 150/1251 ( 12%)]  Loss: 3.526 (3.62)  Time: 0.668s, 1533.32/s  (0.676s, 1515.68/s)  LR: 7.370e-04  Data: 0.013 (0.026)
Train: 103 [ 200/1251 ( 16%)]  Loss: 3.923 (3.68)  Time: 0.673s, 1521.70/s  (0.674s, 1518.78/s)  LR: 7.369e-04  Data: 0.015 (0.023)
Train: 103 [ 250/1251 ( 20%)]  Loss: 3.298 (3.61)  Time: 0.667s, 1534.93/s  (0.673s, 1521.05/s)  LR: 7.367e-04  Data: 0.015 (0.021)
Train: 103 [ 300/1251 ( 24%)]  Loss: 3.475 (3.59)  Time: 0.679s, 1507.14/s  (0.673s, 1520.77/s)  LR: 7.365e-04  Data: 0.013 (0.020)
Train: 103 [ 350/1251 ( 28%)]  Loss: 3.177 (3.54)  Time: 0.672s, 1524.52/s  (0.673s, 1520.54/s)  LR: 7.363e-04  Data: 0.014 (0.019)
Train: 103 [ 400/1251 ( 32%)]  Loss: 3.650 (3.55)  Time: 0.672s, 1523.62/s  (0.674s, 1520.09/s)  LR: 7.361e-04  Data: 0.013 (0.018)
Train: 103 [ 450/1251 ( 36%)]  Loss: 3.598 (3.56)  Time: 0.674s, 1518.52/s  (0.674s, 1519.59/s)  LR: 7.359e-04  Data: 0.013 (0.018)
Train: 103 [ 500/1251 ( 40%)]  Loss: 3.648 (3.57)  Time: 0.661s, 1549.89/s  (0.674s, 1519.54/s)  LR: 7.358e-04  Data: 0.016 (0.017)
Train: 103 [ 550/1251 ( 44%)]  Loss: 3.860 (3.59)  Time: 0.672s, 1524.01/s  (0.674s, 1520.11/s)  LR: 7.356e-04  Data: 0.014 (0.017)
Train: 103 [ 600/1251 ( 48%)]  Loss: 3.560 (3.59)  Time: 0.675s, 1516.28/s  (0.674s, 1520.14/s)  LR: 7.354e-04  Data: 0.013 (0.017)
Train: 103 [ 650/1251 ( 52%)]  Loss: 3.786 (3.60)  Time: 0.674s, 1519.55/s  (0.673s, 1520.50/s)  LR: 7.352e-04  Data: 0.014 (0.016)
Train: 103 [ 700/1251 ( 56%)]  Loss: 3.806 (3.62)  Time: 0.677s, 1512.28/s  (0.673s, 1520.69/s)  LR: 7.350e-04  Data: 0.013 (0.016)
Train: 103 [ 750/1251 ( 60%)]  Loss: 3.640 (3.62)  Time: 0.671s, 1526.70/s  (0.673s, 1520.79/s)  LR: 7.348e-04  Data: 0.013 (0.016)
Train: 103 [ 800/1251 ( 64%)]  Loss: 3.492 (3.61)  Time: 0.666s, 1538.35/s  (0.673s, 1520.96/s)  LR: 7.346e-04  Data: 0.014 (0.016)
Train: 103 [ 850/1251 ( 68%)]  Loss: 3.675 (3.61)  Time: 0.675s, 1516.71/s  (0.673s, 1521.10/s)  LR: 7.345e-04  Data: 0.013 (0.016)
Train: 103 [ 900/1251 ( 72%)]  Loss: 3.483 (3.61)  Time: 0.681s, 1504.54/s  (0.673s, 1521.34/s)  LR: 7.343e-04  Data: 0.017 (0.016)
Train: 103 [ 950/1251 ( 76%)]  Loss: 3.602 (3.61)  Time: 0.669s, 1530.99/s  (0.673s, 1521.38/s)  LR: 7.341e-04  Data: 0.013 (0.016)
Train: 103 [1000/1251 ( 80%)]  Loss: 3.561 (3.60)  Time: 0.666s, 1537.09/s  (0.673s, 1521.38/s)  LR: 7.339e-04  Data: 0.016 (0.015)
Train: 103 [1050/1251 ( 84%)]  Loss: 3.640 (3.61)  Time: 0.675s, 1516.49/s  (0.673s, 1521.63/s)  LR: 7.337e-04  Data: 0.012 (0.015)
Train: 103 [1100/1251 ( 88%)]  Loss: 3.830 (3.62)  Time: 0.673s, 1521.51/s  (0.673s, 1521.92/s)  LR: 7.335e-04  Data: 0.013 (0.015)
Train: 103 [1150/1251 ( 92%)]  Loss: 3.698 (3.62)  Time: 0.674s, 1519.54/s  (0.673s, 1522.05/s)  LR: 7.334e-04  Data: 0.012 (0.015)
Train: 103 [1200/1251 ( 96%)]  Loss: 3.372 (3.61)  Time: 0.664s, 1541.59/s  (0.673s, 1522.19/s)  LR: 7.332e-04  Data: 0.013 (0.015)
Train: 103 [1250/1251 (100%)]  Loss: 3.812 (3.62)  Time: 0.658s, 1556.61/s  (0.673s, 1522.43/s)  LR: 7.330e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.761 (2.761)  Loss:  0.5049 (0.5049)  Acc@1: 89.2578 (89.2578)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.168 (0.324)  Loss:  0.5981 (1.0800)  Acc@1: 85.7311 (74.6660)  Acc@5: 97.0519 (92.6460)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-103.pth.tar', 74.66600000732421)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-97.pth.tar', 74.6040000048828)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-95.pth.tar', 74.58199998535156)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-102.pth.tar', 74.53200013916016)

Train: 104 [   0/1251 (  0%)]  Loss: 3.497 (3.50)  Time: 3.431s,  298.46/s  (3.431s,  298.46/s)  LR: 7.330e-04  Data: 1.745 (1.745)
Train: 104 [  50/1251 (  4%)]  Loss: 2.948 (3.22)  Time: 0.662s, 1546.54/s  (0.691s, 1482.34/s)  LR: 7.328e-04  Data: 0.017 (0.048)
Train: 104 [ 100/1251 (  8%)]  Loss: 3.617 (3.35)  Time: 0.668s, 1531.82/s  (0.677s, 1513.23/s)  LR: 7.326e-04  Data: 0.019 (0.031)
Train: 104 [ 150/1251 ( 12%)]  Loss: 3.940 (3.50)  Time: 0.658s, 1557.35/s  (0.674s, 1520.36/s)  LR: 7.324e-04  Data: 0.019 (0.025)
Train: 104 [ 200/1251 ( 16%)]  Loss: 3.755 (3.55)  Time: 0.670s, 1528.94/s  (0.672s, 1523.06/s)  LR: 7.322e-04  Data: 0.013 (0.022)
Train: 104 [ 250/1251 ( 20%)]  Loss: 3.463 (3.54)  Time: 0.668s, 1533.00/s  (0.672s, 1524.60/s)  LR: 7.321e-04  Data: 0.013 (0.021)
Train: 104 [ 300/1251 ( 24%)]  Loss: 3.752 (3.57)  Time: 0.668s, 1533.56/s  (0.671s, 1525.51/s)  LR: 7.319e-04  Data: 0.013 (0.019)
Train: 104 [ 350/1251 ( 28%)]  Loss: 3.495 (3.56)  Time: 0.669s, 1530.86/s  (0.671s, 1525.16/s)  LR: 7.317e-04  Data: 0.012 (0.019)
Train: 104 [ 400/1251 ( 32%)]  Loss: 4.152 (3.62)  Time: 0.671s, 1525.83/s  (0.671s, 1525.56/s)  LR: 7.315e-04  Data: 0.012 (0.018)
Train: 104 [ 450/1251 ( 36%)]  Loss: 3.707 (3.63)  Time: 0.666s, 1537.85/s  (0.671s, 1526.05/s)  LR: 7.313e-04  Data: 0.014 (0.017)
Train: 104 [ 500/1251 ( 40%)]  Loss: 3.501 (3.62)  Time: 0.661s, 1548.26/s  (0.671s, 1526.49/s)  LR: 7.311e-04  Data: 0.013 (0.017)
Train: 104 [ 550/1251 ( 44%)]  Loss: 3.685 (3.63)  Time: 0.673s, 1521.38/s  (0.671s, 1526.53/s)  LR: 7.310e-04  Data: 0.012 (0.017)
Train: 104 [ 600/1251 ( 48%)]  Loss: 3.754 (3.64)  Time: 0.672s, 1523.07/s  (0.671s, 1526.18/s)  LR: 7.308e-04  Data: 0.013 (0.016)
Train: 104 [ 650/1251 ( 52%)]  Loss: 3.734 (3.64)  Time: 0.681s, 1502.66/s  (0.671s, 1525.87/s)  LR: 7.306e-04  Data: 0.013 (0.016)
Train: 104 [ 700/1251 ( 56%)]  Loss: 3.931 (3.66)  Time: 0.681s, 1504.40/s  (0.671s, 1525.52/s)  LR: 7.304e-04  Data: 0.012 (0.016)
Train: 104 [ 750/1251 ( 60%)]  Loss: 3.586 (3.66)  Time: 0.667s, 1534.86/s  (0.671s, 1525.52/s)  LR: 7.302e-04  Data: 0.013 (0.016)
Train: 104 [ 800/1251 ( 64%)]  Loss: 3.472 (3.65)  Time: 0.673s, 1521.34/s  (0.671s, 1525.44/s)  LR: 7.300e-04  Data: 0.013 (0.016)
Train: 104 [ 850/1251 ( 68%)]  Loss: 3.531 (3.64)  Time: 0.675s, 1517.80/s  (0.671s, 1524.99/s)  LR: 7.298e-04  Data: 0.019 (0.016)
Train: 104 [ 900/1251 ( 72%)]  Loss: 3.605 (3.64)  Time: 0.672s, 1524.92/s  (0.672s, 1524.67/s)  LR: 7.297e-04  Data: 0.013 (0.016)
Train: 104 [ 950/1251 ( 76%)]  Loss: 3.801 (3.65)  Time: 0.675s, 1517.29/s  (0.672s, 1524.58/s)  LR: 7.295e-04  Data: 0.015 (0.015)
Train: 104 [1000/1251 ( 80%)]  Loss: 3.890 (3.66)  Time: 0.665s, 1540.18/s  (0.672s, 1524.41/s)  LR: 7.293e-04  Data: 0.016 (0.015)
Train: 104 [1050/1251 ( 84%)]  Loss: 3.323 (3.64)  Time: 0.673s, 1522.07/s  (0.672s, 1524.24/s)  LR: 7.291e-04  Data: 0.014 (0.015)
Train: 104 [1100/1251 ( 88%)]  Loss: 3.547 (3.64)  Time: 0.674s, 1520.15/s  (0.672s, 1524.15/s)  LR: 7.289e-04  Data: 0.016 (0.015)
Train: 104 [1150/1251 ( 92%)]  Loss: 3.593 (3.64)  Time: 0.672s, 1524.06/s  (0.672s, 1524.28/s)  LR: 7.287e-04  Data: 0.016 (0.015)
Train: 104 [1200/1251 ( 96%)]  Loss: 3.587 (3.63)  Time: 0.661s, 1549.20/s  (0.672s, 1524.60/s)  LR: 7.285e-04  Data: 0.012 (0.015)
Train: 104 [1250/1251 (100%)]  Loss: 3.795 (3.64)  Time: 0.661s, 1549.70/s  (0.671s, 1524.98/s)  LR: 7.284e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.974 (2.974)  Loss:  0.4863 (0.4863)  Acc@1: 90.0391 (90.0391)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.166 (0.327)  Loss:  0.5840 (1.0759)  Acc@1: 85.3774 (74.4920)  Acc@5: 97.5236 (92.6920)
Train: 105 [   0/1251 (  0%)]  Loss: 3.695 (3.69)  Time: 3.723s,  275.02/s  (3.723s,  275.02/s)  LR: 7.284e-04  Data: 1.626 (1.626)
Train: 105 [  50/1251 (  4%)]  Loss: 3.736 (3.72)  Time: 0.663s, 1544.99/s  (0.701s, 1461.63/s)  LR: 7.282e-04  Data: 0.016 (0.045)
Train: 105 [ 100/1251 (  8%)]  Loss: 3.521 (3.65)  Time: 0.664s, 1541.16/s  (0.682s, 1500.46/s)  LR: 7.280e-04  Data: 0.015 (0.029)
Train: 105 [ 150/1251 ( 12%)]  Loss: 3.597 (3.64)  Time: 0.667s, 1535.70/s  (0.678s, 1509.73/s)  LR: 7.278e-04  Data: 0.014 (0.024)
Train: 105 [ 200/1251 ( 16%)]  Loss: 3.612 (3.63)  Time: 0.669s, 1529.80/s  (0.676s, 1515.34/s)  LR: 7.276e-04  Data: 0.013 (0.022)
Train: 105 [ 250/1251 ( 20%)]  Loss: 3.450 (3.60)  Time: 0.672s, 1523.56/s  (0.675s, 1517.72/s)  LR: 7.274e-04  Data: 0.012 (0.020)
Train: 105 [ 300/1251 ( 24%)]  Loss: 3.467 (3.58)  Time: 0.669s, 1530.18/s  (0.674s, 1519.95/s)  LR: 7.272e-04  Data: 0.014 (0.019)
Train: 105 [ 350/1251 ( 28%)]  Loss: 3.477 (3.57)  Time: 0.672s, 1524.47/s  (0.673s, 1521.76/s)  LR: 7.271e-04  Data: 0.012 (0.018)
Train: 105 [ 400/1251 ( 32%)]  Loss: 3.611 (3.57)  Time: 0.678s, 1510.90/s  (0.673s, 1522.52/s)  LR: 7.269e-04  Data: 0.013 (0.018)
Train: 105 [ 450/1251 ( 36%)]  Loss: 3.392 (3.56)  Time: 0.666s, 1537.90/s  (0.672s, 1522.76/s)  LR: 7.267e-04  Data: 0.012 (0.017)
Train: 105 [ 500/1251 ( 40%)]  Loss: 3.475 (3.55)  Time: 0.674s, 1518.92/s  (0.672s, 1523.07/s)  LR: 7.265e-04  Data: 0.013 (0.017)
Train: 105 [ 550/1251 ( 44%)]  Loss: 3.581 (3.55)  Time: 0.668s, 1533.71/s  (0.672s, 1523.21/s)  LR: 7.263e-04  Data: 0.013 (0.016)
Train: 105 [ 600/1251 ( 48%)]  Loss: 3.771 (3.57)  Time: 0.672s, 1524.78/s  (0.672s, 1523.31/s)  LR: 7.261e-04  Data: 0.013 (0.016)
Train: 105 [ 650/1251 ( 52%)]  Loss: 3.437 (3.56)  Time: 0.668s, 1533.01/s  (0.672s, 1523.16/s)  LR: 7.259e-04  Data: 0.013 (0.016)
Train: 105 [ 700/1251 ( 56%)]  Loss: 3.899 (3.58)  Time: 0.670s, 1527.86/s  (0.672s, 1522.92/s)  LR: 7.258e-04  Data: 0.012 (0.016)
Train: 105 [ 750/1251 ( 60%)]  Loss: 3.438 (3.57)  Time: 0.683s, 1499.02/s  (0.673s, 1522.60/s)  LR: 7.256e-04  Data: 0.016 (0.016)
Train: 105 [ 800/1251 ( 64%)]  Loss: 3.407 (3.56)  Time: 0.679s, 1508.13/s  (0.673s, 1522.28/s)  LR: 7.254e-04  Data: 0.013 (0.015)
Train: 105 [ 850/1251 ( 68%)]  Loss: 3.682 (3.57)  Time: 0.680s, 1505.73/s  (0.673s, 1521.99/s)  LR: 7.252e-04  Data: 0.016 (0.015)
Train: 105 [ 900/1251 ( 72%)]  Loss: 3.551 (3.57)  Time: 0.674s, 1520.19/s  (0.673s, 1521.97/s)  LR: 7.250e-04  Data: 0.013 (0.015)
Train: 105 [ 950/1251 ( 76%)]  Loss: 3.482 (3.56)  Time: 0.674s, 1520.15/s  (0.673s, 1521.95/s)  LR: 7.248e-04  Data: 0.016 (0.015)
Train: 105 [1000/1251 ( 80%)]  Loss: 3.666 (3.57)  Time: 0.673s, 1522.00/s  (0.673s, 1522.02/s)  LR: 7.246e-04  Data: 0.013 (0.015)
Train: 105 [1050/1251 ( 84%)]  Loss: 3.457 (3.56)  Time: 0.683s, 1498.63/s  (0.673s, 1522.10/s)  LR: 7.245e-04  Data: 0.014 (0.015)
Train: 105 [1100/1251 ( 88%)]  Loss: 3.807 (3.57)  Time: 0.669s, 1531.27/s  (0.673s, 1522.27/s)  LR: 7.243e-04  Data: 0.013 (0.015)
Train: 105 [1150/1251 ( 92%)]  Loss: 3.343 (3.56)  Time: 0.681s, 1504.42/s  (0.673s, 1522.55/s)  LR: 7.241e-04  Data: 0.013 (0.015)
Train: 105 [1200/1251 ( 96%)]  Loss: 3.810 (3.57)  Time: 0.657s, 1558.85/s  (0.672s, 1522.71/s)  LR: 7.239e-04  Data: 0.012 (0.015)
Train: 105 [1250/1251 (100%)]  Loss: 3.779 (3.58)  Time: 0.661s, 1548.35/s  (0.672s, 1522.99/s)  LR: 7.237e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.787 (2.787)  Loss:  0.5737 (0.5737)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.162 (0.326)  Loss:  0.5825 (1.0887)  Acc@1: 86.9104 (74.6920)  Acc@5: 97.2877 (92.7220)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-105.pth.tar', 74.6920000024414)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-103.pth.tar', 74.66600000732421)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-97.pth.tar', 74.6040000048828)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-95.pth.tar', 74.58199998535156)

Train: 106 [   0/1251 (  0%)]  Loss: 3.761 (3.76)  Time: 3.968s,  258.09/s  (3.968s,  258.09/s)  LR: 7.237e-04  Data: 2.051 (2.051)
Train: 106 [  50/1251 (  4%)]  Loss: 3.735 (3.75)  Time: 0.668s, 1532.37/s  (0.697s, 1469.57/s)  LR: 7.235e-04  Data: 0.016 (0.054)
Train: 106 [ 100/1251 (  8%)]  Loss: 3.304 (3.60)  Time: 0.676s, 1515.54/s  (0.680s, 1504.86/s)  LR: 7.233e-04  Data: 0.014 (0.034)
Train: 106 [ 150/1251 ( 12%)]  Loss: 3.608 (3.60)  Time: 0.681s, 1504.49/s  (0.677s, 1513.61/s)  LR: 7.231e-04  Data: 0.016 (0.027)
Train: 106 [ 200/1251 ( 16%)]  Loss: 3.669 (3.62)  Time: 0.668s, 1534.00/s  (0.675s, 1517.62/s)  LR: 7.230e-04  Data: 0.014 (0.024)
Train: 106 [ 250/1251 ( 20%)]  Loss: 3.243 (3.55)  Time: 0.654s, 1564.76/s  (0.674s, 1519.37/s)  LR: 7.228e-04  Data: 0.013 (0.022)
Train: 106 [ 300/1251 ( 24%)]  Loss: 3.496 (3.55)  Time: 0.670s, 1529.23/s  (0.674s, 1520.29/s)  LR: 7.226e-04  Data: 0.015 (0.020)
Train: 106 [ 350/1251 ( 28%)]  Loss: 3.498 (3.54)  Time: 0.668s, 1532.69/s  (0.673s, 1521.39/s)  LR: 7.224e-04  Data: 0.014 (0.019)
Train: 106 [ 400/1251 ( 32%)]  Loss: 3.627 (3.55)  Time: 0.665s, 1539.92/s  (0.673s, 1522.09/s)  LR: 7.222e-04  Data: 0.016 (0.019)
Train: 106 [ 450/1251 ( 36%)]  Loss: 3.100 (3.50)  Time: 0.670s, 1529.01/s  (0.673s, 1522.35/s)  LR: 7.220e-04  Data: 0.013 (0.018)
Train: 106 [ 500/1251 ( 40%)]  Loss: 3.386 (3.49)  Time: 0.665s, 1539.87/s  (0.673s, 1522.56/s)  LR: 7.218e-04  Data: 0.012 (0.018)
Train: 106 [ 550/1251 ( 44%)]  Loss: 3.763 (3.52)  Time: 0.675s, 1515.91/s  (0.673s, 1522.59/s)  LR: 7.216e-04  Data: 0.015 (0.017)
Train: 106 [ 600/1251 ( 48%)]  Loss: 3.705 (3.53)  Time: 0.670s, 1528.10/s  (0.673s, 1522.55/s)  LR: 7.215e-04  Data: 0.013 (0.017)
Train: 106 [ 650/1251 ( 52%)]  Loss: 4.025 (3.57)  Time: 0.669s, 1529.88/s  (0.673s, 1522.49/s)  LR: 7.213e-04  Data: 0.013 (0.017)
Train: 106 [ 700/1251 ( 56%)]  Loss: 3.719 (3.58)  Time: 0.675s, 1517.04/s  (0.673s, 1522.53/s)  LR: 7.211e-04  Data: 0.013 (0.017)
Train: 106 [ 750/1251 ( 60%)]  Loss: 3.828 (3.59)  Time: 0.673s, 1522.01/s  (0.673s, 1522.43/s)  LR: 7.209e-04  Data: 0.015 (0.016)
Train: 106 [ 800/1251 ( 64%)]  Loss: 3.663 (3.60)  Time: 0.675s, 1516.97/s  (0.673s, 1522.66/s)  LR: 7.207e-04  Data: 0.013 (0.016)
Train: 106 [ 850/1251 ( 68%)]  Loss: 3.907 (3.61)  Time: 0.678s, 1510.17/s  (0.672s, 1522.71/s)  LR: 7.205e-04  Data: 0.014 (0.016)
Train: 106 [ 900/1251 ( 72%)]  Loss: 3.324 (3.60)  Time: 0.688s, 1488.74/s  (0.672s, 1522.69/s)  LR: 7.203e-04  Data: 0.013 (0.016)
Train: 106 [ 950/1251 ( 76%)]  Loss: 3.616 (3.60)  Time: 0.679s, 1508.37/s  (0.672s, 1522.75/s)  LR: 7.202e-04  Data: 0.014 (0.016)
Train: 106 [1000/1251 ( 80%)]  Loss: 3.383 (3.59)  Time: 0.672s, 1524.29/s  (0.672s, 1522.78/s)  LR: 7.200e-04  Data: 0.016 (0.016)
Train: 106 [1050/1251 ( 84%)]  Loss: 3.666 (3.59)  Time: 0.686s, 1492.59/s  (0.673s, 1522.59/s)  LR: 7.198e-04  Data: 0.013 (0.016)
Train: 106 [1100/1251 ( 88%)]  Loss: 3.572 (3.59)  Time: 0.682s, 1500.64/s  (0.673s, 1522.50/s)  LR: 7.196e-04  Data: 0.013 (0.016)
Train: 106 [1150/1251 ( 92%)]  Loss: 3.576 (3.59)  Time: 0.671s, 1526.52/s  (0.673s, 1522.42/s)  LR: 7.194e-04  Data: 0.012 (0.015)
Train: 106 [1200/1251 ( 96%)]  Loss: 3.616 (3.59)  Time: 0.686s, 1492.28/s  (0.673s, 1522.59/s)  LR: 7.192e-04  Data: 0.013 (0.015)
Train: 106 [1250/1251 (100%)]  Loss: 3.334 (3.58)  Time: 0.659s, 1554.32/s  (0.672s, 1522.73/s)  LR: 7.190e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.803 (2.803)  Loss:  0.5054 (0.5054)  Acc@1: 89.9414 (89.9414)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.164 (0.334)  Loss:  0.6196 (1.0811)  Acc@1: 85.4953 (74.8220)  Acc@5: 97.0519 (92.6400)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-106.pth.tar', 74.82200003417968)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-105.pth.tar', 74.6920000024414)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-103.pth.tar', 74.66600000732421)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-97.pth.tar', 74.6040000048828)

Train: 107 [   0/1251 (  0%)]  Loss: 3.391 (3.39)  Time: 3.728s,  274.69/s  (3.728s,  274.69/s)  LR: 7.190e-04  Data: 1.611 (1.611)
Train: 107 [  50/1251 (  4%)]  Loss: 3.530 (3.46)  Time: 0.656s, 1559.90/s  (0.699s, 1464.51/s)  LR: 7.188e-04  Data: 0.014 (0.045)
Train: 107 [ 100/1251 (  8%)]  Loss: 3.677 (3.53)  Time: 0.664s, 1542.35/s  (0.680s, 1505.11/s)  LR: 7.186e-04  Data: 0.013 (0.029)
Train: 107 [ 150/1251 ( 12%)]  Loss: 3.065 (3.42)  Time: 0.668s, 1533.09/s  (0.675s, 1516.03/s)  LR: 7.185e-04  Data: 0.014 (0.024)
Train: 107 [ 200/1251 ( 16%)]  Loss: 3.784 (3.49)  Time: 0.662s, 1547.85/s  (0.674s, 1520.37/s)  LR: 7.183e-04  Data: 0.016 (0.022)
Train: 107 [ 250/1251 ( 20%)]  Loss: 3.664 (3.52)  Time: 0.665s, 1540.15/s  (0.672s, 1523.53/s)  LR: 7.181e-04  Data: 0.013 (0.020)
Train: 107 [ 300/1251 ( 24%)]  Loss: 3.737 (3.55)  Time: 0.669s, 1531.08/s  (0.671s, 1525.71/s)  LR: 7.179e-04  Data: 0.013 (0.019)
Train: 107 [ 350/1251 ( 28%)]  Loss: 3.653 (3.56)  Time: 0.674s, 1518.70/s  (0.671s, 1526.19/s)  LR: 7.177e-04  Data: 0.014 (0.018)
Train: 107 [ 400/1251 ( 32%)]  Loss: 3.873 (3.60)  Time: 0.668s, 1531.89/s  (0.671s, 1526.06/s)  LR: 7.175e-04  Data: 0.013 (0.018)
Train: 107 [ 450/1251 ( 36%)]  Loss: 3.844 (3.62)  Time: 0.666s, 1537.12/s  (0.671s, 1527.02/s)  LR: 7.173e-04  Data: 0.013 (0.017)
Train: 107 [ 500/1251 ( 40%)]  Loss: 3.700 (3.63)  Time: 0.673s, 1521.49/s  (0.670s, 1527.32/s)  LR: 7.171e-04  Data: 0.013 (0.017)
Train: 107 [ 550/1251 ( 44%)]  Loss: 3.555 (3.62)  Time: 0.668s, 1532.60/s  (0.670s, 1527.88/s)  LR: 7.170e-04  Data: 0.012 (0.017)
Train: 107 [ 600/1251 ( 48%)]  Loss: 3.696 (3.63)  Time: 0.675s, 1518.06/s  (0.670s, 1528.30/s)  LR: 7.168e-04  Data: 0.012 (0.016)
Train: 107 [ 650/1251 ( 52%)]  Loss: 3.523 (3.62)  Time: 0.673s, 1521.11/s  (0.670s, 1528.49/s)  LR: 7.166e-04  Data: 0.013 (0.016)
Train: 107 [ 700/1251 ( 56%)]  Loss: 3.821 (3.63)  Time: 0.664s, 1541.98/s  (0.670s, 1528.98/s)  LR: 7.164e-04  Data: 0.014 (0.016)
Train: 107 [ 750/1251 ( 60%)]  Loss: 3.749 (3.64)  Time: 0.679s, 1507.35/s  (0.670s, 1529.02/s)  LR: 7.162e-04  Data: 0.011 (0.016)
Train: 107 [ 800/1251 ( 64%)]  Loss: 3.498 (3.63)  Time: 0.668s, 1532.36/s  (0.670s, 1529.04/s)  LR: 7.160e-04  Data: 0.012 (0.016)
Train: 107 [ 850/1251 ( 68%)]  Loss: 3.861 (3.65)  Time: 0.678s, 1509.63/s  (0.670s, 1529.01/s)  LR: 7.158e-04  Data: 0.013 (0.016)
Train: 107 [ 900/1251 ( 72%)]  Loss: 3.535 (3.64)  Time: 0.670s, 1528.37/s  (0.670s, 1529.20/s)  LR: 7.156e-04  Data: 0.016 (0.016)
Train: 107 [ 950/1251 ( 76%)]  Loss: 3.860 (3.65)  Time: 0.668s, 1533.22/s  (0.670s, 1529.18/s)  LR: 7.155e-04  Data: 0.016 (0.015)
Train: 107 [1000/1251 ( 80%)]  Loss: 3.693 (3.65)  Time: 0.667s, 1535.35/s  (0.670s, 1529.15/s)  LR: 7.153e-04  Data: 0.013 (0.015)
Train: 107 [1050/1251 ( 84%)]  Loss: 3.808 (3.66)  Time: 0.682s, 1501.52/s  (0.670s, 1528.95/s)  LR: 7.151e-04  Data: 0.013 (0.015)
Train: 107 [1100/1251 ( 88%)]  Loss: 3.735 (3.66)  Time: 0.672s, 1523.19/s  (0.670s, 1528.60/s)  LR: 7.149e-04  Data: 0.013 (0.015)
Train: 107 [1150/1251 ( 92%)]  Loss: 3.818 (3.67)  Time: 0.671s, 1525.23/s  (0.670s, 1528.41/s)  LR: 7.147e-04  Data: 0.013 (0.015)
Train: 107 [1200/1251 ( 96%)]  Loss: 3.591 (3.67)  Time: 0.674s, 1519.83/s  (0.670s, 1528.29/s)  LR: 7.145e-04  Data: 0.012 (0.015)
Train: 107 [1250/1251 (100%)]  Loss: 3.374 (3.66)  Time: 0.660s, 1550.89/s  (0.670s, 1528.29/s)  LR: 7.143e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.009 (3.009)  Loss:  0.4771 (0.4771)  Acc@1: 90.3320 (90.3320)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.161 (0.320)  Loss:  0.5923 (1.0505)  Acc@1: 85.2594 (74.8100)  Acc@5: 97.4057 (92.8340)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-106.pth.tar', 74.82200003417968)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-107.pth.tar', 74.80999993164062)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-105.pth.tar', 74.6920000024414)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-103.pth.tar', 74.66600000732421)

Train: 108 [   0/1251 (  0%)]  Loss: 3.933 (3.93)  Time: 3.091s,  331.27/s  (3.091s,  331.27/s)  LR: 7.143e-04  Data: 1.673 (1.673)
Train: 108 [  50/1251 (  4%)]  Loss: 3.762 (3.85)  Time: 0.657s, 1559.36/s  (0.697s, 1468.30/s)  LR: 7.141e-04  Data: 0.013 (0.046)
Train: 108 [ 100/1251 (  8%)]  Loss: 3.608 (3.77)  Time: 0.664s, 1542.18/s  (0.682s, 1501.28/s)  LR: 7.139e-04  Data: 0.012 (0.030)
Train: 108 [ 150/1251 ( 12%)]  Loss: 3.653 (3.74)  Time: 0.675s, 1516.10/s  (0.678s, 1510.52/s)  LR: 7.138e-04  Data: 0.018 (0.025)
Train: 108 [ 200/1251 ( 16%)]  Loss: 3.563 (3.70)  Time: 0.679s, 1507.22/s  (0.677s, 1512.71/s)  LR: 7.136e-04  Data: 0.012 (0.022)
Train: 108 [ 250/1251 ( 20%)]  Loss: 3.315 (3.64)  Time: 0.653s, 1568.53/s  (0.676s, 1515.77/s)  LR: 7.134e-04  Data: 0.017 (0.020)
Train: 108 [ 300/1251 ( 24%)]  Loss: 3.470 (3.61)  Time: 0.673s, 1521.68/s  (0.675s, 1518.02/s)  LR: 7.132e-04  Data: 0.013 (0.019)
Train: 108 [ 350/1251 ( 28%)]  Loss: 3.669 (3.62)  Time: 0.670s, 1529.23/s  (0.674s, 1518.79/s)  LR: 7.130e-04  Data: 0.015 (0.018)
Train: 108 [ 400/1251 ( 32%)]  Loss: 3.946 (3.66)  Time: 0.671s, 1526.34/s  (0.674s, 1519.42/s)  LR: 7.128e-04  Data: 0.013 (0.018)
Train: 108 [ 450/1251 ( 36%)]  Loss: 3.787 (3.67)  Time: 0.679s, 1508.32/s  (0.674s, 1519.52/s)  LR: 7.126e-04  Data: 0.012 (0.017)
Train: 108 [ 500/1251 ( 40%)]  Loss: 3.377 (3.64)  Time: 0.680s, 1506.91/s  (0.674s, 1519.57/s)  LR: 7.124e-04  Data: 0.013 (0.017)
Train: 108 [ 550/1251 ( 44%)]  Loss: 3.759 (3.65)  Time: 0.673s, 1521.85/s  (0.674s, 1519.39/s)  LR: 7.122e-04  Data: 0.012 (0.017)
Train: 108 [ 600/1251 ( 48%)]  Loss: 3.789 (3.66)  Time: 0.670s, 1527.46/s  (0.674s, 1519.38/s)  LR: 7.121e-04  Data: 0.013 (0.016)
Train: 108 [ 650/1251 ( 52%)]  Loss: 3.848 (3.68)  Time: 0.671s, 1526.88/s  (0.674s, 1519.48/s)  LR: 7.119e-04  Data: 0.016 (0.016)
Train: 108 [ 700/1251 ( 56%)]  Loss: 3.665 (3.68)  Time: 0.674s, 1519.20/s  (0.674s, 1519.73/s)  LR: 7.117e-04  Data: 0.013 (0.016)
Train: 108 [ 750/1251 ( 60%)]  Loss: 3.738 (3.68)  Time: 0.672s, 1523.64/s  (0.674s, 1520.10/s)  LR: 7.115e-04  Data: 0.015 (0.016)
Train: 108 [ 800/1251 ( 64%)]  Loss: 3.392 (3.66)  Time: 0.673s, 1521.19/s  (0.674s, 1520.19/s)  LR: 7.113e-04  Data: 0.014 (0.016)
Train: 108 [ 850/1251 ( 68%)]  Loss: 3.695 (3.66)  Time: 0.674s, 1520.32/s  (0.674s, 1520.36/s)  LR: 7.111e-04  Data: 0.012 (0.016)
Train: 108 [ 900/1251 ( 72%)]  Loss: 3.384 (3.65)  Time: 0.675s, 1516.33/s  (0.673s, 1520.54/s)  LR: 7.109e-04  Data: 0.013 (0.015)
Train: 108 [ 950/1251 ( 76%)]  Loss: 3.588 (3.65)  Time: 0.664s, 1541.12/s  (0.673s, 1520.70/s)  LR: 7.107e-04  Data: 0.014 (0.015)
Train: 108 [1000/1251 ( 80%)]  Loss: 3.703 (3.65)  Time: 0.674s, 1519.67/s  (0.673s, 1520.86/s)  LR: 7.105e-04  Data: 0.013 (0.015)
Train: 108 [1050/1251 ( 84%)]  Loss: 3.668 (3.65)  Time: 0.669s, 1530.48/s  (0.673s, 1521.07/s)  LR: 7.104e-04  Data: 0.013 (0.015)
Train: 108 [1100/1251 ( 88%)]  Loss: 3.372 (3.64)  Time: 0.667s, 1536.33/s  (0.673s, 1521.24/s)  LR: 7.102e-04  Data: 0.013 (0.015)
Train: 108 [1150/1251 ( 92%)]  Loss: 3.547 (3.63)  Time: 0.677s, 1513.24/s  (0.673s, 1521.54/s)  LR: 7.100e-04  Data: 0.016 (0.015)
Train: 108 [1200/1251 ( 96%)]  Loss: 3.285 (3.62)  Time: 0.657s, 1558.98/s  (0.673s, 1521.91/s)  LR: 7.098e-04  Data: 0.014 (0.015)
Train: 108 [1250/1251 (100%)]  Loss: 3.518 (3.62)  Time: 0.654s, 1566.09/s  (0.673s, 1522.07/s)  LR: 7.096e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.794 (2.794)  Loss:  0.5459 (0.5459)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.164 (0.320)  Loss:  0.6685 (1.1050)  Acc@1: 84.7877 (74.5520)  Acc@5: 96.3443 (92.5160)
Train: 109 [   0/1251 (  0%)]  Loss: 3.808 (3.81)  Time: 3.474s,  294.80/s  (3.474s,  294.80/s)  LR: 7.096e-04  Data: 1.585 (1.585)
Train: 109 [  50/1251 (  4%)]  Loss: 3.405 (3.61)  Time: 0.650s, 1574.81/s  (0.700s, 1462.80/s)  LR: 7.094e-04  Data: 0.013 (0.044)
Train: 109 [ 100/1251 (  8%)]  Loss: 3.601 (3.60)  Time: 0.660s, 1552.22/s  (0.682s, 1500.92/s)  LR: 7.092e-04  Data: 0.016 (0.029)
Train: 109 [ 150/1251 ( 12%)]  Loss: 3.975 (3.70)  Time: 0.660s, 1551.53/s  (0.677s, 1513.41/s)  LR: 7.090e-04  Data: 0.014 (0.024)
Train: 109 [ 200/1251 ( 16%)]  Loss: 3.734 (3.70)  Time: 0.670s, 1527.33/s  (0.674s, 1518.45/s)  LR: 7.088e-04  Data: 0.016 (0.021)
Train: 109 [ 250/1251 ( 20%)]  Loss: 3.588 (3.69)  Time: 0.674s, 1518.30/s  (0.673s, 1521.05/s)  LR: 7.086e-04  Data: 0.013 (0.020)
Train: 109 [ 300/1251 ( 24%)]  Loss: 3.462 (3.65)  Time: 0.672s, 1522.69/s  (0.673s, 1522.44/s)  LR: 7.085e-04  Data: 0.017 (0.019)
Train: 109 [ 350/1251 ( 28%)]  Loss: 3.974 (3.69)  Time: 0.669s, 1530.90/s  (0.672s, 1523.39/s)  LR: 7.083e-04  Data: 0.012 (0.018)
Train: 109 [ 400/1251 ( 32%)]  Loss: 3.799 (3.71)  Time: 0.666s, 1536.44/s  (0.672s, 1523.92/s)  LR: 7.081e-04  Data: 0.012 (0.018)
Train: 109 [ 450/1251 ( 36%)]  Loss: 3.394 (3.67)  Time: 0.667s, 1534.75/s  (0.672s, 1524.00/s)  LR: 7.079e-04  Data: 0.012 (0.017)
Train: 109 [ 500/1251 ( 40%)]  Loss: 3.847 (3.69)  Time: 0.670s, 1528.88/s  (0.672s, 1524.30/s)  LR: 7.077e-04  Data: 0.013 (0.017)
Train: 109 [ 550/1251 ( 44%)]  Loss: 3.478 (3.67)  Time: 0.676s, 1515.30/s  (0.672s, 1524.51/s)  LR: 7.075e-04  Data: 0.012 (0.016)
Train: 109 [ 600/1251 ( 48%)]  Loss: 3.766 (3.68)  Time: 0.665s, 1539.69/s  (0.671s, 1525.07/s)  LR: 7.073e-04  Data: 0.013 (0.016)
Train: 109 [ 650/1251 ( 52%)]  Loss: 3.655 (3.68)  Time: 0.676s, 1514.08/s  (0.671s, 1525.45/s)  LR: 7.071e-04  Data: 0.014 (0.016)
Train: 109 [ 700/1251 ( 56%)]  Loss: 4.060 (3.70)  Time: 0.680s, 1504.80/s  (0.671s, 1525.49/s)  LR: 7.069e-04  Data: 0.014 (0.016)
Train: 109 [ 750/1251 ( 60%)]  Loss: 3.694 (3.70)  Time: 0.681s, 1504.50/s  (0.671s, 1525.32/s)  LR: 7.068e-04  Data: 0.012 (0.016)
Train: 109 [ 800/1251 ( 64%)]  Loss: 3.750 (3.71)  Time: 0.675s, 1517.34/s  (0.671s, 1525.47/s)  LR: 7.066e-04  Data: 0.014 (0.016)
Train: 109 [ 850/1251 ( 68%)]  Loss: 3.688 (3.70)  Time: 0.675s, 1515.98/s  (0.671s, 1525.56/s)  LR: 7.064e-04  Data: 0.014 (0.015)
Train: 109 [ 900/1251 ( 72%)]  Loss: 3.699 (3.70)  Time: 0.675s, 1517.97/s  (0.671s, 1525.66/s)  LR: 7.062e-04  Data: 0.013 (0.015)
Train: 109 [ 950/1251 ( 76%)]  Loss: 3.783 (3.71)  Time: 0.668s, 1533.07/s  (0.671s, 1525.81/s)  LR: 7.060e-04  Data: 0.012 (0.015)
Train: 109 [1000/1251 ( 80%)]  Loss: 3.596 (3.70)  Time: 0.670s, 1529.07/s  (0.671s, 1526.03/s)  LR: 7.058e-04  Data: 0.013 (0.015)
Train: 109 [1050/1251 ( 84%)]  Loss: 3.679 (3.70)  Time: 0.669s, 1531.48/s  (0.671s, 1526.34/s)  LR: 7.056e-04  Data: 0.015 (0.015)
Train: 109 [1100/1251 ( 88%)]  Loss: 3.751 (3.70)  Time: 0.670s, 1528.75/s  (0.671s, 1526.50/s)  LR: 7.054e-04  Data: 0.013 (0.015)
Train: 109 [1150/1251 ( 92%)]  Loss: 3.713 (3.70)  Time: 0.670s, 1527.76/s  (0.671s, 1526.68/s)  LR: 7.052e-04  Data: 0.015 (0.015)
Train: 109 [1200/1251 ( 96%)]  Loss: 3.573 (3.70)  Time: 0.666s, 1538.37/s  (0.671s, 1526.78/s)  LR: 7.050e-04  Data: 0.012 (0.015)
Train: 109 [1250/1251 (100%)]  Loss: 3.344 (3.69)  Time: 0.670s, 1527.44/s  (0.671s, 1527.07/s)  LR: 7.049e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.936 (2.936)  Loss:  0.5151 (0.5151)  Acc@1: 88.6719 (88.6719)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.166 (0.325)  Loss:  0.5874 (1.0669)  Acc@1: 84.1981 (74.7860)  Acc@5: 97.6415 (92.7760)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-106.pth.tar', 74.82200003417968)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-107.pth.tar', 74.80999993164062)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-109.pth.tar', 74.78599998779296)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-105.pth.tar', 74.6920000024414)

Train: 110 [   0/1251 (  0%)]  Loss: 3.771 (3.77)  Time: 3.803s,  269.29/s  (3.803s,  269.29/s)  LR: 7.048e-04  Data: 1.790 (1.790)
Train: 110 [  50/1251 (  4%)]  Loss: 3.525 (3.65)  Time: 0.652s, 1570.07/s  (0.696s, 1471.94/s)  LR: 7.047e-04  Data: 0.013 (0.049)
Train: 110 [ 100/1251 (  8%)]  Loss: 3.432 (3.58)  Time: 0.664s, 1542.88/s  (0.680s, 1505.41/s)  LR: 7.045e-04  Data: 0.014 (0.032)
Train: 110 [ 150/1251 ( 12%)]  Loss: 3.466 (3.55)  Time: 0.671s, 1526.98/s  (0.677s, 1513.31/s)  LR: 7.043e-04  Data: 0.013 (0.026)
Train: 110 [ 200/1251 ( 16%)]  Loss: 3.605 (3.56)  Time: 0.674s, 1518.41/s  (0.675s, 1517.93/s)  LR: 7.041e-04  Data: 0.013 (0.023)
Train: 110 [ 250/1251 ( 20%)]  Loss: 4.024 (3.64)  Time: 0.657s, 1559.32/s  (0.673s, 1520.76/s)  LR: 7.039e-04  Data: 0.012 (0.021)
Train: 110 [ 300/1251 ( 24%)]  Loss: 3.682 (3.64)  Time: 0.665s, 1540.94/s  (0.673s, 1522.65/s)  LR: 7.037e-04  Data: 0.013 (0.020)
Train: 110 [ 350/1251 ( 28%)]  Loss: 3.925 (3.68)  Time: 0.667s, 1535.76/s  (0.672s, 1524.38/s)  LR: 7.035e-04  Data: 0.013 (0.019)
Train: 110 [ 400/1251 ( 32%)]  Loss: 3.896 (3.70)  Time: 0.662s, 1546.83/s  (0.671s, 1525.51/s)  LR: 7.033e-04  Data: 0.013 (0.018)
Train: 110 [ 450/1251 ( 36%)]  Loss: 3.940 (3.73)  Time: 0.669s, 1530.87/s  (0.671s, 1526.11/s)  LR: 7.031e-04  Data: 0.015 (0.018)
Train: 110 [ 500/1251 ( 40%)]  Loss: 3.898 (3.74)  Time: 0.677s, 1512.93/s  (0.671s, 1526.55/s)  LR: 7.029e-04  Data: 0.014 (0.017)
Train: 110 [ 550/1251 ( 44%)]  Loss: 3.835 (3.75)  Time: 0.673s, 1522.20/s  (0.671s, 1526.44/s)  LR: 7.028e-04  Data: 0.012 (0.017)
Train: 110 [ 600/1251 ( 48%)]  Loss: 3.640 (3.74)  Time: 0.669s, 1529.50/s  (0.671s, 1526.31/s)  LR: 7.026e-04  Data: 0.013 (0.017)
Train: 110 [ 650/1251 ( 52%)]  Loss: 3.667 (3.74)  Time: 0.672s, 1524.36/s  (0.671s, 1526.76/s)  LR: 7.024e-04  Data: 0.018 (0.016)
Train: 110 [ 700/1251 ( 56%)]  Loss: 3.484 (3.72)  Time: 0.671s, 1526.09/s  (0.671s, 1527.13/s)  LR: 7.022e-04  Data: 0.013 (0.016)
Train: 110 [ 750/1251 ( 60%)]  Loss: 3.477 (3.70)  Time: 0.676s, 1515.05/s  (0.670s, 1527.53/s)  LR: 7.020e-04  Data: 0.015 (0.016)
Train: 110 [ 800/1251 ( 64%)]  Loss: 3.356 (3.68)  Time: 0.655s, 1564.49/s  (0.670s, 1527.93/s)  LR: 7.018e-04  Data: 0.012 (0.016)
Train: 110 [ 850/1251 ( 68%)]  Loss: 3.748 (3.69)  Time: 0.680s, 1506.00/s  (0.670s, 1528.23/s)  LR: 7.016e-04  Data: 0.015 (0.016)
Train: 110 [ 900/1251 ( 72%)]  Loss: 3.728 (3.69)  Time: 0.668s, 1532.80/s  (0.670s, 1528.78/s)  LR: 7.014e-04  Data: 0.012 (0.016)
Train: 110 [ 950/1251 ( 76%)]  Loss: 3.819 (3.70)  Time: 0.666s, 1538.10/s  (0.670s, 1529.19/s)  LR: 7.012e-04  Data: 0.017 (0.015)
Train: 110 [1000/1251 ( 80%)]  Loss: 3.392 (3.68)  Time: 0.670s, 1528.76/s  (0.670s, 1529.15/s)  LR: 7.010e-04  Data: 0.012 (0.015)
Train: 110 [1050/1251 ( 84%)]  Loss: 3.706 (3.68)  Time: 0.668s, 1533.99/s  (0.670s, 1529.17/s)  LR: 7.008e-04  Data: 0.013 (0.015)
Train: 110 [1100/1251 ( 88%)]  Loss: 3.361 (3.67)  Time: 0.662s, 1547.25/s  (0.670s, 1529.29/s)  LR: 7.007e-04  Data: 0.014 (0.015)
Train: 110 [1150/1251 ( 92%)]  Loss: 3.674 (3.67)  Time: 0.668s, 1532.05/s  (0.670s, 1529.39/s)  LR: 7.005e-04  Data: 0.017 (0.015)
Train: 110 [1200/1251 ( 96%)]  Loss: 3.545 (3.66)  Time: 0.666s, 1537.74/s  (0.670s, 1529.19/s)  LR: 7.003e-04  Data: 0.015 (0.015)
Train: 110 [1250/1251 (100%)]  Loss: 3.641 (3.66)  Time: 0.658s, 1556.77/s  (0.670s, 1529.16/s)  LR: 7.001e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.025 (3.025)  Loss:  0.5093 (0.5093)  Acc@1: 89.5508 (89.5508)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.168 (0.315)  Loss:  0.6284 (1.0556)  Acc@1: 85.0236 (75.0060)  Acc@5: 96.1085 (92.7720)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-110.pth.tar', 75.00599995849609)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-106.pth.tar', 74.82200003417968)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-107.pth.tar', 74.80999993164062)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-109.pth.tar', 74.78599998779296)

Train: 111 [   0/1251 (  0%)]  Loss: 3.685 (3.68)  Time: 3.868s,  264.74/s  (3.868s,  264.74/s)  LR: 7.001e-04  Data: 1.632 (1.632)
Train: 111 [  50/1251 (  4%)]  Loss: 3.695 (3.69)  Time: 0.664s, 1543.03/s  (0.704s, 1454.95/s)  LR: 6.999e-04  Data: 0.017 (0.046)
Train: 111 [ 100/1251 (  8%)]  Loss: 3.455 (3.61)  Time: 0.666s, 1537.86/s  (0.684s, 1497.30/s)  LR: 6.997e-04  Data: 0.014 (0.030)
Train: 111 [ 150/1251 ( 12%)]  Loss: 3.838 (3.67)  Time: 0.665s, 1540.28/s  (0.678s, 1509.28/s)  LR: 6.995e-04  Data: 0.013 (0.025)
Train: 111 [ 200/1251 ( 16%)]  Loss: 3.471 (3.63)  Time: 0.667s, 1534.13/s  (0.677s, 1513.17/s)  LR: 6.993e-04  Data: 0.013 (0.022)
Train: 111 [ 250/1251 ( 20%)]  Loss: 3.252 (3.57)  Time: 0.667s, 1534.45/s  (0.675s, 1516.80/s)  LR: 6.991e-04  Data: 0.015 (0.020)
Train: 111 [ 300/1251 ( 24%)]  Loss: 3.827 (3.60)  Time: 0.672s, 1522.86/s  (0.675s, 1517.93/s)  LR: 6.989e-04  Data: 0.013 (0.019)
Train: 111 [ 350/1251 ( 28%)]  Loss: 3.794 (3.63)  Time: 0.671s, 1527.17/s  (0.674s, 1519.39/s)  LR: 6.987e-04  Data: 0.014 (0.018)
Train: 111 [ 400/1251 ( 32%)]  Loss: 3.608 (3.63)  Time: 0.667s, 1535.17/s  (0.673s, 1520.58/s)  LR: 6.985e-04  Data: 0.013 (0.018)
Train: 111 [ 450/1251 ( 36%)]  Loss: 3.521 (3.61)  Time: 0.671s, 1526.15/s  (0.673s, 1520.93/s)  LR: 6.984e-04  Data: 0.013 (0.017)
Train: 111 [ 500/1251 ( 40%)]  Loss: 3.320 (3.59)  Time: 0.657s, 1559.12/s  (0.673s, 1521.62/s)  LR: 6.982e-04  Data: 0.015 (0.017)
Train: 111 [ 550/1251 ( 44%)]  Loss: 3.806 (3.61)  Time: 0.667s, 1535.11/s  (0.673s, 1522.05/s)  LR: 6.980e-04  Data: 0.013 (0.017)
Train: 111 [ 600/1251 ( 48%)]  Loss: 3.497 (3.60)  Time: 0.668s, 1532.91/s  (0.673s, 1522.42/s)  LR: 6.978e-04  Data: 0.014 (0.016)
Train: 111 [ 650/1251 ( 52%)]  Loss: 3.930 (3.62)  Time: 0.672s, 1524.26/s  (0.672s, 1522.90/s)  LR: 6.976e-04  Data: 0.013 (0.016)
Train: 111 [ 700/1251 ( 56%)]  Loss: 3.225 (3.59)  Time: 0.674s, 1520.06/s  (0.672s, 1523.11/s)  LR: 6.974e-04  Data: 0.013 (0.016)
Train: 111 [ 750/1251 ( 60%)]  Loss: 3.737 (3.60)  Time: 0.678s, 1510.91/s  (0.672s, 1523.22/s)  LR: 6.972e-04  Data: 0.012 (0.016)
Train: 111 [ 800/1251 ( 64%)]  Loss: 3.686 (3.61)  Time: 0.671s, 1526.64/s  (0.672s, 1523.46/s)  LR: 6.970e-04  Data: 0.014 (0.016)
Train: 111 [ 850/1251 ( 68%)]  Loss: 3.432 (3.60)  Time: 0.673s, 1520.62/s  (0.672s, 1523.81/s)  LR: 6.968e-04  Data: 0.013 (0.016)
Train: 111 [ 900/1251 ( 72%)]  Loss: 3.723 (3.61)  Time: 0.663s, 1544.16/s  (0.672s, 1524.35/s)  LR: 6.966e-04  Data: 0.012 (0.015)
Train: 111 [ 950/1251 ( 76%)]  Loss: 3.426 (3.60)  Time: 0.672s, 1523.08/s  (0.672s, 1524.65/s)  LR: 6.964e-04  Data: 0.015 (0.015)
Train: 111 [1000/1251 ( 80%)]  Loss: 3.743 (3.60)  Time: 0.670s, 1528.42/s  (0.672s, 1524.85/s)  LR: 6.962e-04  Data: 0.013 (0.015)
Train: 111 [1050/1251 ( 84%)]  Loss: 3.439 (3.60)  Time: 0.672s, 1522.95/s  (0.671s, 1525.08/s)  LR: 6.961e-04  Data: 0.013 (0.015)
Train: 111 [1100/1251 ( 88%)]  Loss: 3.328 (3.58)  Time: 0.664s, 1542.96/s  (0.671s, 1525.24/s)  LR: 6.959e-04  Data: 0.013 (0.015)
Train: 111 [1150/1251 ( 92%)]  Loss: 3.259 (3.57)  Time: 0.669s, 1529.83/s  (0.671s, 1525.25/s)  LR: 6.957e-04  Data: 0.016 (0.015)
Train: 111 [1200/1251 ( 96%)]  Loss: 3.539 (3.57)  Time: 0.668s, 1533.23/s  (0.671s, 1525.17/s)  LR: 6.955e-04  Data: 0.013 (0.015)
Train: 111 [1250/1251 (100%)]  Loss: 3.655 (3.57)  Time: 0.654s, 1565.20/s  (0.671s, 1525.14/s)  LR: 6.953e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.870 (2.870)  Loss:  0.5488 (0.5488)  Acc@1: 89.7461 (89.7461)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.166 (0.319)  Loss:  0.6416 (1.1009)  Acc@1: 85.1415 (74.9020)  Acc@5: 96.4623 (92.7640)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-110.pth.tar', 75.00599995849609)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-111.pth.tar', 74.90200013916015)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-106.pth.tar', 74.82200003417968)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-107.pth.tar', 74.80999993164062)

Train: 112 [   0/1251 (  0%)]  Loss: 3.572 (3.57)  Time: 4.066s,  251.86/s  (4.066s,  251.86/s)  LR: 6.953e-04  Data: 1.644 (1.644)
Train: 112 [  50/1251 (  4%)]  Loss: 3.411 (3.49)  Time: 0.659s, 1552.76/s  (0.702s, 1458.36/s)  LR: 6.951e-04  Data: 0.015 (0.046)
Train: 112 [ 100/1251 (  8%)]  Loss: 3.555 (3.51)  Time: 0.671s, 1526.56/s  (0.682s, 1500.89/s)  LR: 6.949e-04  Data: 0.014 (0.030)
Train: 112 [ 150/1251 ( 12%)]  Loss: 3.878 (3.60)  Time: 0.667s, 1535.66/s  (0.677s, 1511.78/s)  LR: 6.947e-04  Data: 0.013 (0.025)
Train: 112 [ 200/1251 ( 16%)]  Loss: 3.631 (3.61)  Time: 0.675s, 1518.08/s  (0.674s, 1518.16/s)  LR: 6.945e-04  Data: 0.013 (0.022)
Train: 112 [ 250/1251 ( 20%)]  Loss: 3.763 (3.64)  Time: 0.668s, 1532.72/s  (0.673s, 1520.63/s)  LR: 6.943e-04  Data: 0.018 (0.020)
Train: 112 [ 300/1251 ( 24%)]  Loss: 3.599 (3.63)  Time: 0.677s, 1512.17/s  (0.673s, 1522.28/s)  LR: 6.941e-04  Data: 0.014 (0.019)
Train: 112 [ 350/1251 ( 28%)]  Loss: 3.610 (3.63)  Time: 0.662s, 1546.89/s  (0.672s, 1523.21/s)  LR: 6.939e-04  Data: 0.015 (0.019)
Train: 112 [ 400/1251 ( 32%)]  Loss: 3.374 (3.60)  Time: 0.669s, 1530.10/s  (0.672s, 1523.54/s)  LR: 6.937e-04  Data: 0.013 (0.018)
Train: 112 [ 450/1251 ( 36%)]  Loss: 3.953 (3.63)  Time: 0.670s, 1528.80/s  (0.672s, 1523.97/s)  LR: 6.936e-04  Data: 0.017 (0.018)
Train: 112 [ 500/1251 ( 40%)]  Loss: 3.592 (3.63)  Time: 0.669s, 1530.26/s  (0.672s, 1523.70/s)  LR: 6.934e-04  Data: 0.013 (0.017)
Train: 112 [ 550/1251 ( 44%)]  Loss: 3.443 (3.62)  Time: 0.672s, 1523.04/s  (0.672s, 1524.16/s)  LR: 6.932e-04  Data: 0.013 (0.017)
Train: 112 [ 600/1251 ( 48%)]  Loss: 3.349 (3.59)  Time: 0.675s, 1516.10/s  (0.672s, 1524.35/s)  LR: 6.930e-04  Data: 0.012 (0.017)
Train: 112 [ 650/1251 ( 52%)]  Loss: 3.585 (3.59)  Time: 0.665s, 1538.79/s  (0.672s, 1524.45/s)  LR: 6.928e-04  Data: 0.012 (0.016)
Train: 112 [ 700/1251 ( 56%)]  Loss: 3.538 (3.59)  Time: 0.674s, 1518.50/s  (0.672s, 1524.78/s)  LR: 6.926e-04  Data: 0.013 (0.016)
Train: 112 [ 750/1251 ( 60%)]  Loss: 3.755 (3.60)  Time: 0.667s, 1534.72/s  (0.672s, 1524.91/s)  LR: 6.924e-04  Data: 0.013 (0.016)
Train: 112 [ 800/1251 ( 64%)]  Loss: 3.612 (3.60)  Time: 0.672s, 1523.90/s  (0.671s, 1525.05/s)  LR: 6.922e-04  Data: 0.013 (0.016)
Train: 112 [ 850/1251 ( 68%)]  Loss: 3.512 (3.60)  Time: 0.684s, 1498.00/s  (0.671s, 1525.12/s)  LR: 6.920e-04  Data: 0.013 (0.016)
Train: 112 [ 900/1251 ( 72%)]  Loss: 3.865 (3.61)  Time: 0.663s, 1543.34/s  (0.671s, 1524.98/s)  LR: 6.918e-04  Data: 0.014 (0.016)
Train: 112 [ 950/1251 ( 76%)]  Loss: 3.492 (3.60)  Time: 0.664s, 1542.63/s  (0.671s, 1525.19/s)  LR: 6.916e-04  Data: 0.012 (0.015)
Train: 112 [1000/1251 ( 80%)]  Loss: 3.672 (3.61)  Time: 0.672s, 1522.78/s  (0.671s, 1525.36/s)  LR: 6.914e-04  Data: 0.013 (0.015)
Train: 112 [1050/1251 ( 84%)]  Loss: 3.639 (3.61)  Time: 0.667s, 1535.09/s  (0.671s, 1525.48/s)  LR: 6.912e-04  Data: 0.018 (0.015)
Train: 112 [1100/1251 ( 88%)]  Loss: 3.424 (3.60)  Time: 0.662s, 1546.95/s  (0.671s, 1525.54/s)  LR: 6.911e-04  Data: 0.014 (0.015)
Train: 112 [1150/1251 ( 92%)]  Loss: 3.748 (3.61)  Time: 0.664s, 1543.19/s  (0.671s, 1525.69/s)  LR: 6.909e-04  Data: 0.012 (0.015)
Train: 112 [1200/1251 ( 96%)]  Loss: 3.708 (3.61)  Time: 0.669s, 1531.28/s  (0.671s, 1525.55/s)  LR: 6.907e-04  Data: 0.016 (0.015)
Train: 112 [1250/1251 (100%)]  Loss: 3.853 (3.62)  Time: 0.658s, 1556.12/s  (0.671s, 1525.71/s)  LR: 6.905e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.149 (3.149)  Loss:  0.5547 (0.5547)  Acc@1: 89.4531 (89.4531)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.164 (0.319)  Loss:  0.6030 (1.0579)  Acc@1: 86.4387 (75.3420)  Acc@5: 97.1698 (92.8220)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-112.pth.tar', 75.34200005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-110.pth.tar', 75.00599995849609)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-111.pth.tar', 74.90200013916015)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-106.pth.tar', 74.82200003417968)

Train: 113 [   0/1251 (  0%)]  Loss: 3.597 (3.60)  Time: 3.745s,  273.41/s  (3.745s,  273.41/s)  LR: 6.905e-04  Data: 1.737 (1.737)
Train: 113 [  50/1251 (  4%)]  Loss: 3.452 (3.52)  Time: 0.659s, 1553.44/s  (0.695s, 1473.98/s)  LR: 6.903e-04  Data: 0.013 (0.048)
Train: 113 [ 100/1251 (  8%)]  Loss: 3.871 (3.64)  Time: 0.674s, 1519.46/s  (0.679s, 1507.54/s)  LR: 6.901e-04  Data: 0.014 (0.031)
Train: 113 [ 150/1251 ( 12%)]  Loss: 3.592 (3.63)  Time: 0.671s, 1525.89/s  (0.676s, 1515.62/s)  LR: 6.899e-04  Data: 0.018 (0.025)
Train: 113 [ 200/1251 ( 16%)]  Loss: 3.409 (3.58)  Time: 0.670s, 1529.01/s  (0.674s, 1518.45/s)  LR: 6.897e-04  Data: 0.013 (0.022)
Train: 113 [ 250/1251 ( 20%)]  Loss: 3.648 (3.59)  Time: 0.673s, 1520.93/s  (0.674s, 1520.18/s)  LR: 6.895e-04  Data: 0.013 (0.021)
Train: 113 [ 300/1251 ( 24%)]  Loss: 3.594 (3.59)  Time: 0.678s, 1509.69/s  (0.673s, 1521.66/s)  LR: 6.893e-04  Data: 0.013 (0.019)
Train: 113 [ 350/1251 ( 28%)]  Loss: 3.298 (3.56)  Time: 0.665s, 1539.39/s  (0.672s, 1522.98/s)  LR: 6.891e-04  Data: 0.015 (0.019)
Train: 113 [ 400/1251 ( 32%)]  Loss: 3.685 (3.57)  Time: 0.670s, 1529.38/s  (0.672s, 1524.50/s)  LR: 6.889e-04  Data: 0.016 (0.018)
Train: 113 [ 450/1251 ( 36%)]  Loss: 3.413 (3.56)  Time: 0.667s, 1535.71/s  (0.671s, 1525.29/s)  LR: 6.887e-04  Data: 0.013 (0.018)
Train: 113 [ 500/1251 ( 40%)]  Loss: 3.058 (3.51)  Time: 0.668s, 1533.37/s  (0.671s, 1525.73/s)  LR: 6.885e-04  Data: 0.013 (0.017)
Train: 113 [ 550/1251 ( 44%)]  Loss: 3.670 (3.52)  Time: 0.675s, 1517.50/s  (0.671s, 1526.13/s)  LR: 6.883e-04  Data: 0.016 (0.017)
Train: 113 [ 600/1251 ( 48%)]  Loss: 3.635 (3.53)  Time: 0.663s, 1544.34/s  (0.671s, 1526.45/s)  LR: 6.882e-04  Data: 0.013 (0.017)
Train: 113 [ 650/1251 ( 52%)]  Loss: 3.608 (3.54)  Time: 0.658s, 1555.27/s  (0.671s, 1526.93/s)  LR: 6.880e-04  Data: 0.012 (0.016)
Train: 113 [ 700/1251 ( 56%)]  Loss: 3.789 (3.55)  Time: 0.676s, 1514.84/s  (0.671s, 1527.04/s)  LR: 6.878e-04  Data: 0.013 (0.016)
Train: 113 [ 750/1251 ( 60%)]  Loss: 3.926 (3.58)  Time: 0.672s, 1522.85/s  (0.670s, 1527.44/s)  LR: 6.876e-04  Data: 0.013 (0.016)
Train: 113 [ 800/1251 ( 64%)]  Loss: 3.390 (3.57)  Time: 0.674s, 1519.32/s  (0.670s, 1527.60/s)  LR: 6.874e-04  Data: 0.014 (0.016)
Train: 113 [ 850/1251 ( 68%)]  Loss: 3.542 (3.57)  Time: 0.664s, 1541.91/s  (0.670s, 1527.60/s)  LR: 6.872e-04  Data: 0.013 (0.016)
Train: 113 [ 900/1251 ( 72%)]  Loss: 3.918 (3.58)  Time: 0.673s, 1521.75/s  (0.670s, 1527.66/s)  LR: 6.870e-04  Data: 0.013 (0.016)
Train: 113 [ 950/1251 ( 76%)]  Loss: 3.527 (3.58)  Time: 0.667s, 1535.11/s  (0.670s, 1527.76/s)  LR: 6.868e-04  Data: 0.012 (0.015)
Train: 113 [1000/1251 ( 80%)]  Loss: 3.290 (3.57)  Time: 0.665s, 1539.71/s  (0.670s, 1527.88/s)  LR: 6.866e-04  Data: 0.013 (0.015)
Train: 113 [1050/1251 ( 84%)]  Loss: 3.601 (3.57)  Time: 0.671s, 1525.69/s  (0.670s, 1527.97/s)  LR: 6.864e-04  Data: 0.017 (0.015)
Train: 113 [1100/1251 ( 88%)]  Loss: 3.855 (3.58)  Time: 0.666s, 1537.28/s  (0.670s, 1528.13/s)  LR: 6.862e-04  Data: 0.013 (0.015)
Train: 113 [1150/1251 ( 92%)]  Loss: 3.913 (3.60)  Time: 0.670s, 1527.84/s  (0.670s, 1528.29/s)  LR: 6.860e-04  Data: 0.015 (0.015)
Train: 113 [1200/1251 ( 96%)]  Loss: 3.839 (3.60)  Time: 0.674s, 1519.08/s  (0.670s, 1528.32/s)  LR: 6.858e-04  Data: 0.018 (0.015)
Train: 113 [1250/1251 (100%)]  Loss: 3.452 (3.60)  Time: 0.654s, 1566.70/s  (0.670s, 1528.56/s)  LR: 6.856e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.159 (3.159)  Loss:  0.5200 (0.5200)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.165 (0.321)  Loss:  0.5996 (1.0730)  Acc@1: 86.7924 (75.1120)  Acc@5: 97.5236 (92.9220)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-112.pth.tar', 75.34200005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-113.pth.tar', 75.11199995117188)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-110.pth.tar', 75.00599995849609)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-111.pth.tar', 74.90200013916015)

Train: 114 [   0/1251 (  0%)]  Loss: 3.933 (3.93)  Time: 4.086s,  250.63/s  (4.086s,  250.63/s)  LR: 6.856e-04  Data: 1.994 (1.994)
Train: 114 [  50/1251 (  4%)]  Loss: 3.668 (3.80)  Time: 0.652s, 1571.60/s  (0.697s, 1468.25/s)  LR: 6.854e-04  Data: 0.013 (0.052)
Train: 114 [ 100/1251 (  8%)]  Loss: 3.807 (3.80)  Time: 0.664s, 1541.43/s  (0.678s, 1509.48/s)  LR: 6.853e-04  Data: 0.013 (0.033)
Train: 114 [ 150/1251 ( 12%)]  Loss: 3.461 (3.72)  Time: 0.667s, 1536.28/s  (0.674s, 1519.34/s)  LR: 6.851e-04  Data: 0.013 (0.026)
Train: 114 [ 200/1251 ( 16%)]  Loss: 3.575 (3.69)  Time: 0.669s, 1531.11/s  (0.672s, 1524.57/s)  LR: 6.849e-04  Data: 0.014 (0.023)
Train: 114 [ 250/1251 ( 20%)]  Loss: 3.606 (3.67)  Time: 0.654s, 1564.62/s  (0.670s, 1527.50/s)  LR: 6.847e-04  Data: 0.014 (0.021)
Train: 114 [ 300/1251 ( 24%)]  Loss: 3.778 (3.69)  Time: 0.672s, 1524.36/s  (0.670s, 1528.64/s)  LR: 6.845e-04  Data: 0.014 (0.020)
Train: 114 [ 350/1251 ( 28%)]  Loss: 3.131 (3.62)  Time: 0.673s, 1522.02/s  (0.670s, 1529.27/s)  LR: 6.843e-04  Data: 0.014 (0.019)
Train: 114 [ 400/1251 ( 32%)]  Loss: 3.353 (3.59)  Time: 0.665s, 1539.96/s  (0.669s, 1529.80/s)  LR: 6.841e-04  Data: 0.015 (0.018)
Train: 114 [ 450/1251 ( 36%)]  Loss: 3.697 (3.60)  Time: 0.671s, 1526.98/s  (0.669s, 1530.11/s)  LR: 6.839e-04  Data: 0.013 (0.018)
Train: 114 [ 500/1251 ( 40%)]  Loss: 3.692 (3.61)  Time: 0.668s, 1533.28/s  (0.669s, 1530.57/s)  LR: 6.837e-04  Data: 0.012 (0.017)
Train: 114 [ 550/1251 ( 44%)]  Loss: 3.583 (3.61)  Time: 0.671s, 1526.02/s  (0.669s, 1530.52/s)  LR: 6.835e-04  Data: 0.014 (0.017)
Train: 114 [ 600/1251 ( 48%)]  Loss: 3.474 (3.60)  Time: 0.669s, 1531.37/s  (0.669s, 1530.94/s)  LR: 6.833e-04  Data: 0.014 (0.017)
Train: 114 [ 650/1251 ( 52%)]  Loss: 3.766 (3.61)  Time: 0.670s, 1528.37/s  (0.669s, 1531.54/s)  LR: 6.831e-04  Data: 0.016 (0.016)
Train: 114 [ 700/1251 ( 56%)]  Loss: 4.034 (3.64)  Time: 0.665s, 1539.83/s  (0.668s, 1531.82/s)  LR: 6.829e-04  Data: 0.013 (0.016)
Train: 114 [ 750/1251 ( 60%)]  Loss: 3.284 (3.62)  Time: 0.655s, 1564.38/s  (0.668s, 1532.27/s)  LR: 6.827e-04  Data: 0.015 (0.016)
Train: 114 [ 800/1251 ( 64%)]  Loss: 3.198 (3.59)  Time: 0.663s, 1545.63/s  (0.668s, 1532.43/s)  LR: 6.825e-04  Data: 0.013 (0.016)
Train: 114 [ 850/1251 ( 68%)]  Loss: 3.535 (3.59)  Time: 0.661s, 1548.45/s  (0.668s, 1532.49/s)  LR: 6.823e-04  Data: 0.014 (0.016)
Train: 114 [ 900/1251 ( 72%)]  Loss: 3.438 (3.58)  Time: 0.672s, 1523.62/s  (0.668s, 1532.38/s)  LR: 6.821e-04  Data: 0.013 (0.016)
Train: 114 [ 950/1251 ( 76%)]  Loss: 4.018 (3.60)  Time: 0.662s, 1546.53/s  (0.668s, 1532.14/s)  LR: 6.820e-04  Data: 0.014 (0.015)
Train: 114 [1000/1251 ( 80%)]  Loss: 3.498 (3.60)  Time: 0.663s, 1545.34/s  (0.668s, 1531.99/s)  LR: 6.818e-04  Data: 0.013 (0.015)
Train: 114 [1050/1251 ( 84%)]  Loss: 3.936 (3.61)  Time: 0.665s, 1539.61/s  (0.668s, 1531.96/s)  LR: 6.816e-04  Data: 0.013 (0.015)
Train: 114 [1100/1251 ( 88%)]  Loss: 3.616 (3.61)  Time: 0.664s, 1542.38/s  (0.668s, 1531.99/s)  LR: 6.814e-04  Data: 0.014 (0.015)
Train: 114 [1150/1251 ( 92%)]  Loss: 3.579 (3.61)  Time: 0.662s, 1546.81/s  (0.668s, 1532.10/s)  LR: 6.812e-04  Data: 0.013 (0.015)
Train: 114 [1200/1251 ( 96%)]  Loss: 3.489 (3.61)  Time: 0.669s, 1531.09/s  (0.668s, 1532.09/s)  LR: 6.810e-04  Data: 0.013 (0.015)
Train: 114 [1250/1251 (100%)]  Loss: 3.740 (3.61)  Time: 0.652s, 1569.98/s  (0.668s, 1532.14/s)  LR: 6.808e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.915 (2.915)  Loss:  0.4561 (0.4561)  Acc@1: 90.5273 (90.5273)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.173 (0.320)  Loss:  0.6250 (1.0413)  Acc@1: 84.3160 (74.9060)  Acc@5: 96.9340 (92.7320)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-112.pth.tar', 75.34200005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-113.pth.tar', 75.11199995117188)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-110.pth.tar', 75.00599995849609)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-114.pth.tar', 74.9060000390625)

Train: 115 [   0/1251 (  0%)]  Loss: 3.712 (3.71)  Time: 3.771s,  271.52/s  (3.771s,  271.52/s)  LR: 6.808e-04  Data: 1.695 (1.695)
Train: 115 [  50/1251 (  4%)]  Loss: 3.211 (3.46)  Time: 0.661s, 1548.09/s  (0.699s, 1464.71/s)  LR: 6.806e-04  Data: 0.014 (0.047)
Train: 115 [ 100/1251 (  8%)]  Loss: 3.524 (3.48)  Time: 0.680s, 1506.32/s  (0.683s, 1498.43/s)  LR: 6.804e-04  Data: 0.014 (0.031)
Train: 115 [ 150/1251 ( 12%)]  Loss: 3.294 (3.44)  Time: 0.668s, 1532.00/s  (0.679s, 1509.04/s)  LR: 6.802e-04  Data: 0.013 (0.025)
Train: 115 [ 200/1251 ( 16%)]  Loss: 3.410 (3.43)  Time: 0.671s, 1526.55/s  (0.677s, 1513.17/s)  LR: 6.800e-04  Data: 0.012 (0.022)
Train: 115 [ 250/1251 ( 20%)]  Loss: 3.587 (3.46)  Time: 0.673s, 1521.10/s  (0.676s, 1515.62/s)  LR: 6.798e-04  Data: 0.013 (0.020)
Train: 115 [ 300/1251 ( 24%)]  Loss: 3.518 (3.47)  Time: 0.677s, 1512.02/s  (0.675s, 1517.01/s)  LR: 6.796e-04  Data: 0.013 (0.019)
Train: 115 [ 350/1251 ( 28%)]  Loss: 3.575 (3.48)  Time: 0.665s, 1539.79/s  (0.674s, 1518.31/s)  LR: 6.794e-04  Data: 0.016 (0.018)
Train: 115 [ 400/1251 ( 32%)]  Loss: 3.670 (3.50)  Time: 0.669s, 1529.98/s  (0.674s, 1518.84/s)  LR: 6.792e-04  Data: 0.013 (0.018)
Train: 115 [ 450/1251 ( 36%)]  Loss: 3.709 (3.52)  Time: 0.675s, 1517.98/s  (0.674s, 1518.73/s)  LR: 6.790e-04  Data: 0.012 (0.017)
Train: 115 [ 500/1251 ( 40%)]  Loss: 3.704 (3.54)  Time: 0.685s, 1495.75/s  (0.674s, 1519.00/s)  LR: 6.788e-04  Data: 0.013 (0.017)
Train: 115 [ 550/1251 ( 44%)]  Loss: 3.157 (3.51)  Time: 0.682s, 1501.79/s  (0.674s, 1519.89/s)  LR: 6.786e-04  Data: 0.013 (0.017)
Train: 115 [ 600/1251 ( 48%)]  Loss: 3.501 (3.51)  Time: 0.682s, 1502.43/s  (0.674s, 1520.07/s)  LR: 6.784e-04  Data: 0.013 (0.016)
Train: 115 [ 650/1251 ( 52%)]  Loss: 3.874 (3.53)  Time: 0.669s, 1529.96/s  (0.674s, 1520.30/s)  LR: 6.783e-04  Data: 0.014 (0.016)
Train: 115 [ 700/1251 ( 56%)]  Loss: 3.566 (3.53)  Time: 0.669s, 1530.63/s  (0.673s, 1520.81/s)  LR: 6.781e-04  Data: 0.012 (0.016)
Train: 115 [ 750/1251 ( 60%)]  Loss: 3.297 (3.52)  Time: 0.666s, 1536.87/s  (0.673s, 1521.16/s)  LR: 6.779e-04  Data: 0.015 (0.016)
Train: 115 [ 800/1251 ( 64%)]  Loss: 3.685 (3.53)  Time: 0.672s, 1524.40/s  (0.673s, 1521.73/s)  LR: 6.777e-04  Data: 0.013 (0.016)
Train: 115 [ 850/1251 ( 68%)]  Loss: 3.360 (3.52)  Time: 0.668s, 1533.73/s  (0.673s, 1522.02/s)  LR: 6.775e-04  Data: 0.014 (0.016)
Train: 115 [ 900/1251 ( 72%)]  Loss: 3.488 (3.52)  Time: 0.663s, 1544.24/s  (0.673s, 1522.39/s)  LR: 6.773e-04  Data: 0.012 (0.015)
Train: 115 [ 950/1251 ( 76%)]  Loss: 3.405 (3.51)  Time: 0.672s, 1523.46/s  (0.673s, 1522.44/s)  LR: 6.771e-04  Data: 0.013 (0.015)
Train: 115 [1000/1251 ( 80%)]  Loss: 3.309 (3.50)  Time: 0.670s, 1528.81/s  (0.673s, 1522.48/s)  LR: 6.769e-04  Data: 0.016 (0.015)
Train: 115 [1050/1251 ( 84%)]  Loss: 3.715 (3.51)  Time: 0.673s, 1522.24/s  (0.673s, 1522.62/s)  LR: 6.767e-04  Data: 0.012 (0.015)
Train: 115 [1100/1251 ( 88%)]  Loss: 3.544 (3.51)  Time: 0.673s, 1521.30/s  (0.672s, 1522.71/s)  LR: 6.765e-04  Data: 0.014 (0.015)
Train: 115 [1150/1251 ( 92%)]  Loss: 3.667 (3.52)  Time: 0.678s, 1510.36/s  (0.672s, 1522.89/s)  LR: 6.763e-04  Data: 0.013 (0.015)
Train: 115 [1200/1251 ( 96%)]  Loss: 3.855 (3.53)  Time: 0.658s, 1556.49/s  (0.672s, 1522.92/s)  LR: 6.761e-04  Data: 0.015 (0.015)
Train: 115 [1250/1251 (100%)]  Loss: 3.727 (3.54)  Time: 0.670s, 1529.26/s  (0.672s, 1522.99/s)  LR: 6.759e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.788 (2.788)  Loss:  0.4907 (0.4907)  Acc@1: 89.6484 (89.6484)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.163 (0.317)  Loss:  0.6177 (1.0580)  Acc@1: 84.3160 (75.1100)  Acc@5: 97.4057 (92.8380)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-112.pth.tar', 75.34200005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-113.pth.tar', 75.11199995117188)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-115.pth.tar', 75.1100000390625)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-110.pth.tar', 75.00599995849609)

Train: 116 [   0/1251 (  0%)]  Loss: 3.592 (3.59)  Time: 4.119s,  248.58/s  (4.119s,  248.58/s)  LR: 6.759e-04  Data: 1.987 (1.987)
Train: 116 [  50/1251 (  4%)]  Loss: 3.350 (3.47)  Time: 0.662s, 1547.46/s  (0.710s, 1442.90/s)  LR: 6.757e-04  Data: 0.013 (0.052)
Train: 116 [ 100/1251 (  8%)]  Loss: 3.506 (3.48)  Time: 0.668s, 1532.27/s  (0.689s, 1485.78/s)  LR: 6.755e-04  Data: 0.013 (0.033)
Train: 116 [ 150/1251 ( 12%)]  Loss: 3.592 (3.51)  Time: 0.660s, 1551.45/s  (0.683s, 1498.26/s)  LR: 6.753e-04  Data: 0.013 (0.027)
Train: 116 [ 200/1251 ( 16%)]  Loss: 3.513 (3.51)  Time: 0.681s, 1503.71/s  (0.681s, 1504.33/s)  LR: 6.751e-04  Data: 0.016 (0.023)
Train: 116 [ 250/1251 ( 20%)]  Loss: 3.623 (3.53)  Time: 0.671s, 1525.21/s  (0.679s, 1508.39/s)  LR: 6.749e-04  Data: 0.016 (0.021)
Train: 116 [ 300/1251 ( 24%)]  Loss: 3.506 (3.53)  Time: 0.675s, 1516.10/s  (0.677s, 1511.62/s)  LR: 6.747e-04  Data: 0.014 (0.020)
Train: 116 [ 350/1251 ( 28%)]  Loss: 3.710 (3.55)  Time: 0.671s, 1526.06/s  (0.677s, 1512.43/s)  LR: 6.745e-04  Data: 0.012 (0.019)
Train: 116 [ 400/1251 ( 32%)]  Loss: 3.266 (3.52)  Time: 0.670s, 1528.22/s  (0.676s, 1514.11/s)  LR: 6.743e-04  Data: 0.018 (0.019)
Train: 116 [ 450/1251 ( 36%)]  Loss: 3.503 (3.52)  Time: 0.669s, 1529.96/s  (0.676s, 1514.97/s)  LR: 6.742e-04  Data: 0.013 (0.018)
Train: 116 [ 500/1251 ( 40%)]  Loss: 3.672 (3.53)  Time: 0.669s, 1529.80/s  (0.675s, 1515.99/s)  LR: 6.740e-04  Data: 0.013 (0.018)
Train: 116 [ 550/1251 ( 44%)]  Loss: 3.702 (3.54)  Time: 0.683s, 1498.50/s  (0.675s, 1516.89/s)  LR: 6.738e-04  Data: 0.015 (0.017)
Train: 116 [ 600/1251 ( 48%)]  Loss: 3.545 (3.54)  Time: 0.670s, 1528.74/s  (0.675s, 1517.27/s)  LR: 6.736e-04  Data: 0.017 (0.017)
Train: 116 [ 650/1251 ( 52%)]  Loss: 3.530 (3.54)  Time: 0.673s, 1522.17/s  (0.675s, 1517.80/s)  LR: 6.734e-04  Data: 0.011 (0.017)
Train: 116 [ 700/1251 ( 56%)]  Loss: 3.464 (3.54)  Time: 0.671s, 1525.90/s  (0.674s, 1518.22/s)  LR: 6.732e-04  Data: 0.013 (0.017)
Train: 116 [ 750/1251 ( 60%)]  Loss: 3.676 (3.55)  Time: 0.672s, 1523.12/s  (0.674s, 1518.74/s)  LR: 6.730e-04  Data: 0.017 (0.016)
Train: 116 [ 800/1251 ( 64%)]  Loss: 3.511 (3.54)  Time: 0.672s, 1523.84/s  (0.674s, 1519.14/s)  LR: 6.728e-04  Data: 0.015 (0.016)
Train: 116 [ 850/1251 ( 68%)]  Loss: 3.590 (3.55)  Time: 0.670s, 1529.20/s  (0.674s, 1519.51/s)  LR: 6.726e-04  Data: 0.013 (0.016)
Train: 116 [ 900/1251 ( 72%)]  Loss: 3.739 (3.56)  Time: 0.677s, 1513.52/s  (0.674s, 1519.74/s)  LR: 6.724e-04  Data: 0.014 (0.016)
Train: 116 [ 950/1251 ( 76%)]  Loss: 3.735 (3.57)  Time: 0.666s, 1536.64/s  (0.674s, 1520.13/s)  LR: 6.722e-04  Data: 0.017 (0.016)
Train: 116 [1000/1251 ( 80%)]  Loss: 3.842 (3.58)  Time: 0.671s, 1526.94/s  (0.673s, 1520.59/s)  LR: 6.720e-04  Data: 0.014 (0.016)
Train: 116 [1050/1251 ( 84%)]  Loss: 3.813 (3.59)  Time: 0.672s, 1523.84/s  (0.673s, 1520.96/s)  LR: 6.718e-04  Data: 0.012 (0.016)
Train: 116 [1100/1251 ( 88%)]  Loss: 3.423 (3.58)  Time: 0.663s, 1545.06/s  (0.673s, 1521.22/s)  LR: 6.716e-04  Data: 0.013 (0.016)
Train: 116 [1150/1251 ( 92%)]  Loss: 3.657 (3.59)  Time: 0.668s, 1532.09/s  (0.673s, 1521.44/s)  LR: 6.714e-04  Data: 0.013 (0.015)
Train: 116 [1200/1251 ( 96%)]  Loss: 3.698 (3.59)  Time: 0.679s, 1507.26/s  (0.673s, 1521.64/s)  LR: 6.712e-04  Data: 0.013 (0.015)
Train: 116 [1250/1251 (100%)]  Loss: 3.578 (3.59)  Time: 0.658s, 1555.42/s  (0.673s, 1521.95/s)  LR: 6.710e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.170 (3.170)  Loss:  0.5020 (0.5020)  Acc@1: 89.3555 (89.3555)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.168 (0.319)  Loss:  0.6455 (1.0472)  Acc@1: 85.4953 (75.4120)  Acc@5: 95.9906 (92.9320)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-116.pth.tar', 75.41200016357422)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-112.pth.tar', 75.34200005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-113.pth.tar', 75.11199995117188)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-115.pth.tar', 75.1100000390625)

Train: 117 [   0/1251 (  0%)]  Loss: 3.113 (3.11)  Time: 3.993s,  256.43/s  (3.993s,  256.43/s)  LR: 6.710e-04  Data: 1.622 (1.622)
Train: 117 [  50/1251 (  4%)]  Loss: 3.789 (3.45)  Time: 0.666s, 1537.44/s  (0.699s, 1464.94/s)  LR: 6.708e-04  Data: 0.016 (0.045)
Train: 117 [ 100/1251 (  8%)]  Loss: 3.766 (3.56)  Time: 0.671s, 1526.34/s  (0.682s, 1501.90/s)  LR: 6.706e-04  Data: 0.016 (0.030)
Train: 117 [ 150/1251 ( 12%)]  Loss: 2.969 (3.41)  Time: 0.664s, 1541.15/s  (0.676s, 1514.08/s)  LR: 6.704e-04  Data: 0.015 (0.025)
Train: 117 [ 200/1251 ( 16%)]  Loss: 3.646 (3.46)  Time: 0.664s, 1541.92/s  (0.674s, 1518.76/s)  LR: 6.702e-04  Data: 0.014 (0.022)
Train: 117 [ 250/1251 ( 20%)]  Loss: 3.836 (3.52)  Time: 0.670s, 1528.79/s  (0.673s, 1520.88/s)  LR: 6.700e-04  Data: 0.013 (0.020)
Train: 117 [ 300/1251 ( 24%)]  Loss: 3.886 (3.57)  Time: 0.664s, 1541.88/s  (0.672s, 1522.96/s)  LR: 6.698e-04  Data: 0.014 (0.019)
Train: 117 [ 350/1251 ( 28%)]  Loss: 3.428 (3.55)  Time: 0.660s, 1552.25/s  (0.671s, 1525.13/s)  LR: 6.696e-04  Data: 0.012 (0.018)
Train: 117 [ 400/1251 ( 32%)]  Loss: 3.516 (3.55)  Time: 0.670s, 1527.41/s  (0.671s, 1526.74/s)  LR: 6.694e-04  Data: 0.014 (0.018)
Train: 117 [ 450/1251 ( 36%)]  Loss: 3.713 (3.57)  Time: 0.674s, 1518.43/s  (0.670s, 1527.64/s)  LR: 6.693e-04  Data: 0.013 (0.017)
Train: 117 [ 500/1251 ( 40%)]  Loss: 3.826 (3.59)  Time: 0.670s, 1529.00/s  (0.670s, 1528.43/s)  LR: 6.691e-04  Data: 0.013 (0.017)
Train: 117 [ 550/1251 ( 44%)]  Loss: 3.545 (3.59)  Time: 0.671s, 1526.69/s  (0.670s, 1529.01/s)  LR: 6.689e-04  Data: 0.012 (0.016)
Train: 117 [ 600/1251 ( 48%)]  Loss: 3.728 (3.60)  Time: 0.661s, 1549.54/s  (0.669s, 1529.60/s)  LR: 6.687e-04  Data: 0.018 (0.016)
Train: 117 [ 650/1251 ( 52%)]  Loss: 4.001 (3.63)  Time: 0.660s, 1551.46/s  (0.669s, 1529.77/s)  LR: 6.685e-04  Data: 0.016 (0.016)
Train: 117 [ 700/1251 ( 56%)]  Loss: 3.511 (3.62)  Time: 0.667s, 1535.66/s  (0.669s, 1530.08/s)  LR: 6.683e-04  Data: 0.012 (0.016)
Train: 117 [ 750/1251 ( 60%)]  Loss: 3.623 (3.62)  Time: 0.672s, 1523.86/s  (0.669s, 1530.34/s)  LR: 6.681e-04  Data: 0.014 (0.016)
Train: 117 [ 800/1251 ( 64%)]  Loss: 3.620 (3.62)  Time: 0.667s, 1534.66/s  (0.669s, 1530.56/s)  LR: 6.679e-04  Data: 0.015 (0.016)
Train: 117 [ 850/1251 ( 68%)]  Loss: 3.315 (3.60)  Time: 0.669s, 1531.04/s  (0.669s, 1530.91/s)  LR: 6.677e-04  Data: 0.013 (0.016)
Train: 117 [ 900/1251 ( 72%)]  Loss: 3.422 (3.59)  Time: 0.674s, 1519.47/s  (0.669s, 1530.84/s)  LR: 6.675e-04  Data: 0.013 (0.015)
Train: 117 [ 950/1251 ( 76%)]  Loss: 3.480 (3.59)  Time: 0.672s, 1524.56/s  (0.669s, 1530.73/s)  LR: 6.673e-04  Data: 0.016 (0.015)
Train: 117 [1000/1251 ( 80%)]  Loss: 3.693 (3.59)  Time: 0.671s, 1527.02/s  (0.669s, 1530.39/s)  LR: 6.671e-04  Data: 0.014 (0.015)
Train: 117 [1050/1251 ( 84%)]  Loss: 3.778 (3.60)  Time: 0.679s, 1507.29/s  (0.669s, 1530.31/s)  LR: 6.669e-04  Data: 0.013 (0.015)
Train: 117 [1100/1251 ( 88%)]  Loss: 3.242 (3.58)  Time: 0.672s, 1524.34/s  (0.669s, 1530.00/s)  LR: 6.667e-04  Data: 0.013 (0.015)
Train: 117 [1150/1251 ( 92%)]  Loss: 3.685 (3.59)  Time: 0.668s, 1531.88/s  (0.669s, 1529.65/s)  LR: 6.665e-04  Data: 0.014 (0.015)
Train: 117 [1200/1251 ( 96%)]  Loss: 3.405 (3.58)  Time: 0.677s, 1513.21/s  (0.670s, 1529.46/s)  LR: 6.663e-04  Data: 0.014 (0.015)
Train: 117 [1250/1251 (100%)]  Loss: 3.715 (3.59)  Time: 0.660s, 1552.10/s  (0.670s, 1529.41/s)  LR: 6.661e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.784 (2.784)  Loss:  0.4641 (0.4641)  Acc@1: 89.6484 (89.6484)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.162 (0.319)  Loss:  0.5742 (1.0511)  Acc@1: 85.1415 (75.1560)  Acc@5: 97.2877 (92.8520)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-116.pth.tar', 75.41200016357422)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-112.pth.tar', 75.34200005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-117.pth.tar', 75.1559998803711)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-113.pth.tar', 75.11199995117188)

Train: 118 [   0/1251 (  0%)]  Loss: 3.791 (3.79)  Time: 3.587s,  285.44/s  (3.587s,  285.44/s)  LR: 6.661e-04  Data: 1.774 (1.774)
Train: 118 [  50/1251 (  4%)]  Loss: 3.962 (3.88)  Time: 0.665s, 1540.69/s  (0.698s, 1466.48/s)  LR: 6.659e-04  Data: 0.014 (0.049)
Train: 118 [ 100/1251 (  8%)]  Loss: 3.570 (3.77)  Time: 0.659s, 1554.04/s  (0.679s, 1507.06/s)  LR: 6.657e-04  Data: 0.012 (0.031)
Train: 118 [ 150/1251 ( 12%)]  Loss: 3.565 (3.72)  Time: 0.666s, 1538.55/s  (0.675s, 1517.43/s)  LR: 6.655e-04  Data: 0.015 (0.026)
Train: 118 [ 200/1251 ( 16%)]  Loss: 3.538 (3.69)  Time: 0.664s, 1543.14/s  (0.673s, 1521.33/s)  LR: 6.653e-04  Data: 0.012 (0.023)
Train: 118 [ 250/1251 ( 20%)]  Loss: 3.630 (3.68)  Time: 0.670s, 1527.90/s  (0.672s, 1522.72/s)  LR: 6.651e-04  Data: 0.013 (0.021)
Train: 118 [ 300/1251 ( 24%)]  Loss: 3.309 (3.62)  Time: 0.681s, 1503.51/s  (0.672s, 1524.30/s)  LR: 6.649e-04  Data: 0.014 (0.020)
Train: 118 [ 350/1251 ( 28%)]  Loss: 3.600 (3.62)  Time: 0.663s, 1543.43/s  (0.671s, 1525.51/s)  LR: 6.647e-04  Data: 0.012 (0.019)
Train: 118 [ 400/1251 ( 32%)]  Loss: 3.839 (3.64)  Time: 0.662s, 1546.71/s  (0.671s, 1526.58/s)  LR: 6.645e-04  Data: 0.015 (0.018)
Train: 118 [ 450/1251 ( 36%)]  Loss: 3.542 (3.63)  Time: 0.671s, 1526.70/s  (0.671s, 1527.19/s)  LR: 6.643e-04  Data: 0.013 (0.018)
Train: 118 [ 500/1251 ( 40%)]  Loss: 3.713 (3.64)  Time: 0.671s, 1525.16/s  (0.670s, 1527.77/s)  LR: 6.641e-04  Data: 0.016 (0.018)
Train: 118 [ 550/1251 ( 44%)]  Loss: 3.467 (3.63)  Time: 0.674s, 1520.39/s  (0.670s, 1527.68/s)  LR: 6.639e-04  Data: 0.015 (0.017)
Train: 118 [ 600/1251 ( 48%)]  Loss: 3.492 (3.62)  Time: 0.670s, 1527.44/s  (0.670s, 1527.82/s)  LR: 6.637e-04  Data: 0.013 (0.017)
Train: 118 [ 650/1251 ( 52%)]  Loss: 3.672 (3.62)  Time: 0.671s, 1526.00/s  (0.670s, 1528.05/s)  LR: 6.635e-04  Data: 0.013 (0.017)
Train: 118 [ 700/1251 ( 56%)]  Loss: 3.476 (3.61)  Time: 0.660s, 1550.77/s  (0.670s, 1527.82/s)  LR: 6.634e-04  Data: 0.013 (0.017)
Train: 118 [ 750/1251 ( 60%)]  Loss: 3.748 (3.62)  Time: 0.677s, 1512.43/s  (0.670s, 1527.83/s)  LR: 6.632e-04  Data: 0.012 (0.016)
Train: 118 [ 800/1251 ( 64%)]  Loss: 3.503 (3.61)  Time: 0.671s, 1526.39/s  (0.670s, 1527.82/s)  LR: 6.630e-04  Data: 0.014 (0.016)
Train: 118 [ 850/1251 ( 68%)]  Loss: 3.749 (3.62)  Time: 0.669s, 1529.93/s  (0.670s, 1527.88/s)  LR: 6.628e-04  Data: 0.013 (0.016)
Train: 118 [ 900/1251 ( 72%)]  Loss: 3.806 (3.63)  Time: 0.675s, 1517.74/s  (0.670s, 1528.03/s)  LR: 6.626e-04  Data: 0.012 (0.016)
Train: 118 [ 950/1251 ( 76%)]  Loss: 3.629 (3.63)  Time: 0.672s, 1524.56/s  (0.670s, 1528.09/s)  LR: 6.624e-04  Data: 0.014 (0.016)
Train: 118 [1000/1251 ( 80%)]  Loss: 3.549 (3.63)  Time: 0.671s, 1526.55/s  (0.670s, 1528.02/s)  LR: 6.622e-04  Data: 0.012 (0.016)
Train: 118 [1050/1251 ( 84%)]  Loss: 3.736 (3.63)  Time: 0.672s, 1522.99/s  (0.670s, 1527.77/s)  LR: 6.620e-04  Data: 0.013 (0.016)
Train: 118 [1100/1251 ( 88%)]  Loss: 3.297 (3.62)  Time: 0.680s, 1505.82/s  (0.670s, 1527.61/s)  LR: 6.618e-04  Data: 0.013 (0.015)
Train: 118 [1150/1251 ( 92%)]  Loss: 3.785 (3.62)  Time: 0.670s, 1528.99/s  (0.670s, 1527.30/s)  LR: 6.616e-04  Data: 0.015 (0.015)
Train: 118 [1200/1251 ( 96%)]  Loss: 3.781 (3.63)  Time: 0.670s, 1529.06/s  (0.671s, 1527.11/s)  LR: 6.614e-04  Data: 0.013 (0.015)
Train: 118 [1250/1251 (100%)]  Loss: 3.723 (3.63)  Time: 0.655s, 1563.40/s  (0.671s, 1527.18/s)  LR: 6.612e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.809 (2.809)  Loss:  0.5620 (0.5620)  Acc@1: 89.0625 (89.0625)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.168 (0.321)  Loss:  0.5820 (1.0454)  Acc@1: 86.0849 (75.3580)  Acc@5: 97.4057 (93.1620)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-116.pth.tar', 75.41200016357422)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-118.pth.tar', 75.35800003173829)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-112.pth.tar', 75.34200005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-117.pth.tar', 75.1559998803711)

Train: 119 [   0/1251 (  0%)]  Loss: 3.773 (3.77)  Time: 3.884s,  263.68/s  (3.884s,  263.68/s)  LR: 6.612e-04  Data: 1.532 (1.532)
Train: 119 [  50/1251 (  4%)]  Loss: 3.873 (3.82)  Time: 0.656s, 1560.27/s  (0.696s, 1470.84/s)  LR: 6.610e-04  Data: 0.015 (0.044)
Train: 119 [ 100/1251 (  8%)]  Loss: 3.553 (3.73)  Time: 0.673s, 1521.95/s  (0.681s, 1502.76/s)  LR: 6.608e-04  Data: 0.013 (0.029)
Train: 119 [ 150/1251 ( 12%)]  Loss: 3.610 (3.70)  Time: 0.667s, 1536.37/s  (0.678s, 1511.28/s)  LR: 6.606e-04  Data: 0.013 (0.024)
Train: 119 [ 200/1251 ( 16%)]  Loss: 3.973 (3.76)  Time: 0.670s, 1527.70/s  (0.676s, 1515.84/s)  LR: 6.604e-04  Data: 0.013 (0.021)
Train: 119 [ 250/1251 ( 20%)]  Loss: 3.704 (3.75)  Time: 0.671s, 1525.53/s  (0.675s, 1517.57/s)  LR: 6.602e-04  Data: 0.015 (0.020)
Train: 119 [ 300/1251 ( 24%)]  Loss: 3.480 (3.71)  Time: 0.674s, 1518.24/s  (0.675s, 1518.11/s)  LR: 6.600e-04  Data: 0.013 (0.019)
Train: 119 [ 350/1251 ( 28%)]  Loss: 3.897 (3.73)  Time: 0.673s, 1520.61/s  (0.674s, 1518.50/s)  LR: 6.598e-04  Data: 0.012 (0.018)
Train: 119 [ 400/1251 ( 32%)]  Loss: 3.546 (3.71)  Time: 0.665s, 1539.82/s  (0.674s, 1519.49/s)  LR: 6.596e-04  Data: 0.014 (0.018)
Train: 119 [ 450/1251 ( 36%)]  Loss: 3.590 (3.70)  Time: 0.662s, 1546.10/s  (0.674s, 1519.70/s)  LR: 6.594e-04  Data: 0.013 (0.017)
Train: 119 [ 500/1251 ( 40%)]  Loss: 3.466 (3.68)  Time: 0.679s, 1508.94/s  (0.674s, 1519.73/s)  LR: 6.592e-04  Data: 0.013 (0.017)
Train: 119 [ 550/1251 ( 44%)]  Loss: 3.665 (3.68)  Time: 0.673s, 1521.44/s  (0.674s, 1519.81/s)  LR: 6.590e-04  Data: 0.016 (0.016)
Train: 119 [ 600/1251 ( 48%)]  Loss: 3.467 (3.66)  Time: 0.665s, 1540.04/s  (0.674s, 1520.03/s)  LR: 6.588e-04  Data: 0.013 (0.016)
Train: 119 [ 650/1251 ( 52%)]  Loss: 3.409 (3.64)  Time: 0.678s, 1509.37/s  (0.674s, 1519.96/s)  LR: 6.586e-04  Data: 0.013 (0.016)
Train: 119 [ 700/1251 ( 56%)]  Loss: 3.873 (3.66)  Time: 0.668s, 1532.09/s  (0.674s, 1519.99/s)  LR: 6.584e-04  Data: 0.013 (0.016)
Train: 119 [ 750/1251 ( 60%)]  Loss: 3.821 (3.67)  Time: 0.678s, 1511.36/s  (0.674s, 1520.27/s)  LR: 6.582e-04  Data: 0.014 (0.016)
Train: 119 [ 800/1251 ( 64%)]  Loss: 3.829 (3.68)  Time: 0.664s, 1542.75/s  (0.673s, 1520.62/s)  LR: 6.580e-04  Data: 0.012 (0.016)
Train: 119 [ 850/1251 ( 68%)]  Loss: 3.845 (3.69)  Time: 0.665s, 1539.06/s  (0.673s, 1520.84/s)  LR: 6.578e-04  Data: 0.015 (0.015)
Train: 119 [ 900/1251 ( 72%)]  Loss: 3.652 (3.69)  Time: 0.673s, 1521.52/s  (0.673s, 1520.95/s)  LR: 6.576e-04  Data: 0.013 (0.015)
Train: 119 [ 950/1251 ( 76%)]  Loss: 3.831 (3.69)  Time: 0.681s, 1503.08/s  (0.673s, 1521.27/s)  LR: 6.574e-04  Data: 0.012 (0.015)
Train: 119 [1000/1251 ( 80%)]  Loss: 3.735 (3.69)  Time: 0.682s, 1501.08/s  (0.673s, 1521.41/s)  LR: 6.572e-04  Data: 0.015 (0.015)
Train: 119 [1050/1251 ( 84%)]  Loss: 3.570 (3.69)  Time: 0.671s, 1525.96/s  (0.673s, 1521.54/s)  LR: 6.570e-04  Data: 0.013 (0.015)
Train: 119 [1100/1251 ( 88%)]  Loss: 3.655 (3.69)  Time: 0.673s, 1521.82/s  (0.673s, 1521.70/s)  LR: 6.568e-04  Data: 0.013 (0.015)
Train: 119 [1150/1251 ( 92%)]  Loss: 3.422 (3.68)  Time: 0.676s, 1515.13/s  (0.673s, 1521.91/s)  LR: 6.566e-04  Data: 0.015 (0.015)
Train: 119 [1200/1251 ( 96%)]  Loss: 3.509 (3.67)  Time: 0.674s, 1518.85/s  (0.673s, 1521.93/s)  LR: 6.564e-04  Data: 0.016 (0.015)
Train: 119 [1250/1251 (100%)]  Loss: 3.691 (3.67)  Time: 0.661s, 1549.49/s  (0.673s, 1522.04/s)  LR: 6.562e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.980 (2.980)  Loss:  0.4819 (0.4819)  Acc@1: 90.5273 (90.5273)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.167 (0.326)  Loss:  0.6372 (1.0571)  Acc@1: 84.0802 (75.5000)  Acc@5: 97.0519 (92.9660)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-119.pth.tar', 75.50000006591797)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-116.pth.tar', 75.41200016357422)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-118.pth.tar', 75.35800003173829)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-112.pth.tar', 75.34200005615234)

Train: 120 [   0/1251 (  0%)]  Loss: 3.430 (3.43)  Time: 4.187s,  244.54/s  (4.187s,  244.54/s)  LR: 6.562e-04  Data: 1.615 (1.615)
Train: 120 [  50/1251 (  4%)]  Loss: 3.408 (3.42)  Time: 0.674s, 1519.82/s  (0.701s, 1460.02/s)  LR: 6.560e-04  Data: 0.013 (0.045)
Train: 120 [ 100/1251 (  8%)]  Loss: 3.508 (3.45)  Time: 0.663s, 1544.65/s  (0.681s, 1502.74/s)  LR: 6.558e-04  Data: 0.013 (0.030)
Train: 120 [ 150/1251 ( 12%)]  Loss: 3.822 (3.54)  Time: 0.671s, 1525.21/s  (0.676s, 1515.05/s)  LR: 6.556e-04  Data: 0.013 (0.025)
Train: 120 [ 200/1251 ( 16%)]  Loss: 3.508 (3.54)  Time: 0.676s, 1515.15/s  (0.674s, 1518.69/s)  LR: 6.554e-04  Data: 0.015 (0.022)
Train: 120 [ 250/1251 ( 20%)]  Loss: 3.522 (3.53)  Time: 0.670s, 1527.37/s  (0.673s, 1520.72/s)  LR: 6.552e-04  Data: 0.013 (0.020)
Train: 120 [ 300/1251 ( 24%)]  Loss: 3.622 (3.55)  Time: 0.670s, 1527.66/s  (0.673s, 1522.13/s)  LR: 6.550e-04  Data: 0.015 (0.019)
Train: 120 [ 350/1251 ( 28%)]  Loss: 3.427 (3.53)  Time: 0.661s, 1549.53/s  (0.672s, 1522.91/s)  LR: 6.548e-04  Data: 0.013 (0.018)
Train: 120 [ 400/1251 ( 32%)]  Loss: 3.650 (3.54)  Time: 0.672s, 1524.86/s  (0.672s, 1523.55/s)  LR: 6.546e-04  Data: 0.013 (0.018)
Train: 120 [ 450/1251 ( 36%)]  Loss: 3.616 (3.55)  Time: 0.671s, 1526.18/s  (0.672s, 1523.83/s)  LR: 6.544e-04  Data: 0.013 (0.017)
Train: 120 [ 500/1251 ( 40%)]  Loss: 3.613 (3.56)  Time: 0.667s, 1534.54/s  (0.672s, 1524.47/s)  LR: 6.543e-04  Data: 0.013 (0.017)
Train: 120 [ 550/1251 ( 44%)]  Loss: 3.533 (3.55)  Time: 0.678s, 1510.33/s  (0.672s, 1524.51/s)  LR: 6.541e-04  Data: 0.013 (0.017)
Train: 120 [ 600/1251 ( 48%)]  Loss: 3.935 (3.58)  Time: 0.668s, 1533.14/s  (0.672s, 1524.58/s)  LR: 6.539e-04  Data: 0.013 (0.016)
Train: 120 [ 650/1251 ( 52%)]  Loss: 3.363 (3.57)  Time: 0.669s, 1530.02/s  (0.672s, 1524.60/s)  LR: 6.537e-04  Data: 0.017 (0.016)
Train: 120 [ 700/1251 ( 56%)]  Loss: 3.506 (3.56)  Time: 0.666s, 1537.00/s  (0.672s, 1524.65/s)  LR: 6.535e-04  Data: 0.013 (0.016)
Train: 120 [ 750/1251 ( 60%)]  Loss: 3.652 (3.57)  Time: 0.674s, 1520.25/s  (0.672s, 1524.48/s)  LR: 6.533e-04  Data: 0.012 (0.016)
Train: 120 [ 800/1251 ( 64%)]  Loss: 3.449 (3.56)  Time: 0.676s, 1513.74/s  (0.672s, 1524.54/s)  LR: 6.531e-04  Data: 0.013 (0.016)
Train: 120 [ 850/1251 ( 68%)]  Loss: 3.190 (3.54)  Time: 0.661s, 1549.29/s  (0.672s, 1524.33/s)  LR: 6.529e-04  Data: 0.013 (0.016)
Train: 120 [ 900/1251 ( 72%)]  Loss: 3.814 (3.56)  Time: 0.671s, 1526.90/s  (0.672s, 1524.08/s)  LR: 6.527e-04  Data: 0.013 (0.015)
Train: 120 [ 950/1251 ( 76%)]  Loss: 3.662 (3.56)  Time: 0.671s, 1527.21/s  (0.672s, 1524.00/s)  LR: 6.525e-04  Data: 0.012 (0.015)
Train: 120 [1000/1251 ( 80%)]  Loss: 3.285 (3.55)  Time: 0.672s, 1524.27/s  (0.672s, 1523.96/s)  LR: 6.523e-04  Data: 0.016 (0.015)
Train: 120 [1050/1251 ( 84%)]  Loss: 3.698 (3.56)  Time: 0.681s, 1504.15/s  (0.672s, 1523.83/s)  LR: 6.521e-04  Data: 0.013 (0.015)
Train: 120 [1100/1251 ( 88%)]  Loss: 3.680 (3.56)  Time: 0.660s, 1550.82/s  (0.672s, 1523.84/s)  LR: 6.519e-04  Data: 0.013 (0.015)
Train: 120 [1150/1251 ( 92%)]  Loss: 3.520 (3.56)  Time: 0.662s, 1547.55/s  (0.672s, 1523.79/s)  LR: 6.517e-04  Data: 0.014 (0.015)
Train: 120 [1200/1251 ( 96%)]  Loss: 3.788 (3.57)  Time: 0.679s, 1507.35/s  (0.672s, 1523.77/s)  LR: 6.515e-04  Data: 0.017 (0.015)
Train: 120 [1250/1251 (100%)]  Loss: 3.565 (3.57)  Time: 0.665s, 1540.32/s  (0.672s, 1523.81/s)  LR: 6.513e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.860 (2.860)  Loss:  0.4968 (0.4968)  Acc@1: 89.9414 (89.9414)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.166 (0.321)  Loss:  0.6416 (1.0469)  Acc@1: 84.9057 (75.4900)  Acc@5: 97.2877 (93.0580)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-119.pth.tar', 75.50000006591797)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-120.pth.tar', 75.4900000366211)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-116.pth.tar', 75.41200016357422)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-118.pth.tar', 75.35800003173829)

Train: 121 [   0/1251 (  0%)]  Loss: 3.611 (3.61)  Time: 3.919s,  261.27/s  (3.919s,  261.27/s)  LR: 6.513e-04  Data: 1.687 (1.687)
Train: 121 [  50/1251 (  4%)]  Loss: 3.446 (3.53)  Time: 0.658s, 1555.74/s  (0.697s, 1469.23/s)  LR: 6.511e-04  Data: 0.017 (0.046)
Train: 121 [ 100/1251 (  8%)]  Loss: 3.661 (3.57)  Time: 0.676s, 1515.07/s  (0.681s, 1503.57/s)  LR: 6.509e-04  Data: 0.013 (0.030)
Train: 121 [ 150/1251 ( 12%)]  Loss: 3.666 (3.60)  Time: 0.667s, 1534.90/s  (0.677s, 1513.16/s)  LR: 6.507e-04  Data: 0.013 (0.024)
Train: 121 [ 200/1251 ( 16%)]  Loss: 3.809 (3.64)  Time: 0.675s, 1516.90/s  (0.675s, 1516.27/s)  LR: 6.505e-04  Data: 0.014 (0.022)
Train: 121 [ 250/1251 ( 20%)]  Loss: 3.876 (3.68)  Time: 0.674s, 1518.16/s  (0.674s, 1518.46/s)  LR: 6.503e-04  Data: 0.013 (0.020)
Train: 121 [ 300/1251 ( 24%)]  Loss: 3.490 (3.65)  Time: 0.681s, 1504.20/s  (0.674s, 1520.06/s)  LR: 6.501e-04  Data: 0.013 (0.019)
Train: 121 [ 350/1251 ( 28%)]  Loss: 3.702 (3.66)  Time: 0.671s, 1525.00/s  (0.673s, 1521.07/s)  LR: 6.499e-04  Data: 0.015 (0.018)
Train: 121 [ 400/1251 ( 32%)]  Loss: 3.301 (3.62)  Time: 0.667s, 1534.14/s  (0.673s, 1521.88/s)  LR: 6.497e-04  Data: 0.014 (0.018)
Train: 121 [ 450/1251 ( 36%)]  Loss: 3.421 (3.60)  Time: 0.663s, 1545.60/s  (0.673s, 1522.61/s)  LR: 6.495e-04  Data: 0.014 (0.017)
Train: 121 [ 500/1251 ( 40%)]  Loss: 3.454 (3.59)  Time: 0.667s, 1534.28/s  (0.672s, 1523.31/s)  LR: 6.493e-04  Data: 0.013 (0.017)
Train: 121 [ 550/1251 ( 44%)]  Loss: 3.436 (3.57)  Time: 0.674s, 1519.81/s  (0.672s, 1523.60/s)  LR: 6.491e-04  Data: 0.014 (0.017)
Train: 121 [ 600/1251 ( 48%)]  Loss: 3.556 (3.57)  Time: 0.667s, 1536.10/s  (0.672s, 1524.19/s)  LR: 6.489e-04  Data: 0.013 (0.016)
Train: 121 [ 650/1251 ( 52%)]  Loss: 3.803 (3.59)  Time: 0.668s, 1533.58/s  (0.672s, 1524.24/s)  LR: 6.487e-04  Data: 0.013 (0.016)
Train: 121 [ 700/1251 ( 56%)]  Loss: 3.607 (3.59)  Time: 0.671s, 1527.20/s  (0.672s, 1524.43/s)  LR: 6.485e-04  Data: 0.015 (0.016)
Train: 121 [ 750/1251 ( 60%)]  Loss: 3.344 (3.57)  Time: 0.678s, 1511.12/s  (0.672s, 1524.66/s)  LR: 6.483e-04  Data: 0.013 (0.016)
Train: 121 [ 800/1251 ( 64%)]  Loss: 3.561 (3.57)  Time: 0.670s, 1528.54/s  (0.672s, 1524.83/s)  LR: 6.481e-04  Data: 0.013 (0.016)
Train: 121 [ 850/1251 ( 68%)]  Loss: 3.180 (3.55)  Time: 0.680s, 1505.59/s  (0.672s, 1524.92/s)  LR: 6.479e-04  Data: 0.013 (0.016)
Train: 121 [ 900/1251 ( 72%)]  Loss: 3.570 (3.55)  Time: 0.671s, 1526.85/s  (0.672s, 1524.86/s)  LR: 6.477e-04  Data: 0.013 (0.015)
Train: 121 [ 950/1251 ( 76%)]  Loss: 3.641 (3.56)  Time: 0.674s, 1520.37/s  (0.671s, 1525.04/s)  LR: 6.475e-04  Data: 0.016 (0.015)
Train: 121 [1000/1251 ( 80%)]  Loss: 3.768 (3.57)  Time: 0.668s, 1532.13/s  (0.671s, 1525.00/s)  LR: 6.473e-04  Data: 0.013 (0.015)
Train: 121 [1050/1251 ( 84%)]  Loss: 3.445 (3.56)  Time: 0.671s, 1525.99/s  (0.672s, 1524.84/s)  LR: 6.471e-04  Data: 0.016 (0.015)
Train: 121 [1100/1251 ( 88%)]  Loss: 3.534 (3.56)  Time: 0.676s, 1514.98/s  (0.672s, 1524.84/s)  LR: 6.469e-04  Data: 0.013 (0.015)
Train: 121 [1150/1251 ( 92%)]  Loss: 3.620 (3.56)  Time: 0.672s, 1522.92/s  (0.671s, 1525.05/s)  LR: 6.467e-04  Data: 0.012 (0.015)
Train: 121 [1200/1251 ( 96%)]  Loss: 3.597 (3.56)  Time: 0.685s, 1493.85/s  (0.671s, 1525.03/s)  LR: 6.465e-04  Data: 0.013 (0.015)
Train: 121 [1250/1251 (100%)]  Loss: 3.681 (3.57)  Time: 0.657s, 1559.71/s  (0.671s, 1525.10/s)  LR: 6.463e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.915 (2.915)  Loss:  0.5654 (0.5654)  Acc@1: 88.8672 (88.8672)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.164 (0.326)  Loss:  0.6167 (1.0795)  Acc@1: 86.0849 (75.3240)  Acc@5: 97.2877 (92.9480)
Train: 122 [   0/1251 (  0%)]  Loss: 3.273 (3.27)  Time: 3.175s,  322.55/s  (3.175s,  322.55/s)  LR: 6.463e-04  Data: 1.599 (1.599)
Train: 122 [  50/1251 (  4%)]  Loss: 3.383 (3.33)  Time: 0.658s, 1556.84/s  (0.689s, 1485.83/s)  LR: 6.461e-04  Data: 0.014 (0.045)
Train: 122 [ 100/1251 (  8%)]  Loss: 3.575 (3.41)  Time: 0.662s, 1547.60/s  (0.676s, 1514.34/s)  LR: 6.459e-04  Data: 0.013 (0.030)
Train: 122 [ 150/1251 ( 12%)]  Loss: 3.569 (3.45)  Time: 0.675s, 1517.19/s  (0.673s, 1520.85/s)  LR: 6.457e-04  Data: 0.019 (0.025)
Train: 122 [ 200/1251 ( 16%)]  Loss: 3.427 (3.45)  Time: 0.667s, 1534.50/s  (0.672s, 1523.31/s)  LR: 6.455e-04  Data: 0.013 (0.022)
Train: 122 [ 250/1251 ( 20%)]  Loss: 3.364 (3.43)  Time: 0.678s, 1509.35/s  (0.672s, 1524.75/s)  LR: 6.453e-04  Data: 0.016 (0.020)
Train: 122 [ 300/1251 ( 24%)]  Loss: 3.492 (3.44)  Time: 0.667s, 1534.27/s  (0.671s, 1525.34/s)  LR: 6.451e-04  Data: 0.013 (0.019)
Train: 122 [ 350/1251 ( 28%)]  Loss: 3.777 (3.48)  Time: 0.674s, 1518.26/s  (0.671s, 1524.98/s)  LR: 6.449e-04  Data: 0.012 (0.018)
Train: 122 [ 400/1251 ( 32%)]  Loss: 3.905 (3.53)  Time: 0.674s, 1519.87/s  (0.671s, 1525.33/s)  LR: 6.447e-04  Data: 0.012 (0.018)
Train: 122 [ 450/1251 ( 36%)]  Loss: 3.624 (3.54)  Time: 0.668s, 1533.76/s  (0.671s, 1525.61/s)  LR: 6.445e-04  Data: 0.012 (0.017)
Train: 122 [ 500/1251 ( 40%)]  Loss: 3.426 (3.53)  Time: 0.675s, 1516.58/s  (0.671s, 1525.73/s)  LR: 6.443e-04  Data: 0.013 (0.017)
Train: 122 [ 550/1251 ( 44%)]  Loss: 3.711 (3.54)  Time: 0.663s, 1543.56/s  (0.671s, 1525.81/s)  LR: 6.441e-04  Data: 0.014 (0.017)
Train: 122 [ 600/1251 ( 48%)]  Loss: 3.419 (3.53)  Time: 0.668s, 1532.28/s  (0.671s, 1525.50/s)  LR: 6.439e-04  Data: 0.013 (0.016)
Train: 122 [ 650/1251 ( 52%)]  Loss: 3.659 (3.54)  Time: 0.670s, 1527.89/s  (0.671s, 1525.44/s)  LR: 6.437e-04  Data: 0.014 (0.016)
Train: 122 [ 700/1251 ( 56%)]  Loss: 3.398 (3.53)  Time: 0.677s, 1512.94/s  (0.671s, 1525.34/s)  LR: 6.435e-04  Data: 0.012 (0.016)
Train: 122 [ 750/1251 ( 60%)]  Loss: 3.177 (3.51)  Time: 0.673s, 1521.73/s  (0.671s, 1525.32/s)  LR: 6.433e-04  Data: 0.013 (0.016)
Train: 122 [ 800/1251 ( 64%)]  Loss: 3.628 (3.52)  Time: 0.667s, 1535.34/s  (0.671s, 1525.41/s)  LR: 6.431e-04  Data: 0.012 (0.016)
Train: 122 [ 850/1251 ( 68%)]  Loss: 3.785 (3.53)  Time: 0.667s, 1534.22/s  (0.671s, 1525.17/s)  LR: 6.429e-04  Data: 0.013 (0.015)
Train: 122 [ 900/1251 ( 72%)]  Loss: 3.848 (3.55)  Time: 0.672s, 1524.42/s  (0.672s, 1524.89/s)  LR: 6.427e-04  Data: 0.014 (0.015)
Train: 122 [ 950/1251 ( 76%)]  Loss: 3.662 (3.56)  Time: 0.664s, 1542.85/s  (0.671s, 1525.08/s)  LR: 6.425e-04  Data: 0.013 (0.015)
Train: 122 [1000/1251 ( 80%)]  Loss: 3.624 (3.56)  Time: 0.673s, 1522.29/s  (0.671s, 1524.98/s)  LR: 6.423e-04  Data: 0.012 (0.015)
Train: 122 [1050/1251 ( 84%)]  Loss: 3.886 (3.57)  Time: 0.667s, 1534.61/s  (0.671s, 1525.05/s)  LR: 6.421e-04  Data: 0.013 (0.015)
Train: 122 [1100/1251 ( 88%)]  Loss: 3.741 (3.58)  Time: 0.668s, 1533.22/s  (0.671s, 1525.01/s)  LR: 6.419e-04  Data: 0.012 (0.015)
Train: 122 [1150/1251 ( 92%)]  Loss: 3.578 (3.58)  Time: 0.675s, 1517.42/s  (0.671s, 1525.14/s)  LR: 6.417e-04  Data: 0.014 (0.015)
Train: 122 [1200/1251 ( 96%)]  Loss: 3.882 (3.59)  Time: 0.660s, 1550.51/s  (0.671s, 1525.12/s)  LR: 6.415e-04  Data: 0.012 (0.015)
Train: 122 [1250/1251 (100%)]  Loss: 3.409 (3.59)  Time: 0.663s, 1545.58/s  (0.671s, 1525.24/s)  LR: 6.413e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.786 (2.786)  Loss:  0.4839 (0.4839)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.162 (0.324)  Loss:  0.5728 (1.0358)  Acc@1: 85.9670 (75.4960)  Acc@5: 97.1698 (93.0400)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-119.pth.tar', 75.50000006591797)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-122.pth.tar', 75.49600010986327)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-120.pth.tar', 75.4900000366211)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-116.pth.tar', 75.41200016357422)

Train: 123 [   0/1251 (  0%)]  Loss: 3.462 (3.46)  Time: 3.387s,  302.34/s  (3.387s,  302.34/s)  LR: 6.413e-04  Data: 1.517 (1.517)
Train: 123 [  50/1251 (  4%)]  Loss: 3.383 (3.42)  Time: 0.658s, 1555.86/s  (0.688s, 1488.33/s)  LR: 6.411e-04  Data: 0.017 (0.044)
Train: 123 [ 100/1251 (  8%)]  Loss: 3.388 (3.41)  Time: 0.658s, 1557.35/s  (0.674s, 1518.89/s)  LR: 6.409e-04  Data: 0.012 (0.029)
Train: 123 [ 150/1251 ( 12%)]  Loss: 3.817 (3.51)  Time: 0.677s, 1513.39/s  (0.671s, 1526.18/s)  LR: 6.407e-04  Data: 0.013 (0.024)
Train: 123 [ 200/1251 ( 16%)]  Loss: 3.713 (3.55)  Time: 0.668s, 1533.32/s  (0.670s, 1529.09/s)  LR: 6.405e-04  Data: 0.013 (0.022)
Train: 123 [ 250/1251 ( 20%)]  Loss: 3.504 (3.54)  Time: 0.659s, 1554.23/s  (0.670s, 1529.50/s)  LR: 6.403e-04  Data: 0.013 (0.020)
Train: 123 [ 300/1251 ( 24%)]  Loss: 3.361 (3.52)  Time: 0.669s, 1530.60/s  (0.670s, 1528.97/s)  LR: 6.401e-04  Data: 0.014 (0.019)
Train: 123 [ 350/1251 ( 28%)]  Loss: 3.459 (3.51)  Time: 0.674s, 1518.32/s  (0.670s, 1528.31/s)  LR: 6.399e-04  Data: 0.013 (0.018)
Train: 123 [ 400/1251 ( 32%)]  Loss: 3.501 (3.51)  Time: 0.672s, 1524.30/s  (0.670s, 1527.75/s)  LR: 6.397e-04  Data: 0.013 (0.018)
Train: 123 [ 450/1251 ( 36%)]  Loss: 3.677 (3.53)  Time: 0.671s, 1525.48/s  (0.670s, 1527.43/s)  LR: 6.395e-04  Data: 0.014 (0.017)
Train: 123 [ 500/1251 ( 40%)]  Loss: 3.930 (3.56)  Time: 0.677s, 1511.92/s  (0.670s, 1527.47/s)  LR: 6.393e-04  Data: 0.014 (0.017)
Train: 123 [ 550/1251 ( 44%)]  Loss: 3.472 (3.56)  Time: 0.676s, 1514.89/s  (0.670s, 1527.24/s)  LR: 6.391e-04  Data: 0.013 (0.016)
Train: 123 [ 600/1251 ( 48%)]  Loss: 3.754 (3.57)  Time: 0.668s, 1532.10/s  (0.671s, 1526.42/s)  LR: 6.389e-04  Data: 0.015 (0.016)
Train: 123 [ 650/1251 ( 52%)]  Loss: 3.697 (3.58)  Time: 0.669s, 1531.18/s  (0.671s, 1525.89/s)  LR: 6.387e-04  Data: 0.013 (0.016)
Train: 123 [ 700/1251 ( 56%)]  Loss: 3.973 (3.61)  Time: 0.673s, 1521.19/s  (0.671s, 1525.56/s)  LR: 6.385e-04  Data: 0.014 (0.016)
Train: 123 [ 750/1251 ( 60%)]  Loss: 3.510 (3.60)  Time: 0.670s, 1529.49/s  (0.671s, 1525.22/s)  LR: 6.383e-04  Data: 0.013 (0.016)
Train: 123 [ 800/1251 ( 64%)]  Loss: 3.941 (3.62)  Time: 0.672s, 1522.76/s  (0.672s, 1524.40/s)  LR: 6.381e-04  Data: 0.013 (0.016)
Train: 123 [ 850/1251 ( 68%)]  Loss: 3.692 (3.62)  Time: 0.675s, 1517.19/s  (0.672s, 1524.16/s)  LR: 6.379e-04  Data: 0.013 (0.015)
Train: 123 [ 900/1251 ( 72%)]  Loss: 3.563 (3.62)  Time: 0.680s, 1506.59/s  (0.672s, 1523.95/s)  LR: 6.377e-04  Data: 0.018 (0.015)
Train: 123 [ 950/1251 ( 76%)]  Loss: 3.289 (3.60)  Time: 0.672s, 1523.18/s  (0.672s, 1523.89/s)  LR: 6.375e-04  Data: 0.013 (0.015)
Train: 123 [1000/1251 ( 80%)]  Loss: 3.586 (3.60)  Time: 0.676s, 1514.35/s  (0.672s, 1523.79/s)  LR: 6.373e-04  Data: 0.012 (0.015)
Train: 123 [1050/1251 ( 84%)]  Loss: 3.470 (3.60)  Time: 0.675s, 1517.45/s  (0.672s, 1523.78/s)  LR: 6.371e-04  Data: 0.017 (0.015)
Train: 123 [1100/1251 ( 88%)]  Loss: 3.405 (3.59)  Time: 0.677s, 1513.38/s  (0.672s, 1523.94/s)  LR: 6.369e-04  Data: 0.013 (0.015)
Train: 123 [1150/1251 ( 92%)]  Loss: 3.333 (3.58)  Time: 0.673s, 1522.56/s  (0.672s, 1523.91/s)  LR: 6.367e-04  Data: 0.013 (0.015)
Train: 123 [1200/1251 ( 96%)]  Loss: 3.480 (3.57)  Time: 0.663s, 1543.97/s  (0.672s, 1523.83/s)  LR: 6.365e-04  Data: 0.012 (0.015)
Train: 123 [1250/1251 (100%)]  Loss: 3.723 (3.58)  Time: 0.653s, 1568.34/s  (0.672s, 1523.92/s)  LR: 6.363e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.904 (2.904)  Loss:  0.4729 (0.4729)  Acc@1: 89.6484 (89.6484)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.162 (0.333)  Loss:  0.5742 (1.0228)  Acc@1: 86.4387 (75.5800)  Acc@5: 96.6981 (93.1520)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-123.pth.tar', 75.58000005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-119.pth.tar', 75.50000006591797)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-122.pth.tar', 75.49600010986327)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-120.pth.tar', 75.4900000366211)

Train: 124 [   0/1251 (  0%)]  Loss: 3.768 (3.77)  Time: 3.941s,  259.82/s  (3.941s,  259.82/s)  LR: 6.363e-04  Data: 1.751 (1.751)
Train: 124 [  50/1251 (  4%)]  Loss: 3.392 (3.58)  Time: 0.647s, 1583.02/s  (0.696s, 1471.61/s)  LR: 6.361e-04  Data: 0.013 (0.048)
Train: 124 [ 100/1251 (  8%)]  Loss: 3.784 (3.65)  Time: 0.665s, 1540.27/s  (0.676s, 1515.19/s)  LR: 6.359e-04  Data: 0.017 (0.031)
Train: 124 [ 150/1251 ( 12%)]  Loss: 3.333 (3.57)  Time: 0.670s, 1527.60/s  (0.671s, 1526.99/s)  LR: 6.357e-04  Data: 0.014 (0.025)
Train: 124 [ 200/1251 ( 16%)]  Loss: 4.022 (3.66)  Time: 0.674s, 1518.40/s  (0.669s, 1530.16/s)  LR: 6.355e-04  Data: 0.013 (0.022)
Train: 124 [ 250/1251 ( 20%)]  Loss: 3.775 (3.68)  Time: 0.673s, 1521.11/s  (0.669s, 1531.01/s)  LR: 6.353e-04  Data: 0.013 (0.021)
Train: 124 [ 300/1251 ( 24%)]  Loss: 3.319 (3.63)  Time: 0.666s, 1536.43/s  (0.668s, 1531.83/s)  LR: 6.351e-04  Data: 0.015 (0.019)
Train: 124 [ 350/1251 ( 28%)]  Loss: 3.784 (3.65)  Time: 0.674s, 1519.31/s  (0.668s, 1532.24/s)  LR: 6.349e-04  Data: 0.012 (0.019)
Train: 124 [ 400/1251 ( 32%)]  Loss: 3.593 (3.64)  Time: 0.660s, 1552.45/s  (0.668s, 1532.92/s)  LR: 6.347e-04  Data: 0.014 (0.018)
Train: 124 [ 450/1251 ( 36%)]  Loss: 3.714 (3.65)  Time: 0.666s, 1538.43/s  (0.668s, 1533.16/s)  LR: 6.345e-04  Data: 0.014 (0.018)
Train: 124 [ 500/1251 ( 40%)]  Loss: 3.552 (3.64)  Time: 0.672s, 1524.47/s  (0.668s, 1532.53/s)  LR: 6.343e-04  Data: 0.013 (0.017)
Train: 124 [ 550/1251 ( 44%)]  Loss: 3.554 (3.63)  Time: 0.678s, 1510.35/s  (0.668s, 1532.07/s)  LR: 6.341e-04  Data: 0.019 (0.017)
Train: 124 [ 600/1251 ( 48%)]  Loss: 3.219 (3.60)  Time: 0.668s, 1532.46/s  (0.669s, 1531.79/s)  LR: 6.339e-04  Data: 0.012 (0.017)
Train: 124 [ 650/1251 ( 52%)]  Loss: 3.481 (3.59)  Time: 0.669s, 1530.59/s  (0.669s, 1531.65/s)  LR: 6.337e-04  Data: 0.014 (0.016)
Train: 124 [ 700/1251 ( 56%)]  Loss: 3.235 (3.57)  Time: 0.673s, 1521.77/s  (0.668s, 1531.97/s)  LR: 6.335e-04  Data: 0.014 (0.016)
Train: 124 [ 750/1251 ( 60%)]  Loss: 3.675 (3.58)  Time: 0.672s, 1522.79/s  (0.668s, 1532.25/s)  LR: 6.333e-04  Data: 0.013 (0.016)
Train: 124 [ 800/1251 ( 64%)]  Loss: 3.781 (3.59)  Time: 0.668s, 1533.13/s  (0.668s, 1532.30/s)  LR: 6.331e-04  Data: 0.013 (0.016)
Train: 124 [ 850/1251 ( 68%)]  Loss: 3.619 (3.59)  Time: 0.665s, 1539.51/s  (0.668s, 1532.43/s)  LR: 6.329e-04  Data: 0.013 (0.016)
Train: 124 [ 900/1251 ( 72%)]  Loss: 3.702 (3.59)  Time: 0.667s, 1535.35/s  (0.668s, 1532.61/s)  LR: 6.327e-04  Data: 0.013 (0.016)
Train: 124 [ 950/1251 ( 76%)]  Loss: 3.426 (3.59)  Time: 0.665s, 1539.37/s  (0.668s, 1532.66/s)  LR: 6.325e-04  Data: 0.013 (0.015)
Train: 124 [1000/1251 ( 80%)]  Loss: 3.476 (3.58)  Time: 0.664s, 1542.68/s  (0.668s, 1532.68/s)  LR: 6.323e-04  Data: 0.013 (0.015)
Train: 124 [1050/1251 ( 84%)]  Loss: 3.342 (3.57)  Time: 0.670s, 1528.90/s  (0.668s, 1533.02/s)  LR: 6.321e-04  Data: 0.014 (0.015)
Train: 124 [1100/1251 ( 88%)]  Loss: 3.428 (3.56)  Time: 0.663s, 1543.83/s  (0.668s, 1533.04/s)  LR: 6.319e-04  Data: 0.014 (0.015)
Train: 124 [1150/1251 ( 92%)]  Loss: 3.462 (3.56)  Time: 0.667s, 1535.36/s  (0.668s, 1532.82/s)  LR: 6.317e-04  Data: 0.013 (0.015)
Train: 124 [1200/1251 ( 96%)]  Loss: 3.632 (3.56)  Time: 0.662s, 1547.92/s  (0.668s, 1532.82/s)  LR: 6.315e-04  Data: 0.013 (0.015)
Train: 124 [1250/1251 (100%)]  Loss: 3.881 (3.58)  Time: 0.651s, 1572.71/s  (0.668s, 1533.05/s)  LR: 6.313e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.759 (2.759)  Loss:  0.5215 (0.5215)  Acc@1: 89.8438 (89.8438)  Acc@5: 97.4609 (97.4609)
Test: [  48/48]  Time: 0.162 (0.325)  Loss:  0.5542 (1.0357)  Acc@1: 86.2028 (75.3500)  Acc@5: 97.1698 (93.0300)
Train: 125 [   0/1251 (  0%)]  Loss: 3.621 (3.62)  Time: 3.795s,  269.80/s  (3.795s,  269.80/s)  LR: 6.313e-04  Data: 1.900 (1.900)
Train: 125 [  50/1251 (  4%)]  Loss: 3.485 (3.55)  Time: 0.654s, 1565.81/s  (0.687s, 1490.09/s)  LR: 6.311e-04  Data: 0.013 (0.051)
Train: 125 [ 100/1251 (  8%)]  Loss: 3.546 (3.55)  Time: 0.664s, 1542.48/s  (0.672s, 1523.97/s)  LR: 6.309e-04  Data: 0.016 (0.033)
Train: 125 [ 150/1251 ( 12%)]  Loss: 3.753 (3.60)  Time: 0.671s, 1526.19/s  (0.669s, 1530.67/s)  LR: 6.307e-04  Data: 0.017 (0.026)
Train: 125 [ 200/1251 ( 16%)]  Loss: 3.579 (3.60)  Time: 0.665s, 1540.89/s  (0.668s, 1531.96/s)  LR: 6.305e-04  Data: 0.013 (0.023)
Train: 125 [ 250/1251 ( 20%)]  Loss: 3.737 (3.62)  Time: 0.664s, 1543.02/s  (0.669s, 1531.73/s)  LR: 6.303e-04  Data: 0.012 (0.021)
Train: 125 [ 300/1251 ( 24%)]  Loss: 3.510 (3.60)  Time: 0.674s, 1519.38/s  (0.669s, 1531.54/s)  LR: 6.301e-04  Data: 0.013 (0.020)
Train: 125 [ 350/1251 ( 28%)]  Loss: 3.633 (3.61)  Time: 0.669s, 1530.20/s  (0.669s, 1531.70/s)  LR: 6.298e-04  Data: 0.014 (0.019)
Train: 125 [ 400/1251 ( 32%)]  Loss: 3.570 (3.60)  Time: 0.660s, 1550.83/s  (0.668s, 1532.09/s)  LR: 6.296e-04  Data: 0.012 (0.018)
Train: 125 [ 450/1251 ( 36%)]  Loss: 3.510 (3.59)  Time: 0.668s, 1532.79/s  (0.668s, 1532.34/s)  LR: 6.294e-04  Data: 0.013 (0.018)
Train: 125 [ 500/1251 ( 40%)]  Loss: 3.824 (3.62)  Time: 0.658s, 1555.91/s  (0.668s, 1533.01/s)  LR: 6.292e-04  Data: 0.016 (0.017)
Train: 125 [ 550/1251 ( 44%)]  Loss: 3.852 (3.63)  Time: 0.667s, 1536.21/s  (0.668s, 1532.73/s)  LR: 6.290e-04  Data: 0.014 (0.017)
Train: 125 [ 600/1251 ( 48%)]  Loss: 3.388 (3.62)  Time: 0.665s, 1538.94/s  (0.668s, 1532.69/s)  LR: 6.288e-04  Data: 0.013 (0.017)
Train: 125 [ 650/1251 ( 52%)]  Loss: 3.503 (3.61)  Time: 0.673s, 1521.33/s  (0.668s, 1532.71/s)  LR: 6.286e-04  Data: 0.012 (0.017)
Train: 125 [ 700/1251 ( 56%)]  Loss: 3.134 (3.58)  Time: 0.665s, 1540.21/s  (0.668s, 1532.64/s)  LR: 6.284e-04  Data: 0.014 (0.016)
Train: 125 [ 750/1251 ( 60%)]  Loss: 3.551 (3.57)  Time: 0.674s, 1519.73/s  (0.668s, 1532.42/s)  LR: 6.282e-04  Data: 0.013 (0.016)
Train: 125 [ 800/1251 ( 64%)]  Loss: 3.581 (3.58)  Time: 0.676s, 1515.79/s  (0.668s, 1532.21/s)  LR: 6.280e-04  Data: 0.013 (0.016)
Train: 125 [ 850/1251 ( 68%)]  Loss: 3.440 (3.57)  Time: 0.669s, 1531.58/s  (0.668s, 1532.12/s)  LR: 6.278e-04  Data: 0.013 (0.016)
Train: 125 [ 900/1251 ( 72%)]  Loss: 3.917 (3.59)  Time: 0.669s, 1530.35/s  (0.668s, 1532.14/s)  LR: 6.276e-04  Data: 0.016 (0.016)
Train: 125 [ 950/1251 ( 76%)]  Loss: 4.007 (3.61)  Time: 0.671s, 1525.18/s  (0.668s, 1531.94/s)  LR: 6.274e-04  Data: 0.012 (0.016)
Train: 125 [1000/1251 ( 80%)]  Loss: 3.695 (3.61)  Time: 0.665s, 1540.24/s  (0.668s, 1531.87/s)  LR: 6.272e-04  Data: 0.014 (0.016)
Train: 125 [1050/1251 ( 84%)]  Loss: 3.787 (3.62)  Time: 0.668s, 1532.22/s  (0.668s, 1531.80/s)  LR: 6.270e-04  Data: 0.013 (0.015)
Train: 125 [1100/1251 ( 88%)]  Loss: 3.511 (3.61)  Time: 0.672s, 1523.83/s  (0.669s, 1531.45/s)  LR: 6.268e-04  Data: 0.012 (0.015)
Train: 125 [1150/1251 ( 92%)]  Loss: 3.502 (3.61)  Time: 0.676s, 1514.03/s  (0.669s, 1531.32/s)  LR: 6.266e-04  Data: 0.011 (0.015)
Train: 125 [1200/1251 ( 96%)]  Loss: 3.481 (3.60)  Time: 0.670s, 1528.18/s  (0.669s, 1531.11/s)  LR: 6.264e-04  Data: 0.013 (0.015)
Train: 125 [1250/1251 (100%)]  Loss: 3.560 (3.60)  Time: 0.661s, 1548.47/s  (0.669s, 1530.90/s)  LR: 6.262e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.819 (2.819)  Loss:  0.5068 (0.5068)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.161 (0.320)  Loss:  0.5645 (1.0251)  Acc@1: 85.9670 (75.5980)  Acc@5: 96.6981 (93.1500)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-125.pth.tar', 75.59800010986328)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-123.pth.tar', 75.58000005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-119.pth.tar', 75.50000006591797)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-122.pth.tar', 75.49600010986327)

Train: 126 [   0/1251 (  0%)]  Loss: 3.590 (3.59)  Time: 3.895s,  262.87/s  (3.895s,  262.87/s)  LR: 6.262e-04  Data: 1.724 (1.724)
Train: 126 [  50/1251 (  4%)]  Loss: 3.392 (3.49)  Time: 0.664s, 1542.89/s  (0.701s, 1460.46/s)  LR: 6.260e-04  Data: 0.014 (0.047)
Train: 126 [ 100/1251 (  8%)]  Loss: 3.382 (3.45)  Time: 0.672s, 1524.13/s  (0.683s, 1499.14/s)  LR: 6.258e-04  Data: 0.013 (0.031)
Train: 126 [ 150/1251 ( 12%)]  Loss: 3.688 (3.51)  Time: 0.668s, 1533.19/s  (0.678s, 1510.85/s)  LR: 6.256e-04  Data: 0.012 (0.025)
Train: 126 [ 200/1251 ( 16%)]  Loss: 3.202 (3.45)  Time: 0.676s, 1514.07/s  (0.676s, 1514.12/s)  LR: 6.254e-04  Data: 0.012 (0.022)
Train: 126 [ 250/1251 ( 20%)]  Loss: 3.827 (3.51)  Time: 0.667s, 1534.47/s  (0.675s, 1516.67/s)  LR: 6.252e-04  Data: 0.013 (0.020)
Train: 126 [ 300/1251 ( 24%)]  Loss: 3.585 (3.52)  Time: 0.677s, 1511.52/s  (0.674s, 1518.64/s)  LR: 6.250e-04  Data: 0.013 (0.019)
Train: 126 [ 350/1251 ( 28%)]  Loss: 3.917 (3.57)  Time: 0.665s, 1539.76/s  (0.674s, 1519.21/s)  LR: 6.248e-04  Data: 0.014 (0.018)
Train: 126 [ 400/1251 ( 32%)]  Loss: 3.277 (3.54)  Time: 0.670s, 1529.43/s  (0.674s, 1519.61/s)  LR: 6.246e-04  Data: 0.013 (0.018)
Train: 126 [ 450/1251 ( 36%)]  Loss: 3.504 (3.54)  Time: 0.677s, 1511.60/s  (0.674s, 1519.99/s)  LR: 6.244e-04  Data: 0.013 (0.017)
Train: 126 [ 500/1251 ( 40%)]  Loss: 3.830 (3.56)  Time: 0.659s, 1554.49/s  (0.673s, 1520.46/s)  LR: 6.242e-04  Data: 0.016 (0.017)
Train: 126 [ 550/1251 ( 44%)]  Loss: 3.576 (3.56)  Time: 0.664s, 1542.28/s  (0.673s, 1520.83/s)  LR: 6.240e-04  Data: 0.015 (0.017)
Train: 126 [ 600/1251 ( 48%)]  Loss: 3.493 (3.56)  Time: 0.679s, 1508.41/s  (0.673s, 1521.21/s)  LR: 6.238e-04  Data: 0.015 (0.016)
Train: 126 [ 650/1251 ( 52%)]  Loss: 3.601 (3.56)  Time: 0.673s, 1522.08/s  (0.673s, 1521.53/s)  LR: 6.236e-04  Data: 0.015 (0.016)
Train: 126 [ 700/1251 ( 56%)]  Loss: 3.305 (3.54)  Time: 0.681s, 1502.71/s  (0.673s, 1521.59/s)  LR: 6.234e-04  Data: 0.016 (0.016)
Train: 126 [ 750/1251 ( 60%)]  Loss: 3.359 (3.53)  Time: 0.671s, 1526.47/s  (0.673s, 1521.41/s)  LR: 6.232e-04  Data: 0.012 (0.016)
Train: 126 [ 800/1251 ( 64%)]  Loss: 3.279 (3.52)  Time: 0.672s, 1523.06/s  (0.673s, 1521.43/s)  LR: 6.230e-04  Data: 0.014 (0.016)
Train: 126 [ 850/1251 ( 68%)]  Loss: 3.708 (3.53)  Time: 0.675s, 1517.94/s  (0.673s, 1521.59/s)  LR: 6.228e-04  Data: 0.013 (0.016)
Train: 126 [ 900/1251 ( 72%)]  Loss: 3.264 (3.51)  Time: 0.671s, 1525.35/s  (0.673s, 1521.82/s)  LR: 6.226e-04  Data: 0.013 (0.016)
Train: 126 [ 950/1251 ( 76%)]  Loss: 3.646 (3.52)  Time: 0.679s, 1507.90/s  (0.673s, 1521.96/s)  LR: 6.224e-04  Data: 0.012 (0.016)
Train: 126 [1000/1251 ( 80%)]  Loss: 3.679 (3.53)  Time: 0.669s, 1529.76/s  (0.673s, 1522.18/s)  LR: 6.222e-04  Data: 0.012 (0.015)
Train: 126 [1050/1251 ( 84%)]  Loss: 3.597 (3.53)  Time: 0.669s, 1531.55/s  (0.673s, 1522.37/s)  LR: 6.220e-04  Data: 0.013 (0.015)
Train: 126 [1100/1251 ( 88%)]  Loss: 3.573 (3.53)  Time: 0.672s, 1524.60/s  (0.673s, 1522.62/s)  LR: 6.218e-04  Data: 0.013 (0.015)
Train: 126 [1150/1251 ( 92%)]  Loss: 3.683 (3.54)  Time: 0.665s, 1540.13/s  (0.672s, 1522.74/s)  LR: 6.216e-04  Data: 0.014 (0.015)
Train: 126 [1200/1251 ( 96%)]  Loss: 3.414 (3.53)  Time: 0.676s, 1513.78/s  (0.672s, 1522.86/s)  LR: 6.214e-04  Data: 0.012 (0.015)
Train: 126 [1250/1251 (100%)]  Loss: 3.606 (3.54)  Time: 0.659s, 1554.35/s  (0.672s, 1523.18/s)  LR: 6.212e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.995 (2.995)  Loss:  0.5098 (0.5098)  Acc@1: 90.0391 (90.0391)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.161 (0.324)  Loss:  0.5615 (1.0312)  Acc@1: 85.6132 (75.5900)  Acc@5: 97.6415 (93.1740)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-125.pth.tar', 75.59800010986328)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-126.pth.tar', 75.59000008544922)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-123.pth.tar', 75.58000005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-119.pth.tar', 75.50000006591797)

Train: 127 [   0/1251 (  0%)]  Loss: 3.512 (3.51)  Time: 3.511s,  291.68/s  (3.511s,  291.68/s)  LR: 6.212e-04  Data: 1.682 (1.682)
Train: 127 [  50/1251 (  4%)]  Loss: 3.509 (3.51)  Time: 0.650s, 1574.25/s  (0.692s, 1479.16/s)  LR: 6.210e-04  Data: 0.013 (0.046)
Train: 127 [ 100/1251 (  8%)]  Loss: 3.380 (3.47)  Time: 0.671s, 1526.99/s  (0.676s, 1513.70/s)  LR: 6.208e-04  Data: 0.016 (0.030)
Train: 127 [ 150/1251 ( 12%)]  Loss: 3.445 (3.46)  Time: 0.670s, 1528.78/s  (0.674s, 1519.85/s)  LR: 6.206e-04  Data: 0.013 (0.025)
Train: 127 [ 200/1251 ( 16%)]  Loss: 3.418 (3.45)  Time: 0.664s, 1541.68/s  (0.673s, 1522.16/s)  LR: 6.204e-04  Data: 0.014 (0.022)
Train: 127 [ 250/1251 ( 20%)]  Loss: 3.365 (3.44)  Time: 0.675s, 1517.94/s  (0.672s, 1523.06/s)  LR: 6.202e-04  Data: 0.013 (0.020)
Train: 127 [ 300/1251 ( 24%)]  Loss: 3.739 (3.48)  Time: 0.672s, 1522.81/s  (0.672s, 1523.77/s)  LR: 6.200e-04  Data: 0.012 (0.019)
Train: 127 [ 350/1251 ( 28%)]  Loss: 3.324 (3.46)  Time: 0.671s, 1526.65/s  (0.672s, 1524.26/s)  LR: 6.198e-04  Data: 0.015 (0.018)
Train: 127 [ 400/1251 ( 32%)]  Loss: 3.596 (3.48)  Time: 0.680s, 1505.36/s  (0.672s, 1524.49/s)  LR: 6.195e-04  Data: 0.013 (0.018)
Train: 127 [ 450/1251 ( 36%)]  Loss: 3.589 (3.49)  Time: 0.668s, 1533.80/s  (0.672s, 1524.73/s)  LR: 6.193e-04  Data: 0.014 (0.017)
Train: 127 [ 500/1251 ( 40%)]  Loss: 3.658 (3.50)  Time: 0.679s, 1507.29/s  (0.672s, 1524.64/s)  LR: 6.191e-04  Data: 0.013 (0.017)
Train: 127 [ 550/1251 ( 44%)]  Loss: 3.641 (3.51)  Time: 0.672s, 1524.89/s  (0.672s, 1524.79/s)  LR: 6.189e-04  Data: 0.011 (0.016)
Train: 127 [ 600/1251 ( 48%)]  Loss: 3.410 (3.51)  Time: 0.673s, 1521.90/s  (0.672s, 1524.32/s)  LR: 6.187e-04  Data: 0.013 (0.016)
Train: 127 [ 650/1251 ( 52%)]  Loss: 3.755 (3.52)  Time: 0.671s, 1525.79/s  (0.672s, 1524.41/s)  LR: 6.185e-04  Data: 0.013 (0.016)
Train: 127 [ 700/1251 ( 56%)]  Loss: 3.462 (3.52)  Time: 0.669s, 1529.91/s  (0.672s, 1524.33/s)  LR: 6.183e-04  Data: 0.013 (0.016)
Train: 127 [ 750/1251 ( 60%)]  Loss: 3.593 (3.52)  Time: 0.675s, 1515.97/s  (0.672s, 1524.53/s)  LR: 6.181e-04  Data: 0.014 (0.016)
Train: 127 [ 800/1251 ( 64%)]  Loss: 3.500 (3.52)  Time: 0.674s, 1519.91/s  (0.672s, 1524.89/s)  LR: 6.179e-04  Data: 0.013 (0.016)
Train: 127 [ 850/1251 ( 68%)]  Loss: 3.458 (3.52)  Time: 0.684s, 1496.81/s  (0.671s, 1525.10/s)  LR: 6.177e-04  Data: 0.013 (0.015)
Train: 127 [ 900/1251 ( 72%)]  Loss: 3.544 (3.52)  Time: 0.675s, 1516.82/s  (0.671s, 1525.04/s)  LR: 6.175e-04  Data: 0.012 (0.015)
Train: 127 [ 950/1251 ( 76%)]  Loss: 3.779 (3.53)  Time: 0.679s, 1508.09/s  (0.671s, 1525.14/s)  LR: 6.173e-04  Data: 0.014 (0.015)
Train: 127 [1000/1251 ( 80%)]  Loss: 3.334 (3.52)  Time: 0.668s, 1533.45/s  (0.671s, 1525.34/s)  LR: 6.171e-04  Data: 0.016 (0.015)
Train: 127 [1050/1251 ( 84%)]  Loss: 3.729 (3.53)  Time: 0.663s, 1544.18/s  (0.671s, 1525.47/s)  LR: 6.169e-04  Data: 0.013 (0.015)
Train: 127 [1100/1251 ( 88%)]  Loss: 3.754 (3.54)  Time: 0.671s, 1526.88/s  (0.671s, 1525.72/s)  LR: 6.167e-04  Data: 0.013 (0.015)
Train: 127 [1150/1251 ( 92%)]  Loss: 3.838 (3.56)  Time: 0.668s, 1531.95/s  (0.671s, 1525.68/s)  LR: 6.165e-04  Data: 0.014 (0.015)
Train: 127 [1200/1251 ( 96%)]  Loss: 3.402 (3.55)  Time: 0.664s, 1541.07/s  (0.671s, 1526.02/s)  LR: 6.163e-04  Data: 0.013 (0.015)
Train: 127 [1250/1251 (100%)]  Loss: 3.481 (3.55)  Time: 0.663s, 1545.30/s  (0.671s, 1526.25/s)  LR: 6.161e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.775 (2.775)  Loss:  0.4580 (0.4580)  Acc@1: 90.5273 (90.5273)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.161 (0.324)  Loss:  0.6021 (1.0173)  Acc@1: 85.3774 (75.7220)  Acc@5: 96.5802 (93.1960)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-127.pth.tar', 75.72199998291016)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-125.pth.tar', 75.59800010986328)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-126.pth.tar', 75.59000008544922)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-123.pth.tar', 75.58000005615234)

Train: 128 [   0/1251 (  0%)]  Loss: 3.336 (3.34)  Time: 3.792s,  270.01/s  (3.792s,  270.01/s)  LR: 6.161e-04  Data: 1.819 (1.819)
Train: 128 [  50/1251 (  4%)]  Loss: 3.328 (3.33)  Time: 0.659s, 1554.54/s  (0.678s, 1510.34/s)  LR: 6.159e-04  Data: 0.014 (0.049)
Train: 128 [ 100/1251 (  8%)]  Loss: 3.724 (3.46)  Time: 0.673s, 1521.08/s  (0.669s, 1530.67/s)  LR: 6.157e-04  Data: 0.013 (0.031)
Train: 128 [ 150/1251 ( 12%)]  Loss: 3.670 (3.51)  Time: 0.675s, 1516.22/s  (0.668s, 1531.88/s)  LR: 6.155e-04  Data: 0.014 (0.025)
Train: 128 [ 200/1251 ( 16%)]  Loss: 3.476 (3.51)  Time: 0.670s, 1528.15/s  (0.668s, 1531.82/s)  LR: 6.153e-04  Data: 0.014 (0.022)
Train: 128 [ 250/1251 ( 20%)]  Loss: 3.177 (3.45)  Time: 0.668s, 1533.13/s  (0.669s, 1531.37/s)  LR: 6.151e-04  Data: 0.013 (0.021)
Train: 128 [ 300/1251 ( 24%)]  Loss: 3.582 (3.47)  Time: 0.671s, 1525.30/s  (0.669s, 1531.15/s)  LR: 6.149e-04  Data: 0.016 (0.020)
Train: 128 [ 350/1251 ( 28%)]  Loss: 3.299 (3.45)  Time: 0.665s, 1539.33/s  (0.669s, 1530.17/s)  LR: 6.147e-04  Data: 0.013 (0.019)
Train: 128 [ 400/1251 ( 32%)]  Loss: 3.612 (3.47)  Time: 0.666s, 1538.36/s  (0.669s, 1529.71/s)  LR: 6.145e-04  Data: 0.013 (0.018)
Train: 128 [ 450/1251 ( 36%)]  Loss: 3.577 (3.48)  Time: 0.673s, 1522.18/s  (0.670s, 1529.10/s)  LR: 6.143e-04  Data: 0.013 (0.018)
Train: 128 [ 500/1251 ( 40%)]  Loss: 3.359 (3.47)  Time: 0.671s, 1527.17/s  (0.670s, 1528.62/s)  LR: 6.141e-04  Data: 0.013 (0.017)
Train: 128 [ 550/1251 ( 44%)]  Loss: 3.541 (3.47)  Time: 0.675s, 1517.91/s  (0.670s, 1527.98/s)  LR: 6.139e-04  Data: 0.014 (0.017)
Train: 128 [ 600/1251 ( 48%)]  Loss: 3.407 (3.47)  Time: 0.668s, 1532.83/s  (0.670s, 1527.60/s)  LR: 6.137e-04  Data: 0.014 (0.017)
Train: 128 [ 650/1251 ( 52%)]  Loss: 3.535 (3.47)  Time: 0.680s, 1506.87/s  (0.670s, 1527.31/s)  LR: 6.135e-04  Data: 0.013 (0.016)
Train: 128 [ 700/1251 ( 56%)]  Loss: 3.653 (3.49)  Time: 0.673s, 1522.56/s  (0.671s, 1527.07/s)  LR: 6.133e-04  Data: 0.013 (0.016)
Train: 128 [ 750/1251 ( 60%)]  Loss: 3.379 (3.48)  Time: 0.671s, 1525.51/s  (0.671s, 1526.99/s)  LR: 6.131e-04  Data: 0.013 (0.016)
Train: 128 [ 800/1251 ( 64%)]  Loss: 3.806 (3.50)  Time: 0.677s, 1511.61/s  (0.671s, 1526.67/s)  LR: 6.129e-04  Data: 0.012 (0.016)
Train: 128 [ 850/1251 ( 68%)]  Loss: 3.435 (3.49)  Time: 0.660s, 1552.27/s  (0.671s, 1526.75/s)  LR: 6.127e-04  Data: 0.013 (0.016)
Train: 128 [ 900/1251 ( 72%)]  Loss: 3.436 (3.49)  Time: 0.674s, 1520.40/s  (0.671s, 1526.96/s)  LR: 6.124e-04  Data: 0.012 (0.016)
Train: 128 [ 950/1251 ( 76%)]  Loss: 3.814 (3.51)  Time: 0.671s, 1525.77/s  (0.671s, 1527.16/s)  LR: 6.122e-04  Data: 0.015 (0.015)
Train: 128 [1000/1251 ( 80%)]  Loss: 3.737 (3.52)  Time: 0.669s, 1530.24/s  (0.670s, 1527.26/s)  LR: 6.120e-04  Data: 0.012 (0.015)
Train: 128 [1050/1251 ( 84%)]  Loss: 3.531 (3.52)  Time: 0.673s, 1522.59/s  (0.670s, 1527.53/s)  LR: 6.118e-04  Data: 0.013 (0.015)
Train: 128 [1100/1251 ( 88%)]  Loss: 3.676 (3.53)  Time: 0.669s, 1530.14/s  (0.670s, 1527.78/s)  LR: 6.116e-04  Data: 0.013 (0.015)
Train: 128 [1150/1251 ( 92%)]  Loss: 3.530 (3.53)  Time: 0.661s, 1549.27/s  (0.670s, 1527.98/s)  LR: 6.114e-04  Data: 0.012 (0.015)
Train: 128 [1200/1251 ( 96%)]  Loss: 3.632 (3.53)  Time: 0.667s, 1535.56/s  (0.670s, 1528.17/s)  LR: 6.112e-04  Data: 0.014 (0.015)
Train: 128 [1250/1251 (100%)]  Loss: 3.587 (3.53)  Time: 0.654s, 1565.86/s  (0.670s, 1528.63/s)  LR: 6.110e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.874 (2.874)  Loss:  0.5254 (0.5254)  Acc@1: 89.5508 (89.5508)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.161 (0.329)  Loss:  0.6489 (1.0452)  Acc@1: 83.6085 (75.5660)  Acc@5: 97.0519 (93.1680)
Train: 129 [   0/1251 (  0%)]  Loss: 3.589 (3.59)  Time: 3.769s,  271.67/s  (3.769s,  271.67/s)  LR: 6.110e-04  Data: 1.823 (1.823)
Train: 129 [  50/1251 (  4%)]  Loss: 3.452 (3.52)  Time: 0.655s, 1562.90/s  (0.694s, 1474.65/s)  LR: 6.108e-04  Data: 0.013 (0.049)
Train: 129 [ 100/1251 (  8%)]  Loss: 3.446 (3.50)  Time: 0.668s, 1532.27/s  (0.678s, 1510.84/s)  LR: 6.106e-04  Data: 0.013 (0.032)
Train: 129 [ 150/1251 ( 12%)]  Loss: 3.583 (3.52)  Time: 0.662s, 1545.87/s  (0.673s, 1520.92/s)  LR: 6.104e-04  Data: 0.012 (0.026)
Train: 129 [ 200/1251 ( 16%)]  Loss: 3.588 (3.53)  Time: 0.676s, 1514.79/s  (0.672s, 1524.28/s)  LR: 6.102e-04  Data: 0.017 (0.023)
Train: 129 [ 250/1251 ( 20%)]  Loss: 3.645 (3.55)  Time: 0.672s, 1523.65/s  (0.671s, 1525.65/s)  LR: 6.100e-04  Data: 0.015 (0.021)
Train: 129 [ 300/1251 ( 24%)]  Loss: 3.993 (3.61)  Time: 0.667s, 1534.40/s  (0.671s, 1526.26/s)  LR: 6.098e-04  Data: 0.012 (0.020)
Train: 129 [ 350/1251 ( 28%)]  Loss: 3.390 (3.59)  Time: 0.670s, 1529.44/s  (0.671s, 1526.17/s)  LR: 6.096e-04  Data: 0.018 (0.019)
Train: 129 [ 400/1251 ( 32%)]  Loss: 3.626 (3.59)  Time: 0.673s, 1520.46/s  (0.671s, 1526.13/s)  LR: 6.094e-04  Data: 0.013 (0.018)
Train: 129 [ 450/1251 ( 36%)]  Loss: 3.716 (3.60)  Time: 0.673s, 1521.23/s  (0.671s, 1526.36/s)  LR: 6.092e-04  Data: 0.014 (0.018)
Train: 129 [ 500/1251 ( 40%)]  Loss: 3.340 (3.58)  Time: 0.668s, 1532.76/s  (0.671s, 1526.96/s)  LR: 6.090e-04  Data: 0.013 (0.017)
Train: 129 [ 550/1251 ( 44%)]  Loss: 3.542 (3.58)  Time: 0.677s, 1512.59/s  (0.671s, 1527.04/s)  LR: 6.088e-04  Data: 0.013 (0.017)
Train: 129 [ 600/1251 ( 48%)]  Loss: 3.274 (3.55)  Time: 0.679s, 1507.40/s  (0.671s, 1526.63/s)  LR: 6.086e-04  Data: 0.014 (0.017)
Train: 129 [ 650/1251 ( 52%)]  Loss: 3.768 (3.57)  Time: 0.671s, 1526.29/s  (0.671s, 1526.73/s)  LR: 6.084e-04  Data: 0.013 (0.016)
Train: 129 [ 700/1251 ( 56%)]  Loss: 3.448 (3.56)  Time: 0.661s, 1548.21/s  (0.671s, 1526.47/s)  LR: 6.082e-04  Data: 0.013 (0.016)
Train: 129 [ 750/1251 ( 60%)]  Loss: 3.718 (3.57)  Time: 0.674s, 1520.11/s  (0.671s, 1526.29/s)  LR: 6.080e-04  Data: 0.014 (0.016)
Train: 129 [ 800/1251 ( 64%)]  Loss: 3.457 (3.56)  Time: 0.667s, 1534.23/s  (0.671s, 1526.31/s)  LR: 6.078e-04  Data: 0.013 (0.016)
Train: 129 [ 850/1251 ( 68%)]  Loss: 3.411 (3.55)  Time: 0.673s, 1521.18/s  (0.671s, 1526.29/s)  LR: 6.076e-04  Data: 0.013 (0.016)
Train: 129 [ 900/1251 ( 72%)]  Loss: 3.419 (3.55)  Time: 0.673s, 1522.62/s  (0.671s, 1526.53/s)  LR: 6.074e-04  Data: 0.013 (0.016)
Train: 129 [ 950/1251 ( 76%)]  Loss: 3.793 (3.56)  Time: 0.680s, 1505.27/s  (0.671s, 1526.57/s)  LR: 6.072e-04  Data: 0.015 (0.015)
Train: 129 [1000/1251 ( 80%)]  Loss: 3.769 (3.57)  Time: 0.673s, 1522.44/s  (0.671s, 1526.62/s)  LR: 6.070e-04  Data: 0.014 (0.015)
Train: 129 [1050/1251 ( 84%)]  Loss: 3.789 (3.58)  Time: 0.670s, 1527.44/s  (0.671s, 1526.87/s)  LR: 6.068e-04  Data: 0.013 (0.015)
Train: 129 [1100/1251 ( 88%)]  Loss: 3.725 (3.59)  Time: 0.666s, 1537.93/s  (0.671s, 1527.16/s)  LR: 6.065e-04  Data: 0.013 (0.015)
Train: 129 [1150/1251 ( 92%)]  Loss: 3.628 (3.59)  Time: 0.673s, 1521.58/s  (0.670s, 1527.36/s)  LR: 6.063e-04  Data: 0.014 (0.015)
Train: 129 [1200/1251 ( 96%)]  Loss: 3.675 (3.59)  Time: 0.664s, 1542.09/s  (0.670s, 1527.50/s)  LR: 6.061e-04  Data: 0.013 (0.015)
Train: 129 [1250/1251 (100%)]  Loss: 3.758 (3.60)  Time: 0.656s, 1560.48/s  (0.670s, 1527.80/s)  LR: 6.059e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.965 (2.965)  Loss:  0.5254 (0.5254)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.161 (0.327)  Loss:  0.6099 (1.0425)  Acc@1: 86.0849 (75.8640)  Acc@5: 96.6981 (93.1120)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-129.pth.tar', 75.86400016113281)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-127.pth.tar', 75.72199998291016)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-125.pth.tar', 75.59800010986328)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-126.pth.tar', 75.59000008544922)

Train: 130 [   0/1251 (  0%)]  Loss: 3.512 (3.51)  Time: 3.547s,  288.67/s  (3.547s,  288.67/s)  LR: 6.059e-04  Data: 1.937 (1.937)
Train: 130 [  50/1251 (  4%)]  Loss: 3.630 (3.57)  Time: 0.655s, 1563.23/s  (0.685s, 1493.95/s)  LR: 6.057e-04  Data: 0.012 (0.051)
Train: 130 [ 100/1251 (  8%)]  Loss: 3.764 (3.64)  Time: 0.659s, 1553.42/s  (0.673s, 1522.41/s)  LR: 6.055e-04  Data: 0.014 (0.033)
Train: 130 [ 150/1251 ( 12%)]  Loss: 3.821 (3.68)  Time: 0.667s, 1534.60/s  (0.671s, 1526.86/s)  LR: 6.053e-04  Data: 0.013 (0.027)
Train: 130 [ 200/1251 ( 16%)]  Loss: 3.428 (3.63)  Time: 0.663s, 1544.55/s  (0.670s, 1529.23/s)  LR: 6.051e-04  Data: 0.015 (0.024)
Train: 130 [ 250/1251 ( 20%)]  Loss: 3.519 (3.61)  Time: 0.661s, 1549.54/s  (0.669s, 1529.51/s)  LR: 6.049e-04  Data: 0.014 (0.022)
Train: 130 [ 300/1251 ( 24%)]  Loss: 3.845 (3.65)  Time: 0.665s, 1540.61/s  (0.669s, 1529.53/s)  LR: 6.047e-04  Data: 0.013 (0.020)
Train: 130 [ 350/1251 ( 28%)]  Loss: 3.656 (3.65)  Time: 0.665s, 1539.83/s  (0.669s, 1529.80/s)  LR: 6.045e-04  Data: 0.012 (0.019)
Train: 130 [ 400/1251 ( 32%)]  Loss: 3.171 (3.59)  Time: 0.675s, 1517.41/s  (0.669s, 1529.79/s)  LR: 6.043e-04  Data: 0.013 (0.019)
Train: 130 [ 450/1251 ( 36%)]  Loss: 3.494 (3.58)  Time: 0.666s, 1536.96/s  (0.669s, 1530.17/s)  LR: 6.041e-04  Data: 0.015 (0.018)
Train: 130 [ 500/1251 ( 40%)]  Loss: 3.528 (3.58)  Time: 0.680s, 1505.46/s  (0.669s, 1530.05/s)  LR: 6.039e-04  Data: 0.013 (0.018)
Train: 130 [ 550/1251 ( 44%)]  Loss: 3.494 (3.57)  Time: 0.667s, 1535.91/s  (0.669s, 1529.92/s)  LR: 6.037e-04  Data: 0.016 (0.017)
Train: 130 [ 600/1251 ( 48%)]  Loss: 3.277 (3.55)  Time: 0.663s, 1545.20/s  (0.669s, 1529.89/s)  LR: 6.035e-04  Data: 0.014 (0.017)
Train: 130 [ 650/1251 ( 52%)]  Loss: 3.578 (3.55)  Time: 0.670s, 1527.53/s  (0.669s, 1530.20/s)  LR: 6.033e-04  Data: 0.014 (0.017)
Train: 130 [ 700/1251 ( 56%)]  Loss: 3.682 (3.56)  Time: 0.670s, 1527.58/s  (0.669s, 1529.79/s)  LR: 6.031e-04  Data: 0.012 (0.017)
Train: 130 [ 750/1251 ( 60%)]  Loss: 3.915 (3.58)  Time: 0.669s, 1529.63/s  (0.669s, 1529.53/s)  LR: 6.029e-04  Data: 0.015 (0.016)
Train: 130 [ 800/1251 ( 64%)]  Loss: 3.505 (3.58)  Time: 0.669s, 1530.05/s  (0.670s, 1529.32/s)  LR: 6.027e-04  Data: 0.013 (0.016)
Train: 130 [ 850/1251 ( 68%)]  Loss: 3.318 (3.56)  Time: 0.673s, 1521.48/s  (0.670s, 1529.29/s)  LR: 6.025e-04  Data: 0.016 (0.016)
Train: 130 [ 900/1251 ( 72%)]  Loss: 3.373 (3.55)  Time: 0.677s, 1513.36/s  (0.670s, 1529.35/s)  LR: 6.023e-04  Data: 0.013 (0.016)
Train: 130 [ 950/1251 ( 76%)]  Loss: 3.678 (3.56)  Time: 0.666s, 1538.27/s  (0.670s, 1529.28/s)  LR: 6.021e-04  Data: 0.016 (0.016)
Train: 130 [1000/1251 ( 80%)]  Loss: 3.361 (3.55)  Time: 0.659s, 1554.98/s  (0.670s, 1529.27/s)  LR: 6.019e-04  Data: 0.013 (0.016)
Train: 130 [1050/1251 ( 84%)]  Loss: 3.375 (3.54)  Time: 0.678s, 1509.90/s  (0.670s, 1529.06/s)  LR: 6.017e-04  Data: 0.015 (0.016)
Train: 130 [1100/1251 ( 88%)]  Loss: 3.482 (3.54)  Time: 0.670s, 1529.21/s  (0.670s, 1528.81/s)  LR: 6.014e-04  Data: 0.016 (0.016)
Train: 130 [1150/1251 ( 92%)]  Loss: 3.633 (3.54)  Time: 0.668s, 1533.08/s  (0.670s, 1528.70/s)  LR: 6.012e-04  Data: 0.013 (0.016)
Train: 130 [1200/1251 ( 96%)]  Loss: 3.681 (3.55)  Time: 0.672s, 1523.66/s  (0.670s, 1528.59/s)  LR: 6.010e-04  Data: 0.013 (0.015)
Train: 130 [1250/1251 (100%)]  Loss: 3.509 (3.55)  Time: 0.664s, 1542.10/s  (0.670s, 1528.65/s)  LR: 6.008e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.861 (2.861)  Loss:  0.4500 (0.4500)  Acc@1: 90.1367 (90.1367)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.164 (0.333)  Loss:  0.5859 (1.0146)  Acc@1: 85.3774 (75.8360)  Acc@5: 96.6981 (93.3320)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-129.pth.tar', 75.86400016113281)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-130.pth.tar', 75.83600011230469)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-127.pth.tar', 75.72199998291016)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-125.pth.tar', 75.59800010986328)

Train: 131 [   0/1251 (  0%)]  Loss: 3.567 (3.57)  Time: 4.057s,  252.38/s  (4.057s,  252.38/s)  LR: 6.008e-04  Data: 1.494 (1.494)
Train: 131 [  50/1251 (  4%)]  Loss: 3.565 (3.57)  Time: 0.654s, 1564.84/s  (0.694s, 1476.16/s)  LR: 6.006e-04  Data: 0.014 (0.043)
Train: 131 [ 100/1251 (  8%)]  Loss: 3.600 (3.58)  Time: 0.666s, 1537.44/s  (0.679s, 1507.93/s)  LR: 6.004e-04  Data: 0.016 (0.029)
Train: 131 [ 150/1251 ( 12%)]  Loss: 3.444 (3.54)  Time: 0.673s, 1522.50/s  (0.675s, 1517.16/s)  LR: 6.002e-04  Data: 0.012 (0.024)
Train: 131 [ 200/1251 ( 16%)]  Loss: 3.545 (3.54)  Time: 0.683s, 1500.36/s  (0.674s, 1519.55/s)  LR: 6.000e-04  Data: 0.013 (0.021)
Train: 131 [ 250/1251 ( 20%)]  Loss: 3.555 (3.55)  Time: 0.676s, 1515.08/s  (0.674s, 1519.91/s)  LR: 5.998e-04  Data: 0.013 (0.020)
Train: 131 [ 300/1251 ( 24%)]  Loss: 3.613 (3.56)  Time: 0.674s, 1520.16/s  (0.674s, 1520.20/s)  LR: 5.996e-04  Data: 0.016 (0.019)
Train: 131 [ 350/1251 ( 28%)]  Loss: 3.764 (3.58)  Time: 0.678s, 1510.00/s  (0.673s, 1521.40/s)  LR: 5.994e-04  Data: 0.012 (0.018)
Train: 131 [ 400/1251 ( 32%)]  Loss: 3.787 (3.60)  Time: 0.673s, 1520.63/s  (0.673s, 1521.68/s)  LR: 5.992e-04  Data: 0.013 (0.017)
Train: 131 [ 450/1251 ( 36%)]  Loss: 3.514 (3.60)  Time: 0.672s, 1523.25/s  (0.673s, 1521.66/s)  LR: 5.990e-04  Data: 0.013 (0.017)
Train: 131 [ 500/1251 ( 40%)]  Loss: 3.379 (3.58)  Time: 0.666s, 1538.55/s  (0.673s, 1522.42/s)  LR: 5.988e-04  Data: 0.013 (0.017)
Train: 131 [ 550/1251 ( 44%)]  Loss: 3.435 (3.56)  Time: 0.680s, 1505.22/s  (0.672s, 1522.89/s)  LR: 5.986e-04  Data: 0.016 (0.016)
Train: 131 [ 600/1251 ( 48%)]  Loss: 3.535 (3.56)  Time: 0.673s, 1520.55/s  (0.673s, 1522.67/s)  LR: 5.984e-04  Data: 0.012 (0.016)
Train: 131 [ 650/1251 ( 52%)]  Loss: 3.552 (3.56)  Time: 0.664s, 1542.85/s  (0.672s, 1522.78/s)  LR: 5.982e-04  Data: 0.015 (0.016)
Train: 131 [ 700/1251 ( 56%)]  Loss: 3.511 (3.56)  Time: 0.667s, 1534.80/s  (0.672s, 1522.97/s)  LR: 5.980e-04  Data: 0.015 (0.016)
Train: 131 [ 750/1251 ( 60%)]  Loss: 3.541 (3.56)  Time: 0.677s, 1511.65/s  (0.672s, 1523.29/s)  LR: 5.978e-04  Data: 0.013 (0.016)
Train: 131 [ 800/1251 ( 64%)]  Loss: 3.351 (3.54)  Time: 0.675s, 1517.44/s  (0.672s, 1523.30/s)  LR: 5.976e-04  Data: 0.013 (0.016)
Train: 131 [ 850/1251 ( 68%)]  Loss: 3.551 (3.55)  Time: 0.669s, 1531.69/s  (0.672s, 1523.41/s)  LR: 5.974e-04  Data: 0.013 (0.015)
Train: 131 [ 900/1251 ( 72%)]  Loss: 3.642 (3.55)  Time: 0.661s, 1550.09/s  (0.672s, 1523.78/s)  LR: 5.972e-04  Data: 0.013 (0.015)
Train: 131 [ 950/1251 ( 76%)]  Loss: 3.602 (3.55)  Time: 0.668s, 1532.00/s  (0.672s, 1523.65/s)  LR: 5.969e-04  Data: 0.012 (0.015)
Train: 131 [1000/1251 ( 80%)]  Loss: 3.688 (3.56)  Time: 0.672s, 1523.39/s  (0.672s, 1523.45/s)  LR: 5.967e-04  Data: 0.012 (0.015)
Train: 131 [1050/1251 ( 84%)]  Loss: 3.373 (3.55)  Time: 0.677s, 1513.39/s  (0.672s, 1523.42/s)  LR: 5.965e-04  Data: 0.014 (0.015)
Train: 131 [1100/1251 ( 88%)]  Loss: 3.595 (3.55)  Time: 0.666s, 1536.59/s  (0.672s, 1523.69/s)  LR: 5.963e-04  Data: 0.014 (0.015)
Train: 131 [1150/1251 ( 92%)]  Loss: 3.551 (3.55)  Time: 0.671s, 1526.35/s  (0.672s, 1523.71/s)  LR: 5.961e-04  Data: 0.013 (0.015)
Train: 131 [1200/1251 ( 96%)]  Loss: 3.756 (3.56)  Time: 0.666s, 1538.17/s  (0.672s, 1523.77/s)  LR: 5.959e-04  Data: 0.012 (0.015)
Train: 131 [1250/1251 (100%)]  Loss: 3.791 (3.57)  Time: 0.663s, 1544.56/s  (0.672s, 1523.94/s)  LR: 5.957e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.689 (2.689)  Loss:  0.5078 (0.5078)  Acc@1: 90.1367 (90.1367)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.161 (0.325)  Loss:  0.6152 (1.0367)  Acc@1: 86.3208 (75.8700)  Acc@5: 97.7594 (93.3160)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-131.pth.tar', 75.87000000488281)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-129.pth.tar', 75.86400016113281)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-130.pth.tar', 75.83600011230469)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-127.pth.tar', 75.72199998291016)

Train: 132 [   0/1251 (  0%)]  Loss: 3.584 (3.58)  Time: 3.957s,  258.76/s  (3.957s,  258.76/s)  LR: 5.957e-04  Data: 1.525 (1.525)
Train: 132 [  50/1251 (  4%)]  Loss: 3.670 (3.63)  Time: 0.654s, 1565.23/s  (0.699s, 1463.96/s)  LR: 5.955e-04  Data: 0.014 (0.043)
Train: 132 [ 100/1251 (  8%)]  Loss: 3.455 (3.57)  Time: 0.676s, 1514.69/s  (0.683s, 1498.76/s)  LR: 5.953e-04  Data: 0.015 (0.029)
Train: 132 [ 150/1251 ( 12%)]  Loss: 3.435 (3.54)  Time: 0.671s, 1527.03/s  (0.679s, 1507.28/s)  LR: 5.951e-04  Data: 0.015 (0.024)
Train: 132 [ 200/1251 ( 16%)]  Loss: 3.428 (3.51)  Time: 0.686s, 1492.26/s  (0.678s, 1510.35/s)  LR: 5.949e-04  Data: 0.015 (0.021)
Train: 132 [ 250/1251 ( 20%)]  Loss: 3.572 (3.52)  Time: 0.676s, 1515.28/s  (0.678s, 1510.58/s)  LR: 5.947e-04  Data: 0.013 (0.020)
Train: 132 [ 300/1251 ( 24%)]  Loss: 3.810 (3.56)  Time: 0.664s, 1541.27/s  (0.678s, 1511.42/s)  LR: 5.945e-04  Data: 0.015 (0.019)
Train: 132 [ 350/1251 ( 28%)]  Loss: 3.545 (3.56)  Time: 0.672s, 1523.79/s  (0.677s, 1512.18/s)  LR: 5.943e-04  Data: 0.013 (0.018)
Train: 132 [ 400/1251 ( 32%)]  Loss: 3.422 (3.55)  Time: 0.675s, 1517.87/s  (0.677s, 1513.14/s)  LR: 5.941e-04  Data: 0.014 (0.017)
Train: 132 [ 450/1251 ( 36%)]  Loss: 3.472 (3.54)  Time: 0.669s, 1529.57/s  (0.676s, 1513.98/s)  LR: 5.939e-04  Data: 0.013 (0.017)
Train: 132 [ 500/1251 ( 40%)]  Loss: 3.689 (3.55)  Time: 0.673s, 1521.73/s  (0.676s, 1514.72/s)  LR: 5.937e-04  Data: 0.013 (0.017)
Train: 132 [ 550/1251 ( 44%)]  Loss: 3.399 (3.54)  Time: 0.663s, 1545.59/s  (0.676s, 1515.47/s)  LR: 5.935e-04  Data: 0.013 (0.016)
Train: 132 [ 600/1251 ( 48%)]  Loss: 3.629 (3.55)  Time: 0.665s, 1540.59/s  (0.675s, 1516.42/s)  LR: 5.933e-04  Data: 0.014 (0.016)
Train: 132 [ 650/1251 ( 52%)]  Loss: 3.418 (3.54)  Time: 0.684s, 1497.65/s  (0.675s, 1517.08/s)  LR: 5.931e-04  Data: 0.013 (0.016)
Train: 132 [ 700/1251 ( 56%)]  Loss: 3.515 (3.54)  Time: 0.681s, 1502.75/s  (0.675s, 1517.41/s)  LR: 5.929e-04  Data: 0.013 (0.016)
Train: 132 [ 750/1251 ( 60%)]  Loss: 3.359 (3.53)  Time: 0.672s, 1523.12/s  (0.675s, 1517.62/s)  LR: 5.926e-04  Data: 0.012 (0.016)
Train: 132 [ 800/1251 ( 64%)]  Loss: 3.841 (3.54)  Time: 0.678s, 1509.51/s  (0.675s, 1517.73/s)  LR: 5.924e-04  Data: 0.013 (0.016)
Train: 132 [ 850/1251 ( 68%)]  Loss: 3.511 (3.54)  Time: 0.678s, 1510.41/s  (0.675s, 1517.98/s)  LR: 5.922e-04  Data: 0.015 (0.015)
Train: 132 [ 900/1251 ( 72%)]  Loss: 3.602 (3.55)  Time: 0.678s, 1511.42/s  (0.674s, 1518.26/s)  LR: 5.920e-04  Data: 0.014 (0.015)
Train: 132 [ 950/1251 ( 76%)]  Loss: 3.422 (3.54)  Time: 0.674s, 1519.73/s  (0.674s, 1518.34/s)  LR: 5.918e-04  Data: 0.014 (0.015)
Train: 132 [1000/1251 ( 80%)]  Loss: 3.386 (3.53)  Time: 0.669s, 1530.58/s  (0.674s, 1518.45/s)  LR: 5.916e-04  Data: 0.013 (0.015)
Train: 132 [1050/1251 ( 84%)]  Loss: 3.312 (3.52)  Time: 0.673s, 1520.46/s  (0.674s, 1518.71/s)  LR: 5.914e-04  Data: 0.014 (0.015)
Train: 132 [1100/1251 ( 88%)]  Loss: 3.448 (3.52)  Time: 0.660s, 1552.33/s  (0.674s, 1519.21/s)  LR: 5.912e-04  Data: 0.013 (0.015)
Train: 132 [1150/1251 ( 92%)]  Loss: 3.334 (3.51)  Time: 0.680s, 1504.90/s  (0.674s, 1519.68/s)  LR: 5.910e-04  Data: 0.013 (0.015)
Train: 132 [1200/1251 ( 96%)]  Loss: 3.947 (3.53)  Time: 0.677s, 1512.06/s  (0.674s, 1520.10/s)  LR: 5.908e-04  Data: 0.013 (0.015)
Train: 132 [1250/1251 (100%)]  Loss: 3.759 (3.54)  Time: 0.660s, 1550.58/s  (0.673s, 1520.53/s)  LR: 5.906e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.956 (2.956)  Loss:  0.5137 (0.5137)  Acc@1: 89.9414 (89.9414)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.168 (0.325)  Loss:  0.6064 (1.0141)  Acc@1: 85.1415 (76.0300)  Acc@5: 96.5802 (93.3720)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-132.pth.tar', 76.03000000976563)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-131.pth.tar', 75.87000000488281)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-129.pth.tar', 75.86400016113281)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-130.pth.tar', 75.83600011230469)

Train: 133 [   0/1251 (  0%)]  Loss: 3.399 (3.40)  Time: 3.426s,  298.91/s  (3.426s,  298.91/s)  LR: 5.906e-04  Data: 1.575 (1.575)
Train: 133 [  50/1251 (  4%)]  Loss: 3.323 (3.36)  Time: 0.659s, 1553.73/s  (0.686s, 1493.58/s)  LR: 5.904e-04  Data: 0.013 (0.045)
Train: 133 [ 100/1251 (  8%)]  Loss: 3.315 (3.35)  Time: 0.661s, 1548.19/s  (0.672s, 1523.23/s)  LR: 5.902e-04  Data: 0.013 (0.029)
Train: 133 [ 150/1251 ( 12%)]  Loss: 3.432 (3.37)  Time: 0.667s, 1535.90/s  (0.669s, 1531.37/s)  LR: 5.900e-04  Data: 0.013 (0.024)
Train: 133 [ 200/1251 ( 16%)]  Loss: 3.310 (3.36)  Time: 0.660s, 1552.13/s  (0.668s, 1533.91/s)  LR: 5.898e-04  Data: 0.014 (0.021)
Train: 133 [ 250/1251 ( 20%)]  Loss: 3.769 (3.42)  Time: 0.670s, 1527.47/s  (0.667s, 1534.56/s)  LR: 5.896e-04  Data: 0.012 (0.020)
Train: 133 [ 300/1251 ( 24%)]  Loss: 3.259 (3.40)  Time: 0.660s, 1552.41/s  (0.667s, 1534.46/s)  LR: 5.894e-04  Data: 0.013 (0.019)
Train: 133 [ 350/1251 ( 28%)]  Loss: 3.477 (3.41)  Time: 0.666s, 1537.99/s  (0.667s, 1534.47/s)  LR: 5.892e-04  Data: 0.013 (0.018)
Train: 133 [ 400/1251 ( 32%)]  Loss: 3.642 (3.44)  Time: 0.664s, 1543.16/s  (0.667s, 1534.13/s)  LR: 5.890e-04  Data: 0.012 (0.018)
Train: 133 [ 450/1251 ( 36%)]  Loss: 3.747 (3.47)  Time: 0.666s, 1536.98/s  (0.668s, 1533.96/s)  LR: 5.888e-04  Data: 0.015 (0.017)
Train: 133 [ 500/1251 ( 40%)]  Loss: 3.565 (3.48)  Time: 0.672s, 1524.16/s  (0.668s, 1533.72/s)  LR: 5.885e-04  Data: 0.012 (0.017)
Train: 133 [ 550/1251 ( 44%)]  Loss: 3.440 (3.47)  Time: 0.681s, 1502.82/s  (0.668s, 1533.63/s)  LR: 5.883e-04  Data: 0.018 (0.016)
Train: 133 [ 600/1251 ( 48%)]  Loss: 3.700 (3.49)  Time: 0.666s, 1537.69/s  (0.668s, 1533.49/s)  LR: 5.881e-04  Data: 0.014 (0.016)
Train: 133 [ 650/1251 ( 52%)]  Loss: 3.450 (3.49)  Time: 0.665s, 1539.21/s  (0.668s, 1533.66/s)  LR: 5.879e-04  Data: 0.012 (0.016)
Train: 133 [ 700/1251 ( 56%)]  Loss: 3.413 (3.48)  Time: 0.674s, 1519.67/s  (0.668s, 1533.69/s)  LR: 5.877e-04  Data: 0.013 (0.016)
Train: 133 [ 750/1251 ( 60%)]  Loss: 3.808 (3.50)  Time: 0.665s, 1538.81/s  (0.668s, 1533.46/s)  LR: 5.875e-04  Data: 0.014 (0.016)
Train: 133 [ 800/1251 ( 64%)]  Loss: 3.420 (3.50)  Time: 0.669s, 1531.05/s  (0.668s, 1533.44/s)  LR: 5.873e-04  Data: 0.013 (0.016)
Train: 133 [ 850/1251 ( 68%)]  Loss: 3.520 (3.50)  Time: 0.671s, 1526.39/s  (0.668s, 1533.20/s)  LR: 5.871e-04  Data: 0.014 (0.015)
Train: 133 [ 900/1251 ( 72%)]  Loss: 3.268 (3.49)  Time: 0.677s, 1513.05/s  (0.668s, 1532.85/s)  LR: 5.869e-04  Data: 0.013 (0.015)
Train: 133 [ 950/1251 ( 76%)]  Loss: 3.725 (3.50)  Time: 0.672s, 1523.10/s  (0.668s, 1532.24/s)  LR: 5.867e-04  Data: 0.017 (0.015)
Train: 133 [1000/1251 ( 80%)]  Loss: 3.465 (3.50)  Time: 0.674s, 1519.85/s  (0.668s, 1531.82/s)  LR: 5.865e-04  Data: 0.013 (0.015)
Train: 133 [1050/1251 ( 84%)]  Loss: 3.520 (3.50)  Time: 0.671s, 1527.18/s  (0.669s, 1531.68/s)  LR: 5.863e-04  Data: 0.013 (0.015)
Train: 133 [1100/1251 ( 88%)]  Loss: 3.610 (3.50)  Time: 0.670s, 1528.93/s  (0.669s, 1531.43/s)  LR: 5.861e-04  Data: 0.013 (0.015)
Train: 133 [1150/1251 ( 92%)]  Loss: 3.496 (3.50)  Time: 0.670s, 1528.44/s  (0.669s, 1531.27/s)  LR: 5.859e-04  Data: 0.014 (0.015)
Train: 133 [1200/1251 ( 96%)]  Loss: 3.568 (3.51)  Time: 0.677s, 1513.21/s  (0.669s, 1531.31/s)  LR: 5.857e-04  Data: 0.012 (0.015)
Train: 133 [1250/1251 (100%)]  Loss: 3.278 (3.50)  Time: 0.657s, 1558.21/s  (0.669s, 1531.36/s)  LR: 5.855e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.872 (2.872)  Loss:  0.4517 (0.4517)  Acc@1: 90.2344 (90.2344)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.167 (0.319)  Loss:  0.6035 (1.0150)  Acc@1: 85.1415 (75.8720)  Acc@5: 96.9340 (93.3240)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-132.pth.tar', 76.03000000976563)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-133.pth.tar', 75.87200000976563)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-131.pth.tar', 75.87000000488281)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-129.pth.tar', 75.86400016113281)

Train: 134 [   0/1251 (  0%)]  Loss: 3.382 (3.38)  Time: 4.054s,  252.61/s  (4.054s,  252.61/s)  LR: 5.855e-04  Data: 1.795 (1.795)
Train: 134 [  50/1251 (  4%)]  Loss: 3.580 (3.48)  Time: 0.648s, 1580.32/s  (0.700s, 1462.13/s)  LR: 5.853e-04  Data: 0.016 (0.049)
Train: 134 [ 100/1251 (  8%)]  Loss: 3.530 (3.50)  Time: 0.659s, 1552.83/s  (0.680s, 1505.37/s)  LR: 5.851e-04  Data: 0.013 (0.031)
Train: 134 [ 150/1251 ( 12%)]  Loss: 3.591 (3.52)  Time: 0.665s, 1540.97/s  (0.675s, 1517.49/s)  LR: 5.848e-04  Data: 0.014 (0.025)
Train: 134 [ 200/1251 ( 16%)]  Loss: 3.457 (3.51)  Time: 0.669s, 1531.14/s  (0.673s, 1522.66/s)  LR: 5.846e-04  Data: 0.014 (0.023)
Train: 134 [ 250/1251 ( 20%)]  Loss: 3.594 (3.52)  Time: 0.670s, 1527.73/s  (0.672s, 1524.61/s)  LR: 5.844e-04  Data: 0.012 (0.021)
Train: 134 [ 300/1251 ( 24%)]  Loss: 3.563 (3.53)  Time: 0.667s, 1534.76/s  (0.671s, 1526.14/s)  LR: 5.842e-04  Data: 0.013 (0.020)
Train: 134 [ 350/1251 ( 28%)]  Loss: 3.288 (3.50)  Time: 0.667s, 1534.28/s  (0.671s, 1526.73/s)  LR: 5.840e-04  Data: 0.016 (0.019)
Train: 134 [ 400/1251 ( 32%)]  Loss: 3.617 (3.51)  Time: 0.678s, 1511.11/s  (0.671s, 1526.59/s)  LR: 5.838e-04  Data: 0.013 (0.018)
Train: 134 [ 450/1251 ( 36%)]  Loss: 3.471 (3.51)  Time: 0.672s, 1524.83/s  (0.671s, 1526.37/s)  LR: 5.836e-04  Data: 0.013 (0.018)
Train: 134 [ 500/1251 ( 40%)]  Loss: 3.733 (3.53)  Time: 0.657s, 1558.63/s  (0.671s, 1526.52/s)  LR: 5.834e-04  Data: 0.012 (0.017)
Train: 134 [ 550/1251 ( 44%)]  Loss: 3.672 (3.54)  Time: 0.669s, 1530.87/s  (0.671s, 1526.56/s)  LR: 5.832e-04  Data: 0.015 (0.017)
Train: 134 [ 600/1251 ( 48%)]  Loss: 3.592 (3.54)  Time: 0.675s, 1517.50/s  (0.671s, 1526.44/s)  LR: 5.830e-04  Data: 0.013 (0.017)
Train: 134 [ 650/1251 ( 52%)]  Loss: 3.702 (3.56)  Time: 0.663s, 1543.93/s  (0.671s, 1526.22/s)  LR: 5.828e-04  Data: 0.014 (0.016)
Train: 134 [ 700/1251 ( 56%)]  Loss: 3.509 (3.55)  Time: 0.665s, 1539.27/s  (0.671s, 1525.99/s)  LR: 5.826e-04  Data: 0.015 (0.016)
Train: 134 [ 750/1251 ( 60%)]  Loss: 3.615 (3.56)  Time: 0.675s, 1517.54/s  (0.671s, 1525.70/s)  LR: 5.824e-04  Data: 0.012 (0.016)
Train: 134 [ 800/1251 ( 64%)]  Loss: 3.670 (3.56)  Time: 0.682s, 1500.66/s  (0.671s, 1525.67/s)  LR: 5.822e-04  Data: 0.014 (0.016)
Train: 134 [ 850/1251 ( 68%)]  Loss: 3.240 (3.54)  Time: 0.675s, 1517.82/s  (0.671s, 1525.56/s)  LR: 5.820e-04  Data: 0.013 (0.016)
Train: 134 [ 900/1251 ( 72%)]  Loss: 3.681 (3.55)  Time: 0.668s, 1532.00/s  (0.671s, 1525.54/s)  LR: 5.818e-04  Data: 0.013 (0.016)
Train: 134 [ 950/1251 ( 76%)]  Loss: 3.362 (3.54)  Time: 0.672s, 1523.13/s  (0.671s, 1525.55/s)  LR: 5.816e-04  Data: 0.014 (0.016)
Train: 134 [1000/1251 ( 80%)]  Loss: 3.602 (3.55)  Time: 0.669s, 1530.31/s  (0.671s, 1525.33/s)  LR: 5.814e-04  Data: 0.013 (0.015)
Train: 134 [1050/1251 ( 84%)]  Loss: 3.533 (3.54)  Time: 0.671s, 1526.73/s  (0.671s, 1525.23/s)  LR: 5.811e-04  Data: 0.013 (0.015)
Train: 134 [1100/1251 ( 88%)]  Loss: 3.459 (3.54)  Time: 0.654s, 1566.23/s  (0.671s, 1525.10/s)  LR: 5.809e-04  Data: 0.013 (0.015)
Train: 134 [1150/1251 ( 92%)]  Loss: 3.655 (3.55)  Time: 0.674s, 1519.50/s  (0.671s, 1525.10/s)  LR: 5.807e-04  Data: 0.017 (0.015)
Train: 134 [1200/1251 ( 96%)]  Loss: 3.658 (3.55)  Time: 0.675s, 1517.72/s  (0.671s, 1525.21/s)  LR: 5.805e-04  Data: 0.013 (0.015)
Train: 134 [1250/1251 (100%)]  Loss: 3.365 (3.54)  Time: 0.662s, 1547.96/s  (0.671s, 1525.48/s)  LR: 5.803e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.746 (2.746)  Loss:  0.4866 (0.4866)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.166 (0.324)  Loss:  0.5698 (1.0023)  Acc@1: 86.4387 (76.1440)  Acc@5: 97.2877 (93.3780)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-134.pth.tar', 76.14399992675781)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-132.pth.tar', 76.03000000976563)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-133.pth.tar', 75.87200000976563)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-131.pth.tar', 75.87000000488281)

Train: 135 [   0/1251 (  0%)]  Loss: 3.341 (3.34)  Time: 4.054s,  252.59/s  (4.054s,  252.59/s)  LR: 5.803e-04  Data: 1.773 (1.773)
Train: 135 [  50/1251 (  4%)]  Loss: 3.268 (3.30)  Time: 0.652s, 1569.68/s  (0.702s, 1458.67/s)  LR: 5.801e-04  Data: 0.013 (0.048)
Train: 135 [ 100/1251 (  8%)]  Loss: 3.552 (3.39)  Time: 0.668s, 1531.83/s  (0.683s, 1500.12/s)  LR: 5.799e-04  Data: 0.013 (0.031)
Train: 135 [ 150/1251 ( 12%)]  Loss: 3.040 (3.30)  Time: 0.659s, 1553.33/s  (0.677s, 1512.45/s)  LR: 5.797e-04  Data: 0.014 (0.025)
Train: 135 [ 200/1251 ( 16%)]  Loss: 3.243 (3.29)  Time: 0.674s, 1520.36/s  (0.674s, 1518.47/s)  LR: 5.795e-04  Data: 0.013 (0.023)
Train: 135 [ 250/1251 ( 20%)]  Loss: 3.542 (3.33)  Time: 0.664s, 1541.99/s  (0.673s, 1521.73/s)  LR: 5.793e-04  Data: 0.013 (0.021)
Train: 135 [ 300/1251 ( 24%)]  Loss: 3.520 (3.36)  Time: 0.672s, 1523.05/s  (0.672s, 1522.93/s)  LR: 5.791e-04  Data: 0.015 (0.020)
Train: 135 [ 350/1251 ( 28%)]  Loss: 3.601 (3.39)  Time: 0.679s, 1507.03/s  (0.672s, 1523.53/s)  LR: 5.789e-04  Data: 0.015 (0.019)
Train: 135 [ 400/1251 ( 32%)]  Loss: 3.293 (3.38)  Time: 0.670s, 1529.31/s  (0.672s, 1524.81/s)  LR: 5.787e-04  Data: 0.013 (0.018)
Train: 135 [ 450/1251 ( 36%)]  Loss: 3.598 (3.40)  Time: 0.667s, 1536.27/s  (0.671s, 1525.43/s)  LR: 5.785e-04  Data: 0.014 (0.018)
Train: 135 [ 500/1251 ( 40%)]  Loss: 3.890 (3.44)  Time: 0.670s, 1529.40/s  (0.671s, 1525.87/s)  LR: 5.783e-04  Data: 0.012 (0.017)
Train: 135 [ 550/1251 ( 44%)]  Loss: 3.227 (3.43)  Time: 0.672s, 1523.50/s  (0.671s, 1526.14/s)  LR: 5.781e-04  Data: 0.014 (0.017)
Train: 135 [ 600/1251 ( 48%)]  Loss: 3.652 (3.44)  Time: 0.668s, 1533.25/s  (0.671s, 1526.19/s)  LR: 5.779e-04  Data: 0.013 (0.017)
Train: 135 [ 650/1251 ( 52%)]  Loss: 3.379 (3.44)  Time: 0.669s, 1531.16/s  (0.671s, 1526.32/s)  LR: 5.776e-04  Data: 0.013 (0.017)
Train: 135 [ 700/1251 ( 56%)]  Loss: 3.570 (3.45)  Time: 0.672s, 1524.08/s  (0.671s, 1526.53/s)  LR: 5.774e-04  Data: 0.013 (0.016)
Train: 135 [ 750/1251 ( 60%)]  Loss: 3.397 (3.44)  Time: 0.670s, 1527.60/s  (0.671s, 1526.71/s)  LR: 5.772e-04  Data: 0.012 (0.016)
Train: 135 [ 800/1251 ( 64%)]  Loss: 3.509 (3.45)  Time: 0.674s, 1519.75/s  (0.671s, 1526.85/s)  LR: 5.770e-04  Data: 0.012 (0.016)
Train: 135 [ 850/1251 ( 68%)]  Loss: 3.436 (3.45)  Time: 0.678s, 1510.96/s  (0.671s, 1527.10/s)  LR: 5.768e-04  Data: 0.013 (0.016)
Train: 135 [ 900/1251 ( 72%)]  Loss: 3.512 (3.45)  Time: 0.662s, 1546.32/s  (0.670s, 1527.61/s)  LR: 5.766e-04  Data: 0.013 (0.016)
Train: 135 [ 950/1251 ( 76%)]  Loss: 3.587 (3.46)  Time: 0.670s, 1528.73/s  (0.670s, 1527.81/s)  LR: 5.764e-04  Data: 0.020 (0.016)
Train: 135 [1000/1251 ( 80%)]  Loss: 3.848 (3.48)  Time: 0.663s, 1544.36/s  (0.670s, 1528.01/s)  LR: 5.762e-04  Data: 0.013 (0.015)
Train: 135 [1050/1251 ( 84%)]  Loss: 3.403 (3.47)  Time: 0.673s, 1522.51/s  (0.670s, 1528.35/s)  LR: 5.760e-04  Data: 0.013 (0.015)
Train: 135 [1100/1251 ( 88%)]  Loss: 3.613 (3.48)  Time: 0.671s, 1525.27/s  (0.670s, 1528.46/s)  LR: 5.758e-04  Data: 0.013 (0.015)
Train: 135 [1150/1251 ( 92%)]  Loss: 3.502 (3.48)  Time: 0.674s, 1519.19/s  (0.670s, 1528.56/s)  LR: 5.756e-04  Data: 0.012 (0.015)
Train: 135 [1200/1251 ( 96%)]  Loss: 3.419 (3.48)  Time: 0.673s, 1521.32/s  (0.670s, 1528.55/s)  LR: 5.754e-04  Data: 0.012 (0.015)
Train: 135 [1250/1251 (100%)]  Loss: 3.453 (3.48)  Time: 0.665s, 1538.93/s  (0.670s, 1528.57/s)  LR: 5.752e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.965 (2.965)  Loss:  0.4995 (0.4995)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.162 (0.325)  Loss:  0.5957 (1.0248)  Acc@1: 85.4953 (76.0720)  Acc@5: 97.2877 (93.4260)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-134.pth.tar', 76.14399992675781)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-135.pth.tar', 76.07200003417968)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-132.pth.tar', 76.03000000976563)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-133.pth.tar', 75.87200000976563)

Train: 136 [   0/1251 (  0%)]  Loss: 3.413 (3.41)  Time: 4.519s,  226.61/s  (4.519s,  226.61/s)  LR: 5.752e-04  Data: 1.969 (1.969)
Train: 136 [  50/1251 (  4%)]  Loss: 3.538 (3.48)  Time: 0.655s, 1563.33/s  (0.707s, 1447.77/s)  LR: 5.750e-04  Data: 0.013 (0.053)
Train: 136 [ 100/1251 (  8%)]  Loss: 3.601 (3.52)  Time: 0.655s, 1563.90/s  (0.685s, 1494.92/s)  LR: 5.748e-04  Data: 0.013 (0.033)
Train: 136 [ 150/1251 ( 12%)]  Loss: 3.554 (3.53)  Time: 0.644s, 1590.46/s  (0.678s, 1511.26/s)  LR: 5.746e-04  Data: 0.014 (0.027)
Train: 136 [ 200/1251 ( 16%)]  Loss: 3.801 (3.58)  Time: 0.662s, 1547.87/s  (0.674s, 1518.79/s)  LR: 5.743e-04  Data: 0.014 (0.024)
Train: 136 [ 250/1251 ( 20%)]  Loss: 3.343 (3.54)  Time: 0.667s, 1534.62/s  (0.673s, 1521.77/s)  LR: 5.741e-04  Data: 0.013 (0.022)
Train: 136 [ 300/1251 ( 24%)]  Loss: 3.649 (3.56)  Time: 0.663s, 1544.76/s  (0.672s, 1524.86/s)  LR: 5.739e-04  Data: 0.016 (0.020)
Train: 136 [ 350/1251 ( 28%)]  Loss: 3.909 (3.60)  Time: 0.665s, 1539.35/s  (0.671s, 1526.21/s)  LR: 5.737e-04  Data: 0.014 (0.019)
Train: 136 [ 400/1251 ( 32%)]  Loss: 3.469 (3.59)  Time: 0.673s, 1520.79/s  (0.670s, 1527.43/s)  LR: 5.735e-04  Data: 0.013 (0.019)
Train: 136 [ 450/1251 ( 36%)]  Loss: 3.651 (3.59)  Time: 0.670s, 1528.78/s  (0.670s, 1528.32/s)  LR: 5.733e-04  Data: 0.014 (0.018)
Train: 136 [ 500/1251 ( 40%)]  Loss: 3.464 (3.58)  Time: 0.666s, 1538.25/s  (0.670s, 1529.18/s)  LR: 5.731e-04  Data: 0.014 (0.018)
Train: 136 [ 550/1251 ( 44%)]  Loss: 3.528 (3.58)  Time: 0.671s, 1526.20/s  (0.669s, 1529.82/s)  LR: 5.729e-04  Data: 0.015 (0.017)
Train: 136 [ 600/1251 ( 48%)]  Loss: 3.522 (3.57)  Time: 0.661s, 1549.88/s  (0.669s, 1530.46/s)  LR: 5.727e-04  Data: 0.018 (0.017)
Train: 136 [ 650/1251 ( 52%)]  Loss: 3.464 (3.56)  Time: 0.667s, 1535.32/s  (0.669s, 1531.07/s)  LR: 5.725e-04  Data: 0.018 (0.017)
Train: 136 [ 700/1251 ( 56%)]  Loss: 3.806 (3.58)  Time: 0.671s, 1525.04/s  (0.669s, 1531.53/s)  LR: 5.723e-04  Data: 0.013 (0.017)
Train: 136 [ 750/1251 ( 60%)]  Loss: 3.301 (3.56)  Time: 0.668s, 1533.44/s  (0.669s, 1531.78/s)  LR: 5.721e-04  Data: 0.014 (0.016)
Train: 136 [ 800/1251 ( 64%)]  Loss: 3.537 (3.56)  Time: 0.664s, 1541.99/s  (0.668s, 1532.08/s)  LR: 5.719e-04  Data: 0.013 (0.016)
Train: 136 [ 850/1251 ( 68%)]  Loss: 3.203 (3.54)  Time: 0.677s, 1513.09/s  (0.668s, 1532.48/s)  LR: 5.717e-04  Data: 0.014 (0.016)
Train: 136 [ 900/1251 ( 72%)]  Loss: 3.711 (3.55)  Time: 0.678s, 1510.45/s  (0.668s, 1532.85/s)  LR: 5.715e-04  Data: 0.016 (0.016)
Train: 136 [ 950/1251 ( 76%)]  Loss: 3.715 (3.56)  Time: 0.663s, 1545.37/s  (0.668s, 1532.96/s)  LR: 5.713e-04  Data: 0.013 (0.016)
Train: 136 [1000/1251 ( 80%)]  Loss: 3.206 (3.54)  Time: 0.671s, 1526.10/s  (0.668s, 1533.16/s)  LR: 5.710e-04  Data: 0.013 (0.016)
Train: 136 [1050/1251 ( 84%)]  Loss: 3.376 (3.53)  Time: 0.670s, 1528.38/s  (0.668s, 1533.17/s)  LR: 5.708e-04  Data: 0.014 (0.016)
Train: 136 [1100/1251 ( 88%)]  Loss: 3.513 (3.53)  Time: 0.667s, 1534.37/s  (0.668s, 1533.10/s)  LR: 5.706e-04  Data: 0.013 (0.015)
Train: 136 [1150/1251 ( 92%)]  Loss: 3.658 (3.54)  Time: 0.666s, 1538.62/s  (0.668s, 1533.02/s)  LR: 5.704e-04  Data: 0.013 (0.015)
Train: 136 [1200/1251 ( 96%)]  Loss: 3.136 (3.52)  Time: 0.668s, 1533.15/s  (0.668s, 1532.90/s)  LR: 5.702e-04  Data: 0.016 (0.015)
Train: 136 [1250/1251 (100%)]  Loss: 3.494 (3.52)  Time: 0.652s, 1570.64/s  (0.668s, 1532.89/s)  LR: 5.700e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.949 (2.949)  Loss:  0.4526 (0.4526)  Acc@1: 90.2344 (90.2344)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.164 (0.324)  Loss:  0.5405 (1.0108)  Acc@1: 86.3208 (75.8240)  Acc@5: 97.1698 (93.4260)
Train: 137 [   0/1251 (  0%)]  Loss: 3.539 (3.54)  Time: 4.224s,  242.40/s  (4.224s,  242.40/s)  LR: 5.700e-04  Data: 1.581 (1.581)
Train: 137 [  50/1251 (  4%)]  Loss: 3.694 (3.62)  Time: 0.665s, 1540.57/s  (0.707s, 1449.02/s)  LR: 5.698e-04  Data: 0.013 (0.044)
Train: 137 [ 100/1251 (  8%)]  Loss: 3.387 (3.54)  Time: 0.667s, 1534.66/s  (0.686s, 1492.02/s)  LR: 5.696e-04  Data: 0.013 (0.029)
Train: 137 [ 150/1251 ( 12%)]  Loss: 3.075 (3.42)  Time: 0.662s, 1547.66/s  (0.680s, 1505.03/s)  LR: 5.694e-04  Data: 0.013 (0.024)
Train: 137 [ 200/1251 ( 16%)]  Loss: 3.731 (3.49)  Time: 0.673s, 1520.61/s  (0.678s, 1510.13/s)  LR: 5.692e-04  Data: 0.014 (0.021)
Train: 137 [ 250/1251 ( 20%)]  Loss: 3.778 (3.53)  Time: 0.679s, 1508.60/s  (0.677s, 1513.37/s)  LR: 5.690e-04  Data: 0.013 (0.020)
Train: 137 [ 300/1251 ( 24%)]  Loss: 3.558 (3.54)  Time: 0.675s, 1516.83/s  (0.676s, 1515.69/s)  LR: 5.688e-04  Data: 0.019 (0.019)
Train: 137 [ 350/1251 ( 28%)]  Loss: 3.476 (3.53)  Time: 0.669s, 1530.23/s  (0.675s, 1517.63/s)  LR: 5.686e-04  Data: 0.012 (0.018)
Train: 137 [ 400/1251 ( 32%)]  Loss: 3.502 (3.53)  Time: 0.665s, 1540.39/s  (0.674s, 1518.66/s)  LR: 5.684e-04  Data: 0.014 (0.017)
Train: 137 [ 450/1251 ( 36%)]  Loss: 3.585 (3.53)  Time: 0.666s, 1537.81/s  (0.674s, 1520.13/s)  LR: 5.682e-04  Data: 0.015 (0.017)
Train: 137 [ 500/1251 ( 40%)]  Loss: 3.693 (3.55)  Time: 0.666s, 1536.60/s  (0.673s, 1520.86/s)  LR: 5.680e-04  Data: 0.014 (0.017)
Train: 137 [ 550/1251 ( 44%)]  Loss: 3.413 (3.54)  Time: 0.683s, 1498.83/s  (0.673s, 1520.55/s)  LR: 5.677e-04  Data: 0.012 (0.016)
Train: 137 [ 600/1251 ( 48%)]  Loss: 3.363 (3.52)  Time: 0.665s, 1540.07/s  (0.674s, 1520.39/s)  LR: 5.675e-04  Data: 0.017 (0.016)
Train: 137 [ 650/1251 ( 52%)]  Loss: 3.369 (3.51)  Time: 0.678s, 1509.68/s  (0.674s, 1520.39/s)  LR: 5.673e-04  Data: 0.013 (0.016)
Train: 137 [ 700/1251 ( 56%)]  Loss: 3.551 (3.51)  Time: 0.674s, 1519.70/s  (0.673s, 1520.58/s)  LR: 5.671e-04  Data: 0.013 (0.016)
Train: 137 [ 750/1251 ( 60%)]  Loss: 3.696 (3.53)  Time: 0.683s, 1500.25/s  (0.673s, 1520.93/s)  LR: 5.669e-04  Data: 0.012 (0.016)
Train: 137 [ 800/1251 ( 64%)]  Loss: 3.613 (3.53)  Time: 0.667s, 1534.95/s  (0.673s, 1521.27/s)  LR: 5.667e-04  Data: 0.013 (0.015)
Train: 137 [ 850/1251 ( 68%)]  Loss: 3.307 (3.52)  Time: 0.672s, 1523.35/s  (0.673s, 1521.88/s)  LR: 5.665e-04  Data: 0.012 (0.015)
Train: 137 [ 900/1251 ( 72%)]  Loss: 3.651 (3.53)  Time: 0.651s, 1572.81/s  (0.672s, 1522.69/s)  LR: 5.663e-04  Data: 0.013 (0.015)
Train: 137 [ 950/1251 ( 76%)]  Loss: 3.248 (3.51)  Time: 0.664s, 1542.01/s  (0.672s, 1523.31/s)  LR: 5.661e-04  Data: 0.013 (0.015)
Train: 137 [1000/1251 ( 80%)]  Loss: 3.198 (3.50)  Time: 0.667s, 1536.28/s  (0.672s, 1523.90/s)  LR: 5.659e-04  Data: 0.014 (0.015)
Train: 137 [1050/1251 ( 84%)]  Loss: 3.220 (3.48)  Time: 0.667s, 1535.65/s  (0.672s, 1524.24/s)  LR: 5.657e-04  Data: 0.018 (0.015)
Train: 137 [1100/1251 ( 88%)]  Loss: 3.125 (3.47)  Time: 0.670s, 1527.78/s  (0.672s, 1524.39/s)  LR: 5.655e-04  Data: 0.012 (0.015)
Train: 137 [1150/1251 ( 92%)]  Loss: 3.042 (3.45)  Time: 0.663s, 1545.42/s  (0.672s, 1524.67/s)  LR: 5.653e-04  Data: 0.020 (0.015)
Train: 137 [1200/1251 ( 96%)]  Loss: 3.469 (3.45)  Time: 0.659s, 1553.65/s  (0.671s, 1524.97/s)  LR: 5.651e-04  Data: 0.014 (0.015)
Train: 137 [1250/1251 (100%)]  Loss: 3.430 (3.45)  Time: 0.650s, 1574.81/s  (0.671s, 1525.53/s)  LR: 5.649e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.884 (2.884)  Loss:  0.4360 (0.4360)  Acc@1: 90.7227 (90.7227)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.161 (0.330)  Loss:  0.5547 (0.9946)  Acc@1: 86.3208 (76.2020)  Acc@5: 97.9953 (93.4600)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-137.pth.tar', 76.20200013427734)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-134.pth.tar', 76.14399992675781)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-135.pth.tar', 76.07200003417968)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-132.pth.tar', 76.03000000976563)

Train: 138 [   0/1251 (  0%)]  Loss: 3.691 (3.69)  Time: 3.846s,  266.24/s  (3.846s,  266.24/s)  LR: 5.648e-04  Data: 2.118 (2.118)
Train: 138 [  50/1251 (  4%)]  Loss: 3.113 (3.40)  Time: 0.649s, 1578.99/s  (0.691s, 1482.29/s)  LR: 5.646e-04  Data: 0.012 (0.056)
Train: 138 [ 100/1251 (  8%)]  Loss: 3.676 (3.49)  Time: 0.667s, 1534.64/s  (0.676s, 1514.98/s)  LR: 5.644e-04  Data: 0.013 (0.035)
Train: 138 [ 150/1251 ( 12%)]  Loss: 3.554 (3.51)  Time: 0.681s, 1504.53/s  (0.673s, 1521.49/s)  LR: 5.642e-04  Data: 0.012 (0.028)
Train: 138 [ 200/1251 ( 16%)]  Loss: 3.466 (3.50)  Time: 0.665s, 1539.48/s  (0.672s, 1523.60/s)  LR: 5.640e-04  Data: 0.015 (0.024)
Train: 138 [ 250/1251 ( 20%)]  Loss: 3.163 (3.44)  Time: 0.672s, 1524.35/s  (0.671s, 1525.17/s)  LR: 5.638e-04  Data: 0.013 (0.022)
Train: 138 [ 300/1251 ( 24%)]  Loss: 3.496 (3.45)  Time: 0.662s, 1545.86/s  (0.671s, 1525.63/s)  LR: 5.636e-04  Data: 0.013 (0.021)
Train: 138 [ 350/1251 ( 28%)]  Loss: 3.692 (3.48)  Time: 0.668s, 1533.82/s  (0.671s, 1526.27/s)  LR: 5.634e-04  Data: 0.013 (0.020)
Train: 138 [ 400/1251 ( 32%)]  Loss: 3.742 (3.51)  Time: 0.662s, 1545.70/s  (0.671s, 1526.41/s)  LR: 5.632e-04  Data: 0.014 (0.019)
Train: 138 [ 450/1251 ( 36%)]  Loss: 3.486 (3.51)  Time: 0.670s, 1527.33/s  (0.671s, 1526.27/s)  LR: 5.630e-04  Data: 0.014 (0.019)
Train: 138 [ 500/1251 ( 40%)]  Loss: 3.807 (3.53)  Time: 0.670s, 1528.66/s  (0.671s, 1526.19/s)  LR: 5.628e-04  Data: 0.013 (0.018)
Train: 138 [ 550/1251 ( 44%)]  Loss: 3.650 (3.54)  Time: 0.661s, 1549.92/s  (0.671s, 1526.27/s)  LR: 5.626e-04  Data: 0.013 (0.018)
Train: 138 [ 600/1251 ( 48%)]  Loss: 3.320 (3.53)  Time: 0.674s, 1518.64/s  (0.671s, 1526.11/s)  LR: 5.624e-04  Data: 0.013 (0.017)
Train: 138 [ 650/1251 ( 52%)]  Loss: 3.272 (3.51)  Time: 0.670s, 1528.19/s  (0.671s, 1526.04/s)  LR: 5.622e-04  Data: 0.013 (0.017)
Train: 138 [ 700/1251 ( 56%)]  Loss: 3.550 (3.51)  Time: 0.671s, 1525.81/s  (0.671s, 1525.83/s)  LR: 5.620e-04  Data: 0.014 (0.017)
Train: 138 [ 750/1251 ( 60%)]  Loss: 3.757 (3.53)  Time: 0.669s, 1531.31/s  (0.671s, 1525.64/s)  LR: 5.617e-04  Data: 0.015 (0.017)
Train: 138 [ 800/1251 ( 64%)]  Loss: 3.580 (3.53)  Time: 0.672s, 1524.27/s  (0.671s, 1525.84/s)  LR: 5.615e-04  Data: 0.013 (0.017)
Train: 138 [ 850/1251 ( 68%)]  Loss: 3.548 (3.53)  Time: 0.669s, 1531.56/s  (0.671s, 1526.28/s)  LR: 5.613e-04  Data: 0.013 (0.016)
Train: 138 [ 900/1251 ( 72%)]  Loss: 3.232 (3.52)  Time: 0.681s, 1503.90/s  (0.671s, 1526.49/s)  LR: 5.611e-04  Data: 0.013 (0.016)
Train: 138 [ 950/1251 ( 76%)]  Loss: 3.872 (3.53)  Time: 0.663s, 1544.10/s  (0.671s, 1526.70/s)  LR: 5.609e-04  Data: 0.017 (0.016)
Train: 138 [1000/1251 ( 80%)]  Loss: 3.250 (3.52)  Time: 0.672s, 1524.24/s  (0.671s, 1526.95/s)  LR: 5.607e-04  Data: 0.015 (0.016)
Train: 138 [1050/1251 ( 84%)]  Loss: 3.609 (3.52)  Time: 0.673s, 1522.27/s  (0.671s, 1527.02/s)  LR: 5.605e-04  Data: 0.013 (0.016)
Train: 138 [1100/1251 ( 88%)]  Loss: 3.255 (3.51)  Time: 0.665s, 1539.82/s  (0.670s, 1527.35/s)  LR: 5.603e-04  Data: 0.015 (0.016)
Train: 138 [1150/1251 ( 92%)]  Loss: 3.325 (3.50)  Time: 0.671s, 1526.69/s  (0.670s, 1527.67/s)  LR: 5.601e-04  Data: 0.019 (0.016)
Train: 138 [1200/1251 ( 96%)]  Loss: 3.253 (3.49)  Time: 0.659s, 1553.33/s  (0.670s, 1528.02/s)  LR: 5.599e-04  Data: 0.016 (0.016)
Train: 138 [1250/1251 (100%)]  Loss: 3.277 (3.49)  Time: 0.651s, 1573.29/s  (0.670s, 1528.47/s)  LR: 5.597e-04  Data: 0.000 (0.016)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.853 (2.853)  Loss:  0.4871 (0.4871)  Acc@1: 90.5273 (90.5273)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.166 (0.332)  Loss:  0.6113 (1.0156)  Acc@1: 85.3774 (76.0160)  Acc@5: 96.6981 (93.5380)
Train: 139 [   0/1251 (  0%)]  Loss: 3.319 (3.32)  Time: 3.429s,  298.60/s  (3.429s,  298.60/s)  LR: 5.597e-04  Data: 1.869 (1.869)
Train: 139 [  50/1251 (  4%)]  Loss: 3.511 (3.42)  Time: 0.656s, 1561.02/s  (0.688s, 1489.25/s)  LR: 5.595e-04  Data: 0.014 (0.050)
Train: 139 [ 100/1251 (  8%)]  Loss: 3.435 (3.42)  Time: 0.664s, 1541.33/s  (0.674s, 1519.79/s)  LR: 5.593e-04  Data: 0.012 (0.032)
Train: 139 [ 150/1251 ( 12%)]  Loss: 3.689 (3.49)  Time: 0.658s, 1555.08/s  (0.671s, 1525.81/s)  LR: 5.591e-04  Data: 0.013 (0.026)
Train: 139 [ 200/1251 ( 16%)]  Loss: 3.148 (3.42)  Time: 0.672s, 1524.25/s  (0.670s, 1528.99/s)  LR: 5.588e-04  Data: 0.012 (0.023)
Train: 139 [ 250/1251 ( 20%)]  Loss: 3.642 (3.46)  Time: 0.663s, 1545.01/s  (0.669s, 1530.06/s)  LR: 5.586e-04  Data: 0.013 (0.021)
Train: 139 [ 300/1251 ( 24%)]  Loss: 3.594 (3.48)  Time: 0.676s, 1515.81/s  (0.669s, 1530.32/s)  LR: 5.584e-04  Data: 0.013 (0.020)
Train: 139 [ 350/1251 ( 28%)]  Loss: 3.312 (3.46)  Time: 0.668s, 1532.54/s  (0.669s, 1531.20/s)  LR: 5.582e-04  Data: 0.014 (0.019)
Train: 139 [ 400/1251 ( 32%)]  Loss: 3.733 (3.49)  Time: 0.669s, 1529.67/s  (0.669s, 1530.98/s)  LR: 5.580e-04  Data: 0.013 (0.018)
Train: 139 [ 450/1251 ( 36%)]  Loss: 3.286 (3.47)  Time: 0.671s, 1526.69/s  (0.669s, 1530.79/s)  LR: 5.578e-04  Data: 0.013 (0.018)
Train: 139 [ 500/1251 ( 40%)]  Loss: 3.624 (3.48)  Time: 0.672s, 1524.15/s  (0.669s, 1530.40/s)  LR: 5.576e-04  Data: 0.013 (0.017)
Train: 139 [ 550/1251 ( 44%)]  Loss: 3.689 (3.50)  Time: 0.671s, 1526.66/s  (0.669s, 1530.33/s)  LR: 5.574e-04  Data: 0.012 (0.017)
Train: 139 [ 600/1251 ( 48%)]  Loss: 3.493 (3.50)  Time: 0.670s, 1528.60/s  (0.669s, 1530.02/s)  LR: 5.572e-04  Data: 0.013 (0.017)
Train: 139 [ 650/1251 ( 52%)]  Loss: 3.672 (3.51)  Time: 0.679s, 1508.44/s  (0.669s, 1529.77/s)  LR: 5.570e-04  Data: 0.014 (0.016)
Train: 139 [ 700/1251 ( 56%)]  Loss: 3.621 (3.52)  Time: 0.668s, 1533.77/s  (0.669s, 1529.56/s)  LR: 5.568e-04  Data: 0.013 (0.016)
Train: 139 [ 750/1251 ( 60%)]  Loss: 3.555 (3.52)  Time: 0.676s, 1515.23/s  (0.670s, 1529.48/s)  LR: 5.566e-04  Data: 0.014 (0.016)
Train: 139 [ 800/1251 ( 64%)]  Loss: 3.535 (3.52)  Time: 0.669s, 1530.21/s  (0.670s, 1529.37/s)  LR: 5.564e-04  Data: 0.016 (0.016)
Train: 139 [ 850/1251 ( 68%)]  Loss: 3.414 (3.52)  Time: 0.675s, 1517.45/s  (0.670s, 1529.17/s)  LR: 5.562e-04  Data: 0.012 (0.016)
Train: 139 [ 900/1251 ( 72%)]  Loss: 3.986 (3.54)  Time: 0.676s, 1515.67/s  (0.670s, 1529.06/s)  LR: 5.560e-04  Data: 0.015 (0.016)
Train: 139 [ 950/1251 ( 76%)]  Loss: 3.466 (3.54)  Time: 0.682s, 1502.28/s  (0.670s, 1528.68/s)  LR: 5.557e-04  Data: 0.016 (0.016)
Train: 139 [1000/1251 ( 80%)]  Loss: 3.485 (3.53)  Time: 0.670s, 1527.33/s  (0.670s, 1528.35/s)  LR: 5.555e-04  Data: 0.012 (0.015)
Train: 139 [1050/1251 ( 84%)]  Loss: 3.324 (3.52)  Time: 0.669s, 1530.86/s  (0.670s, 1528.22/s)  LR: 5.553e-04  Data: 0.018 (0.015)
Train: 139 [1100/1251 ( 88%)]  Loss: 3.442 (3.52)  Time: 0.670s, 1528.59/s  (0.670s, 1528.13/s)  LR: 5.551e-04  Data: 0.014 (0.015)
Train: 139 [1150/1251 ( 92%)]  Loss: 3.704 (3.53)  Time: 0.676s, 1514.42/s  (0.670s, 1527.80/s)  LR: 5.549e-04  Data: 0.013 (0.015)
Train: 139 [1200/1251 ( 96%)]  Loss: 3.452 (3.53)  Time: 0.666s, 1536.59/s  (0.670s, 1527.64/s)  LR: 5.547e-04  Data: 0.013 (0.015)
Train: 139 [1250/1251 (100%)]  Loss: 3.443 (3.52)  Time: 0.658s, 1555.43/s  (0.670s, 1527.48/s)  LR: 5.545e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.808 (2.808)  Loss:  0.4937 (0.4937)  Acc@1: 89.4531 (89.4531)  Acc@5: 97.2656 (97.2656)
Test: [  48/48]  Time: 0.168 (0.325)  Loss:  0.5654 (0.9968)  Acc@1: 85.8491 (76.2000)  Acc@5: 96.5802 (93.5540)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-137.pth.tar', 76.20200013427734)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-139.pth.tar', 76.19999992919922)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-134.pth.tar', 76.14399992675781)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-135.pth.tar', 76.07200003417968)

Train: 140 [   0/1251 (  0%)]  Loss: 3.493 (3.49)  Time: 3.504s,  292.24/s  (3.504s,  292.24/s)  LR: 5.545e-04  Data: 2.063 (2.063)
Train: 140 [  50/1251 (  4%)]  Loss: 3.549 (3.52)  Time: 0.658s, 1557.40/s  (0.693s, 1478.28/s)  LR: 5.543e-04  Data: 0.013 (0.054)
Train: 140 [ 100/1251 (  8%)]  Loss: 3.298 (3.45)  Time: 0.666s, 1537.38/s  (0.679s, 1508.74/s)  LR: 5.541e-04  Data: 0.013 (0.034)
Train: 140 [ 150/1251 ( 12%)]  Loss: 3.238 (3.39)  Time: 0.668s, 1532.95/s  (0.676s, 1515.43/s)  LR: 5.539e-04  Data: 0.015 (0.028)
Train: 140 [ 200/1251 ( 16%)]  Loss: 3.577 (3.43)  Time: 0.659s, 1553.06/s  (0.674s, 1518.69/s)  LR: 5.537e-04  Data: 0.014 (0.024)
Train: 140 [ 250/1251 ( 20%)]  Loss: 3.863 (3.50)  Time: 0.674s, 1520.39/s  (0.673s, 1521.47/s)  LR: 5.535e-04  Data: 0.016 (0.022)
Train: 140 [ 300/1251 ( 24%)]  Loss: 3.597 (3.52)  Time: 0.663s, 1543.66/s  (0.672s, 1524.09/s)  LR: 5.533e-04  Data: 0.014 (0.021)
Train: 140 [ 350/1251 ( 28%)]  Loss: 3.155 (3.47)  Time: 0.672s, 1523.74/s  (0.671s, 1526.01/s)  LR: 5.530e-04  Data: 0.013 (0.020)
Train: 140 [ 400/1251 ( 32%)]  Loss: 3.787 (3.51)  Time: 0.665s, 1539.08/s  (0.671s, 1526.91/s)  LR: 5.528e-04  Data: 0.013 (0.019)
Train: 140 [ 450/1251 ( 36%)]  Loss: 3.178 (3.47)  Time: 0.672s, 1523.31/s  (0.670s, 1527.56/s)  LR: 5.526e-04  Data: 0.012 (0.018)
Train: 140 [ 500/1251 ( 40%)]  Loss: 3.550 (3.48)  Time: 0.670s, 1529.04/s  (0.670s, 1528.21/s)  LR: 5.524e-04  Data: 0.013 (0.018)
Train: 140 [ 550/1251 ( 44%)]  Loss: 3.778 (3.51)  Time: 0.666s, 1536.53/s  (0.670s, 1528.60/s)  LR: 5.522e-04  Data: 0.019 (0.018)
Train: 140 [ 600/1251 ( 48%)]  Loss: 3.374 (3.50)  Time: 0.672s, 1523.25/s  (0.670s, 1528.53/s)  LR: 5.520e-04  Data: 0.015 (0.017)
Train: 140 [ 650/1251 ( 52%)]  Loss: 3.330 (3.48)  Time: 0.661s, 1550.02/s  (0.670s, 1528.62/s)  LR: 5.518e-04  Data: 0.012 (0.017)
Train: 140 [ 700/1251 ( 56%)]  Loss: 3.436 (3.48)  Time: 0.680s, 1506.94/s  (0.670s, 1528.43/s)  LR: 5.516e-04  Data: 0.013 (0.017)
Train: 140 [ 750/1251 ( 60%)]  Loss: 3.625 (3.49)  Time: 0.668s, 1532.34/s  (0.670s, 1528.62/s)  LR: 5.514e-04  Data: 0.013 (0.016)
Train: 140 [ 800/1251 ( 64%)]  Loss: 3.449 (3.49)  Time: 0.674s, 1518.68/s  (0.670s, 1528.97/s)  LR: 5.512e-04  Data: 0.016 (0.016)
Train: 140 [ 850/1251 ( 68%)]  Loss: 3.657 (3.50)  Time: 0.672s, 1524.48/s  (0.670s, 1529.20/s)  LR: 5.510e-04  Data: 0.012 (0.016)
Train: 140 [ 900/1251 ( 72%)]  Loss: 3.419 (3.49)  Time: 0.678s, 1510.37/s  (0.670s, 1529.28/s)  LR: 5.508e-04  Data: 0.014 (0.016)
Train: 140 [ 950/1251 ( 76%)]  Loss: 3.831 (3.51)  Time: 0.673s, 1522.24/s  (0.670s, 1529.44/s)  LR: 5.506e-04  Data: 0.014 (0.016)
Train: 140 [1000/1251 ( 80%)]  Loss: 3.753 (3.52)  Time: 0.674s, 1520.18/s  (0.669s, 1529.58/s)  LR: 5.504e-04  Data: 0.014 (0.016)
Train: 140 [1050/1251 ( 84%)]  Loss: 3.373 (3.51)  Time: 0.662s, 1545.88/s  (0.669s, 1529.65/s)  LR: 5.501e-04  Data: 0.014 (0.016)
Train: 140 [1100/1251 ( 88%)]  Loss: 3.531 (3.51)  Time: 0.668s, 1533.08/s  (0.669s, 1529.90/s)  LR: 5.499e-04  Data: 0.013 (0.016)
Train: 140 [1150/1251 ( 92%)]  Loss: 3.593 (3.52)  Time: 0.665s, 1540.69/s  (0.669s, 1529.94/s)  LR: 5.497e-04  Data: 0.012 (0.016)
Train: 140 [1200/1251 ( 96%)]  Loss: 3.292 (3.51)  Time: 0.679s, 1507.50/s  (0.669s, 1530.10/s)  LR: 5.495e-04  Data: 0.014 (0.015)
Train: 140 [1250/1251 (100%)]  Loss: 3.554 (3.51)  Time: 0.658s, 1556.33/s  (0.669s, 1530.10/s)  LR: 5.493e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.786 (2.786)  Loss:  0.5015 (0.5015)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.162 (0.320)  Loss:  0.6089 (1.0114)  Acc@1: 86.9104 (76.2880)  Acc@5: 96.4623 (93.5220)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-140.pth.tar', 76.28800000244141)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-137.pth.tar', 76.20200013427734)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-139.pth.tar', 76.19999992919922)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-134.pth.tar', 76.14399992675781)

Train: 141 [   0/1251 (  0%)]  Loss: 3.559 (3.56)  Time: 4.086s,  250.64/s  (4.086s,  250.64/s)  LR: 5.493e-04  Data: 1.687 (1.687)
Train: 141 [  50/1251 (  4%)]  Loss: 3.459 (3.51)  Time: 0.656s, 1561.65/s  (0.700s, 1463.17/s)  LR: 5.491e-04  Data: 0.013 (0.046)
Train: 141 [ 100/1251 (  8%)]  Loss: 3.218 (3.41)  Time: 0.672s, 1523.24/s  (0.682s, 1502.46/s)  LR: 5.489e-04  Data: 0.013 (0.030)
Train: 141 [ 150/1251 ( 12%)]  Loss: 3.385 (3.41)  Time: 0.665s, 1538.78/s  (0.677s, 1512.37/s)  LR: 5.487e-04  Data: 0.016 (0.025)
Train: 141 [ 200/1251 ( 16%)]  Loss: 3.181 (3.36)  Time: 0.673s, 1520.67/s  (0.675s, 1516.41/s)  LR: 5.485e-04  Data: 0.015 (0.022)
Train: 141 [ 250/1251 ( 20%)]  Loss: 3.575 (3.40)  Time: 0.666s, 1537.08/s  (0.674s, 1519.22/s)  LR: 5.483e-04  Data: 0.014 (0.021)
Train: 141 [ 300/1251 ( 24%)]  Loss: 3.564 (3.42)  Time: 0.665s, 1540.57/s  (0.674s, 1520.36/s)  LR: 5.481e-04  Data: 0.013 (0.019)
Train: 141 [ 350/1251 ( 28%)]  Loss: 3.519 (3.43)  Time: 0.669s, 1531.77/s  (0.673s, 1522.20/s)  LR: 5.479e-04  Data: 0.012 (0.019)
Train: 141 [ 400/1251 ( 32%)]  Loss: 3.580 (3.45)  Time: 0.661s, 1548.99/s  (0.672s, 1523.65/s)  LR: 5.477e-04  Data: 0.014 (0.018)
Train: 141 [ 450/1251 ( 36%)]  Loss: 3.490 (3.45)  Time: 0.664s, 1542.19/s  (0.672s, 1524.94/s)  LR: 5.474e-04  Data: 0.012 (0.018)
Train: 141 [ 500/1251 ( 40%)]  Loss: 3.712 (3.48)  Time: 0.672s, 1524.81/s  (0.671s, 1525.13/s)  LR: 5.472e-04  Data: 0.014 (0.017)
Train: 141 [ 550/1251 ( 44%)]  Loss: 3.449 (3.47)  Time: 0.668s, 1532.87/s  (0.671s, 1525.87/s)  LR: 5.470e-04  Data: 0.017 (0.017)
Train: 141 [ 600/1251 ( 48%)]  Loss: 3.674 (3.49)  Time: 0.674s, 1518.29/s  (0.671s, 1526.32/s)  LR: 5.468e-04  Data: 0.013 (0.017)
Train: 141 [ 650/1251 ( 52%)]  Loss: 3.730 (3.51)  Time: 0.663s, 1544.13/s  (0.671s, 1526.73/s)  LR: 5.466e-04  Data: 0.013 (0.016)
Train: 141 [ 700/1251 ( 56%)]  Loss: 3.556 (3.51)  Time: 0.669s, 1529.52/s  (0.670s, 1527.26/s)  LR: 5.464e-04  Data: 0.013 (0.016)
Train: 141 [ 750/1251 ( 60%)]  Loss: 3.777 (3.53)  Time: 0.667s, 1535.47/s  (0.670s, 1528.03/s)  LR: 5.462e-04  Data: 0.015 (0.016)
Train: 141 [ 800/1251 ( 64%)]  Loss: 3.535 (3.53)  Time: 0.666s, 1536.76/s  (0.670s, 1528.60/s)  LR: 5.460e-04  Data: 0.014 (0.016)
Train: 141 [ 850/1251 ( 68%)]  Loss: 3.631 (3.53)  Time: 0.669s, 1531.10/s  (0.670s, 1528.94/s)  LR: 5.458e-04  Data: 0.013 (0.016)
Train: 141 [ 900/1251 ( 72%)]  Loss: 3.893 (3.55)  Time: 0.660s, 1551.70/s  (0.670s, 1529.27/s)  LR: 5.456e-04  Data: 0.015 (0.016)
Train: 141 [ 950/1251 ( 76%)]  Loss: 3.799 (3.56)  Time: 0.677s, 1513.10/s  (0.669s, 1529.66/s)  LR: 5.454e-04  Data: 0.015 (0.016)
Train: 141 [1000/1251 ( 80%)]  Loss: 3.534 (3.56)  Time: 0.675s, 1516.43/s  (0.669s, 1529.86/s)  LR: 5.452e-04  Data: 0.013 (0.015)
Train: 141 [1050/1251 ( 84%)]  Loss: 3.697 (3.57)  Time: 0.670s, 1528.74/s  (0.669s, 1530.17/s)  LR: 5.450e-04  Data: 0.012 (0.015)
Train: 141 [1100/1251 ( 88%)]  Loss: 3.767 (3.58)  Time: 0.666s, 1536.79/s  (0.669s, 1530.12/s)  LR: 5.448e-04  Data: 0.012 (0.015)
Train: 141 [1150/1251 ( 92%)]  Loss: 3.804 (3.59)  Time: 0.667s, 1534.55/s  (0.669s, 1530.07/s)  LR: 5.445e-04  Data: 0.012 (0.015)
Train: 141 [1200/1251 ( 96%)]  Loss: 3.237 (3.57)  Time: 0.664s, 1541.85/s  (0.669s, 1530.11/s)  LR: 5.443e-04  Data: 0.013 (0.015)
Train: 141 [1250/1251 (100%)]  Loss: 3.374 (3.57)  Time: 0.658s, 1556.55/s  (0.669s, 1530.24/s)  LR: 5.441e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.872 (2.872)  Loss:  0.4692 (0.4692)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.168 (0.324)  Loss:  0.5850 (1.0038)  Acc@1: 85.7311 (76.2960)  Acc@5: 97.1698 (93.3960)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-141.pth.tar', 76.29600013671875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-140.pth.tar', 76.28800000244141)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-137.pth.tar', 76.20200013427734)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-139.pth.tar', 76.19999992919922)

Train: 142 [   0/1251 (  0%)]  Loss: 3.235 (3.24)  Time: 3.908s,  262.06/s  (3.908s,  262.06/s)  LR: 5.441e-04  Data: 1.762 (1.762)
Train: 142 [  50/1251 (  4%)]  Loss: 3.519 (3.38)  Time: 0.655s, 1563.42/s  (0.697s, 1468.22/s)  LR: 5.439e-04  Data: 0.014 (0.048)
Train: 142 [ 100/1251 (  8%)]  Loss: 3.376 (3.38)  Time: 0.670s, 1529.18/s  (0.679s, 1507.34/s)  LR: 5.437e-04  Data: 0.014 (0.031)
Train: 142 [ 150/1251 ( 12%)]  Loss: 3.412 (3.39)  Time: 0.668s, 1533.83/s  (0.675s, 1516.83/s)  LR: 5.435e-04  Data: 0.016 (0.026)
Train: 142 [ 200/1251 ( 16%)]  Loss: 3.902 (3.49)  Time: 0.668s, 1532.97/s  (0.674s, 1520.37/s)  LR: 5.433e-04  Data: 0.012 (0.023)
Train: 142 [ 250/1251 ( 20%)]  Loss: 3.467 (3.49)  Time: 0.669s, 1531.09/s  (0.673s, 1521.71/s)  LR: 5.431e-04  Data: 0.014 (0.021)
Train: 142 [ 300/1251 ( 24%)]  Loss: 3.580 (3.50)  Time: 0.670s, 1528.03/s  (0.672s, 1522.82/s)  LR: 5.429e-04  Data: 0.014 (0.020)
Train: 142 [ 350/1251 ( 28%)]  Loss: 3.429 (3.49)  Time: 0.680s, 1506.71/s  (0.672s, 1523.15/s)  LR: 5.427e-04  Data: 0.017 (0.019)
Train: 142 [ 400/1251 ( 32%)]  Loss: 3.379 (3.48)  Time: 0.672s, 1524.70/s  (0.672s, 1523.83/s)  LR: 5.425e-04  Data: 0.013 (0.018)
Train: 142 [ 450/1251 ( 36%)]  Loss: 3.573 (3.49)  Time: 0.675s, 1517.15/s  (0.672s, 1523.79/s)  LR: 5.423e-04  Data: 0.013 (0.018)
Train: 142 [ 500/1251 ( 40%)]  Loss: 3.642 (3.50)  Time: 0.669s, 1530.55/s  (0.672s, 1524.18/s)  LR: 5.421e-04  Data: 0.013 (0.017)
Train: 142 [ 550/1251 ( 44%)]  Loss: 3.550 (3.51)  Time: 0.673s, 1520.68/s  (0.671s, 1524.96/s)  LR: 5.418e-04  Data: 0.018 (0.017)
Train: 142 [ 600/1251 ( 48%)]  Loss: 3.868 (3.53)  Time: 0.665s, 1539.26/s  (0.671s, 1525.34/s)  LR: 5.416e-04  Data: 0.014 (0.017)
Train: 142 [ 650/1251 ( 52%)]  Loss: 3.397 (3.52)  Time: 0.667s, 1535.02/s  (0.671s, 1525.77/s)  LR: 5.414e-04  Data: 0.013 (0.016)
Train: 142 [ 700/1251 ( 56%)]  Loss: 3.424 (3.52)  Time: 0.666s, 1536.60/s  (0.671s, 1526.15/s)  LR: 5.412e-04  Data: 0.016 (0.016)
Train: 142 [ 750/1251 ( 60%)]  Loss: 3.409 (3.51)  Time: 0.670s, 1527.68/s  (0.671s, 1526.53/s)  LR: 5.410e-04  Data: 0.014 (0.016)
Train: 142 [ 800/1251 ( 64%)]  Loss: 3.269 (3.50)  Time: 0.665s, 1540.56/s  (0.671s, 1526.88/s)  LR: 5.408e-04  Data: 0.013 (0.016)
Train: 142 [ 850/1251 ( 68%)]  Loss: 3.486 (3.50)  Time: 0.674s, 1519.30/s  (0.671s, 1526.83/s)  LR: 5.406e-04  Data: 0.017 (0.016)
Train: 142 [ 900/1251 ( 72%)]  Loss: 3.568 (3.50)  Time: 0.672s, 1523.63/s  (0.671s, 1526.76/s)  LR: 5.404e-04  Data: 0.013 (0.016)
Train: 142 [ 950/1251 ( 76%)]  Loss: 3.471 (3.50)  Time: 0.669s, 1529.83/s  (0.671s, 1526.85/s)  LR: 5.402e-04  Data: 0.013 (0.016)
Train: 142 [1000/1251 ( 80%)]  Loss: 3.612 (3.50)  Time: 0.663s, 1543.76/s  (0.671s, 1526.86/s)  LR: 5.400e-04  Data: 0.014 (0.015)
Train: 142 [1050/1251 ( 84%)]  Loss: 3.397 (3.50)  Time: 0.666s, 1538.36/s  (0.671s, 1526.96/s)  LR: 5.398e-04  Data: 0.014 (0.015)
Train: 142 [1100/1251 ( 88%)]  Loss: 3.526 (3.50)  Time: 0.657s, 1557.50/s  (0.670s, 1527.29/s)  LR: 5.396e-04  Data: 0.015 (0.015)
Train: 142 [1150/1251 ( 92%)]  Loss: 3.475 (3.50)  Time: 0.662s, 1546.29/s  (0.670s, 1527.69/s)  LR: 5.394e-04  Data: 0.014 (0.015)
Train: 142 [1200/1251 ( 96%)]  Loss: 3.526 (3.50)  Time: 0.661s, 1548.16/s  (0.670s, 1527.97/s)  LR: 5.391e-04  Data: 0.013 (0.015)
Train: 142 [1250/1251 (100%)]  Loss: 3.605 (3.50)  Time: 0.661s, 1549.67/s  (0.670s, 1528.18/s)  LR: 5.389e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.263 (3.263)  Loss:  0.4624 (0.4624)  Acc@1: 89.5508 (89.5508)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.162 (0.328)  Loss:  0.6436 (1.0007)  Acc@1: 84.5519 (76.3940)  Acc@5: 96.5802 (93.4800)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-142.pth.tar', 76.39400014160157)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-141.pth.tar', 76.29600013671875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-140.pth.tar', 76.28800000244141)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-137.pth.tar', 76.20200013427734)

Train: 143 [   0/1251 (  0%)]  Loss: 3.325 (3.33)  Time: 3.433s,  298.29/s  (3.433s,  298.29/s)  LR: 5.389e-04  Data: 1.577 (1.577)
Train: 143 [  50/1251 (  4%)]  Loss: 3.324 (3.32)  Time: 0.651s, 1572.18/s  (0.687s, 1490.19/s)  LR: 5.387e-04  Data: 0.013 (0.044)
Train: 143 [ 100/1251 (  8%)]  Loss: 3.404 (3.35)  Time: 0.662s, 1546.72/s  (0.671s, 1526.26/s)  LR: 5.385e-04  Data: 0.014 (0.029)
Train: 143 [ 150/1251 ( 12%)]  Loss: 3.713 (3.44)  Time: 0.655s, 1563.38/s  (0.667s, 1535.10/s)  LR: 5.383e-04  Data: 0.013 (0.024)
Train: 143 [ 200/1251 ( 16%)]  Loss: 3.706 (3.49)  Time: 0.659s, 1553.06/s  (0.666s, 1537.53/s)  LR: 5.381e-04  Data: 0.013 (0.021)
Train: 143 [ 250/1251 ( 20%)]  Loss: 3.494 (3.49)  Time: 0.669s, 1530.60/s  (0.666s, 1536.74/s)  LR: 5.379e-04  Data: 0.013 (0.020)
Train: 143 [ 300/1251 ( 24%)]  Loss: 3.739 (3.53)  Time: 0.657s, 1557.70/s  (0.666s, 1536.87/s)  LR: 5.377e-04  Data: 0.013 (0.019)
Train: 143 [ 350/1251 ( 28%)]  Loss: 3.325 (3.50)  Time: 0.669s, 1530.57/s  (0.667s, 1536.15/s)  LR: 5.375e-04  Data: 0.017 (0.018)
Train: 143 [ 400/1251 ( 32%)]  Loss: 3.693 (3.52)  Time: 0.665s, 1540.65/s  (0.667s, 1535.25/s)  LR: 5.373e-04  Data: 0.012 (0.018)
Train: 143 [ 450/1251 ( 36%)]  Loss: 3.679 (3.54)  Time: 0.674s, 1520.15/s  (0.667s, 1534.85/s)  LR: 5.371e-04  Data: 0.013 (0.017)
Train: 143 [ 500/1251 ( 40%)]  Loss: 3.469 (3.53)  Time: 0.671s, 1525.60/s  (0.667s, 1534.56/s)  LR: 5.369e-04  Data: 0.013 (0.017)
Train: 143 [ 550/1251 ( 44%)]  Loss: 3.215 (3.51)  Time: 0.663s, 1543.89/s  (0.667s, 1534.71/s)  LR: 5.366e-04  Data: 0.012 (0.016)
Train: 143 [ 600/1251 ( 48%)]  Loss: 3.573 (3.51)  Time: 0.660s, 1551.26/s  (0.667s, 1534.77/s)  LR: 5.364e-04  Data: 0.014 (0.016)
Train: 143 [ 650/1251 ( 52%)]  Loss: 3.381 (3.50)  Time: 0.669s, 1531.52/s  (0.667s, 1534.39/s)  LR: 5.362e-04  Data: 0.014 (0.016)
Train: 143 [ 700/1251 ( 56%)]  Loss: 3.629 (3.51)  Time: 0.668s, 1533.37/s  (0.668s, 1534.02/s)  LR: 5.360e-04  Data: 0.012 (0.016)
Train: 143 [ 750/1251 ( 60%)]  Loss: 3.837 (3.53)  Time: 0.668s, 1532.21/s  (0.668s, 1533.73/s)  LR: 5.358e-04  Data: 0.013 (0.016)
Train: 143 [ 800/1251 ( 64%)]  Loss: 3.711 (3.54)  Time: 0.669s, 1531.77/s  (0.668s, 1533.30/s)  LR: 5.356e-04  Data: 0.013 (0.015)
Train: 143 [ 850/1251 ( 68%)]  Loss: 3.290 (3.53)  Time: 0.657s, 1559.44/s  (0.668s, 1533.25/s)  LR: 5.354e-04  Data: 0.013 (0.015)
Train: 143 [ 900/1251 ( 72%)]  Loss: 3.431 (3.52)  Time: 0.673s, 1522.64/s  (0.668s, 1533.10/s)  LR: 5.352e-04  Data: 0.012 (0.015)
Train: 143 [ 950/1251 ( 76%)]  Loss: 3.538 (3.52)  Time: 0.670s, 1529.33/s  (0.668s, 1533.26/s)  LR: 5.350e-04  Data: 0.014 (0.015)
Train: 143 [1000/1251 ( 80%)]  Loss: 3.213 (3.51)  Time: 0.658s, 1556.89/s  (0.668s, 1533.50/s)  LR: 5.348e-04  Data: 0.014 (0.015)
Train: 143 [1050/1251 ( 84%)]  Loss: 3.563 (3.51)  Time: 0.673s, 1522.23/s  (0.668s, 1533.77/s)  LR: 5.346e-04  Data: 0.014 (0.015)
Train: 143 [1100/1251 ( 88%)]  Loss: 3.128 (3.49)  Time: 0.664s, 1542.35/s  (0.668s, 1533.86/s)  LR: 5.344e-04  Data: 0.013 (0.015)
Train: 143 [1150/1251 ( 92%)]  Loss: 3.746 (3.51)  Time: 0.666s, 1538.44/s  (0.668s, 1534.03/s)  LR: 5.342e-04  Data: 0.017 (0.015)
Train: 143 [1200/1251 ( 96%)]  Loss: 3.559 (3.51)  Time: 0.668s, 1534.00/s  (0.667s, 1534.30/s)  LR: 5.339e-04  Data: 0.012 (0.015)
Train: 143 [1250/1251 (100%)]  Loss: 3.405 (3.50)  Time: 0.663s, 1545.02/s  (0.667s, 1534.60/s)  LR: 5.337e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.820 (2.820)  Loss:  0.5264 (0.5264)  Acc@1: 90.2344 (90.2344)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.162 (0.332)  Loss:  0.6069 (1.0220)  Acc@1: 85.9670 (76.1900)  Acc@5: 97.2877 (93.4500)
Train: 144 [   0/1251 (  0%)]  Loss: 3.546 (3.55)  Time: 3.387s,  302.36/s  (3.387s,  302.36/s)  LR: 5.337e-04  Data: 1.863 (1.863)
Train: 144 [  50/1251 (  4%)]  Loss: 3.249 (3.40)  Time: 0.648s, 1580.94/s  (0.686s, 1492.80/s)  LR: 5.335e-04  Data: 0.014 (0.050)
Train: 144 [ 100/1251 (  8%)]  Loss: 3.603 (3.47)  Time: 0.664s, 1542.08/s  (0.672s, 1524.35/s)  LR: 5.333e-04  Data: 0.012 (0.032)
Train: 144 [ 150/1251 ( 12%)]  Loss: 3.433 (3.46)  Time: 0.676s, 1513.91/s  (0.668s, 1532.19/s)  LR: 5.331e-04  Data: 0.014 (0.026)
Train: 144 [ 200/1251 ( 16%)]  Loss: 3.486 (3.46)  Time: 0.662s, 1547.09/s  (0.668s, 1533.75/s)  LR: 5.329e-04  Data: 0.013 (0.023)
Train: 144 [ 250/1251 ( 20%)]  Loss: 3.274 (3.43)  Time: 0.671s, 1526.06/s  (0.667s, 1535.12/s)  LR: 5.327e-04  Data: 0.014 (0.021)
Train: 144 [ 300/1251 ( 24%)]  Loss: 3.434 (3.43)  Time: 0.667s, 1534.14/s  (0.667s, 1535.01/s)  LR: 5.325e-04  Data: 0.014 (0.020)
Train: 144 [ 350/1251 ( 28%)]  Loss: 3.441 (3.43)  Time: 0.671s, 1526.10/s  (0.667s, 1534.99/s)  LR: 5.323e-04  Data: 0.013 (0.019)
Train: 144 [ 400/1251 ( 32%)]  Loss: 3.482 (3.44)  Time: 0.669s, 1530.30/s  (0.667s, 1534.96/s)  LR: 5.321e-04  Data: 0.013 (0.018)
Train: 144 [ 450/1251 ( 36%)]  Loss: 3.468 (3.44)  Time: 0.679s, 1508.24/s  (0.667s, 1534.80/s)  LR: 5.319e-04  Data: 0.013 (0.018)
Train: 144 [ 500/1251 ( 40%)]  Loss: 3.217 (3.42)  Time: 0.672s, 1523.76/s  (0.667s, 1534.86/s)  LR: 5.317e-04  Data: 0.013 (0.017)
Train: 144 [ 550/1251 ( 44%)]  Loss: 3.664 (3.44)  Time: 0.681s, 1502.76/s  (0.667s, 1535.22/s)  LR: 5.314e-04  Data: 0.012 (0.017)
Train: 144 [ 600/1251 ( 48%)]  Loss: 3.333 (3.43)  Time: 0.659s, 1553.79/s  (0.667s, 1535.12/s)  LR: 5.312e-04  Data: 0.015 (0.017)
Train: 144 [ 650/1251 ( 52%)]  Loss: 3.418 (3.43)  Time: 0.668s, 1532.39/s  (0.667s, 1534.95/s)  LR: 5.310e-04  Data: 0.012 (0.016)
Train: 144 [ 700/1251 ( 56%)]  Loss: 3.683 (3.45)  Time: 0.664s, 1541.44/s  (0.667s, 1535.14/s)  LR: 5.308e-04  Data: 0.012 (0.016)
Train: 144 [ 750/1251 ( 60%)]  Loss: 3.815 (3.47)  Time: 0.669s, 1530.25/s  (0.667s, 1535.51/s)  LR: 5.306e-04  Data: 0.012 (0.016)
Train: 144 [ 800/1251 ( 64%)]  Loss: 3.473 (3.47)  Time: 0.668s, 1532.27/s  (0.667s, 1535.73/s)  LR: 5.304e-04  Data: 0.016 (0.016)
Train: 144 [ 850/1251 ( 68%)]  Loss: 3.383 (3.47)  Time: 0.670s, 1529.25/s  (0.667s, 1535.70/s)  LR: 5.302e-04  Data: 0.016 (0.016)
Train: 144 [ 900/1251 ( 72%)]  Loss: 3.539 (3.47)  Time: 0.666s, 1536.49/s  (0.667s, 1535.69/s)  LR: 5.300e-04  Data: 0.018 (0.016)
Train: 144 [ 950/1251 ( 76%)]  Loss: 3.776 (3.49)  Time: 0.671s, 1525.91/s  (0.667s, 1535.58/s)  LR: 5.298e-04  Data: 0.015 (0.015)
Train: 144 [1000/1251 ( 80%)]  Loss: 3.608 (3.49)  Time: 0.655s, 1563.96/s  (0.667s, 1535.50/s)  LR: 5.296e-04  Data: 0.014 (0.015)
Train: 144 [1050/1251 ( 84%)]  Loss: 2.916 (3.47)  Time: 0.676s, 1514.85/s  (0.667s, 1535.42/s)  LR: 5.294e-04  Data: 0.014 (0.015)
Train: 144 [1100/1251 ( 88%)]  Loss: 3.570 (3.47)  Time: 0.661s, 1550.19/s  (0.667s, 1535.55/s)  LR: 5.292e-04  Data: 0.014 (0.015)
Train: 144 [1150/1251 ( 92%)]  Loss: 3.487 (3.47)  Time: 0.671s, 1525.64/s  (0.667s, 1535.40/s)  LR: 5.290e-04  Data: 0.012 (0.015)
Train: 144 [1200/1251 ( 96%)]  Loss: 3.344 (3.47)  Time: 0.673s, 1520.47/s  (0.667s, 1535.28/s)  LR: 5.287e-04  Data: 0.013 (0.015)
Train: 144 [1250/1251 (100%)]  Loss: 3.594 (3.47)  Time: 0.653s, 1567.51/s  (0.667s, 1535.07/s)  LR: 5.285e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.814 (2.814)  Loss:  0.4941 (0.4941)  Acc@1: 90.7227 (90.7227)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.162 (0.325)  Loss:  0.5630 (1.0090)  Acc@1: 86.5566 (76.2280)  Acc@5: 97.5236 (93.5480)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-142.pth.tar', 76.39400014160157)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-141.pth.tar', 76.29600013671875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-140.pth.tar', 76.28800000244141)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-144.pth.tar', 76.22800010742188)

Train: 145 [   0/1251 (  0%)]  Loss: 3.410 (3.41)  Time: 3.940s,  259.87/s  (3.940s,  259.87/s)  LR: 5.285e-04  Data: 1.743 (1.743)
Train: 145 [  50/1251 (  4%)]  Loss: 3.572 (3.49)  Time: 0.649s, 1576.61/s  (0.698s, 1467.62/s)  LR: 5.283e-04  Data: 0.013 (0.048)
Train: 145 [ 100/1251 (  8%)]  Loss: 3.749 (3.58)  Time: 0.673s, 1521.49/s  (0.679s, 1507.20/s)  LR: 5.281e-04  Data: 0.013 (0.031)
Train: 145 [ 150/1251 ( 12%)]  Loss: 3.287 (3.50)  Time: 0.663s, 1543.93/s  (0.675s, 1517.52/s)  LR: 5.279e-04  Data: 0.013 (0.026)
Train: 145 [ 200/1251 ( 16%)]  Loss: 3.507 (3.51)  Time: 0.671s, 1526.79/s  (0.673s, 1521.23/s)  LR: 5.277e-04  Data: 0.012 (0.023)
Train: 145 [ 250/1251 ( 20%)]  Loss: 3.557 (3.51)  Time: 0.668s, 1533.32/s  (0.672s, 1523.09/s)  LR: 5.275e-04  Data: 0.012 (0.021)
Train: 145 [ 300/1251 ( 24%)]  Loss: 3.388 (3.50)  Time: 0.671s, 1526.01/s  (0.672s, 1523.39/s)  LR: 5.273e-04  Data: 0.015 (0.020)
Train: 145 [ 350/1251 ( 28%)]  Loss: 3.632 (3.51)  Time: 0.671s, 1526.71/s  (0.672s, 1523.75/s)  LR: 5.271e-04  Data: 0.015 (0.019)
Train: 145 [ 400/1251 ( 32%)]  Loss: 3.803 (3.55)  Time: 0.672s, 1524.28/s  (0.672s, 1524.64/s)  LR: 5.269e-04  Data: 0.013 (0.018)
Train: 145 [ 450/1251 ( 36%)]  Loss: 3.429 (3.53)  Time: 0.665s, 1540.49/s  (0.671s, 1525.45/s)  LR: 5.267e-04  Data: 0.014 (0.018)
Train: 145 [ 500/1251 ( 40%)]  Loss: 3.560 (3.54)  Time: 0.665s, 1539.45/s  (0.671s, 1526.25/s)  LR: 5.265e-04  Data: 0.014 (0.017)
Train: 145 [ 550/1251 ( 44%)]  Loss: 3.524 (3.53)  Time: 0.674s, 1519.76/s  (0.671s, 1526.18/s)  LR: 5.262e-04  Data: 0.012 (0.017)
Train: 145 [ 600/1251 ( 48%)]  Loss: 3.163 (3.51)  Time: 0.672s, 1524.78/s  (0.671s, 1525.87/s)  LR: 5.260e-04  Data: 0.015 (0.017)
Train: 145 [ 650/1251 ( 52%)]  Loss: 3.622 (3.51)  Time: 0.669s, 1529.76/s  (0.671s, 1525.11/s)  LR: 5.258e-04  Data: 0.016 (0.016)
Train: 145 [ 700/1251 ( 56%)]  Loss: 3.469 (3.51)  Time: 0.662s, 1546.87/s  (0.671s, 1525.25/s)  LR: 5.256e-04  Data: 0.013 (0.016)
Train: 145 [ 750/1251 ( 60%)]  Loss: 3.605 (3.52)  Time: 0.661s, 1548.42/s  (0.671s, 1525.28/s)  LR: 5.254e-04  Data: 0.016 (0.016)
Train: 145 [ 800/1251 ( 64%)]  Loss: 3.612 (3.52)  Time: 0.678s, 1509.46/s  (0.671s, 1525.16/s)  LR: 5.252e-04  Data: 0.013 (0.016)
Train: 145 [ 850/1251 ( 68%)]  Loss: 3.464 (3.52)  Time: 0.667s, 1535.88/s  (0.671s, 1525.00/s)  LR: 5.250e-04  Data: 0.013 (0.016)
Train: 145 [ 900/1251 ( 72%)]  Loss: 3.702 (3.53)  Time: 0.672s, 1522.77/s  (0.671s, 1524.99/s)  LR: 5.248e-04  Data: 0.014 (0.016)
Train: 145 [ 950/1251 ( 76%)]  Loss: 3.398 (3.52)  Time: 0.674s, 1518.48/s  (0.671s, 1525.23/s)  LR: 5.246e-04  Data: 0.012 (0.016)
Train: 145 [1000/1251 ( 80%)]  Loss: 3.670 (3.53)  Time: 0.671s, 1526.34/s  (0.671s, 1525.09/s)  LR: 5.244e-04  Data: 0.018 (0.015)
Train: 145 [1050/1251 ( 84%)]  Loss: 3.490 (3.53)  Time: 0.678s, 1511.29/s  (0.671s, 1524.97/s)  LR: 5.242e-04  Data: 0.014 (0.015)
Train: 145 [1100/1251 ( 88%)]  Loss: 3.454 (3.52)  Time: 0.674s, 1520.26/s  (0.672s, 1524.78/s)  LR: 5.240e-04  Data: 0.014 (0.015)
Train: 145 [1150/1251 ( 92%)]  Loss: 3.691 (3.53)  Time: 0.670s, 1528.23/s  (0.672s, 1524.57/s)  LR: 5.237e-04  Data: 0.015 (0.015)
Train: 145 [1200/1251 ( 96%)]  Loss: 3.683 (3.54)  Time: 0.681s, 1503.95/s  (0.672s, 1524.20/s)  LR: 5.235e-04  Data: 0.012 (0.015)
Train: 145 [1250/1251 (100%)]  Loss: 3.375 (3.53)  Time: 0.662s, 1546.63/s  (0.672s, 1524.14/s)  LR: 5.233e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.893 (2.893)  Loss:  0.4751 (0.4751)  Acc@1: 90.6250 (90.6250)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.162 (0.328)  Loss:  0.5918 (0.9949)  Acc@1: 84.9057 (76.3000)  Acc@5: 97.6415 (93.4320)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-142.pth.tar', 76.39400014160157)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-145.pth.tar', 76.3000000366211)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-141.pth.tar', 76.29600013671875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-140.pth.tar', 76.28800000244141)

Train: 146 [   0/1251 (  0%)]  Loss: 3.197 (3.20)  Time: 3.754s,  272.79/s  (3.754s,  272.79/s)  LR: 5.233e-04  Data: 1.877 (1.877)
Train: 146 [  50/1251 (  4%)]  Loss: 3.652 (3.42)  Time: 0.647s, 1583.91/s  (0.699s, 1464.14/s)  LR: 5.231e-04  Data: 0.013 (0.051)
Train: 146 [ 100/1251 (  8%)]  Loss: 3.795 (3.55)  Time: 0.656s, 1560.01/s  (0.682s, 1500.72/s)  LR: 5.229e-04  Data: 0.014 (0.032)
Train: 146 [ 150/1251 ( 12%)]  Loss: 3.512 (3.54)  Time: 0.670s, 1527.72/s  (0.678s, 1510.32/s)  LR: 5.227e-04  Data: 0.012 (0.026)
Train: 146 [ 200/1251 ( 16%)]  Loss: 3.302 (3.49)  Time: 0.671s, 1527.13/s  (0.676s, 1514.14/s)  LR: 5.225e-04  Data: 0.018 (0.023)
Train: 146 [ 250/1251 ( 20%)]  Loss: 2.953 (3.40)  Time: 0.666s, 1537.88/s  (0.675s, 1516.04/s)  LR: 5.223e-04  Data: 0.013 (0.021)
Train: 146 [ 300/1251 ( 24%)]  Loss: 3.282 (3.38)  Time: 0.671s, 1525.18/s  (0.675s, 1518.03/s)  LR: 5.221e-04  Data: 0.015 (0.020)
Train: 146 [ 350/1251 ( 28%)]  Loss: 3.488 (3.40)  Time: 0.670s, 1527.95/s  (0.674s, 1519.44/s)  LR: 5.219e-04  Data: 0.012 (0.019)
Train: 146 [ 400/1251 ( 32%)]  Loss: 3.642 (3.42)  Time: 0.670s, 1528.82/s  (0.673s, 1520.81/s)  LR: 5.217e-04  Data: 0.013 (0.018)
Train: 146 [ 450/1251 ( 36%)]  Loss: 3.620 (3.44)  Time: 0.667s, 1534.13/s  (0.673s, 1521.95/s)  LR: 5.215e-04  Data: 0.012 (0.018)
Train: 146 [ 500/1251 ( 40%)]  Loss: 3.336 (3.43)  Time: 0.657s, 1558.77/s  (0.672s, 1522.88/s)  LR: 5.212e-04  Data: 0.014 (0.017)
Train: 146 [ 550/1251 ( 44%)]  Loss: 3.374 (3.43)  Time: 0.666s, 1536.92/s  (0.672s, 1523.81/s)  LR: 5.210e-04  Data: 0.013 (0.017)
Train: 146 [ 600/1251 ( 48%)]  Loss: 3.489 (3.43)  Time: 0.666s, 1538.18/s  (0.672s, 1524.31/s)  LR: 5.208e-04  Data: 0.013 (0.017)
Train: 146 [ 650/1251 ( 52%)]  Loss: 3.559 (3.44)  Time: 0.668s, 1533.20/s  (0.671s, 1525.17/s)  LR: 5.206e-04  Data: 0.016 (0.016)
Train: 146 [ 700/1251 ( 56%)]  Loss: 3.269 (3.43)  Time: 0.667s, 1535.31/s  (0.671s, 1525.76/s)  LR: 5.204e-04  Data: 0.015 (0.016)
Train: 146 [ 750/1251 ( 60%)]  Loss: 3.244 (3.42)  Time: 0.674s, 1520.36/s  (0.671s, 1526.05/s)  LR: 5.202e-04  Data: 0.017 (0.016)
Train: 146 [ 800/1251 ( 64%)]  Loss: 3.582 (3.43)  Time: 0.671s, 1526.86/s  (0.671s, 1526.15/s)  LR: 5.200e-04  Data: 0.013 (0.016)
Train: 146 [ 850/1251 ( 68%)]  Loss: 3.769 (3.45)  Time: 0.673s, 1521.52/s  (0.671s, 1526.00/s)  LR: 5.198e-04  Data: 0.013 (0.016)
Train: 146 [ 900/1251 ( 72%)]  Loss: 3.513 (3.45)  Time: 0.667s, 1535.54/s  (0.671s, 1526.16/s)  LR: 5.196e-04  Data: 0.013 (0.016)
Train: 146 [ 950/1251 ( 76%)]  Loss: 3.595 (3.46)  Time: 0.669s, 1530.21/s  (0.671s, 1526.49/s)  LR: 5.194e-04  Data: 0.014 (0.016)
Train: 146 [1000/1251 ( 80%)]  Loss: 3.438 (3.46)  Time: 0.675s, 1517.45/s  (0.671s, 1526.86/s)  LR: 5.192e-04  Data: 0.013 (0.015)
Train: 146 [1050/1251 ( 84%)]  Loss: 3.428 (3.46)  Time: 0.664s, 1542.85/s  (0.671s, 1527.03/s)  LR: 5.190e-04  Data: 0.015 (0.015)
Train: 146 [1100/1251 ( 88%)]  Loss: 3.345 (3.45)  Time: 0.672s, 1524.42/s  (0.671s, 1527.12/s)  LR: 5.188e-04  Data: 0.014 (0.015)
Train: 146 [1150/1251 ( 92%)]  Loss: 3.433 (3.45)  Time: 0.668s, 1532.78/s  (0.671s, 1527.18/s)  LR: 5.185e-04  Data: 0.015 (0.015)
Train: 146 [1200/1251 ( 96%)]  Loss: 3.579 (3.46)  Time: 0.671s, 1526.80/s  (0.670s, 1527.25/s)  LR: 5.183e-04  Data: 0.013 (0.015)
Train: 146 [1250/1251 (100%)]  Loss: 3.328 (3.45)  Time: 0.656s, 1561.13/s  (0.670s, 1527.45/s)  LR: 5.181e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.712 (2.712)  Loss:  0.4688 (0.4688)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.162 (0.321)  Loss:  0.5928 (1.0116)  Acc@1: 85.4953 (76.4720)  Acc@5: 97.5236 (93.4760)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-146.pth.tar', 76.47199990478515)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-142.pth.tar', 76.39400014160157)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-145.pth.tar', 76.3000000366211)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-141.pth.tar', 76.29600013671875)

Train: 147 [   0/1251 (  0%)]  Loss: 3.539 (3.54)  Time: 3.616s,  283.19/s  (3.616s,  283.19/s)  LR: 5.181e-04  Data: 1.546 (1.546)
Train: 147 [  50/1251 (  4%)]  Loss: 3.650 (3.59)  Time: 0.656s, 1560.32/s  (0.688s, 1487.89/s)  LR: 5.179e-04  Data: 0.013 (0.044)
Train: 147 [ 100/1251 (  8%)]  Loss: 3.141 (3.44)  Time: 0.666s, 1537.33/s  (0.676s, 1514.24/s)  LR: 5.177e-04  Data: 0.013 (0.029)
Train: 147 [ 150/1251 ( 12%)]  Loss: 3.486 (3.45)  Time: 0.670s, 1528.12/s  (0.674s, 1519.91/s)  LR: 5.175e-04  Data: 0.015 (0.024)
Train: 147 [ 200/1251 ( 16%)]  Loss: 3.552 (3.47)  Time: 0.668s, 1531.82/s  (0.673s, 1521.43/s)  LR: 5.173e-04  Data: 0.014 (0.021)
Train: 147 [ 250/1251 ( 20%)]  Loss: 3.595 (3.49)  Time: 0.663s, 1545.66/s  (0.673s, 1522.26/s)  LR: 5.171e-04  Data: 0.013 (0.020)
Train: 147 [ 300/1251 ( 24%)]  Loss: 3.544 (3.50)  Time: 0.674s, 1519.05/s  (0.672s, 1523.75/s)  LR: 5.169e-04  Data: 0.013 (0.019)
Train: 147 [ 350/1251 ( 28%)]  Loss: 3.315 (3.48)  Time: 0.674s, 1520.04/s  (0.672s, 1524.23/s)  LR: 5.167e-04  Data: 0.013 (0.018)
Train: 147 [ 400/1251 ( 32%)]  Loss: 3.452 (3.47)  Time: 0.669s, 1530.59/s  (0.672s, 1524.20/s)  LR: 5.165e-04  Data: 0.013 (0.018)
Train: 147 [ 450/1251 ( 36%)]  Loss: 3.517 (3.48)  Time: 0.677s, 1511.53/s  (0.672s, 1524.91/s)  LR: 5.162e-04  Data: 0.013 (0.017)
Train: 147 [ 500/1251 ( 40%)]  Loss: 3.516 (3.48)  Time: 0.667s, 1536.05/s  (0.671s, 1525.35/s)  LR: 5.160e-04  Data: 0.013 (0.017)
Train: 147 [ 550/1251 ( 44%)]  Loss: 3.530 (3.49)  Time: 0.669s, 1529.97/s  (0.671s, 1525.86/s)  LR: 5.158e-04  Data: 0.013 (0.016)
Train: 147 [ 600/1251 ( 48%)]  Loss: 3.461 (3.48)  Time: 0.667s, 1534.37/s  (0.671s, 1526.45/s)  LR: 5.156e-04  Data: 0.013 (0.016)
Train: 147 [ 650/1251 ( 52%)]  Loss: 3.457 (3.48)  Time: 0.668s, 1532.22/s  (0.670s, 1527.28/s)  LR: 5.154e-04  Data: 0.015 (0.016)
Train: 147 [ 700/1251 ( 56%)]  Loss: 3.448 (3.48)  Time: 0.670s, 1528.04/s  (0.670s, 1527.76/s)  LR: 5.152e-04  Data: 0.012 (0.016)
Train: 147 [ 750/1251 ( 60%)]  Loss: 3.635 (3.49)  Time: 0.661s, 1548.88/s  (0.670s, 1528.29/s)  LR: 5.150e-04  Data: 0.015 (0.016)
Train: 147 [ 800/1251 ( 64%)]  Loss: 3.354 (3.48)  Time: 0.672s, 1524.66/s  (0.670s, 1528.57/s)  LR: 5.148e-04  Data: 0.016 (0.016)
Train: 147 [ 850/1251 ( 68%)]  Loss: 3.701 (3.49)  Time: 0.662s, 1547.41/s  (0.670s, 1529.01/s)  LR: 5.146e-04  Data: 0.014 (0.015)
Train: 147 [ 900/1251 ( 72%)]  Loss: 3.582 (3.50)  Time: 0.668s, 1532.69/s  (0.670s, 1529.28/s)  LR: 5.144e-04  Data: 0.013 (0.015)
Train: 147 [ 950/1251 ( 76%)]  Loss: 3.258 (3.49)  Time: 0.664s, 1542.27/s  (0.669s, 1529.73/s)  LR: 5.142e-04  Data: 0.014 (0.015)
Train: 147 [1000/1251 ( 80%)]  Loss: 3.808 (3.50)  Time: 0.665s, 1540.63/s  (0.669s, 1530.18/s)  LR: 5.140e-04  Data: 0.013 (0.015)
Train: 147 [1050/1251 ( 84%)]  Loss: 3.826 (3.52)  Time: 0.662s, 1546.13/s  (0.669s, 1530.46/s)  LR: 5.138e-04  Data: 0.012 (0.015)
Train: 147 [1100/1251 ( 88%)]  Loss: 3.256 (3.51)  Time: 0.667s, 1535.04/s  (0.669s, 1530.57/s)  LR: 5.135e-04  Data: 0.013 (0.015)
Train: 147 [1150/1251 ( 92%)]  Loss: 3.813 (3.52)  Time: 0.670s, 1527.27/s  (0.669s, 1530.62/s)  LR: 5.133e-04  Data: 0.013 (0.015)
Train: 147 [1200/1251 ( 96%)]  Loss: 2.993 (3.50)  Time: 0.666s, 1538.29/s  (0.669s, 1530.34/s)  LR: 5.131e-04  Data: 0.014 (0.015)
Train: 147 [1250/1251 (100%)]  Loss: 3.459 (3.50)  Time: 0.660s, 1551.00/s  (0.669s, 1530.08/s)  LR: 5.129e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.744 (2.744)  Loss:  0.4377 (0.4377)  Acc@1: 91.3086 (91.3086)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.167 (0.320)  Loss:  0.6431 (0.9848)  Acc@1: 84.1981 (76.5660)  Acc@5: 97.0519 (93.5920)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-147.pth.tar', 76.5660001171875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-146.pth.tar', 76.47199990478515)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-142.pth.tar', 76.39400014160157)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-145.pth.tar', 76.3000000366211)

Train: 148 [   0/1251 (  0%)]  Loss: 3.458 (3.46)  Time: 3.752s,  272.95/s  (3.752s,  272.95/s)  LR: 5.129e-04  Data: 1.585 (1.585)
Train: 148 [  50/1251 (  4%)]  Loss: 3.619 (3.54)  Time: 0.651s, 1573.91/s  (0.698s, 1467.75/s)  LR: 5.127e-04  Data: 0.017 (0.044)
Train: 148 [ 100/1251 (  8%)]  Loss: 3.316 (3.46)  Time: 0.667s, 1534.32/s  (0.680s, 1506.17/s)  LR: 5.125e-04  Data: 0.013 (0.029)
Train: 148 [ 150/1251 ( 12%)]  Loss: 3.346 (3.43)  Time: 0.672s, 1522.96/s  (0.676s, 1514.57/s)  LR: 5.123e-04  Data: 0.013 (0.024)
Train: 148 [ 200/1251 ( 16%)]  Loss: 3.421 (3.43)  Time: 0.665s, 1538.74/s  (0.674s, 1518.55/s)  LR: 5.121e-04  Data: 0.013 (0.022)
Train: 148 [ 250/1251 ( 20%)]  Loss: 3.741 (3.48)  Time: 0.672s, 1524.63/s  (0.673s, 1520.56/s)  LR: 5.119e-04  Data: 0.012 (0.020)
Train: 148 [ 300/1251 ( 24%)]  Loss: 3.241 (3.45)  Time: 0.677s, 1512.20/s  (0.673s, 1521.64/s)  LR: 5.117e-04  Data: 0.013 (0.019)
Train: 148 [ 350/1251 ( 28%)]  Loss: 3.468 (3.45)  Time: 0.669s, 1529.91/s  (0.672s, 1522.73/s)  LR: 5.115e-04  Data: 0.019 (0.018)
Train: 148 [ 400/1251 ( 32%)]  Loss: 3.418 (3.45)  Time: 0.678s, 1511.09/s  (0.672s, 1523.38/s)  LR: 5.112e-04  Data: 0.016 (0.018)
Train: 148 [ 450/1251 ( 36%)]  Loss: 3.676 (3.47)  Time: 0.673s, 1520.94/s  (0.672s, 1523.46/s)  LR: 5.110e-04  Data: 0.013 (0.017)
Train: 148 [ 500/1251 ( 40%)]  Loss: 3.797 (3.50)  Time: 0.671s, 1525.95/s  (0.672s, 1523.07/s)  LR: 5.108e-04  Data: 0.013 (0.017)
Train: 148 [ 550/1251 ( 44%)]  Loss: 3.835 (3.53)  Time: 0.669s, 1531.03/s  (0.672s, 1523.33/s)  LR: 5.106e-04  Data: 0.012 (0.017)
Train: 148 [ 600/1251 ( 48%)]  Loss: 3.585 (3.53)  Time: 0.669s, 1529.59/s  (0.672s, 1523.67/s)  LR: 5.104e-04  Data: 0.015 (0.016)
Train: 148 [ 650/1251 ( 52%)]  Loss: 3.369 (3.52)  Time: 0.673s, 1522.31/s  (0.672s, 1524.19/s)  LR: 5.102e-04  Data: 0.013 (0.016)
Train: 148 [ 700/1251 ( 56%)]  Loss: 3.385 (3.51)  Time: 0.669s, 1531.01/s  (0.672s, 1524.62/s)  LR: 5.100e-04  Data: 0.013 (0.016)
Train: 148 [ 750/1251 ( 60%)]  Loss: 3.517 (3.51)  Time: 0.678s, 1509.93/s  (0.671s, 1525.06/s)  LR: 5.098e-04  Data: 0.013 (0.016)
Train: 148 [ 800/1251 ( 64%)]  Loss: 3.252 (3.50)  Time: 0.668s, 1533.64/s  (0.671s, 1525.43/s)  LR: 5.096e-04  Data: 0.013 (0.016)
Train: 148 [ 850/1251 ( 68%)]  Loss: 3.326 (3.49)  Time: 0.669s, 1531.51/s  (0.671s, 1525.92/s)  LR: 5.094e-04  Data: 0.013 (0.015)
Train: 148 [ 900/1251 ( 72%)]  Loss: 3.373 (3.48)  Time: 0.669s, 1529.74/s  (0.671s, 1526.18/s)  LR: 5.092e-04  Data: 0.014 (0.015)
Train: 148 [ 950/1251 ( 76%)]  Loss: 3.760 (3.50)  Time: 0.670s, 1528.02/s  (0.671s, 1526.35/s)  LR: 5.090e-04  Data: 0.014 (0.015)
Train: 148 [1000/1251 ( 80%)]  Loss: 3.327 (3.49)  Time: 0.674s, 1520.27/s  (0.671s, 1526.49/s)  LR: 5.088e-04  Data: 0.014 (0.015)
Train: 148 [1050/1251 ( 84%)]  Loss: 3.355 (3.48)  Time: 0.667s, 1535.72/s  (0.671s, 1526.52/s)  LR: 5.085e-04  Data: 0.014 (0.015)
Train: 148 [1100/1251 ( 88%)]  Loss: 3.244 (3.47)  Time: 0.668s, 1531.89/s  (0.671s, 1526.55/s)  LR: 5.083e-04  Data: 0.013 (0.015)
Train: 148 [1150/1251 ( 92%)]  Loss: 3.776 (3.48)  Time: 0.675s, 1516.75/s  (0.671s, 1526.79/s)  LR: 5.081e-04  Data: 0.013 (0.015)
Train: 148 [1200/1251 ( 96%)]  Loss: 3.695 (3.49)  Time: 0.663s, 1544.71/s  (0.671s, 1526.88/s)  LR: 5.079e-04  Data: 0.013 (0.015)
Train: 148 [1250/1251 (100%)]  Loss: 3.314 (3.49)  Time: 0.650s, 1574.48/s  (0.671s, 1526.95/s)  LR: 5.077e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.748 (2.748)  Loss:  0.4741 (0.4741)  Acc@1: 90.0391 (90.0391)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.169 (0.326)  Loss:  0.5850 (0.9835)  Acc@1: 85.2594 (76.4540)  Acc@5: 96.8160 (93.6720)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-147.pth.tar', 76.5660001171875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-146.pth.tar', 76.47199990478515)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-148.pth.tar', 76.45399993164062)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-142.pth.tar', 76.39400014160157)

Train: 149 [   0/1251 (  0%)]  Loss: 3.213 (3.21)  Time: 3.404s,  300.81/s  (3.404s,  300.81/s)  LR: 5.077e-04  Data: 1.519 (1.519)
Train: 149 [  50/1251 (  4%)]  Loss: 3.466 (3.34)  Time: 0.650s, 1574.32/s  (0.691s, 1480.89/s)  LR: 5.075e-04  Data: 0.013 (0.044)
Train: 149 [ 100/1251 (  8%)]  Loss: 3.564 (3.41)  Time: 0.658s, 1556.68/s  (0.676s, 1515.74/s)  LR: 5.073e-04  Data: 0.013 (0.029)
Train: 149 [ 150/1251 ( 12%)]  Loss: 3.500 (3.44)  Time: 0.664s, 1543.01/s  (0.671s, 1525.79/s)  LR: 5.071e-04  Data: 0.014 (0.024)
Train: 149 [ 200/1251 ( 16%)]  Loss: 3.285 (3.41)  Time: 0.668s, 1531.96/s  (0.670s, 1529.29/s)  LR: 5.069e-04  Data: 0.018 (0.021)
Train: 149 [ 250/1251 ( 20%)]  Loss: 3.680 (3.45)  Time: 0.671s, 1524.96/s  (0.669s, 1530.66/s)  LR: 5.067e-04  Data: 0.018 (0.020)
Train: 149 [ 300/1251 ( 24%)]  Loss: 3.668 (3.48)  Time: 0.678s, 1511.36/s  (0.669s, 1530.79/s)  LR: 5.065e-04  Data: 0.014 (0.019)
Train: 149 [ 350/1251 ( 28%)]  Loss: 3.511 (3.49)  Time: 0.678s, 1509.54/s  (0.669s, 1530.73/s)  LR: 5.062e-04  Data: 0.013 (0.018)
Train: 149 [ 400/1251 ( 32%)]  Loss: 3.090 (3.44)  Time: 0.665s, 1540.06/s  (0.669s, 1530.86/s)  LR: 5.060e-04  Data: 0.014 (0.017)
Train: 149 [ 450/1251 ( 36%)]  Loss: 3.515 (3.45)  Time: 0.667s, 1535.86/s  (0.669s, 1531.17/s)  LR: 5.058e-04  Data: 0.013 (0.017)
Train: 149 [ 500/1251 ( 40%)]  Loss: 3.530 (3.46)  Time: 0.666s, 1536.72/s  (0.669s, 1531.01/s)  LR: 5.056e-04  Data: 0.013 (0.017)
Train: 149 [ 550/1251 ( 44%)]  Loss: 3.701 (3.48)  Time: 0.668s, 1533.61/s  (0.669s, 1531.33/s)  LR: 5.054e-04  Data: 0.013 (0.017)
Train: 149 [ 600/1251 ( 48%)]  Loss: 3.577 (3.48)  Time: 0.667s, 1534.38/s  (0.668s, 1531.92/s)  LR: 5.052e-04  Data: 0.014 (0.016)
Train: 149 [ 650/1251 ( 52%)]  Loss: 3.224 (3.47)  Time: 0.669s, 1531.53/s  (0.668s, 1532.61/s)  LR: 5.050e-04  Data: 0.013 (0.016)
Train: 149 [ 700/1251 ( 56%)]  Loss: 3.447 (3.46)  Time: 0.672s, 1523.56/s  (0.668s, 1532.97/s)  LR: 5.048e-04  Data: 0.014 (0.016)
Train: 149 [ 750/1251 ( 60%)]  Loss: 3.580 (3.47)  Time: 0.666s, 1538.24/s  (0.668s, 1533.42/s)  LR: 5.046e-04  Data: 0.014 (0.016)
Train: 149 [ 800/1251 ( 64%)]  Loss: 3.379 (3.47)  Time: 0.658s, 1556.15/s  (0.667s, 1534.16/s)  LR: 5.044e-04  Data: 0.013 (0.016)
Train: 149 [ 850/1251 ( 68%)]  Loss: 3.290 (3.46)  Time: 0.659s, 1554.57/s  (0.667s, 1534.52/s)  LR: 5.042e-04  Data: 0.014 (0.016)
Train: 149 [ 900/1251 ( 72%)]  Loss: 3.456 (3.46)  Time: 0.666s, 1537.21/s  (0.667s, 1534.84/s)  LR: 5.040e-04  Data: 0.012 (0.015)
Train: 149 [ 950/1251 ( 76%)]  Loss: 3.716 (3.47)  Time: 0.667s, 1534.77/s  (0.667s, 1535.00/s)  LR: 5.037e-04  Data: 0.013 (0.015)
Train: 149 [1000/1251 ( 80%)]  Loss: 3.443 (3.47)  Time: 0.665s, 1540.39/s  (0.667s, 1535.14/s)  LR: 5.035e-04  Data: 0.014 (0.015)
Train: 149 [1050/1251 ( 84%)]  Loss: 3.261 (3.46)  Time: 0.663s, 1545.06/s  (0.667s, 1535.53/s)  LR: 5.033e-04  Data: 0.016 (0.015)
Train: 149 [1100/1251 ( 88%)]  Loss: 3.486 (3.46)  Time: 0.671s, 1526.67/s  (0.667s, 1535.62/s)  LR: 5.031e-04  Data: 0.015 (0.015)
Train: 149 [1150/1251 ( 92%)]  Loss: 3.476 (3.46)  Time: 0.667s, 1535.78/s  (0.667s, 1535.72/s)  LR: 5.029e-04  Data: 0.013 (0.015)
Train: 149 [1200/1251 ( 96%)]  Loss: 3.503 (3.46)  Time: 0.657s, 1557.51/s  (0.667s, 1535.59/s)  LR: 5.027e-04  Data: 0.016 (0.015)
Train: 149 [1250/1251 (100%)]  Loss: 3.533 (3.47)  Time: 0.658s, 1556.06/s  (0.667s, 1535.61/s)  LR: 5.025e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.866 (2.866)  Loss:  0.4702 (0.4702)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.170 (0.321)  Loss:  0.5913 (0.9827)  Acc@1: 86.2028 (76.7780)  Acc@5: 96.8160 (93.7560)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-149.pth.tar', 76.77800008300781)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-147.pth.tar', 76.5660001171875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-146.pth.tar', 76.47199990478515)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-148.pth.tar', 76.45399993164062)

Train: 150 [   0/1251 (  0%)]  Loss: 3.746 (3.75)  Time: 3.681s,  278.16/s  (3.681s,  278.16/s)  LR: 5.025e-04  Data: 1.885 (1.885)
Train: 150 [  50/1251 (  4%)]  Loss: 3.304 (3.52)  Time: 0.652s, 1570.46/s  (0.689s, 1487.16/s)  LR: 5.023e-04  Data: 0.013 (0.051)
Train: 150 [ 100/1251 (  8%)]  Loss: 3.488 (3.51)  Time: 0.665s, 1538.95/s  (0.675s, 1516.37/s)  LR: 5.021e-04  Data: 0.013 (0.033)
Train: 150 [ 150/1251 ( 12%)]  Loss: 3.539 (3.52)  Time: 0.665s, 1539.02/s  (0.672s, 1523.20/s)  LR: 5.019e-04  Data: 0.015 (0.026)
Train: 150 [ 200/1251 ( 16%)]  Loss: 3.581 (3.53)  Time: 0.671s, 1526.75/s  (0.671s, 1525.43/s)  LR: 5.017e-04  Data: 0.013 (0.023)
Train: 150 [ 250/1251 ( 20%)]  Loss: 3.482 (3.52)  Time: 0.665s, 1539.98/s  (0.670s, 1527.22/s)  LR: 5.015e-04  Data: 0.013 (0.021)
Train: 150 [ 300/1251 ( 24%)]  Loss: 3.487 (3.52)  Time: 0.667s, 1534.76/s  (0.670s, 1528.86/s)  LR: 5.012e-04  Data: 0.013 (0.020)
Train: 150 [ 350/1251 ( 28%)]  Loss: 3.689 (3.54)  Time: 0.665s, 1538.98/s  (0.669s, 1529.66/s)  LR: 5.010e-04  Data: 0.013 (0.019)
Train: 150 [ 400/1251 ( 32%)]  Loss: 3.792 (3.57)  Time: 0.670s, 1528.11/s  (0.669s, 1530.14/s)  LR: 5.008e-04  Data: 0.013 (0.018)
Train: 150 [ 450/1251 ( 36%)]  Loss: 3.737 (3.58)  Time: 0.661s, 1549.20/s  (0.669s, 1530.60/s)  LR: 5.006e-04  Data: 0.013 (0.018)
Train: 150 [ 500/1251 ( 40%)]  Loss: 3.698 (3.59)  Time: 0.679s, 1509.18/s  (0.669s, 1531.32/s)  LR: 5.004e-04  Data: 0.013 (0.017)
Train: 150 [ 550/1251 ( 44%)]  Loss: 3.340 (3.57)  Time: 0.666s, 1536.90/s  (0.669s, 1531.75/s)  LR: 5.002e-04  Data: 0.013 (0.017)
Train: 150 [ 600/1251 ( 48%)]  Loss: 3.536 (3.57)  Time: 0.668s, 1533.54/s  (0.668s, 1532.07/s)  LR: 5.000e-04  Data: 0.013 (0.017)
Train: 150 [ 650/1251 ( 52%)]  Loss: 3.667 (3.58)  Time: 0.664s, 1543.25/s  (0.668s, 1532.12/s)  LR: 4.998e-04  Data: 0.015 (0.017)
Train: 150 [ 700/1251 ( 56%)]  Loss: 3.358 (3.56)  Time: 0.659s, 1553.86/s  (0.668s, 1532.66/s)  LR: 4.996e-04  Data: 0.013 (0.016)
Train: 150 [ 750/1251 ( 60%)]  Loss: 3.611 (3.57)  Time: 0.670s, 1527.54/s  (0.668s, 1532.78/s)  LR: 4.994e-04  Data: 0.012 (0.016)
Train: 150 [ 800/1251 ( 64%)]  Loss: 3.398 (3.56)  Time: 0.666s, 1537.31/s  (0.668s, 1533.07/s)  LR: 4.992e-04  Data: 0.013 (0.016)
Train: 150 [ 850/1251 ( 68%)]  Loss: 3.302 (3.54)  Time: 0.665s, 1539.88/s  (0.668s, 1533.41/s)  LR: 4.990e-04  Data: 0.017 (0.016)
Train: 150 [ 900/1251 ( 72%)]  Loss: 3.397 (3.53)  Time: 0.667s, 1534.53/s  (0.668s, 1533.31/s)  LR: 4.987e-04  Data: 0.013 (0.016)
Train: 150 [ 950/1251 ( 76%)]  Loss: 3.804 (3.55)  Time: 0.670s, 1528.04/s  (0.668s, 1533.29/s)  LR: 4.985e-04  Data: 0.012 (0.016)
Train: 150 [1000/1251 ( 80%)]  Loss: 3.872 (3.56)  Time: 0.678s, 1511.42/s  (0.668s, 1533.16/s)  LR: 4.983e-04  Data: 0.011 (0.016)
Train: 150 [1050/1251 ( 84%)]  Loss: 3.514 (3.56)  Time: 0.668s, 1532.99/s  (0.668s, 1533.08/s)  LR: 4.981e-04  Data: 0.013 (0.015)
Train: 150 [1100/1251 ( 88%)]  Loss: 3.961 (3.58)  Time: 0.667s, 1534.10/s  (0.668s, 1533.18/s)  LR: 4.979e-04  Data: 0.016 (0.015)
Train: 150 [1150/1251 ( 92%)]  Loss: 3.353 (3.57)  Time: 0.670s, 1528.83/s  (0.668s, 1533.08/s)  LR: 4.977e-04  Data: 0.012 (0.015)
Train: 150 [1200/1251 ( 96%)]  Loss: 3.626 (3.57)  Time: 0.667s, 1534.26/s  (0.668s, 1532.88/s)  LR: 4.975e-04  Data: 0.014 (0.015)
Train: 150 [1250/1251 (100%)]  Loss: 3.090 (3.55)  Time: 0.660s, 1551.80/s  (0.668s, 1532.67/s)  LR: 4.973e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.811 (2.811)  Loss:  0.4707 (0.4707)  Acc@1: 91.2109 (91.2109)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.165 (0.337)  Loss:  0.5898 (0.9797)  Acc@1: 85.3774 (76.6620)  Acc@5: 97.1698 (93.7300)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-149.pth.tar', 76.77800008300781)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-150.pth.tar', 76.66199998291016)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-147.pth.tar', 76.5660001171875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-146.pth.tar', 76.47199990478515)

Train: 151 [   0/1251 (  0%)]  Loss: 3.682 (3.68)  Time: 3.993s,  256.46/s  (3.993s,  256.46/s)  LR: 4.973e-04  Data: 1.521 (1.521)
Train: 151 [  50/1251 (  4%)]  Loss: 3.353 (3.52)  Time: 0.657s, 1557.84/s  (0.697s, 1469.17/s)  LR: 4.971e-04  Data: 0.014 (0.043)
Train: 151 [ 100/1251 (  8%)]  Loss: 3.221 (3.42)  Time: 0.651s, 1572.29/s  (0.680s, 1504.86/s)  LR: 4.969e-04  Data: 0.016 (0.028)
Train: 151 [ 150/1251 ( 12%)]  Loss: 3.394 (3.41)  Time: 0.672s, 1522.92/s  (0.676s, 1515.77/s)  LR: 4.967e-04  Data: 0.013 (0.024)
Train: 151 [ 200/1251 ( 16%)]  Loss: 3.250 (3.38)  Time: 0.681s, 1503.04/s  (0.674s, 1520.21/s)  LR: 4.965e-04  Data: 0.013 (0.021)
Train: 151 [ 250/1251 ( 20%)]  Loss: 3.558 (3.41)  Time: 0.669s, 1529.68/s  (0.673s, 1522.30/s)  LR: 4.962e-04  Data: 0.012 (0.020)
Train: 151 [ 300/1251 ( 24%)]  Loss: 3.318 (3.40)  Time: 0.660s, 1551.03/s  (0.672s, 1524.00/s)  LR: 4.960e-04  Data: 0.016 (0.018)
Train: 151 [ 350/1251 ( 28%)]  Loss: 3.397 (3.40)  Time: 0.667s, 1534.90/s  (0.671s, 1525.05/s)  LR: 4.958e-04  Data: 0.012 (0.018)
Train: 151 [ 400/1251 ( 32%)]  Loss: 3.554 (3.41)  Time: 0.660s, 1550.46/s  (0.671s, 1526.37/s)  LR: 4.956e-04  Data: 0.011 (0.017)
Train: 151 [ 450/1251 ( 36%)]  Loss: 3.342 (3.41)  Time: 0.660s, 1550.56/s  (0.670s, 1527.61/s)  LR: 4.954e-04  Data: 0.013 (0.017)
Train: 151 [ 500/1251 ( 40%)]  Loss: 3.341 (3.40)  Time: 0.676s, 1515.69/s  (0.670s, 1528.80/s)  LR: 4.952e-04  Data: 0.016 (0.017)
Train: 151 [ 550/1251 ( 44%)]  Loss: 3.398 (3.40)  Time: 0.660s, 1550.52/s  (0.670s, 1529.26/s)  LR: 4.950e-04  Data: 0.015 (0.016)
Train: 151 [ 600/1251 ( 48%)]  Loss: 3.314 (3.39)  Time: 0.668s, 1532.65/s  (0.669s, 1530.00/s)  LR: 4.948e-04  Data: 0.014 (0.016)
Train: 151 [ 650/1251 ( 52%)]  Loss: 3.177 (3.38)  Time: 0.666s, 1538.45/s  (0.669s, 1530.49/s)  LR: 4.946e-04  Data: 0.013 (0.016)
Train: 151 [ 700/1251 ( 56%)]  Loss: 3.429 (3.38)  Time: 0.663s, 1544.32/s  (0.669s, 1530.69/s)  LR: 4.944e-04  Data: 0.013 (0.016)
Train: 151 [ 750/1251 ( 60%)]  Loss: 3.637 (3.40)  Time: 0.672s, 1523.26/s  (0.669s, 1531.33/s)  LR: 4.942e-04  Data: 0.012 (0.016)
Train: 151 [ 800/1251 ( 64%)]  Loss: 3.796 (3.42)  Time: 0.668s, 1533.68/s  (0.669s, 1531.65/s)  LR: 4.940e-04  Data: 0.016 (0.016)
Train: 151 [ 850/1251 ( 68%)]  Loss: 3.551 (3.43)  Time: 0.669s, 1529.75/s  (0.668s, 1531.96/s)  LR: 4.937e-04  Data: 0.014 (0.016)
Train: 151 [ 900/1251 ( 72%)]  Loss: 3.663 (3.44)  Time: 0.659s, 1554.99/s  (0.668s, 1532.14/s)  LR: 4.935e-04  Data: 0.013 (0.015)
Train: 151 [ 950/1251 ( 76%)]  Loss: 3.561 (3.45)  Time: 0.670s, 1528.55/s  (0.668s, 1532.16/s)  LR: 4.933e-04  Data: 0.013 (0.015)
Train: 151 [1000/1251 ( 80%)]  Loss: 3.465 (3.45)  Time: 0.664s, 1542.89/s  (0.668s, 1532.14/s)  LR: 4.931e-04  Data: 0.013 (0.015)
Train: 151 [1050/1251 ( 84%)]  Loss: 3.479 (3.45)  Time: 0.677s, 1512.95/s  (0.668s, 1531.91/s)  LR: 4.929e-04  Data: 0.014 (0.015)
Train: 151 [1100/1251 ( 88%)]  Loss: 3.601 (3.46)  Time: 0.670s, 1527.40/s  (0.668s, 1531.79/s)  LR: 4.927e-04  Data: 0.013 (0.015)
Train: 151 [1150/1251 ( 92%)]  Loss: 3.384 (3.45)  Time: 0.668s, 1532.83/s  (0.669s, 1531.69/s)  LR: 4.925e-04  Data: 0.016 (0.015)
Train: 151 [1200/1251 ( 96%)]  Loss: 3.605 (3.46)  Time: 0.670s, 1529.46/s  (0.669s, 1531.48/s)  LR: 4.923e-04  Data: 0.013 (0.015)
Train: 151 [1250/1251 (100%)]  Loss: 3.521 (3.46)  Time: 0.656s, 1560.68/s  (0.669s, 1531.17/s)  LR: 4.921e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.893 (2.893)  Loss:  0.4883 (0.4883)  Acc@1: 90.5273 (90.5273)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.169 (0.327)  Loss:  0.5786 (1.0014)  Acc@1: 85.7311 (76.3520)  Acc@5: 97.2877 (93.5620)
Train: 152 [   0/1251 (  0%)]  Loss: 3.340 (3.34)  Time: 3.571s,  286.78/s  (3.571s,  286.78/s)  LR: 4.921e-04  Data: 1.929 (1.929)
Train: 152 [  50/1251 (  4%)]  Loss: 3.467 (3.40)  Time: 0.664s, 1542.22/s  (0.697s, 1469.47/s)  LR: 4.919e-04  Data: 0.012 (0.052)
Train: 152 [ 100/1251 (  8%)]  Loss: 3.671 (3.49)  Time: 0.665s, 1540.21/s  (0.681s, 1502.81/s)  LR: 4.917e-04  Data: 0.014 (0.033)
Train: 152 [ 150/1251 ( 12%)]  Loss: 3.490 (3.49)  Time: 0.678s, 1509.62/s  (0.678s, 1511.43/s)  LR: 4.915e-04  Data: 0.012 (0.026)
Train: 152 [ 200/1251 ( 16%)]  Loss: 3.269 (3.45)  Time: 0.671s, 1525.46/s  (0.676s, 1515.08/s)  LR: 4.912e-04  Data: 0.013 (0.023)
Train: 152 [ 250/1251 ( 20%)]  Loss: 3.602 (3.47)  Time: 0.671s, 1526.28/s  (0.675s, 1517.62/s)  LR: 4.910e-04  Data: 0.014 (0.021)
Train: 152 [ 300/1251 ( 24%)]  Loss: 3.221 (3.44)  Time: 0.674s, 1519.39/s  (0.674s, 1519.07/s)  LR: 4.908e-04  Data: 0.013 (0.020)
Train: 152 [ 350/1251 ( 28%)]  Loss: 3.793 (3.48)  Time: 0.676s, 1515.38/s  (0.674s, 1519.28/s)  LR: 4.906e-04  Data: 0.013 (0.019)
Train: 152 [ 400/1251 ( 32%)]  Loss: 3.344 (3.47)  Time: 0.671s, 1525.47/s  (0.674s, 1520.35/s)  LR: 4.904e-04  Data: 0.012 (0.018)
Train: 152 [ 450/1251 ( 36%)]  Loss: 3.683 (3.49)  Time: 0.674s, 1518.62/s  (0.673s, 1521.06/s)  LR: 4.902e-04  Data: 0.013 (0.018)
Train: 152 [ 500/1251 ( 40%)]  Loss: 3.609 (3.50)  Time: 0.670s, 1527.90/s  (0.673s, 1522.09/s)  LR: 4.900e-04  Data: 0.013 (0.017)
Train: 152 [ 550/1251 ( 44%)]  Loss: 3.510 (3.50)  Time: 0.678s, 1509.50/s  (0.672s, 1523.21/s)  LR: 4.898e-04  Data: 0.014 (0.017)
Train: 152 [ 600/1251 ( 48%)]  Loss: 3.367 (3.49)  Time: 0.665s, 1539.33/s  (0.672s, 1523.83/s)  LR: 4.896e-04  Data: 0.013 (0.017)
Train: 152 [ 650/1251 ( 52%)]  Loss: 3.379 (3.48)  Time: 0.661s, 1548.52/s  (0.672s, 1524.33/s)  LR: 4.894e-04  Data: 0.013 (0.017)
Train: 152 [ 700/1251 ( 56%)]  Loss: 3.314 (3.47)  Time: 0.670s, 1527.48/s  (0.671s, 1524.95/s)  LR: 4.892e-04  Data: 0.013 (0.016)
Train: 152 [ 750/1251 ( 60%)]  Loss: 3.731 (3.49)  Time: 0.667s, 1535.81/s  (0.671s, 1525.49/s)  LR: 4.890e-04  Data: 0.013 (0.016)
Train: 152 [ 800/1251 ( 64%)]  Loss: 3.717 (3.50)  Time: 0.674s, 1520.33/s  (0.671s, 1525.83/s)  LR: 4.887e-04  Data: 0.012 (0.016)
Train: 152 [ 850/1251 ( 68%)]  Loss: 3.585 (3.51)  Time: 0.662s, 1545.85/s  (0.671s, 1526.28/s)  LR: 4.885e-04  Data: 0.013 (0.016)
Train: 152 [ 900/1251 ( 72%)]  Loss: 3.523 (3.51)  Time: 0.668s, 1533.61/s  (0.671s, 1526.67/s)  LR: 4.883e-04  Data: 0.014 (0.016)
Train: 152 [ 950/1251 ( 76%)]  Loss: 3.465 (3.50)  Time: 0.662s, 1546.95/s  (0.671s, 1527.17/s)  LR: 4.881e-04  Data: 0.011 (0.016)
Train: 152 [1000/1251 ( 80%)]  Loss: 3.154 (3.49)  Time: 0.670s, 1528.41/s  (0.670s, 1527.35/s)  LR: 4.879e-04  Data: 0.013 (0.015)
Train: 152 [1050/1251 ( 84%)]  Loss: 3.326 (3.48)  Time: 0.668s, 1532.76/s  (0.670s, 1527.57/s)  LR: 4.877e-04  Data: 0.012 (0.015)
Train: 152 [1100/1251 ( 88%)]  Loss: 3.817 (3.49)  Time: 0.664s, 1541.94/s  (0.670s, 1527.37/s)  LR: 4.875e-04  Data: 0.012 (0.015)
Train: 152 [1150/1251 ( 92%)]  Loss: 3.557 (3.50)  Time: 0.674s, 1519.47/s  (0.671s, 1527.22/s)  LR: 4.873e-04  Data: 0.014 (0.015)
Train: 152 [1200/1251 ( 96%)]  Loss: 3.340 (3.49)  Time: 0.657s, 1557.45/s  (0.670s, 1527.28/s)  LR: 4.871e-04  Data: 0.014 (0.015)
Train: 152 [1250/1251 (100%)]  Loss: 3.226 (3.48)  Time: 0.661s, 1548.59/s  (0.670s, 1527.36/s)  LR: 4.869e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.133 (3.133)  Loss:  0.4536 (0.4536)  Acc@1: 90.8203 (90.8203)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.164 (0.323)  Loss:  0.6006 (1.0037)  Acc@1: 86.4387 (76.8760)  Acc@5: 97.7594 (93.6960)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-152.pth.tar', 76.87600005615235)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-149.pth.tar', 76.77800008300781)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-150.pth.tar', 76.66199998291016)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-147.pth.tar', 76.5660001171875)

Train: 153 [   0/1251 (  0%)]  Loss: 3.393 (3.39)  Time: 3.300s,  310.29/s  (3.300s,  310.29/s)  LR: 4.869e-04  Data: 1.749 (1.749)
Train: 153 [  50/1251 (  4%)]  Loss: 3.552 (3.47)  Time: 0.657s, 1557.74/s  (0.687s, 1489.64/s)  LR: 4.867e-04  Data: 0.013 (0.048)
Train: 153 [ 100/1251 (  8%)]  Loss: 3.175 (3.37)  Time: 0.667s, 1536.09/s  (0.676s, 1515.10/s)  LR: 4.865e-04  Data: 0.013 (0.031)
Train: 153 [ 150/1251 ( 12%)]  Loss: 3.663 (3.45)  Time: 0.666s, 1536.97/s  (0.673s, 1521.19/s)  LR: 4.862e-04  Data: 0.015 (0.025)
Train: 153 [ 200/1251 ( 16%)]  Loss: 3.194 (3.40)  Time: 0.666s, 1536.39/s  (0.672s, 1523.94/s)  LR: 4.860e-04  Data: 0.014 (0.022)
Train: 153 [ 250/1251 ( 20%)]  Loss: 3.102 (3.35)  Time: 0.677s, 1512.10/s  (0.671s, 1525.12/s)  LR: 4.858e-04  Data: 0.013 (0.021)
Train: 153 [ 300/1251 ( 24%)]  Loss: 3.107 (3.31)  Time: 0.671s, 1525.04/s  (0.671s, 1526.11/s)  LR: 4.856e-04  Data: 0.016 (0.020)
Train: 153 [ 350/1251 ( 28%)]  Loss: 3.373 (3.32)  Time: 0.663s, 1544.54/s  (0.670s, 1527.38/s)  LR: 4.854e-04  Data: 0.016 (0.019)
Train: 153 [ 400/1251 ( 32%)]  Loss: 3.583 (3.35)  Time: 0.668s, 1532.95/s  (0.670s, 1528.16/s)  LR: 4.852e-04  Data: 0.013 (0.018)
Train: 153 [ 450/1251 ( 36%)]  Loss: 3.431 (3.36)  Time: 0.666s, 1538.33/s  (0.670s, 1528.87/s)  LR: 4.850e-04  Data: 0.015 (0.018)
Train: 153 [ 500/1251 ( 40%)]  Loss: 3.734 (3.39)  Time: 0.668s, 1532.00/s  (0.670s, 1529.10/s)  LR: 4.848e-04  Data: 0.013 (0.017)
Train: 153 [ 550/1251 ( 44%)]  Loss: 3.678 (3.42)  Time: 0.667s, 1535.76/s  (0.669s, 1529.87/s)  LR: 4.846e-04  Data: 0.013 (0.017)
Train: 153 [ 600/1251 ( 48%)]  Loss: 3.848 (3.45)  Time: 0.667s, 1534.33/s  (0.669s, 1530.14/s)  LR: 4.844e-04  Data: 0.015 (0.017)
Train: 153 [ 650/1251 ( 52%)]  Loss: 3.083 (3.42)  Time: 0.662s, 1547.88/s  (0.669s, 1530.44/s)  LR: 4.842e-04  Data: 0.013 (0.016)
Train: 153 [ 700/1251 ( 56%)]  Loss: 3.506 (3.43)  Time: 0.672s, 1524.68/s  (0.669s, 1530.35/s)  LR: 4.840e-04  Data: 0.012 (0.016)
Train: 153 [ 750/1251 ( 60%)]  Loss: 3.425 (3.43)  Time: 0.663s, 1544.56/s  (0.669s, 1530.34/s)  LR: 4.837e-04  Data: 0.015 (0.016)
Train: 153 [ 800/1251 ( 64%)]  Loss: 3.153 (3.41)  Time: 0.670s, 1528.73/s  (0.669s, 1530.77/s)  LR: 4.835e-04  Data: 0.012 (0.016)
Train: 153 [ 850/1251 ( 68%)]  Loss: 3.308 (3.41)  Time: 0.667s, 1536.17/s  (0.669s, 1530.68/s)  LR: 4.833e-04  Data: 0.013 (0.016)
Train: 153 [ 900/1251 ( 72%)]  Loss: 3.148 (3.39)  Time: 0.671s, 1526.79/s  (0.669s, 1530.61/s)  LR: 4.831e-04  Data: 0.014 (0.016)
Train: 153 [ 950/1251 ( 76%)]  Loss: 3.540 (3.40)  Time: 0.675s, 1516.03/s  (0.669s, 1530.35/s)  LR: 4.829e-04  Data: 0.013 (0.016)
Train: 153 [1000/1251 ( 80%)]  Loss: 3.675 (3.41)  Time: 0.670s, 1528.08/s  (0.669s, 1530.01/s)  LR: 4.827e-04  Data: 0.012 (0.015)
Train: 153 [1050/1251 ( 84%)]  Loss: 3.639 (3.42)  Time: 0.667s, 1534.20/s  (0.669s, 1529.82/s)  LR: 4.825e-04  Data: 0.013 (0.015)
Train: 153 [1100/1251 ( 88%)]  Loss: 3.423 (3.42)  Time: 0.667s, 1534.43/s  (0.669s, 1529.57/s)  LR: 4.823e-04  Data: 0.014 (0.015)
Train: 153 [1150/1251 ( 92%)]  Loss: 3.354 (3.42)  Time: 0.668s, 1532.56/s  (0.670s, 1529.45/s)  LR: 4.821e-04  Data: 0.017 (0.015)
Train: 153 [1200/1251 ( 96%)]  Loss: 3.542 (3.43)  Time: 0.672s, 1523.29/s  (0.670s, 1529.38/s)  LR: 4.819e-04  Data: 0.015 (0.015)
Train: 153 [1250/1251 (100%)]  Loss: 3.166 (3.42)  Time: 0.668s, 1533.34/s  (0.670s, 1529.47/s)  LR: 4.817e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.978 (2.978)  Loss:  0.4849 (0.4849)  Acc@1: 90.6250 (90.6250)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.162 (0.333)  Loss:  0.6030 (0.9970)  Acc@1: 86.2028 (76.6820)  Acc@5: 96.6981 (93.7000)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-152.pth.tar', 76.87600005615235)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-149.pth.tar', 76.77800008300781)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-153.pth.tar', 76.68200008300781)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-150.pth.tar', 76.66199998291016)

Train: 154 [   0/1251 (  0%)]  Loss: 3.328 (3.33)  Time: 4.218s,  242.75/s  (4.218s,  242.75/s)  LR: 4.817e-04  Data: 1.562 (1.562)
Train: 154 [  50/1251 (  4%)]  Loss: 3.208 (3.27)  Time: 0.649s, 1577.03/s  (0.697s, 1469.45/s)  LR: 4.815e-04  Data: 0.013 (0.044)
Train: 154 [ 100/1251 (  8%)]  Loss: 3.206 (3.25)  Time: 0.660s, 1552.11/s  (0.679s, 1509.00/s)  LR: 4.812e-04  Data: 0.014 (0.029)
Train: 154 [ 150/1251 ( 12%)]  Loss: 3.476 (3.30)  Time: 0.661s, 1549.01/s  (0.674s, 1519.61/s)  LR: 4.810e-04  Data: 0.013 (0.024)
Train: 154 [ 200/1251 ( 16%)]  Loss: 2.976 (3.24)  Time: 0.659s, 1554.00/s  (0.673s, 1522.08/s)  LR: 4.808e-04  Data: 0.018 (0.021)
Train: 154 [ 250/1251 ( 20%)]  Loss: 3.700 (3.32)  Time: 0.674s, 1520.03/s  (0.672s, 1523.76/s)  LR: 4.806e-04  Data: 0.013 (0.020)
Train: 154 [ 300/1251 ( 24%)]  Loss: 3.165 (3.29)  Time: 0.675s, 1516.05/s  (0.671s, 1525.06/s)  LR: 4.804e-04  Data: 0.018 (0.019)
Train: 154 [ 350/1251 ( 28%)]  Loss: 3.536 (3.32)  Time: 0.673s, 1520.85/s  (0.671s, 1526.42/s)  LR: 4.802e-04  Data: 0.017 (0.018)
Train: 154 [ 400/1251 ( 32%)]  Loss: 3.884 (3.39)  Time: 0.670s, 1529.14/s  (0.670s, 1527.28/s)  LR: 4.800e-04  Data: 0.014 (0.018)
Train: 154 [ 450/1251 ( 36%)]  Loss: 3.780 (3.43)  Time: 0.671s, 1527.09/s  (0.670s, 1528.20/s)  LR: 4.798e-04  Data: 0.013 (0.017)
Train: 154 [ 500/1251 ( 40%)]  Loss: 3.365 (3.42)  Time: 0.666s, 1537.00/s  (0.670s, 1528.49/s)  LR: 4.796e-04  Data: 0.013 (0.017)
Train: 154 [ 550/1251 ( 44%)]  Loss: 3.336 (3.41)  Time: 0.673s, 1522.67/s  (0.670s, 1528.49/s)  LR: 4.794e-04  Data: 0.013 (0.016)
Train: 154 [ 600/1251 ( 48%)]  Loss: 3.562 (3.42)  Time: 0.661s, 1549.06/s  (0.670s, 1528.84/s)  LR: 4.792e-04  Data: 0.013 (0.016)
Train: 154 [ 650/1251 ( 52%)]  Loss: 3.148 (3.40)  Time: 0.666s, 1536.93/s  (0.670s, 1528.88/s)  LR: 4.790e-04  Data: 0.013 (0.016)
Train: 154 [ 700/1251 ( 56%)]  Loss: 3.473 (3.41)  Time: 0.662s, 1545.88/s  (0.670s, 1529.09/s)  LR: 4.788e-04  Data: 0.013 (0.016)
Train: 154 [ 750/1251 ( 60%)]  Loss: 3.524 (3.42)  Time: 0.668s, 1532.73/s  (0.670s, 1529.08/s)  LR: 4.785e-04  Data: 0.011 (0.016)
Train: 154 [ 800/1251 ( 64%)]  Loss: 3.259 (3.41)  Time: 0.672s, 1523.62/s  (0.670s, 1529.06/s)  LR: 4.783e-04  Data: 0.016 (0.015)
Train: 154 [ 850/1251 ( 68%)]  Loss: 2.956 (3.38)  Time: 0.671s, 1525.38/s  (0.670s, 1528.99/s)  LR: 4.781e-04  Data: 0.013 (0.015)
Train: 154 [ 900/1251 ( 72%)]  Loss: 3.751 (3.40)  Time: 0.681s, 1504.37/s  (0.670s, 1528.93/s)  LR: 4.779e-04  Data: 0.013 (0.015)
Train: 154 [ 950/1251 ( 76%)]  Loss: 3.370 (3.40)  Time: 0.666s, 1537.97/s  (0.670s, 1528.99/s)  LR: 4.777e-04  Data: 0.012 (0.015)
Train: 154 [1000/1251 ( 80%)]  Loss: 3.253 (3.39)  Time: 0.667s, 1535.73/s  (0.670s, 1529.11/s)  LR: 4.775e-04  Data: 0.012 (0.015)
Train: 154 [1050/1251 ( 84%)]  Loss: 3.354 (3.39)  Time: 0.669s, 1531.79/s  (0.670s, 1529.13/s)  LR: 4.773e-04  Data: 0.016 (0.015)
Train: 154 [1100/1251 ( 88%)]  Loss: 3.282 (3.39)  Time: 0.669s, 1530.78/s  (0.670s, 1529.03/s)  LR: 4.771e-04  Data: 0.014 (0.015)
Train: 154 [1150/1251 ( 92%)]  Loss: 3.493 (3.39)  Time: 0.669s, 1529.76/s  (0.670s, 1528.67/s)  LR: 4.769e-04  Data: 0.015 (0.015)
Train: 154 [1200/1251 ( 96%)]  Loss: 3.484 (3.39)  Time: 0.682s, 1501.75/s  (0.670s, 1528.56/s)  LR: 4.767e-04  Data: 0.013 (0.015)
Train: 154 [1250/1251 (100%)]  Loss: 3.677 (3.41)  Time: 0.655s, 1564.00/s  (0.670s, 1528.65/s)  LR: 4.765e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.740 (2.740)  Loss:  0.4600 (0.4600)  Acc@1: 90.4297 (90.4297)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.162 (0.330)  Loss:  0.5850 (0.9891)  Acc@1: 85.0236 (76.6980)  Acc@5: 97.0519 (93.7160)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-152.pth.tar', 76.87600005615235)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-149.pth.tar', 76.77800008300781)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-154.pth.tar', 76.69800008789062)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-153.pth.tar', 76.68200008300781)

Train: 155 [   0/1251 (  0%)]  Loss: 3.533 (3.53)  Time: 4.506s,  227.25/s  (4.506s,  227.25/s)  LR: 4.765e-04  Data: 1.910 (1.910)
Train: 155 [  50/1251 (  4%)]  Loss: 3.102 (3.32)  Time: 0.660s, 1551.49/s  (0.704s, 1454.86/s)  LR: 4.763e-04  Data: 0.016 (0.051)
Train: 155 [ 100/1251 (  8%)]  Loss: 3.277 (3.30)  Time: 0.668s, 1533.99/s  (0.682s, 1500.79/s)  LR: 4.760e-04  Data: 0.014 (0.033)
Train: 155 [ 150/1251 ( 12%)]  Loss: 3.535 (3.36)  Time: 0.678s, 1509.49/s  (0.677s, 1512.02/s)  LR: 4.758e-04  Data: 0.012 (0.027)
Train: 155 [ 200/1251 ( 16%)]  Loss: 3.612 (3.41)  Time: 0.662s, 1546.65/s  (0.675s, 1517.73/s)  LR: 4.756e-04  Data: 0.013 (0.023)
Train: 155 [ 250/1251 ( 20%)]  Loss: 3.115 (3.36)  Time: 0.664s, 1542.14/s  (0.674s, 1520.35/s)  LR: 4.754e-04  Data: 0.014 (0.021)
Train: 155 [ 300/1251 ( 24%)]  Loss: 3.644 (3.40)  Time: 0.668s, 1533.58/s  (0.673s, 1522.21/s)  LR: 4.752e-04  Data: 0.013 (0.020)
Train: 155 [ 350/1251 ( 28%)]  Loss: 3.266 (3.39)  Time: 0.673s, 1521.49/s  (0.672s, 1523.62/s)  LR: 4.750e-04  Data: 0.014 (0.019)
Train: 155 [ 400/1251 ( 32%)]  Loss: 3.573 (3.41)  Time: 0.670s, 1528.60/s  (0.671s, 1525.62/s)  LR: 4.748e-04  Data: 0.014 (0.018)
Train: 155 [ 450/1251 ( 36%)]  Loss: 3.881 (3.45)  Time: 0.672s, 1523.93/s  (0.671s, 1526.64/s)  LR: 4.746e-04  Data: 0.013 (0.018)
Train: 155 [ 500/1251 ( 40%)]  Loss: 3.220 (3.43)  Time: 0.669s, 1530.91/s  (0.670s, 1527.24/s)  LR: 4.744e-04  Data: 0.017 (0.017)
Train: 155 [ 550/1251 ( 44%)]  Loss: 3.121 (3.41)  Time: 0.669s, 1529.62/s  (0.670s, 1528.04/s)  LR: 4.742e-04  Data: 0.015 (0.017)
Train: 155 [ 600/1251 ( 48%)]  Loss: 3.545 (3.42)  Time: 0.681s, 1503.85/s  (0.670s, 1527.87/s)  LR: 4.740e-04  Data: 0.013 (0.017)
Train: 155 [ 650/1251 ( 52%)]  Loss: 3.380 (3.41)  Time: 0.668s, 1533.11/s  (0.670s, 1528.03/s)  LR: 4.738e-04  Data: 0.013 (0.017)
Train: 155 [ 700/1251 ( 56%)]  Loss: 3.594 (3.43)  Time: 0.667s, 1534.34/s  (0.670s, 1528.23/s)  LR: 4.735e-04  Data: 0.012 (0.016)
Train: 155 [ 750/1251 ( 60%)]  Loss: 3.285 (3.42)  Time: 0.687s, 1489.50/s  (0.670s, 1528.60/s)  LR: 4.733e-04  Data: 0.012 (0.016)
Train: 155 [ 800/1251 ( 64%)]  Loss: 3.570 (3.43)  Time: 0.666s, 1537.95/s  (0.670s, 1528.86/s)  LR: 4.731e-04  Data: 0.013 (0.016)
Train: 155 [ 850/1251 ( 68%)]  Loss: 3.566 (3.43)  Time: 0.669s, 1529.92/s  (0.670s, 1529.04/s)  LR: 4.729e-04  Data: 0.013 (0.016)
Train: 155 [ 900/1251 ( 72%)]  Loss: 3.496 (3.44)  Time: 0.666s, 1538.21/s  (0.670s, 1529.01/s)  LR: 4.727e-04  Data: 0.013 (0.016)
Train: 155 [ 950/1251 ( 76%)]  Loss: 3.660 (3.45)  Time: 0.679s, 1508.06/s  (0.670s, 1528.88/s)  LR: 4.725e-04  Data: 0.013 (0.016)
Train: 155 [1000/1251 ( 80%)]  Loss: 3.484 (3.45)  Time: 0.673s, 1520.51/s  (0.670s, 1528.64/s)  LR: 4.723e-04  Data: 0.013 (0.016)
Train: 155 [1050/1251 ( 84%)]  Loss: 3.708 (3.46)  Time: 0.675s, 1517.93/s  (0.670s, 1528.75/s)  LR: 4.721e-04  Data: 0.013 (0.015)
Train: 155 [1100/1251 ( 88%)]  Loss: 3.711 (3.47)  Time: 0.668s, 1533.44/s  (0.670s, 1528.71/s)  LR: 4.719e-04  Data: 0.018 (0.015)
Train: 155 [1150/1251 ( 92%)]  Loss: 3.275 (3.46)  Time: 0.667s, 1534.65/s  (0.670s, 1528.69/s)  LR: 4.717e-04  Data: 0.015 (0.015)
Train: 155 [1200/1251 ( 96%)]  Loss: 3.510 (3.47)  Time: 0.676s, 1514.45/s  (0.670s, 1528.75/s)  LR: 4.715e-04  Data: 0.013 (0.015)
Train: 155 [1250/1251 (100%)]  Loss: 3.568 (3.47)  Time: 0.651s, 1573.42/s  (0.670s, 1528.95/s)  LR: 4.713e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.144 (3.144)  Loss:  0.4685 (0.4685)  Acc@1: 90.0391 (90.0391)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.324)  Loss:  0.5991 (0.9765)  Acc@1: 85.2594 (77.1180)  Acc@5: 97.1698 (93.8540)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-155.pth.tar', 77.11800006103516)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-152.pth.tar', 76.87600005615235)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-149.pth.tar', 76.77800008300781)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-154.pth.tar', 76.69800008789062)

Train: 156 [   0/1251 (  0%)]  Loss: 3.514 (3.51)  Time: 3.443s,  297.43/s  (3.443s,  297.43/s)  LR: 4.713e-04  Data: 1.574 (1.574)
Train: 156 [  50/1251 (  4%)]  Loss: 3.503 (3.51)  Time: 0.653s, 1567.28/s  (0.687s, 1490.02/s)  LR: 4.710e-04  Data: 0.013 (0.044)
Train: 156 [ 100/1251 (  8%)]  Loss: 3.444 (3.49)  Time: 0.669s, 1530.39/s  (0.674s, 1519.10/s)  LR: 4.708e-04  Data: 0.014 (0.029)
Train: 156 [ 150/1251 ( 12%)]  Loss: 3.718 (3.54)  Time: 0.666s, 1538.30/s  (0.671s, 1526.58/s)  LR: 4.706e-04  Data: 0.013 (0.024)
Train: 156 [ 200/1251 ( 16%)]  Loss: 3.769 (3.59)  Time: 0.664s, 1541.82/s  (0.669s, 1529.53/s)  LR: 4.704e-04  Data: 0.014 (0.021)
Train: 156 [ 250/1251 ( 20%)]  Loss: 3.276 (3.54)  Time: 0.665s, 1538.72/s  (0.669s, 1530.62/s)  LR: 4.702e-04  Data: 0.014 (0.020)
Train: 156 [ 300/1251 ( 24%)]  Loss: 3.607 (3.55)  Time: 0.663s, 1545.27/s  (0.669s, 1531.15/s)  LR: 4.700e-04  Data: 0.013 (0.019)
Train: 156 [ 350/1251 ( 28%)]  Loss: 3.508 (3.54)  Time: 0.665s, 1538.93/s  (0.669s, 1531.63/s)  LR: 4.698e-04  Data: 0.014 (0.018)
Train: 156 [ 400/1251 ( 32%)]  Loss: 2.991 (3.48)  Time: 0.668s, 1532.09/s  (0.668s, 1532.36/s)  LR: 4.696e-04  Data: 0.013 (0.018)
Train: 156 [ 450/1251 ( 36%)]  Loss: 3.314 (3.46)  Time: 0.666s, 1538.63/s  (0.668s, 1532.64/s)  LR: 4.694e-04  Data: 0.013 (0.017)
Train: 156 [ 500/1251 ( 40%)]  Loss: 3.701 (3.49)  Time: 0.667s, 1536.25/s  (0.668s, 1532.29/s)  LR: 4.692e-04  Data: 0.013 (0.017)
Train: 156 [ 550/1251 ( 44%)]  Loss: 3.117 (3.46)  Time: 0.674s, 1518.92/s  (0.668s, 1532.04/s)  LR: 4.690e-04  Data: 0.014 (0.017)
Train: 156 [ 600/1251 ( 48%)]  Loss: 3.357 (3.45)  Time: 0.670s, 1528.69/s  (0.669s, 1531.54/s)  LR: 4.688e-04  Data: 0.013 (0.016)
Train: 156 [ 650/1251 ( 52%)]  Loss: 3.383 (3.44)  Time: 0.674s, 1518.40/s  (0.669s, 1531.21/s)  LR: 4.686e-04  Data: 0.014 (0.016)
Train: 156 [ 700/1251 ( 56%)]  Loss: 3.438 (3.44)  Time: 0.670s, 1528.88/s  (0.669s, 1531.00/s)  LR: 4.683e-04  Data: 0.013 (0.016)
Train: 156 [ 750/1251 ( 60%)]  Loss: 3.194 (3.43)  Time: 0.666s, 1538.54/s  (0.669s, 1530.54/s)  LR: 4.681e-04  Data: 0.012 (0.016)
Train: 156 [ 800/1251 ( 64%)]  Loss: 3.721 (3.44)  Time: 0.676s, 1514.93/s  (0.669s, 1530.04/s)  LR: 4.679e-04  Data: 0.013 (0.016)
Train: 156 [ 850/1251 ( 68%)]  Loss: 3.284 (3.44)  Time: 0.672s, 1523.94/s  (0.670s, 1529.49/s)  LR: 4.677e-04  Data: 0.014 (0.016)
Train: 156 [ 900/1251 ( 72%)]  Loss: 3.582 (3.44)  Time: 0.666s, 1537.00/s  (0.669s, 1529.50/s)  LR: 4.675e-04  Data: 0.014 (0.015)
Train: 156 [ 950/1251 ( 76%)]  Loss: 3.259 (3.43)  Time: 0.670s, 1527.76/s  (0.670s, 1529.36/s)  LR: 4.673e-04  Data: 0.015 (0.015)
Train: 156 [1000/1251 ( 80%)]  Loss: 3.643 (3.44)  Time: 0.663s, 1544.67/s  (0.670s, 1529.22/s)  LR: 4.671e-04  Data: 0.013 (0.015)
Train: 156 [1050/1251 ( 84%)]  Loss: 3.457 (3.44)  Time: 0.677s, 1511.86/s  (0.670s, 1528.89/s)  LR: 4.669e-04  Data: 0.013 (0.015)
Train: 156 [1100/1251 ( 88%)]  Loss: 3.807 (3.46)  Time: 0.673s, 1522.09/s  (0.670s, 1528.55/s)  LR: 4.667e-04  Data: 0.013 (0.015)
Train: 156 [1150/1251 ( 92%)]  Loss: 3.590 (3.47)  Time: 0.669s, 1531.27/s  (0.670s, 1528.50/s)  LR: 4.665e-04  Data: 0.013 (0.015)
Train: 156 [1200/1251 ( 96%)]  Loss: 3.529 (3.47)  Time: 0.670s, 1527.58/s  (0.670s, 1528.50/s)  LR: 4.663e-04  Data: 0.013 (0.015)
Train: 156 [1250/1251 (100%)]  Loss: 3.549 (3.47)  Time: 0.662s, 1546.06/s  (0.670s, 1528.71/s)  LR: 4.661e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.140 (3.140)  Loss:  0.4700 (0.4700)  Acc@1: 90.9180 (90.9180)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.164 (0.322)  Loss:  0.6060 (0.9741)  Acc@1: 86.0849 (76.8680)  Acc@5: 96.8160 (93.6620)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-155.pth.tar', 77.11800006103516)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-152.pth.tar', 76.87600005615235)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-156.pth.tar', 76.86800003173828)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-149.pth.tar', 76.77800008300781)

Train: 157 [   0/1251 (  0%)]  Loss: 3.306 (3.31)  Time: 3.510s,  291.73/s  (3.510s,  291.73/s)  LR: 4.661e-04  Data: 2.007 (2.007)
Train: 157 [  50/1251 (  4%)]  Loss: 3.414 (3.36)  Time: 0.655s, 1563.37/s  (0.689s, 1485.17/s)  LR: 4.659e-04  Data: 0.013 (0.053)
Train: 157 [ 100/1251 (  8%)]  Loss: 3.418 (3.38)  Time: 0.668s, 1533.33/s  (0.676s, 1515.29/s)  LR: 4.656e-04  Data: 0.013 (0.033)
Train: 157 [ 150/1251 ( 12%)]  Loss: 3.338 (3.37)  Time: 0.668s, 1532.90/s  (0.672s, 1523.22/s)  LR: 4.654e-04  Data: 0.013 (0.027)
Train: 157 [ 200/1251 ( 16%)]  Loss: 3.220 (3.34)  Time: 0.666s, 1537.94/s  (0.671s, 1526.67/s)  LR: 4.652e-04  Data: 0.016 (0.024)
Train: 157 [ 250/1251 ( 20%)]  Loss: 3.474 (3.36)  Time: 0.673s, 1521.90/s  (0.670s, 1527.83/s)  LR: 4.650e-04  Data: 0.012 (0.022)
Train: 157 [ 300/1251 ( 24%)]  Loss: 3.494 (3.38)  Time: 0.674s, 1518.69/s  (0.670s, 1528.33/s)  LR: 4.648e-04  Data: 0.013 (0.020)
Train: 157 [ 350/1251 ( 28%)]  Loss: 3.448 (3.39)  Time: 0.668s, 1533.48/s  (0.670s, 1529.08/s)  LR: 4.646e-04  Data: 0.013 (0.019)
Train: 157 [ 400/1251 ( 32%)]  Loss: 3.703 (3.42)  Time: 0.661s, 1548.72/s  (0.670s, 1529.05/s)  LR: 4.644e-04  Data: 0.014 (0.019)
Train: 157 [ 450/1251 ( 36%)]  Loss: 3.282 (3.41)  Time: 0.666s, 1536.68/s  (0.670s, 1529.09/s)  LR: 4.642e-04  Data: 0.013 (0.018)
Train: 157 [ 500/1251 ( 40%)]  Loss: 3.718 (3.44)  Time: 0.664s, 1541.41/s  (0.670s, 1529.14/s)  LR: 4.640e-04  Data: 0.014 (0.018)
Train: 157 [ 550/1251 ( 44%)]  Loss: 3.407 (3.44)  Time: 0.660s, 1552.10/s  (0.669s, 1529.64/s)  LR: 4.638e-04  Data: 0.015 (0.017)
Train: 157 [ 600/1251 ( 48%)]  Loss: 3.656 (3.45)  Time: 0.672s, 1524.14/s  (0.669s, 1529.87/s)  LR: 4.636e-04  Data: 0.013 (0.017)
Train: 157 [ 650/1251 ( 52%)]  Loss: 3.435 (3.45)  Time: 0.672s, 1524.50/s  (0.670s, 1529.28/s)  LR: 4.634e-04  Data: 0.013 (0.017)
Train: 157 [ 700/1251 ( 56%)]  Loss: 3.041 (3.42)  Time: 0.671s, 1526.38/s  (0.670s, 1529.15/s)  LR: 4.632e-04  Data: 0.013 (0.017)
Train: 157 [ 750/1251 ( 60%)]  Loss: 3.382 (3.42)  Time: 0.668s, 1532.91/s  (0.670s, 1529.05/s)  LR: 4.629e-04  Data: 0.011 (0.016)
Train: 157 [ 800/1251 ( 64%)]  Loss: 3.495 (3.43)  Time: 0.673s, 1522.20/s  (0.670s, 1529.06/s)  LR: 4.627e-04  Data: 0.012 (0.016)
Train: 157 [ 850/1251 ( 68%)]  Loss: 3.748 (3.44)  Time: 0.663s, 1545.51/s  (0.670s, 1528.95/s)  LR: 4.625e-04  Data: 0.013 (0.016)
Train: 157 [ 900/1251 ( 72%)]  Loss: 3.596 (3.45)  Time: 0.661s, 1549.84/s  (0.670s, 1528.95/s)  LR: 4.623e-04  Data: 0.014 (0.016)
Train: 157 [ 950/1251 ( 76%)]  Loss: 3.138 (3.44)  Time: 0.675s, 1516.89/s  (0.670s, 1528.71/s)  LR: 4.621e-04  Data: 0.013 (0.016)
Train: 157 [1000/1251 ( 80%)]  Loss: 3.214 (3.43)  Time: 0.682s, 1500.90/s  (0.670s, 1528.46/s)  LR: 4.619e-04  Data: 0.013 (0.016)
Train: 157 [1050/1251 ( 84%)]  Loss: 3.491 (3.43)  Time: 0.674s, 1519.32/s  (0.670s, 1528.28/s)  LR: 4.617e-04  Data: 0.013 (0.016)
Train: 157 [1100/1251 ( 88%)]  Loss: 3.534 (3.43)  Time: 0.666s, 1536.64/s  (0.670s, 1528.16/s)  LR: 4.615e-04  Data: 0.013 (0.015)
Train: 157 [1150/1251 ( 92%)]  Loss: 3.172 (3.42)  Time: 0.660s, 1550.70/s  (0.670s, 1528.14/s)  LR: 4.613e-04  Data: 0.013 (0.015)
Train: 157 [1200/1251 ( 96%)]  Loss: 3.530 (3.43)  Time: 0.666s, 1538.40/s  (0.670s, 1527.98/s)  LR: 4.611e-04  Data: 0.013 (0.015)
Train: 157 [1250/1251 (100%)]  Loss: 3.570 (3.43)  Time: 0.660s, 1550.98/s  (0.670s, 1528.18/s)  LR: 4.609e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.865 (2.865)  Loss:  0.4465 (0.4465)  Acc@1: 90.8203 (90.8203)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.163 (0.321)  Loss:  0.5962 (0.9845)  Acc@1: 85.7311 (76.7640)  Acc@5: 97.6415 (93.7820)
Train: 158 [   0/1251 (  0%)]  Loss: 3.311 (3.31)  Time: 4.381s,  233.73/s  (4.381s,  233.73/s)  LR: 4.609e-04  Data: 1.793 (1.793)
Train: 158 [  50/1251 (  4%)]  Loss: 3.456 (3.38)  Time: 0.655s, 1564.08/s  (0.700s, 1462.98/s)  LR: 4.607e-04  Data: 0.012 (0.049)
Train: 158 [ 100/1251 (  8%)]  Loss: 3.511 (3.43)  Time: 0.659s, 1553.81/s  (0.681s, 1504.11/s)  LR: 4.605e-04  Data: 0.012 (0.031)
Train: 158 [ 150/1251 ( 12%)]  Loss: 3.396 (3.42)  Time: 0.666s, 1538.60/s  (0.676s, 1515.57/s)  LR: 4.602e-04  Data: 0.013 (0.025)
Train: 158 [ 200/1251 ( 16%)]  Loss: 3.141 (3.36)  Time: 0.660s, 1551.68/s  (0.673s, 1521.36/s)  LR: 4.600e-04  Data: 0.013 (0.022)
Train: 158 [ 250/1251 ( 20%)]  Loss: 3.498 (3.39)  Time: 0.664s, 1541.19/s  (0.672s, 1524.21/s)  LR: 4.598e-04  Data: 0.013 (0.021)
Train: 158 [ 300/1251 ( 24%)]  Loss: 3.435 (3.39)  Time: 0.662s, 1547.88/s  (0.671s, 1526.91/s)  LR: 4.596e-04  Data: 0.013 (0.020)
Train: 158 [ 350/1251 ( 28%)]  Loss: 3.542 (3.41)  Time: 0.669s, 1530.62/s  (0.670s, 1528.47/s)  LR: 4.594e-04  Data: 0.015 (0.019)
Train: 158 [ 400/1251 ( 32%)]  Loss: 2.994 (3.36)  Time: 0.678s, 1509.28/s  (0.670s, 1528.97/s)  LR: 4.592e-04  Data: 0.014 (0.018)
Train: 158 [ 450/1251 ( 36%)]  Loss: 3.592 (3.39)  Time: 0.664s, 1541.92/s  (0.670s, 1528.67/s)  LR: 4.590e-04  Data: 0.016 (0.018)
Train: 158 [ 500/1251 ( 40%)]  Loss: 3.279 (3.38)  Time: 0.673s, 1521.25/s  (0.670s, 1529.04/s)  LR: 4.588e-04  Data: 0.013 (0.017)
Train: 158 [ 550/1251 ( 44%)]  Loss: 3.367 (3.38)  Time: 0.669s, 1530.04/s  (0.670s, 1528.66/s)  LR: 4.586e-04  Data: 0.014 (0.017)
Train: 158 [ 600/1251 ( 48%)]  Loss: 3.337 (3.37)  Time: 0.664s, 1541.65/s  (0.670s, 1528.61/s)  LR: 4.584e-04  Data: 0.013 (0.017)
Train: 158 [ 650/1251 ( 52%)]  Loss: 3.716 (3.40)  Time: 0.669s, 1529.60/s  (0.670s, 1528.97/s)  LR: 4.582e-04  Data: 0.013 (0.016)
Train: 158 [ 700/1251 ( 56%)]  Loss: 3.444 (3.40)  Time: 0.673s, 1521.18/s  (0.670s, 1529.16/s)  LR: 4.580e-04  Data: 0.014 (0.016)
Train: 158 [ 750/1251 ( 60%)]  Loss: 3.438 (3.40)  Time: 0.679s, 1508.53/s  (0.670s, 1528.94/s)  LR: 4.578e-04  Data: 0.013 (0.016)
Train: 158 [ 800/1251 ( 64%)]  Loss: 3.587 (3.41)  Time: 0.683s, 1499.81/s  (0.670s, 1528.52/s)  LR: 4.575e-04  Data: 0.015 (0.016)
Train: 158 [ 850/1251 ( 68%)]  Loss: 3.259 (3.41)  Time: 0.671s, 1526.98/s  (0.670s, 1528.25/s)  LR: 4.573e-04  Data: 0.013 (0.016)
Train: 158 [ 900/1251 ( 72%)]  Loss: 3.721 (3.42)  Time: 0.669s, 1530.00/s  (0.670s, 1527.96/s)  LR: 4.571e-04  Data: 0.014 (0.016)
Train: 158 [ 950/1251 ( 76%)]  Loss: 3.616 (3.43)  Time: 0.671s, 1526.59/s  (0.670s, 1527.75/s)  LR: 4.569e-04  Data: 0.013 (0.015)
Train: 158 [1000/1251 ( 80%)]  Loss: 3.178 (3.42)  Time: 0.671s, 1526.98/s  (0.670s, 1527.72/s)  LR: 4.567e-04  Data: 0.013 (0.015)
Train: 158 [1050/1251 ( 84%)]  Loss: 3.388 (3.42)  Time: 0.667s, 1534.82/s  (0.670s, 1527.46/s)  LR: 4.565e-04  Data: 0.013 (0.015)
Train: 158 [1100/1251 ( 88%)]  Loss: 3.430 (3.42)  Time: 0.669s, 1530.52/s  (0.671s, 1527.19/s)  LR: 4.563e-04  Data: 0.013 (0.015)
Train: 158 [1150/1251 ( 92%)]  Loss: 3.641 (3.43)  Time: 0.679s, 1508.33/s  (0.671s, 1526.97/s)  LR: 4.561e-04  Data: 0.013 (0.015)
Train: 158 [1200/1251 ( 96%)]  Loss: 3.620 (3.44)  Time: 0.669s, 1530.73/s  (0.671s, 1526.73/s)  LR: 4.559e-04  Data: 0.014 (0.015)
Train: 158 [1250/1251 (100%)]  Loss: 3.624 (3.44)  Time: 0.654s, 1565.54/s  (0.671s, 1526.75/s)  LR: 4.557e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.829 (2.829)  Loss:  0.4758 (0.4758)  Acc@1: 90.3320 (90.3320)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.169 (0.324)  Loss:  0.5908 (0.9792)  Acc@1: 85.0236 (76.6940)  Acc@5: 97.0519 (93.7140)
Train: 159 [   0/1251 (  0%)]  Loss: 3.202 (3.20)  Time: 3.394s,  301.71/s  (3.394s,  301.71/s)  LR: 4.557e-04  Data: 1.787 (1.787)
Train: 159 [  50/1251 (  4%)]  Loss: 3.251 (3.23)  Time: 0.651s, 1574.11/s  (0.687s, 1490.43/s)  LR: 4.555e-04  Data: 0.014 (0.048)
Train: 159 [ 100/1251 (  8%)]  Loss: 3.517 (3.32)  Time: 0.664s, 1541.60/s  (0.676s, 1515.62/s)  LR: 4.553e-04  Data: 0.013 (0.031)
Train: 159 [ 150/1251 ( 12%)]  Loss: 3.467 (3.36)  Time: 0.677s, 1512.56/s  (0.673s, 1522.07/s)  LR: 4.551e-04  Data: 0.016 (0.026)
Train: 159 [ 200/1251 ( 16%)]  Loss: 3.355 (3.36)  Time: 0.668s, 1533.52/s  (0.671s, 1525.31/s)  LR: 4.548e-04  Data: 0.013 (0.023)
Train: 159 [ 250/1251 ( 20%)]  Loss: 3.303 (3.35)  Time: 0.669s, 1531.07/s  (0.671s, 1527.06/s)  LR: 4.546e-04  Data: 0.014 (0.021)
Train: 159 [ 300/1251 ( 24%)]  Loss: 3.336 (3.35)  Time: 0.665s, 1540.18/s  (0.671s, 1526.78/s)  LR: 4.544e-04  Data: 0.014 (0.020)
Train: 159 [ 350/1251 ( 28%)]  Loss: 3.265 (3.34)  Time: 0.666s, 1537.97/s  (0.671s, 1526.13/s)  LR: 4.542e-04  Data: 0.013 (0.019)
Train: 159 [ 400/1251 ( 32%)]  Loss: 3.371 (3.34)  Time: 0.666s, 1536.51/s  (0.671s, 1525.56/s)  LR: 4.540e-04  Data: 0.012 (0.018)
Train: 159 [ 450/1251 ( 36%)]  Loss: 3.427 (3.35)  Time: 0.665s, 1540.02/s  (0.671s, 1525.49/s)  LR: 4.538e-04  Data: 0.013 (0.018)
Train: 159 [ 500/1251 ( 40%)]  Loss: 3.540 (3.37)  Time: 0.660s, 1551.92/s  (0.671s, 1525.38/s)  LR: 4.536e-04  Data: 0.013 (0.017)
Train: 159 [ 550/1251 ( 44%)]  Loss: 3.104 (3.34)  Time: 0.676s, 1513.71/s  (0.671s, 1525.19/s)  LR: 4.534e-04  Data: 0.013 (0.017)
Train: 159 [ 600/1251 ( 48%)]  Loss: 3.293 (3.34)  Time: 0.668s, 1532.77/s  (0.671s, 1525.16/s)  LR: 4.532e-04  Data: 0.013 (0.017)
Train: 159 [ 650/1251 ( 52%)]  Loss: 3.327 (3.34)  Time: 0.670s, 1527.61/s  (0.672s, 1524.79/s)  LR: 4.530e-04  Data: 0.013 (0.016)
Train: 159 [ 700/1251 ( 56%)]  Loss: 3.178 (3.33)  Time: 0.671s, 1526.50/s  (0.672s, 1524.65/s)  LR: 4.528e-04  Data: 0.013 (0.016)
Train: 159 [ 750/1251 ( 60%)]  Loss: 3.187 (3.32)  Time: 0.667s, 1535.62/s  (0.672s, 1524.80/s)  LR: 4.526e-04  Data: 0.013 (0.016)
Train: 159 [ 800/1251 ( 64%)]  Loss: 3.267 (3.32)  Time: 0.667s, 1534.30/s  (0.671s, 1525.02/s)  LR: 4.524e-04  Data: 0.013 (0.016)
Train: 159 [ 850/1251 ( 68%)]  Loss: 3.472 (3.33)  Time: 0.670s, 1527.26/s  (0.671s, 1525.07/s)  LR: 4.522e-04  Data: 0.013 (0.016)
Train: 159 [ 900/1251 ( 72%)]  Loss: 3.326 (3.33)  Time: 0.675s, 1517.55/s  (0.671s, 1525.03/s)  LR: 4.519e-04  Data: 0.015 (0.016)
Train: 159 [ 950/1251 ( 76%)]  Loss: 3.402 (3.33)  Time: 0.673s, 1522.13/s  (0.672s, 1524.89/s)  LR: 4.517e-04  Data: 0.014 (0.015)
Train: 159 [1000/1251 ( 80%)]  Loss: 3.308 (3.33)  Time: 0.667s, 1535.28/s  (0.672s, 1524.55/s)  LR: 4.515e-04  Data: 0.013 (0.015)
Train: 159 [1050/1251 ( 84%)]  Loss: 3.708 (3.35)  Time: 0.666s, 1538.02/s  (0.672s, 1524.35/s)  LR: 4.513e-04  Data: 0.013 (0.015)
Train: 159 [1100/1251 ( 88%)]  Loss: 3.312 (3.34)  Time: 0.666s, 1538.60/s  (0.672s, 1524.33/s)  LR: 4.511e-04  Data: 0.013 (0.015)
Train: 159 [1150/1251 ( 92%)]  Loss: 3.591 (3.35)  Time: 0.665s, 1540.94/s  (0.672s, 1524.51/s)  LR: 4.509e-04  Data: 0.013 (0.015)
Train: 159 [1200/1251 ( 96%)]  Loss: 3.438 (3.36)  Time: 0.678s, 1510.88/s  (0.672s, 1524.58/s)  LR: 4.507e-04  Data: 0.013 (0.015)
Train: 159 [1250/1251 (100%)]  Loss: 3.455 (3.36)  Time: 0.656s, 1559.95/s  (0.672s, 1524.87/s)  LR: 4.505e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.922 (2.922)  Loss:  0.4648 (0.4648)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.8516 (97.8516)
Test: [  48/48]  Time: 0.161 (0.330)  Loss:  0.5405 (0.9546)  Acc@1: 85.4953 (76.7580)  Acc@5: 97.4057 (93.9180)
Train: 160 [   0/1251 (  0%)]  Loss: 3.516 (3.52)  Time: 3.610s,  283.66/s  (3.610s,  283.66/s)  LR: 4.505e-04  Data: 1.754 (1.754)
Train: 160 [  50/1251 (  4%)]  Loss: 3.419 (3.47)  Time: 0.648s, 1580.21/s  (0.692s, 1479.39/s)  LR: 4.503e-04  Data: 0.013 (0.048)
Train: 160 [ 100/1251 (  8%)]  Loss: 3.539 (3.49)  Time: 0.668s, 1532.49/s  (0.677s, 1511.45/s)  LR: 4.501e-04  Data: 0.016 (0.031)
Train: 160 [ 150/1251 ( 12%)]  Loss: 3.344 (3.45)  Time: 0.671s, 1525.88/s  (0.673s, 1521.02/s)  LR: 4.499e-04  Data: 0.014 (0.026)
Train: 160 [ 200/1251 ( 16%)]  Loss: 3.613 (3.49)  Time: 0.653s, 1567.40/s  (0.671s, 1525.91/s)  LR: 4.497e-04  Data: 0.013 (0.023)
Train: 160 [ 250/1251 ( 20%)]  Loss: 3.237 (3.44)  Time: 0.668s, 1533.27/s  (0.670s, 1528.65/s)  LR: 4.495e-04  Data: 0.016 (0.021)
Train: 160 [ 300/1251 ( 24%)]  Loss: 3.354 (3.43)  Time: 0.663s, 1543.76/s  (0.669s, 1530.03/s)  LR: 4.493e-04  Data: 0.013 (0.020)
Train: 160 [ 350/1251 ( 28%)]  Loss: 3.155 (3.40)  Time: 0.669s, 1529.98/s  (0.669s, 1530.47/s)  LR: 4.490e-04  Data: 0.013 (0.019)
Train: 160 [ 400/1251 ( 32%)]  Loss: 3.658 (3.43)  Time: 0.657s, 1558.91/s  (0.669s, 1531.31/s)  LR: 4.488e-04  Data: 0.015 (0.018)
Train: 160 [ 450/1251 ( 36%)]  Loss: 3.478 (3.43)  Time: 0.674s, 1519.35/s  (0.669s, 1531.47/s)  LR: 4.486e-04  Data: 0.017 (0.018)
Train: 160 [ 500/1251 ( 40%)]  Loss: 3.685 (3.45)  Time: 0.680s, 1505.27/s  (0.668s, 1532.13/s)  LR: 4.484e-04  Data: 0.013 (0.017)
Train: 160 [ 550/1251 ( 44%)]  Loss: 3.644 (3.47)  Time: 0.668s, 1532.98/s  (0.668s, 1532.43/s)  LR: 4.482e-04  Data: 0.012 (0.017)
Train: 160 [ 600/1251 ( 48%)]  Loss: 3.541 (3.48)  Time: 0.668s, 1533.69/s  (0.668s, 1532.13/s)  LR: 4.480e-04  Data: 0.014 (0.017)
Train: 160 [ 650/1251 ( 52%)]  Loss: 3.642 (3.49)  Time: 0.661s, 1548.22/s  (0.668s, 1532.18/s)  LR: 4.478e-04  Data: 0.015 (0.016)
Train: 160 [ 700/1251 ( 56%)]  Loss: 3.738 (3.50)  Time: 0.672s, 1522.98/s  (0.668s, 1532.36/s)  LR: 4.476e-04  Data: 0.013 (0.016)
Train: 160 [ 750/1251 ( 60%)]  Loss: 3.386 (3.50)  Time: 0.665s, 1539.02/s  (0.668s, 1532.36/s)  LR: 4.474e-04  Data: 0.013 (0.016)
Train: 160 [ 800/1251 ( 64%)]  Loss: 3.598 (3.50)  Time: 0.667s, 1536.34/s  (0.668s, 1532.49/s)  LR: 4.472e-04  Data: 0.013 (0.016)
Train: 160 [ 850/1251 ( 68%)]  Loss: 3.521 (3.50)  Time: 0.676s, 1515.39/s  (0.668s, 1532.37/s)  LR: 4.470e-04  Data: 0.013 (0.016)
Train: 160 [ 900/1251 ( 72%)]  Loss: 3.502 (3.50)  Time: 0.668s, 1532.44/s  (0.668s, 1532.01/s)  LR: 4.468e-04  Data: 0.014 (0.016)
Train: 160 [ 950/1251 ( 76%)]  Loss: 3.305 (3.49)  Time: 0.672s, 1522.71/s  (0.669s, 1531.45/s)  LR: 4.466e-04  Data: 0.015 (0.016)
Train: 160 [1000/1251 ( 80%)]  Loss: 3.388 (3.49)  Time: 0.671s, 1527.21/s  (0.669s, 1531.35/s)  LR: 4.464e-04  Data: 0.014 (0.016)
Train: 160 [1050/1251 ( 84%)]  Loss: 3.403 (3.48)  Time: 0.671s, 1525.41/s  (0.669s, 1531.16/s)  LR: 4.461e-04  Data: 0.013 (0.015)
Train: 160 [1100/1251 ( 88%)]  Loss: 3.332 (3.48)  Time: 0.671s, 1526.59/s  (0.669s, 1531.06/s)  LR: 4.459e-04  Data: 0.013 (0.015)
Train: 160 [1150/1251 ( 92%)]  Loss: 3.406 (3.48)  Time: 0.662s, 1546.35/s  (0.669s, 1531.11/s)  LR: 4.457e-04  Data: 0.013 (0.015)
Train: 160 [1200/1251 ( 96%)]  Loss: 3.810 (3.49)  Time: 0.664s, 1542.02/s  (0.669s, 1531.18/s)  LR: 4.455e-04  Data: 0.013 (0.015)
Train: 160 [1250/1251 (100%)]  Loss: 3.593 (3.49)  Time: 0.648s, 1580.86/s  (0.669s, 1531.56/s)  LR: 4.453e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.847 (2.847)  Loss:  0.4985 (0.4985)  Acc@1: 89.1602 (89.1602)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.167 (0.319)  Loss:  0.5664 (0.9806)  Acc@1: 85.9670 (76.9660)  Acc@5: 97.7594 (93.9140)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-155.pth.tar', 77.11800006103516)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-160.pth.tar', 76.96600010986329)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-152.pth.tar', 76.87600005615235)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-156.pth.tar', 76.86800003173828)

Train: 161 [   0/1251 (  0%)]  Loss: 3.526 (3.53)  Time: 3.867s,  264.81/s  (3.867s,  264.81/s)  LR: 4.453e-04  Data: 1.865 (1.865)
Train: 161 [  50/1251 (  4%)]  Loss: 3.318 (3.42)  Time: 0.652s, 1570.09/s  (0.699s, 1465.93/s)  LR: 4.451e-04  Data: 0.014 (0.051)
Train: 161 [ 100/1251 (  8%)]  Loss: 3.511 (3.45)  Time: 0.652s, 1569.53/s  (0.678s, 1510.62/s)  LR: 4.449e-04  Data: 0.014 (0.032)
Train: 161 [ 150/1251 ( 12%)]  Loss: 3.665 (3.51)  Time: 0.662s, 1546.73/s  (0.673s, 1521.24/s)  LR: 4.447e-04  Data: 0.013 (0.026)
Train: 161 [ 200/1251 ( 16%)]  Loss: 3.559 (3.52)  Time: 0.665s, 1540.89/s  (0.671s, 1525.80/s)  LR: 4.445e-04  Data: 0.013 (0.023)
Train: 161 [ 250/1251 ( 20%)]  Loss: 3.315 (3.48)  Time: 0.662s, 1546.90/s  (0.670s, 1527.59/s)  LR: 4.443e-04  Data: 0.016 (0.021)
Train: 161 [ 300/1251 ( 24%)]  Loss: 3.388 (3.47)  Time: 0.664s, 1543.03/s  (0.670s, 1528.80/s)  LR: 4.441e-04  Data: 0.013 (0.020)
Train: 161 [ 350/1251 ( 28%)]  Loss: 3.873 (3.52)  Time: 0.666s, 1537.94/s  (0.670s, 1529.21/s)  LR: 4.439e-04  Data: 0.013 (0.019)
Train: 161 [ 400/1251 ( 32%)]  Loss: 3.612 (3.53)  Time: 0.663s, 1544.31/s  (0.669s, 1529.95/s)  LR: 4.437e-04  Data: 0.013 (0.018)
Train: 161 [ 450/1251 ( 36%)]  Loss: 3.487 (3.53)  Time: 0.674s, 1519.13/s  (0.669s, 1530.27/s)  LR: 4.435e-04  Data: 0.016 (0.018)
Train: 161 [ 500/1251 ( 40%)]  Loss: 3.236 (3.50)  Time: 0.668s, 1532.31/s  (0.669s, 1530.97/s)  LR: 4.432e-04  Data: 0.013 (0.017)
Train: 161 [ 550/1251 ( 44%)]  Loss: 3.611 (3.51)  Time: 0.672s, 1523.51/s  (0.669s, 1531.43/s)  LR: 4.430e-04  Data: 0.014 (0.017)
Train: 161 [ 600/1251 ( 48%)]  Loss: 3.745 (3.53)  Time: 0.671s, 1525.73/s  (0.669s, 1531.28/s)  LR: 4.428e-04  Data: 0.013 (0.017)
Train: 161 [ 650/1251 ( 52%)]  Loss: 3.490 (3.52)  Time: 0.677s, 1513.29/s  (0.669s, 1531.13/s)  LR: 4.426e-04  Data: 0.013 (0.016)
Train: 161 [ 700/1251 ( 56%)]  Loss: 3.504 (3.52)  Time: 0.673s, 1521.16/s  (0.669s, 1531.07/s)  LR: 4.424e-04  Data: 0.014 (0.016)
Train: 161 [ 750/1251 ( 60%)]  Loss: 3.536 (3.52)  Time: 0.676s, 1514.48/s  (0.669s, 1530.74/s)  LR: 4.422e-04  Data: 0.013 (0.016)
Train: 161 [ 800/1251 ( 64%)]  Loss: 3.528 (3.52)  Time: 0.668s, 1532.87/s  (0.669s, 1530.38/s)  LR: 4.420e-04  Data: 0.013 (0.016)
Train: 161 [ 850/1251 ( 68%)]  Loss: 3.452 (3.52)  Time: 0.669s, 1530.63/s  (0.669s, 1530.22/s)  LR: 4.418e-04  Data: 0.014 (0.016)
Train: 161 [ 900/1251 ( 72%)]  Loss: 3.655 (3.53)  Time: 0.662s, 1547.46/s  (0.669s, 1530.03/s)  LR: 4.416e-04  Data: 0.013 (0.016)
Train: 161 [ 950/1251 ( 76%)]  Loss: 3.078 (3.50)  Time: 0.675s, 1517.19/s  (0.669s, 1529.95/s)  LR: 4.414e-04  Data: 0.012 (0.016)
Train: 161 [1000/1251 ( 80%)]  Loss: 3.541 (3.51)  Time: 0.671s, 1526.38/s  (0.669s, 1529.75/s)  LR: 4.412e-04  Data: 0.016 (0.015)
Train: 161 [1050/1251 ( 84%)]  Loss: 3.242 (3.49)  Time: 0.674s, 1518.31/s  (0.669s, 1529.62/s)  LR: 4.410e-04  Data: 0.013 (0.015)
Train: 161 [1100/1251 ( 88%)]  Loss: 3.731 (3.50)  Time: 0.669s, 1529.79/s  (0.669s, 1529.67/s)  LR: 4.408e-04  Data: 0.013 (0.015)
Train: 161 [1150/1251 ( 92%)]  Loss: 3.421 (3.50)  Time: 0.663s, 1544.49/s  (0.669s, 1529.73/s)  LR: 4.406e-04  Data: 0.013 (0.015)
Train: 161 [1200/1251 ( 96%)]  Loss: 3.572 (3.50)  Time: 0.666s, 1538.67/s  (0.669s, 1529.53/s)  LR: 4.404e-04  Data: 0.013 (0.015)
Train: 161 [1250/1251 (100%)]  Loss: 3.126 (3.49)  Time: 0.655s, 1562.50/s  (0.669s, 1529.63/s)  LR: 4.401e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.820 (2.820)  Loss:  0.4717 (0.4717)  Acc@1: 90.3320 (90.3320)  Acc@5: 97.6562 (97.6562)
Test: [  48/48]  Time: 0.161 (0.324)  Loss:  0.5679 (0.9602)  Acc@1: 86.3208 (77.3000)  Acc@5: 97.0519 (93.8380)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-161.pth.tar', 77.30000000488282)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-155.pth.tar', 77.11800006103516)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-160.pth.tar', 76.96600010986329)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-152.pth.tar', 76.87600005615235)

Train: 162 [   0/1251 (  0%)]  Loss: 3.356 (3.36)  Time: 3.675s,  278.63/s  (3.675s,  278.63/s)  LR: 4.401e-04  Data: 1.909 (1.909)
Train: 162 [  50/1251 (  4%)]  Loss: 3.111 (3.23)  Time: 0.659s, 1553.18/s  (0.696s, 1471.01/s)  LR: 4.399e-04  Data: 0.016 (0.051)
Train: 162 [ 100/1251 (  8%)]  Loss: 3.598 (3.36)  Time: 0.662s, 1546.32/s  (0.679s, 1507.66/s)  LR: 4.397e-04  Data: 0.014 (0.032)
Train: 162 [ 150/1251 ( 12%)]  Loss: 3.244 (3.33)  Time: 0.673s, 1520.49/s  (0.675s, 1515.96/s)  LR: 4.395e-04  Data: 0.013 (0.026)
Train: 162 [ 200/1251 ( 16%)]  Loss: 3.592 (3.38)  Time: 0.673s, 1521.86/s  (0.674s, 1519.32/s)  LR: 4.393e-04  Data: 0.016 (0.023)
Train: 162 [ 250/1251 ( 20%)]  Loss: 3.257 (3.36)  Time: 0.671s, 1526.11/s  (0.673s, 1522.08/s)  LR: 4.391e-04  Data: 0.012 (0.021)
Train: 162 [ 300/1251 ( 24%)]  Loss: 3.629 (3.40)  Time: 0.671s, 1525.97/s  (0.673s, 1522.47/s)  LR: 4.389e-04  Data: 0.014 (0.020)
Train: 162 [ 350/1251 ( 28%)]  Loss: 3.617 (3.43)  Time: 0.668s, 1532.59/s  (0.672s, 1523.32/s)  LR: 4.387e-04  Data: 0.012 (0.019)
Train: 162 [ 400/1251 ( 32%)]  Loss: 3.419 (3.42)  Time: 0.673s, 1521.91/s  (0.672s, 1523.93/s)  LR: 4.385e-04  Data: 0.013 (0.018)
Train: 162 [ 450/1251 ( 36%)]  Loss: 3.564 (3.44)  Time: 0.667s, 1536.15/s  (0.672s, 1524.25/s)  LR: 4.383e-04  Data: 0.013 (0.018)
Train: 162 [ 500/1251 ( 40%)]  Loss: 3.141 (3.41)  Time: 0.665s, 1539.04/s  (0.672s, 1524.20/s)  LR: 4.381e-04  Data: 0.013 (0.017)
Train: 162 [ 550/1251 ( 44%)]  Loss: 3.370 (3.41)  Time: 0.674s, 1519.04/s  (0.672s, 1524.38/s)  LR: 4.379e-04  Data: 0.012 (0.017)
Train: 162 [ 600/1251 ( 48%)]  Loss: 3.037 (3.38)  Time: 0.668s, 1533.59/s  (0.672s, 1524.41/s)  LR: 4.377e-04  Data: 0.012 (0.016)
Train: 162 [ 650/1251 ( 52%)]  Loss: 3.098 (3.36)  Time: 0.667s, 1535.45/s  (0.672s, 1524.36/s)  LR: 4.375e-04  Data: 0.012 (0.016)
Train: 162 [ 700/1251 ( 56%)]  Loss: 3.590 (3.37)  Time: 0.675s, 1518.05/s  (0.672s, 1524.06/s)  LR: 4.373e-04  Data: 0.014 (0.016)
Train: 162 [ 750/1251 ( 60%)]  Loss: 3.336 (3.37)  Time: 0.671s, 1525.29/s  (0.672s, 1524.44/s)  LR: 4.370e-04  Data: 0.013 (0.016)
Train: 162 [ 800/1251 ( 64%)]  Loss: 3.418 (3.38)  Time: 0.664s, 1541.13/s  (0.672s, 1524.77/s)  LR: 4.368e-04  Data: 0.013 (0.016)
Train: 162 [ 850/1251 ( 68%)]  Loss: 3.153 (3.36)  Time: 0.670s, 1529.25/s  (0.671s, 1525.11/s)  LR: 4.366e-04  Data: 0.013 (0.016)
Train: 162 [ 900/1251 ( 72%)]  Loss: 3.573 (3.37)  Time: 0.677s, 1512.77/s  (0.671s, 1525.24/s)  LR: 4.364e-04  Data: 0.014 (0.016)
Train: 162 [ 950/1251 ( 76%)]  Loss: 3.390 (3.37)  Time: 0.681s, 1504.28/s  (0.671s, 1525.39/s)  LR: 4.362e-04  Data: 0.017 (0.015)
Train: 162 [1000/1251 ( 80%)]  Loss: 3.183 (3.37)  Time: 0.668s, 1533.25/s  (0.671s, 1525.29/s)  LR: 4.360e-04  Data: 0.016 (0.015)
Train: 162 [1050/1251 ( 84%)]  Loss: 3.685 (3.38)  Time: 0.676s, 1515.81/s  (0.671s, 1525.38/s)  LR: 4.358e-04  Data: 0.013 (0.015)
Train: 162 [1100/1251 ( 88%)]  Loss: 3.609 (3.39)  Time: 0.667s, 1535.67/s  (0.671s, 1525.45/s)  LR: 4.356e-04  Data: 0.013 (0.015)
Train: 162 [1150/1251 ( 92%)]  Loss: 3.727 (3.40)  Time: 0.671s, 1527.17/s  (0.671s, 1525.78/s)  LR: 4.354e-04  Data: 0.012 (0.015)
Train: 162 [1200/1251 ( 96%)]  Loss: 3.535 (3.41)  Time: 0.667s, 1536.10/s  (0.671s, 1526.23/s)  LR: 4.352e-04  Data: 0.014 (0.015)
Train: 162 [1250/1251 (100%)]  Loss: 3.392 (3.41)  Time: 0.666s, 1536.92/s  (0.671s, 1526.61/s)  LR: 4.350e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.826 (2.826)  Loss:  0.4568 (0.4568)  Acc@1: 90.9180 (90.9180)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.162 (0.320)  Loss:  0.5537 (0.9667)  Acc@1: 85.9670 (77.1240)  Acc@5: 97.6415 (93.8580)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-161.pth.tar', 77.30000000488282)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-162.pth.tar', 77.12399998046875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-155.pth.tar', 77.11800006103516)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-160.pth.tar', 76.96600010986329)

Train: 163 [   0/1251 (  0%)]  Loss: 3.159 (3.16)  Time: 3.663s,  279.57/s  (3.663s,  279.57/s)  LR: 4.350e-04  Data: 1.485 (1.485)
Train: 163 [  50/1251 (  4%)]  Loss: 3.250 (3.20)  Time: 0.652s, 1571.40/s  (0.688s, 1488.30/s)  LR: 4.348e-04  Data: 0.014 (0.043)
Train: 163 [ 100/1251 (  8%)]  Loss: 3.461 (3.29)  Time: 0.662s, 1547.18/s  (0.672s, 1523.73/s)  LR: 4.346e-04  Data: 0.013 (0.028)
Train: 163 [ 150/1251 ( 12%)]  Loss: 3.502 (3.34)  Time: 0.666s, 1537.88/s  (0.669s, 1531.38/s)  LR: 4.344e-04  Data: 0.013 (0.023)
Train: 163 [ 200/1251 ( 16%)]  Loss: 3.557 (3.39)  Time: 0.676s, 1515.22/s  (0.668s, 1532.88/s)  LR: 4.342e-04  Data: 0.014 (0.021)
Train: 163 [ 250/1251 ( 20%)]  Loss: 3.657 (3.43)  Time: 0.673s, 1520.45/s  (0.668s, 1533.87/s)  LR: 4.339e-04  Data: 0.014 (0.020)
Train: 163 [ 300/1251 ( 24%)]  Loss: 3.079 (3.38)  Time: 0.676s, 1515.70/s  (0.668s, 1533.97/s)  LR: 4.337e-04  Data: 0.013 (0.018)
Train: 163 [ 350/1251 ( 28%)]  Loss: 3.274 (3.37)  Time: 0.665s, 1540.94/s  (0.668s, 1534.04/s)  LR: 4.335e-04  Data: 0.013 (0.018)
Train: 163 [ 400/1251 ( 32%)]  Loss: 3.126 (3.34)  Time: 0.669s, 1531.54/s  (0.668s, 1533.90/s)  LR: 4.333e-04  Data: 0.014 (0.017)
Train: 163 [ 450/1251 ( 36%)]  Loss: 3.192 (3.33)  Time: 0.668s, 1532.57/s  (0.667s, 1534.09/s)  LR: 4.331e-04  Data: 0.013 (0.017)
Train: 163 [ 500/1251 ( 40%)]  Loss: 3.719 (3.36)  Time: 0.680s, 1504.78/s  (0.667s, 1534.36/s)  LR: 4.329e-04  Data: 0.013 (0.017)
Train: 163 [ 550/1251 ( 44%)]  Loss: 3.420 (3.37)  Time: 0.673s, 1520.86/s  (0.667s, 1534.10/s)  LR: 4.327e-04  Data: 0.013 (0.016)
Train: 163 [ 600/1251 ( 48%)]  Loss: 3.269 (3.36)  Time: 0.666s, 1536.49/s  (0.667s, 1534.17/s)  LR: 4.325e-04  Data: 0.015 (0.016)
Train: 163 [ 650/1251 ( 52%)]  Loss: 3.417 (3.36)  Time: 0.659s, 1553.06/s  (0.668s, 1533.89/s)  LR: 4.323e-04  Data: 0.013 (0.016)
Train: 163 [ 700/1251 ( 56%)]  Loss: 3.409 (3.37)  Time: 0.675s, 1516.51/s  (0.668s, 1533.86/s)  LR: 4.321e-04  Data: 0.013 (0.016)
Train: 163 [ 750/1251 ( 60%)]  Loss: 3.525 (3.38)  Time: 0.669s, 1530.47/s  (0.668s, 1533.45/s)  LR: 4.319e-04  Data: 0.015 (0.016)
Train: 163 [ 800/1251 ( 64%)]  Loss: 3.267 (3.37)  Time: 0.664s, 1541.10/s  (0.668s, 1532.99/s)  LR: 4.317e-04  Data: 0.014 (0.015)
Train: 163 [ 850/1251 ( 68%)]  Loss: 3.571 (3.38)  Time: 0.669s, 1529.57/s  (0.668s, 1532.90/s)  LR: 4.315e-04  Data: 0.014 (0.015)
Train: 163 [ 900/1251 ( 72%)]  Loss: 3.560 (3.39)  Time: 0.664s, 1542.75/s  (0.668s, 1533.15/s)  LR: 4.313e-04  Data: 0.014 (0.015)
Train: 163 [ 950/1251 ( 76%)]  Loss: 3.216 (3.38)  Time: 0.670s, 1527.93/s  (0.668s, 1533.02/s)  LR: 4.311e-04  Data: 0.013 (0.015)
Train: 163 [1000/1251 ( 80%)]  Loss: 3.631 (3.39)  Time: 0.674s, 1519.22/s  (0.668s, 1532.87/s)  LR: 4.309e-04  Data: 0.012 (0.015)
Train: 163 [1050/1251 ( 84%)]  Loss: 3.607 (3.40)  Time: 0.669s, 1529.74/s  (0.668s, 1532.76/s)  LR: 4.306e-04  Data: 0.014 (0.015)
Train: 163 [1100/1251 ( 88%)]  Loss: 3.398 (3.40)  Time: 0.662s, 1546.12/s  (0.668s, 1532.78/s)  LR: 4.304e-04  Data: 0.014 (0.015)
Train: 163 [1150/1251 ( 92%)]  Loss: 3.594 (3.41)  Time: 0.673s, 1522.25/s  (0.668s, 1532.83/s)  LR: 4.302e-04  Data: 0.013 (0.015)
Train: 163 [1200/1251 ( 96%)]  Loss: 3.548 (3.42)  Time: 0.669s, 1529.96/s  (0.668s, 1532.90/s)  LR: 4.300e-04  Data: 0.013 (0.015)
Train: 163 [1250/1251 (100%)]  Loss: 3.178 (3.41)  Time: 0.648s, 1579.80/s  (0.668s, 1533.03/s)  LR: 4.298e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.088 (3.088)  Loss:  0.4719 (0.4719)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.170 (0.321)  Loss:  0.5386 (0.9580)  Acc@1: 86.4387 (77.3020)  Acc@5: 97.5236 (94.0220)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-163.pth.tar', 77.30200005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-161.pth.tar', 77.30000000488282)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-162.pth.tar', 77.12399998046875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-155.pth.tar', 77.11800006103516)

Train: 164 [   0/1251 (  0%)]  Loss: 3.320 (3.32)  Time: 3.793s,  269.95/s  (3.793s,  269.95/s)  LR: 4.298e-04  Data: 2.057 (2.057)
Train: 164 [  50/1251 (  4%)]  Loss: 3.259 (3.29)  Time: 0.659s, 1554.81/s  (0.688s, 1487.45/s)  LR: 4.296e-04  Data: 0.014 (0.053)
Train: 164 [ 100/1251 (  8%)]  Loss: 3.147 (3.24)  Time: 0.659s, 1553.83/s  (0.673s, 1522.54/s)  LR: 4.294e-04  Data: 0.014 (0.034)
Train: 164 [ 150/1251 ( 12%)]  Loss: 3.518 (3.31)  Time: 0.660s, 1550.54/s  (0.669s, 1531.36/s)  LR: 4.292e-04  Data: 0.013 (0.027)
Train: 164 [ 200/1251 ( 16%)]  Loss: 3.529 (3.35)  Time: 0.660s, 1552.45/s  (0.667s, 1534.12/s)  LR: 4.290e-04  Data: 0.013 (0.024)
Train: 164 [ 250/1251 ( 20%)]  Loss: 3.725 (3.42)  Time: 0.675s, 1517.21/s  (0.667s, 1535.40/s)  LR: 4.288e-04  Data: 0.013 (0.022)
Train: 164 [ 300/1251 ( 24%)]  Loss: 2.996 (3.36)  Time: 0.663s, 1545.37/s  (0.667s, 1536.00/s)  LR: 4.286e-04  Data: 0.013 (0.020)
Train: 164 [ 350/1251 ( 28%)]  Loss: 3.160 (3.33)  Time: 0.655s, 1563.85/s  (0.666s, 1536.75/s)  LR: 4.284e-04  Data: 0.014 (0.019)
Train: 164 [ 400/1251 ( 32%)]  Loss: 3.365 (3.34)  Time: 0.674s, 1519.62/s  (0.666s, 1537.44/s)  LR: 4.282e-04  Data: 0.013 (0.019)
Train: 164 [ 450/1251 ( 36%)]  Loss: 3.457 (3.35)  Time: 0.664s, 1543.17/s  (0.666s, 1537.41/s)  LR: 4.280e-04  Data: 0.014 (0.018)
Train: 164 [ 500/1251 ( 40%)]  Loss: 3.247 (3.34)  Time: 0.664s, 1541.71/s  (0.666s, 1538.13/s)  LR: 4.278e-04  Data: 0.013 (0.018)
Train: 164 [ 550/1251 ( 44%)]  Loss: 3.677 (3.37)  Time: 0.669s, 1529.50/s  (0.666s, 1537.65/s)  LR: 4.276e-04  Data: 0.013 (0.017)
Train: 164 [ 600/1251 ( 48%)]  Loss: 3.443 (3.37)  Time: 0.660s, 1551.94/s  (0.666s, 1537.53/s)  LR: 4.273e-04  Data: 0.013 (0.017)
Train: 164 [ 650/1251 ( 52%)]  Loss: 3.233 (3.36)  Time: 0.666s, 1537.20/s  (0.666s, 1537.45/s)  LR: 4.271e-04  Data: 0.013 (0.017)
Train: 164 [ 700/1251 ( 56%)]  Loss: 3.357 (3.36)  Time: 0.662s, 1547.88/s  (0.666s, 1537.11/s)  LR: 4.269e-04  Data: 0.013 (0.016)
Train: 164 [ 750/1251 ( 60%)]  Loss: 3.350 (3.36)  Time: 0.671s, 1526.29/s  (0.666s, 1536.87/s)  LR: 4.267e-04  Data: 0.015 (0.016)
Train: 164 [ 800/1251 ( 64%)]  Loss: 3.279 (3.36)  Time: 0.669s, 1529.73/s  (0.666s, 1536.70/s)  LR: 4.265e-04  Data: 0.014 (0.016)
Train: 164 [ 850/1251 ( 68%)]  Loss: 3.578 (3.37)  Time: 0.680s, 1505.06/s  (0.666s, 1536.54/s)  LR: 4.263e-04  Data: 0.013 (0.016)
Train: 164 [ 900/1251 ( 72%)]  Loss: 3.062 (3.35)  Time: 0.674s, 1518.58/s  (0.666s, 1536.60/s)  LR: 4.261e-04  Data: 0.013 (0.016)
Train: 164 [ 950/1251 ( 76%)]  Loss: 3.322 (3.35)  Time: 0.658s, 1557.36/s  (0.666s, 1536.60/s)  LR: 4.259e-04  Data: 0.014 (0.016)
Train: 164 [1000/1251 ( 80%)]  Loss: 3.646 (3.37)  Time: 0.673s, 1521.32/s  (0.666s, 1536.82/s)  LR: 4.257e-04  Data: 0.013 (0.016)
Train: 164 [1050/1251 ( 84%)]  Loss: 3.418 (3.37)  Time: 0.672s, 1524.84/s  (0.666s, 1536.67/s)  LR: 4.255e-04  Data: 0.013 (0.015)
Train: 164 [1100/1251 ( 88%)]  Loss: 3.413 (3.37)  Time: 0.664s, 1542.30/s  (0.666s, 1536.52/s)  LR: 4.253e-04  Data: 0.014 (0.015)
Train: 164 [1150/1251 ( 92%)]  Loss: 3.564 (3.38)  Time: 0.669s, 1530.79/s  (0.667s, 1536.29/s)  LR: 4.251e-04  Data: 0.013 (0.015)
Train: 164 [1200/1251 ( 96%)]  Loss: 3.517 (3.38)  Time: 0.676s, 1514.64/s  (0.667s, 1536.35/s)  LR: 4.249e-04  Data: 0.013 (0.015)
Train: 164 [1250/1251 (100%)]  Loss: 3.725 (3.40)  Time: 0.655s, 1563.16/s  (0.666s, 1536.52/s)  LR: 4.247e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.827 (2.827)  Loss:  0.4797 (0.4797)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.161 (0.323)  Loss:  0.5454 (0.9621)  Acc@1: 86.6745 (77.2960)  Acc@5: 97.6415 (93.9800)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-163.pth.tar', 77.30200005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-161.pth.tar', 77.30000000488282)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-164.pth.tar', 77.29599989990234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-162.pth.tar', 77.12399998046875)

Train: 165 [   0/1251 (  0%)]  Loss: 3.039 (3.04)  Time: 4.187s,  244.56/s  (4.187s,  244.56/s)  LR: 4.247e-04  Data: 1.763 (1.763)
Train: 165 [  50/1251 (  4%)]  Loss: 3.290 (3.16)  Time: 0.648s, 1581.21/s  (0.697s, 1468.37/s)  LR: 4.245e-04  Data: 0.016 (0.048)
Train: 165 [ 100/1251 (  8%)]  Loss: 3.603 (3.31)  Time: 0.660s, 1551.46/s  (0.678s, 1510.25/s)  LR: 4.243e-04  Data: 0.015 (0.031)
Train: 165 [ 150/1251 ( 12%)]  Loss: 3.584 (3.38)  Time: 0.660s, 1550.85/s  (0.673s, 1520.94/s)  LR: 4.241e-04  Data: 0.013 (0.026)
Train: 165 [ 200/1251 ( 16%)]  Loss: 3.614 (3.43)  Time: 0.662s, 1546.60/s  (0.670s, 1527.77/s)  LR: 4.238e-04  Data: 0.013 (0.023)
Train: 165 [ 250/1251 ( 20%)]  Loss: 3.583 (3.45)  Time: 0.655s, 1562.58/s  (0.669s, 1531.01/s)  LR: 4.236e-04  Data: 0.015 (0.021)
Train: 165 [ 300/1251 ( 24%)]  Loss: 3.451 (3.45)  Time: 0.663s, 1544.30/s  (0.668s, 1533.22/s)  LR: 4.234e-04  Data: 0.019 (0.020)
Train: 165 [ 350/1251 ( 28%)]  Loss: 3.329 (3.44)  Time: 0.669s, 1530.96/s  (0.667s, 1534.86/s)  LR: 4.232e-04  Data: 0.013 (0.019)
Train: 165 [ 400/1251 ( 32%)]  Loss: 3.497 (3.44)  Time: 0.667s, 1534.19/s  (0.667s, 1535.09/s)  LR: 4.230e-04  Data: 0.013 (0.018)
Train: 165 [ 450/1251 ( 36%)]  Loss: 3.309 (3.43)  Time: 0.669s, 1531.62/s  (0.667s, 1535.63/s)  LR: 4.228e-04  Data: 0.017 (0.018)
Train: 165 [ 500/1251 ( 40%)]  Loss: 3.661 (3.45)  Time: 0.667s, 1534.83/s  (0.667s, 1535.52/s)  LR: 4.226e-04  Data: 0.013 (0.017)
Train: 165 [ 550/1251 ( 44%)]  Loss: 3.407 (3.45)  Time: 0.665s, 1539.12/s  (0.667s, 1535.57/s)  LR: 4.224e-04  Data: 0.016 (0.017)
Train: 165 [ 600/1251 ( 48%)]  Loss: 3.506 (3.45)  Time: 0.678s, 1510.22/s  (0.667s, 1535.65/s)  LR: 4.222e-04  Data: 0.013 (0.017)
Train: 165 [ 650/1251 ( 52%)]  Loss: 3.170 (3.43)  Time: 0.672s, 1523.52/s  (0.667s, 1535.20/s)  LR: 4.220e-04  Data: 0.014 (0.017)
Train: 165 [ 700/1251 ( 56%)]  Loss: 3.606 (3.44)  Time: 0.656s, 1561.30/s  (0.667s, 1535.15/s)  LR: 4.218e-04  Data: 0.014 (0.016)
Train: 165 [ 750/1251 ( 60%)]  Loss: 2.985 (3.41)  Time: 0.662s, 1547.46/s  (0.667s, 1535.30/s)  LR: 4.216e-04  Data: 0.012 (0.016)
Train: 165 [ 800/1251 ( 64%)]  Loss: 3.578 (3.42)  Time: 0.675s, 1516.61/s  (0.667s, 1535.21/s)  LR: 4.214e-04  Data: 0.013 (0.016)
Train: 165 [ 850/1251 ( 68%)]  Loss: 3.109 (3.41)  Time: 0.672s, 1523.47/s  (0.667s, 1535.03/s)  LR: 4.212e-04  Data: 0.014 (0.016)
Train: 165 [ 900/1251 ( 72%)]  Loss: 3.549 (3.41)  Time: 0.660s, 1552.34/s  (0.667s, 1535.03/s)  LR: 4.210e-04  Data: 0.016 (0.016)
Train: 165 [ 950/1251 ( 76%)]  Loss: 3.334 (3.41)  Time: 0.671s, 1526.44/s  (0.667s, 1534.87/s)  LR: 4.208e-04  Data: 0.014 (0.016)
Train: 165 [1000/1251 ( 80%)]  Loss: 3.451 (3.41)  Time: 0.663s, 1543.58/s  (0.667s, 1534.76/s)  LR: 4.206e-04  Data: 0.013 (0.016)
Train: 165 [1050/1251 ( 84%)]  Loss: 3.332 (3.41)  Time: 0.666s, 1537.76/s  (0.667s, 1534.46/s)  LR: 4.204e-04  Data: 0.014 (0.016)
Train: 165 [1100/1251 ( 88%)]  Loss: 3.474 (3.41)  Time: 0.666s, 1538.02/s  (0.667s, 1534.72/s)  LR: 4.201e-04  Data: 0.013 (0.015)
Train: 165 [1150/1251 ( 92%)]  Loss: 3.144 (3.40)  Time: 0.666s, 1536.46/s  (0.667s, 1534.86/s)  LR: 4.199e-04  Data: 0.013 (0.015)
Train: 165 [1200/1251 ( 96%)]  Loss: 3.335 (3.40)  Time: 0.671s, 1526.21/s  (0.667s, 1534.81/s)  LR: 4.197e-04  Data: 0.014 (0.015)
Train: 165 [1250/1251 (100%)]  Loss: 3.475 (3.40)  Time: 0.651s, 1573.69/s  (0.667s, 1535.00/s)  LR: 4.195e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.756 (2.756)  Loss:  0.4280 (0.4280)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.162 (0.330)  Loss:  0.5630 (0.9638)  Acc@1: 86.4387 (77.0020)  Acc@5: 97.9953 (93.9500)
Train: 166 [   0/1251 (  0%)]  Loss: 3.308 (3.31)  Time: 3.408s,  300.46/s  (3.408s,  300.46/s)  LR: 4.195e-04  Data: 1.733 (1.733)
Train: 166 [  50/1251 (  4%)]  Loss: 3.541 (3.42)  Time: 0.656s, 1561.37/s  (0.686s, 1493.30/s)  LR: 4.193e-04  Data: 0.014 (0.048)
Train: 166 [ 100/1251 (  8%)]  Loss: 3.309 (3.39)  Time: 0.665s, 1539.82/s  (0.672s, 1524.75/s)  LR: 4.191e-04  Data: 0.013 (0.031)
Train: 166 [ 150/1251 ( 12%)]  Loss: 3.414 (3.39)  Time: 0.657s, 1557.95/s  (0.669s, 1531.71/s)  LR: 4.189e-04  Data: 0.013 (0.026)
Train: 166 [ 200/1251 ( 16%)]  Loss: 3.541 (3.42)  Time: 0.661s, 1548.70/s  (0.668s, 1534.01/s)  LR: 4.187e-04  Data: 0.013 (0.023)
Train: 166 [ 250/1251 ( 20%)]  Loss: 3.448 (3.43)  Time: 0.675s, 1517.56/s  (0.667s, 1534.82/s)  LR: 4.185e-04  Data: 0.014 (0.021)
Train: 166 [ 300/1251 ( 24%)]  Loss: 3.501 (3.44)  Time: 0.663s, 1544.17/s  (0.667s, 1535.22/s)  LR: 4.183e-04  Data: 0.014 (0.020)
Train: 166 [ 350/1251 ( 28%)]  Loss: 3.380 (3.43)  Time: 0.661s, 1549.81/s  (0.667s, 1534.56/s)  LR: 4.181e-04  Data: 0.013 (0.019)
Train: 166 [ 400/1251 ( 32%)]  Loss: 3.478 (3.44)  Time: 0.669s, 1529.74/s  (0.667s, 1534.37/s)  LR: 4.179e-04  Data: 0.013 (0.018)
Train: 166 [ 450/1251 ( 36%)]  Loss: 3.480 (3.44)  Time: 0.670s, 1529.12/s  (0.667s, 1534.13/s)  LR: 4.177e-04  Data: 0.017 (0.018)
Train: 166 [ 500/1251 ( 40%)]  Loss: 3.533 (3.45)  Time: 0.680s, 1506.95/s  (0.668s, 1533.48/s)  LR: 4.175e-04  Data: 0.013 (0.018)
Train: 166 [ 550/1251 ( 44%)]  Loss: 3.267 (3.43)  Time: 0.678s, 1509.25/s  (0.668s, 1533.10/s)  LR: 4.173e-04  Data: 0.013 (0.017)
Train: 166 [ 600/1251 ( 48%)]  Loss: 3.543 (3.44)  Time: 0.664s, 1541.30/s  (0.668s, 1532.94/s)  LR: 4.171e-04  Data: 0.013 (0.017)
Train: 166 [ 650/1251 ( 52%)]  Loss: 3.883 (3.47)  Time: 0.661s, 1549.74/s  (0.668s, 1532.55/s)  LR: 4.169e-04  Data: 0.015 (0.017)
Train: 166 [ 700/1251 ( 56%)]  Loss: 3.468 (3.47)  Time: 0.677s, 1511.70/s  (0.668s, 1532.10/s)  LR: 4.167e-04  Data: 0.015 (0.017)
Train: 166 [ 750/1251 ( 60%)]  Loss: 3.391 (3.47)  Time: 0.677s, 1511.77/s  (0.668s, 1531.83/s)  LR: 4.165e-04  Data: 0.012 (0.016)
Train: 166 [ 800/1251 ( 64%)]  Loss: 3.451 (3.47)  Time: 0.669s, 1531.39/s  (0.669s, 1531.56/s)  LR: 4.162e-04  Data: 0.013 (0.016)
Train: 166 [ 850/1251 ( 68%)]  Loss: 3.523 (3.47)  Time: 0.667s, 1534.76/s  (0.669s, 1531.22/s)  LR: 4.160e-04  Data: 0.013 (0.016)
Train: 166 [ 900/1251 ( 72%)]  Loss: 3.575 (3.48)  Time: 0.663s, 1544.09/s  (0.669s, 1531.43/s)  LR: 4.158e-04  Data: 0.013 (0.016)
Train: 166 [ 950/1251 ( 76%)]  Loss: 3.668 (3.49)  Time: 0.673s, 1520.75/s  (0.668s, 1531.81/s)  LR: 4.156e-04  Data: 0.013 (0.016)
Train: 166 [1000/1251 ( 80%)]  Loss: 3.551 (3.49)  Time: 0.665s, 1540.46/s  (0.668s, 1532.27/s)  LR: 4.154e-04  Data: 0.019 (0.016)
Train: 166 [1050/1251 ( 84%)]  Loss: 3.694 (3.50)  Time: 0.672s, 1523.01/s  (0.668s, 1532.33/s)  LR: 4.152e-04  Data: 0.015 (0.016)
Train: 166 [1100/1251 ( 88%)]  Loss: 3.422 (3.49)  Time: 0.672s, 1523.96/s  (0.668s, 1532.40/s)  LR: 4.150e-04  Data: 0.012 (0.016)
Train: 166 [1150/1251 ( 92%)]  Loss: 3.365 (3.49)  Time: 0.661s, 1550.16/s  (0.668s, 1532.70/s)  LR: 4.148e-04  Data: 0.013 (0.015)
Train: 166 [1200/1251 ( 96%)]  Loss: 3.710 (3.50)  Time: 0.668s, 1533.30/s  (0.668s, 1532.95/s)  LR: 4.146e-04  Data: 0.016 (0.015)
Train: 166 [1250/1251 (100%)]  Loss: 3.300 (3.49)  Time: 0.652s, 1569.70/s  (0.668s, 1533.12/s)  LR: 4.144e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.079 (3.079)  Loss:  0.4653 (0.4653)  Acc@1: 90.4297 (90.4297)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.161 (0.324)  Loss:  0.5737 (0.9646)  Acc@1: 85.8491 (77.3900)  Acc@5: 97.4057 (94.0160)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-166.pth.tar', 77.38999992919922)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-163.pth.tar', 77.30200005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-161.pth.tar', 77.30000000488282)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-164.pth.tar', 77.29599989990234)

Train: 167 [   0/1251 (  0%)]  Loss: 3.387 (3.39)  Time: 3.545s,  288.89/s  (3.545s,  288.89/s)  LR: 4.144e-04  Data: 1.950 (1.950)
Train: 167 [  50/1251 (  4%)]  Loss: 3.470 (3.43)  Time: 0.646s, 1585.15/s  (0.690s, 1484.18/s)  LR: 4.142e-04  Data: 0.021 (0.052)
Train: 167 [ 100/1251 (  8%)]  Loss: 3.351 (3.40)  Time: 0.659s, 1553.24/s  (0.674s, 1519.44/s)  LR: 4.140e-04  Data: 0.013 (0.033)
Train: 167 [ 150/1251 ( 12%)]  Loss: 3.098 (3.33)  Time: 0.658s, 1557.22/s  (0.670s, 1528.77/s)  LR: 4.138e-04  Data: 0.013 (0.027)
Train: 167 [ 200/1251 ( 16%)]  Loss: 3.450 (3.35)  Time: 0.658s, 1556.74/s  (0.669s, 1531.45/s)  LR: 4.136e-04  Data: 0.013 (0.024)
Train: 167 [ 250/1251 ( 20%)]  Loss: 3.505 (3.38)  Time: 0.660s, 1551.56/s  (0.668s, 1533.68/s)  LR: 4.134e-04  Data: 0.015 (0.022)
Train: 167 [ 300/1251 ( 24%)]  Loss: 3.445 (3.39)  Time: 0.663s, 1545.47/s  (0.667s, 1534.53/s)  LR: 4.132e-04  Data: 0.013 (0.020)
Train: 167 [ 350/1251 ( 28%)]  Loss: 3.127 (3.35)  Time: 0.660s, 1550.69/s  (0.667s, 1535.66/s)  LR: 4.130e-04  Data: 0.014 (0.019)
Train: 167 [ 400/1251 ( 32%)]  Loss: 3.410 (3.36)  Time: 0.666s, 1536.68/s  (0.667s, 1536.36/s)  LR: 4.128e-04  Data: 0.013 (0.019)
Train: 167 [ 450/1251 ( 36%)]  Loss: 3.303 (3.35)  Time: 0.655s, 1564.40/s  (0.666s, 1536.79/s)  LR: 4.126e-04  Data: 0.017 (0.018)
Train: 167 [ 500/1251 ( 40%)]  Loss: 3.291 (3.35)  Time: 0.658s, 1556.04/s  (0.666s, 1536.84/s)  LR: 4.123e-04  Data: 0.014 (0.018)
Train: 167 [ 550/1251 ( 44%)]  Loss: 3.188 (3.34)  Time: 0.664s, 1543.19/s  (0.666s, 1536.88/s)  LR: 4.121e-04  Data: 0.013 (0.017)
Train: 167 [ 600/1251 ( 48%)]  Loss: 3.567 (3.35)  Time: 0.659s, 1554.53/s  (0.666s, 1537.30/s)  LR: 4.119e-04  Data: 0.013 (0.017)
Train: 167 [ 650/1251 ( 52%)]  Loss: 3.442 (3.36)  Time: 0.668s, 1532.90/s  (0.666s, 1537.15/s)  LR: 4.117e-04  Data: 0.015 (0.017)
Train: 167 [ 700/1251 ( 56%)]  Loss: 3.456 (3.37)  Time: 0.669s, 1531.06/s  (0.666s, 1536.98/s)  LR: 4.115e-04  Data: 0.013 (0.017)
Train: 167 [ 750/1251 ( 60%)]  Loss: 3.332 (3.36)  Time: 0.666s, 1538.31/s  (0.666s, 1536.72/s)  LR: 4.113e-04  Data: 0.014 (0.017)
Train: 167 [ 800/1251 ( 64%)]  Loss: 3.330 (3.36)  Time: 0.670s, 1528.73/s  (0.667s, 1536.30/s)  LR: 4.111e-04  Data: 0.015 (0.016)
Train: 167 [ 850/1251 ( 68%)]  Loss: 3.390 (3.36)  Time: 0.669s, 1530.35/s  (0.667s, 1536.02/s)  LR: 4.109e-04  Data: 0.015 (0.016)
Train: 167 [ 900/1251 ( 72%)]  Loss: 3.220 (3.36)  Time: 0.665s, 1540.68/s  (0.667s, 1535.78/s)  LR: 4.107e-04  Data: 0.013 (0.016)
Train: 167 [ 950/1251 ( 76%)]  Loss: 3.157 (3.35)  Time: 0.665s, 1539.71/s  (0.667s, 1535.58/s)  LR: 4.105e-04  Data: 0.013 (0.016)
Train: 167 [1000/1251 ( 80%)]  Loss: 3.467 (3.35)  Time: 0.666s, 1537.33/s  (0.667s, 1535.58/s)  LR: 4.103e-04  Data: 0.013 (0.016)
Train: 167 [1050/1251 ( 84%)]  Loss: 3.316 (3.35)  Time: 0.659s, 1552.83/s  (0.667s, 1535.53/s)  LR: 4.101e-04  Data: 0.014 (0.016)
Train: 167 [1100/1251 ( 88%)]  Loss: 3.524 (3.36)  Time: 0.676s, 1515.48/s  (0.667s, 1535.74/s)  LR: 4.099e-04  Data: 0.019 (0.016)
Train: 167 [1150/1251 ( 92%)]  Loss: 3.544 (3.37)  Time: 0.662s, 1546.02/s  (0.667s, 1536.02/s)  LR: 4.097e-04  Data: 0.013 (0.016)
Train: 167 [1200/1251 ( 96%)]  Loss: 3.062 (3.35)  Time: 0.667s, 1535.92/s  (0.667s, 1536.33/s)  LR: 4.095e-04  Data: 0.013 (0.016)
Train: 167 [1250/1251 (100%)]  Loss: 3.522 (3.36)  Time: 0.655s, 1563.73/s  (0.666s, 1536.68/s)  LR: 4.093e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.846 (2.846)  Loss:  0.5146 (0.5146)  Acc@1: 89.7461 (89.7461)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.162 (0.323)  Loss:  0.5454 (0.9654)  Acc@1: 85.9670 (77.4960)  Acc@5: 97.4057 (94.1300)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-167.pth.tar', 77.49600010986327)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-166.pth.tar', 77.38999992919922)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-163.pth.tar', 77.30200005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-161.pth.tar', 77.30000000488282)

Train: 168 [   0/1251 (  0%)]  Loss: 2.999 (3.00)  Time: 3.977s,  257.49/s  (3.977s,  257.49/s)  LR: 4.093e-04  Data: 1.623 (1.623)
Train: 168 [  50/1251 (  4%)]  Loss: 3.469 (3.23)  Time: 0.651s, 1574.16/s  (0.694s, 1475.26/s)  LR: 4.091e-04  Data: 0.016 (0.046)
Train: 168 [ 100/1251 (  8%)]  Loss: 3.491 (3.32)  Time: 0.657s, 1558.42/s  (0.677s, 1512.42/s)  LR: 4.089e-04  Data: 0.017 (0.030)
Train: 168 [ 150/1251 ( 12%)]  Loss: 3.453 (3.35)  Time: 0.663s, 1544.68/s  (0.671s, 1525.12/s)  LR: 4.087e-04  Data: 0.013 (0.024)
Train: 168 [ 200/1251 ( 16%)]  Loss: 3.145 (3.31)  Time: 0.668s, 1533.96/s  (0.669s, 1529.57/s)  LR: 4.085e-04  Data: 0.014 (0.022)
Train: 168 [ 250/1251 ( 20%)]  Loss: 3.265 (3.30)  Time: 0.672s, 1524.80/s  (0.669s, 1531.64/s)  LR: 4.083e-04  Data: 0.013 (0.020)
Train: 168 [ 300/1251 ( 24%)]  Loss: 3.162 (3.28)  Time: 0.662s, 1546.52/s  (0.668s, 1532.33/s)  LR: 4.080e-04  Data: 0.013 (0.019)
Train: 168 [ 350/1251 ( 28%)]  Loss: 3.177 (3.27)  Time: 0.669s, 1531.75/s  (0.668s, 1533.27/s)  LR: 4.078e-04  Data: 0.013 (0.018)
Train: 168 [ 400/1251 ( 32%)]  Loss: 3.444 (3.29)  Time: 0.663s, 1545.30/s  (0.668s, 1532.87/s)  LR: 4.076e-04  Data: 0.013 (0.018)
Train: 168 [ 450/1251 ( 36%)]  Loss: 3.610 (3.32)  Time: 0.671s, 1526.89/s  (0.668s, 1532.53/s)  LR: 4.074e-04  Data: 0.014 (0.017)
Train: 168 [ 500/1251 ( 40%)]  Loss: 3.155 (3.31)  Time: 0.671s, 1524.96/s  (0.668s, 1532.67/s)  LR: 4.072e-04  Data: 0.013 (0.017)
Train: 168 [ 550/1251 ( 44%)]  Loss: 3.156 (3.29)  Time: 0.668s, 1532.73/s  (0.668s, 1533.02/s)  LR: 4.070e-04  Data: 0.013 (0.017)
Train: 168 [ 600/1251 ( 48%)]  Loss: 3.378 (3.30)  Time: 0.666s, 1538.65/s  (0.668s, 1532.90/s)  LR: 4.068e-04  Data: 0.013 (0.016)
Train: 168 [ 650/1251 ( 52%)]  Loss: 3.280 (3.30)  Time: 0.671s, 1526.57/s  (0.668s, 1533.07/s)  LR: 4.066e-04  Data: 0.013 (0.016)
Train: 168 [ 700/1251 ( 56%)]  Loss: 3.477 (3.31)  Time: 0.666s, 1536.56/s  (0.668s, 1532.94/s)  LR: 4.064e-04  Data: 0.013 (0.016)
Train: 168 [ 750/1251 ( 60%)]  Loss: 3.403 (3.32)  Time: 0.672s, 1524.46/s  (0.668s, 1532.57/s)  LR: 4.062e-04  Data: 0.013 (0.016)
Train: 168 [ 800/1251 ( 64%)]  Loss: 3.482 (3.33)  Time: 0.670s, 1529.41/s  (0.668s, 1532.48/s)  LR: 4.060e-04  Data: 0.014 (0.016)
Train: 168 [ 850/1251 ( 68%)]  Loss: 3.521 (3.34)  Time: 0.678s, 1511.33/s  (0.668s, 1532.33/s)  LR: 4.058e-04  Data: 0.016 (0.016)
Train: 168 [ 900/1251 ( 72%)]  Loss: 3.241 (3.33)  Time: 0.675s, 1516.21/s  (0.668s, 1531.95/s)  LR: 4.056e-04  Data: 0.016 (0.015)
Train: 168 [ 950/1251 ( 76%)]  Loss: 3.652 (3.35)  Time: 0.672s, 1523.79/s  (0.669s, 1531.74/s)  LR: 4.054e-04  Data: 0.013 (0.015)
Train: 168 [1000/1251 ( 80%)]  Loss: 3.701 (3.36)  Time: 0.668s, 1532.47/s  (0.668s, 1531.85/s)  LR: 4.052e-04  Data: 0.013 (0.015)
Train: 168 [1050/1251 ( 84%)]  Loss: 3.607 (3.38)  Time: 0.668s, 1532.45/s  (0.669s, 1531.75/s)  LR: 4.050e-04  Data: 0.013 (0.015)
Train: 168 [1100/1251 ( 88%)]  Loss: 3.349 (3.37)  Time: 0.664s, 1542.91/s  (0.669s, 1531.64/s)  LR: 4.048e-04  Data: 0.015 (0.015)
Train: 168 [1150/1251 ( 92%)]  Loss: 3.349 (3.37)  Time: 0.680s, 1505.57/s  (0.669s, 1531.49/s)  LR: 4.046e-04  Data: 0.013 (0.015)
Train: 168 [1200/1251 ( 96%)]  Loss: 3.495 (3.38)  Time: 0.674s, 1518.88/s  (0.669s, 1531.33/s)  LR: 4.044e-04  Data: 0.014 (0.015)
Train: 168 [1250/1251 (100%)]  Loss: 3.129 (3.37)  Time: 0.670s, 1528.97/s  (0.669s, 1531.14/s)  LR: 4.042e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.897 (2.897)  Loss:  0.4695 (0.4695)  Acc@1: 90.2344 (90.2344)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.162 (0.325)  Loss:  0.5635 (0.9500)  Acc@1: 85.8491 (77.4800)  Acc@5: 96.9340 (94.0360)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-167.pth.tar', 77.49600010986327)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-168.pth.tar', 77.47999992919922)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-166.pth.tar', 77.38999992919922)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-163.pth.tar', 77.30200005615234)

Train: 169 [   0/1251 (  0%)]  Loss: 3.619 (3.62)  Time: 3.461s,  295.89/s  (3.461s,  295.89/s)  LR: 4.042e-04  Data: 1.616 (1.616)
Train: 169 [  50/1251 (  4%)]  Loss: 3.312 (3.47)  Time: 0.660s, 1550.58/s  (0.687s, 1491.32/s)  LR: 4.040e-04  Data: 0.013 (0.045)
Train: 169 [ 100/1251 (  8%)]  Loss: 3.275 (3.40)  Time: 0.664s, 1542.48/s  (0.675s, 1517.59/s)  LR: 4.038e-04  Data: 0.014 (0.030)
Train: 169 [ 150/1251 ( 12%)]  Loss: 3.219 (3.36)  Time: 0.665s, 1539.54/s  (0.671s, 1524.95/s)  LR: 4.035e-04  Data: 0.012 (0.024)
Train: 169 [ 200/1251 ( 16%)]  Loss: 3.691 (3.42)  Time: 0.666s, 1537.52/s  (0.670s, 1527.40/s)  LR: 4.033e-04  Data: 0.017 (0.022)
Train: 169 [ 250/1251 ( 20%)]  Loss: 3.491 (3.43)  Time: 0.672s, 1523.04/s  (0.669s, 1529.87/s)  LR: 4.031e-04  Data: 0.012 (0.020)
Train: 169 [ 300/1251 ( 24%)]  Loss: 3.612 (3.46)  Time: 0.667s, 1534.64/s  (0.669s, 1530.93/s)  LR: 4.029e-04  Data: 0.014 (0.019)
Train: 169 [ 350/1251 ( 28%)]  Loss: 3.531 (3.47)  Time: 0.669s, 1531.72/s  (0.669s, 1531.42/s)  LR: 4.027e-04  Data: 0.014 (0.018)
Train: 169 [ 400/1251 ( 32%)]  Loss: 3.362 (3.46)  Time: 0.665s, 1539.89/s  (0.668s, 1531.96/s)  LR: 4.025e-04  Data: 0.013 (0.018)
Train: 169 [ 450/1251 ( 36%)]  Loss: 3.385 (3.45)  Time: 0.667s, 1535.37/s  (0.668s, 1532.45/s)  LR: 4.023e-04  Data: 0.015 (0.017)
Train: 169 [ 500/1251 ( 40%)]  Loss: 3.114 (3.42)  Time: 0.665s, 1540.72/s  (0.668s, 1533.19/s)  LR: 4.021e-04  Data: 0.014 (0.017)
Train: 169 [ 550/1251 ( 44%)]  Loss: 3.296 (3.41)  Time: 0.665s, 1539.34/s  (0.668s, 1533.21/s)  LR: 4.019e-04  Data: 0.013 (0.016)
Train: 169 [ 600/1251 ( 48%)]  Loss: 3.396 (3.41)  Time: 0.668s, 1531.95/s  (0.668s, 1533.40/s)  LR: 4.017e-04  Data: 0.014 (0.016)
Train: 169 [ 650/1251 ( 52%)]  Loss: 3.298 (3.40)  Time: 0.673s, 1521.48/s  (0.668s, 1533.57/s)  LR: 4.015e-04  Data: 0.013 (0.016)
Train: 169 [ 700/1251 ( 56%)]  Loss: 3.336 (3.40)  Time: 0.672s, 1524.42/s  (0.668s, 1533.08/s)  LR: 4.013e-04  Data: 0.013 (0.016)
Train: 169 [ 750/1251 ( 60%)]  Loss: 3.227 (3.39)  Time: 0.675s, 1516.48/s  (0.668s, 1532.56/s)  LR: 4.011e-04  Data: 0.015 (0.016)
Train: 169 [ 800/1251 ( 64%)]  Loss: 3.446 (3.39)  Time: 0.663s, 1543.42/s  (0.668s, 1532.45/s)  LR: 4.009e-04  Data: 0.015 (0.016)
Train: 169 [ 850/1251 ( 68%)]  Loss: 3.386 (3.39)  Time: 0.664s, 1541.29/s  (0.668s, 1532.60/s)  LR: 4.007e-04  Data: 0.014 (0.015)
Train: 169 [ 900/1251 ( 72%)]  Loss: 3.664 (3.40)  Time: 0.667s, 1534.67/s  (0.668s, 1532.98/s)  LR: 4.005e-04  Data: 0.012 (0.015)
Train: 169 [ 950/1251 ( 76%)]  Loss: 2.876 (3.38)  Time: 0.666s, 1537.23/s  (0.668s, 1533.12/s)  LR: 4.003e-04  Data: 0.013 (0.015)
Train: 169 [1000/1251 ( 80%)]  Loss: 3.417 (3.38)  Time: 0.654s, 1565.63/s  (0.668s, 1533.47/s)  LR: 4.001e-04  Data: 0.013 (0.015)
Train: 169 [1050/1251 ( 84%)]  Loss: 3.352 (3.38)  Time: 0.668s, 1533.74/s  (0.668s, 1533.71/s)  LR: 3.999e-04  Data: 0.013 (0.015)
Train: 169 [1100/1251 ( 88%)]  Loss: 3.533 (3.38)  Time: 0.666s, 1536.85/s  (0.668s, 1533.99/s)  LR: 3.997e-04  Data: 0.013 (0.015)
Train: 169 [1150/1251 ( 92%)]  Loss: 3.620 (3.39)  Time: 0.671s, 1526.61/s  (0.668s, 1533.96/s)  LR: 3.995e-04  Data: 0.012 (0.015)
Train: 169 [1200/1251 ( 96%)]  Loss: 3.317 (3.39)  Time: 0.668s, 1532.26/s  (0.667s, 1534.10/s)  LR: 3.993e-04  Data: 0.013 (0.015)
Train: 169 [1250/1251 (100%)]  Loss: 3.425 (3.39)  Time: 0.645s, 1588.71/s  (0.667s, 1534.50/s)  LR: 3.991e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.829 (2.829)  Loss:  0.4177 (0.4177)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.161 (0.323)  Loss:  0.5542 (0.9257)  Acc@1: 85.1415 (77.3640)  Acc@5: 97.0519 (94.1460)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-167.pth.tar', 77.49600010986327)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-168.pth.tar', 77.47999992919922)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-166.pth.tar', 77.38999992919922)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-169.pth.tar', 77.36400000976562)

Train: 170 [   0/1251 (  0%)]  Loss: 3.500 (3.50)  Time: 3.758s,  272.47/s  (3.758s,  272.47/s)  LR: 3.991e-04  Data: 1.763 (1.763)
Train: 170 [  50/1251 (  4%)]  Loss: 3.115 (3.31)  Time: 0.649s, 1576.65/s  (0.689s, 1485.85/s)  LR: 3.989e-04  Data: 0.014 (0.048)
Train: 170 [ 100/1251 (  8%)]  Loss: 3.400 (3.34)  Time: 0.666s, 1537.39/s  (0.674s, 1518.81/s)  LR: 3.987e-04  Data: 0.013 (0.031)
Train: 170 [ 150/1251 ( 12%)]  Loss: 3.127 (3.29)  Time: 0.658s, 1555.35/s  (0.670s, 1529.09/s)  LR: 3.984e-04  Data: 0.013 (0.025)
Train: 170 [ 200/1251 ( 16%)]  Loss: 3.751 (3.38)  Time: 0.663s, 1544.24/s  (0.668s, 1533.16/s)  LR: 3.982e-04  Data: 0.014 (0.023)
Train: 170 [ 250/1251 ( 20%)]  Loss: 3.629 (3.42)  Time: 0.671s, 1526.70/s  (0.667s, 1535.13/s)  LR: 3.980e-04  Data: 0.014 (0.021)
Train: 170 [ 300/1251 ( 24%)]  Loss: 3.413 (3.42)  Time: 0.667s, 1534.96/s  (0.667s, 1535.86/s)  LR: 3.978e-04  Data: 0.013 (0.020)
Train: 170 [ 350/1251 ( 28%)]  Loss: 3.246 (3.40)  Time: 0.672s, 1524.54/s  (0.667s, 1536.21/s)  LR: 3.976e-04  Data: 0.012 (0.019)
Train: 170 [ 400/1251 ( 32%)]  Loss: 3.518 (3.41)  Time: 0.663s, 1544.47/s  (0.667s, 1536.20/s)  LR: 3.974e-04  Data: 0.016 (0.018)
Train: 170 [ 450/1251 ( 36%)]  Loss: 3.365 (3.41)  Time: 0.669s, 1531.58/s  (0.667s, 1536.31/s)  LR: 3.972e-04  Data: 0.013 (0.018)
Train: 170 [ 500/1251 ( 40%)]  Loss: 3.488 (3.41)  Time: 0.672s, 1522.99/s  (0.667s, 1536.08/s)  LR: 3.970e-04  Data: 0.012 (0.017)
Train: 170 [ 550/1251 ( 44%)]  Loss: 3.416 (3.41)  Time: 0.671s, 1525.33/s  (0.667s, 1535.89/s)  LR: 3.968e-04  Data: 0.014 (0.017)
Train: 170 [ 600/1251 ( 48%)]  Loss: 3.481 (3.42)  Time: 0.675s, 1518.07/s  (0.667s, 1535.49/s)  LR: 3.966e-04  Data: 0.013 (0.017)
Train: 170 [ 650/1251 ( 52%)]  Loss: 3.165 (3.40)  Time: 0.669s, 1531.17/s  (0.667s, 1534.96/s)  LR: 3.964e-04  Data: 0.012 (0.016)
Train: 170 [ 700/1251 ( 56%)]  Loss: 3.348 (3.40)  Time: 0.666s, 1537.53/s  (0.667s, 1534.77/s)  LR: 3.962e-04  Data: 0.016 (0.016)
Train: 170 [ 750/1251 ( 60%)]  Loss: 3.276 (3.39)  Time: 0.672s, 1522.77/s  (0.667s, 1534.69/s)  LR: 3.960e-04  Data: 0.013 (0.016)
Train: 170 [ 800/1251 ( 64%)]  Loss: 3.388 (3.39)  Time: 0.679s, 1507.80/s  (0.667s, 1534.37/s)  LR: 3.958e-04  Data: 0.013 (0.016)
Train: 170 [ 850/1251 ( 68%)]  Loss: 3.597 (3.40)  Time: 0.672s, 1524.93/s  (0.667s, 1534.13/s)  LR: 3.956e-04  Data: 0.013 (0.016)
Train: 170 [ 900/1251 ( 72%)]  Loss: 3.681 (3.42)  Time: 0.660s, 1550.63/s  (0.667s, 1534.09/s)  LR: 3.954e-04  Data: 0.013 (0.016)
Train: 170 [ 950/1251 ( 76%)]  Loss: 3.414 (3.42)  Time: 0.666s, 1536.42/s  (0.668s, 1533.94/s)  LR: 3.952e-04  Data: 0.013 (0.016)
Train: 170 [1000/1251 ( 80%)]  Loss: 3.675 (3.43)  Time: 0.670s, 1528.12/s  (0.668s, 1533.70/s)  LR: 3.950e-04  Data: 0.013 (0.015)
Train: 170 [1050/1251 ( 84%)]  Loss: 3.675 (3.44)  Time: 0.661s, 1549.26/s  (0.668s, 1533.91/s)  LR: 3.948e-04  Data: 0.014 (0.015)
Train: 170 [1100/1251 ( 88%)]  Loss: 3.175 (3.43)  Time: 0.667s, 1535.83/s  (0.668s, 1534.02/s)  LR: 3.946e-04  Data: 0.015 (0.015)
Train: 170 [1150/1251 ( 92%)]  Loss: 3.183 (3.42)  Time: 0.670s, 1528.94/s  (0.668s, 1533.95/s)  LR: 3.944e-04  Data: 0.013 (0.015)
Train: 170 [1200/1251 ( 96%)]  Loss: 3.695 (3.43)  Time: 0.666s, 1536.46/s  (0.668s, 1533.87/s)  LR: 3.942e-04  Data: 0.012 (0.015)
Train: 170 [1250/1251 (100%)]  Loss: 3.538 (3.43)  Time: 0.654s, 1566.54/s  (0.668s, 1533.98/s)  LR: 3.940e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.853 (2.853)  Loss:  0.4543 (0.4543)  Acc@1: 91.5039 (91.5039)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.162 (0.330)  Loss:  0.5278 (0.9405)  Acc@1: 86.7924 (77.4540)  Acc@5: 97.5236 (94.0440)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-167.pth.tar', 77.49600010986327)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-168.pth.tar', 77.47999992919922)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-170.pth.tar', 77.45399995117188)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-166.pth.tar', 77.38999992919922)

Train: 171 [   0/1251 (  0%)]  Loss: 3.129 (3.13)  Time: 3.460s,  295.97/s  (3.460s,  295.97/s)  LR: 3.940e-04  Data: 1.689 (1.689)
Train: 171 [  50/1251 (  4%)]  Loss: 3.655 (3.39)  Time: 0.650s, 1574.72/s  (0.685s, 1494.25/s)  LR: 3.938e-04  Data: 0.013 (0.047)
Train: 171 [ 100/1251 (  8%)]  Loss: 3.371 (3.39)  Time: 0.655s, 1563.95/s  (0.673s, 1522.24/s)  LR: 3.936e-04  Data: 0.013 (0.031)
Train: 171 [ 150/1251 ( 12%)]  Loss: 3.230 (3.35)  Time: 0.671s, 1526.93/s  (0.670s, 1529.06/s)  LR: 3.934e-04  Data: 0.013 (0.025)
Train: 171 [ 200/1251 ( 16%)]  Loss: 3.462 (3.37)  Time: 0.663s, 1543.55/s  (0.669s, 1531.69/s)  LR: 3.932e-04  Data: 0.013 (0.022)
Train: 171 [ 250/1251 ( 20%)]  Loss: 3.233 (3.35)  Time: 0.675s, 1517.58/s  (0.668s, 1533.39/s)  LR: 3.930e-04  Data: 0.013 (0.020)
Train: 171 [ 300/1251 ( 24%)]  Loss: 3.046 (3.30)  Time: 0.669s, 1531.77/s  (0.667s, 1534.24/s)  LR: 3.928e-04  Data: 0.012 (0.019)
Train: 171 [ 350/1251 ( 28%)]  Loss: 3.362 (3.31)  Time: 0.674s, 1519.21/s  (0.667s, 1534.56/s)  LR: 3.925e-04  Data: 0.012 (0.018)
Train: 171 [ 400/1251 ( 32%)]  Loss: 3.489 (3.33)  Time: 0.677s, 1513.41/s  (0.667s, 1534.53/s)  LR: 3.923e-04  Data: 0.012 (0.018)
Train: 171 [ 450/1251 ( 36%)]  Loss: 3.568 (3.35)  Time: 0.671s, 1526.37/s  (0.667s, 1534.38/s)  LR: 3.921e-04  Data: 0.013 (0.017)
Train: 171 [ 500/1251 ( 40%)]  Loss: 3.293 (3.35)  Time: 0.660s, 1550.67/s  (0.667s, 1534.22/s)  LR: 3.919e-04  Data: 0.013 (0.017)
Train: 171 [ 550/1251 ( 44%)]  Loss: 3.116 (3.33)  Time: 0.654s, 1566.48/s  (0.668s, 1533.88/s)  LR: 3.917e-04  Data: 0.014 (0.017)
Train: 171 [ 600/1251 ( 48%)]  Loss: 3.485 (3.34)  Time: 0.667s, 1535.76/s  (0.668s, 1533.69/s)  LR: 3.915e-04  Data: 0.014 (0.016)
Train: 171 [ 650/1251 ( 52%)]  Loss: 3.358 (3.34)  Time: 0.682s, 1500.88/s  (0.668s, 1533.28/s)  LR: 3.913e-04  Data: 0.013 (0.016)
Train: 171 [ 700/1251 ( 56%)]  Loss: 3.491 (3.35)  Time: 0.679s, 1507.83/s  (0.668s, 1532.77/s)  LR: 3.911e-04  Data: 0.012 (0.016)
Train: 171 [ 750/1251 ( 60%)]  Loss: 3.226 (3.34)  Time: 0.663s, 1544.69/s  (0.668s, 1532.87/s)  LR: 3.909e-04  Data: 0.015 (0.016)
Train: 171 [ 800/1251 ( 64%)]  Loss: 3.452 (3.35)  Time: 0.672s, 1524.23/s  (0.668s, 1532.72/s)  LR: 3.907e-04  Data: 0.014 (0.016)
Train: 171 [ 850/1251 ( 68%)]  Loss: 3.693 (3.37)  Time: 0.659s, 1553.95/s  (0.668s, 1532.65/s)  LR: 3.905e-04  Data: 0.013 (0.016)
Train: 171 [ 900/1251 ( 72%)]  Loss: 3.498 (3.38)  Time: 0.682s, 1501.21/s  (0.668s, 1532.47/s)  LR: 3.903e-04  Data: 0.016 (0.015)
Train: 171 [ 950/1251 ( 76%)]  Loss: 2.997 (3.36)  Time: 0.678s, 1511.05/s  (0.668s, 1532.28/s)  LR: 3.901e-04  Data: 0.016 (0.015)
Train: 171 [1000/1251 ( 80%)]  Loss: 3.534 (3.37)  Time: 0.663s, 1543.35/s  (0.668s, 1532.28/s)  LR: 3.899e-04  Data: 0.013 (0.015)
Train: 171 [1050/1251 ( 84%)]  Loss: 3.670 (3.38)  Time: 0.677s, 1511.67/s  (0.668s, 1532.17/s)  LR: 3.897e-04  Data: 0.012 (0.015)
Train: 171 [1100/1251 ( 88%)]  Loss: 2.923 (3.36)  Time: 0.663s, 1544.74/s  (0.668s, 1532.20/s)  LR: 3.895e-04  Data: 0.013 (0.015)
Train: 171 [1150/1251 ( 92%)]  Loss: 3.312 (3.36)  Time: 0.665s, 1540.21/s  (0.668s, 1532.27/s)  LR: 3.893e-04  Data: 0.013 (0.015)
Train: 171 [1200/1251 ( 96%)]  Loss: 3.207 (3.35)  Time: 0.670s, 1527.25/s  (0.668s, 1532.41/s)  LR: 3.891e-04  Data: 0.012 (0.015)
Train: 171 [1250/1251 (100%)]  Loss: 3.091 (3.34)  Time: 0.646s, 1584.56/s  (0.668s, 1532.59/s)  LR: 3.889e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.810 (2.810)  Loss:  0.4302 (0.4302)  Acc@1: 91.1133 (91.1133)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.161 (0.324)  Loss:  0.5376 (0.9516)  Acc@1: 87.1462 (77.9360)  Acc@5: 97.9953 (94.2020)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-171.pth.tar', 77.93599997558594)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-167.pth.tar', 77.49600010986327)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-168.pth.tar', 77.47999992919922)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-170.pth.tar', 77.45399995117188)

Train: 172 [   0/1251 (  0%)]  Loss: 3.328 (3.33)  Time: 3.534s,  289.76/s  (3.534s,  289.76/s)  LR: 3.889e-04  Data: 1.760 (1.760)
Train: 172 [  50/1251 (  4%)]  Loss: 3.220 (3.27)  Time: 0.652s, 1571.66/s  (0.687s, 1490.32/s)  LR: 3.887e-04  Data: 0.015 (0.048)
Train: 172 [ 100/1251 (  8%)]  Loss: 3.370 (3.31)  Time: 0.658s, 1555.61/s  (0.671s, 1526.32/s)  LR: 3.885e-04  Data: 0.014 (0.031)
Train: 172 [ 150/1251 ( 12%)]  Loss: 3.352 (3.32)  Time: 0.664s, 1542.66/s  (0.667s, 1534.18/s)  LR: 3.883e-04  Data: 0.015 (0.025)
Train: 172 [ 200/1251 ( 16%)]  Loss: 3.041 (3.26)  Time: 0.666s, 1537.05/s  (0.666s, 1537.31/s)  LR: 3.881e-04  Data: 0.014 (0.023)
Train: 172 [ 250/1251 ( 20%)]  Loss: 3.241 (3.26)  Time: 0.657s, 1559.24/s  (0.666s, 1538.35/s)  LR: 3.879e-04  Data: 0.014 (0.021)
Train: 172 [ 300/1251 ( 24%)]  Loss: 3.171 (3.25)  Time: 0.674s, 1520.37/s  (0.666s, 1538.51/s)  LR: 3.877e-04  Data: 0.016 (0.020)
Train: 172 [ 350/1251 ( 28%)]  Loss: 3.209 (3.24)  Time: 0.659s, 1554.39/s  (0.666s, 1538.03/s)  LR: 3.875e-04  Data: 0.013 (0.019)
Train: 172 [ 400/1251 ( 32%)]  Loss: 3.511 (3.27)  Time: 0.667s, 1534.72/s  (0.666s, 1538.07/s)  LR: 3.873e-04  Data: 0.013 (0.018)
Train: 172 [ 450/1251 ( 36%)]  Loss: 3.103 (3.25)  Time: 0.659s, 1553.49/s  (0.666s, 1538.41/s)  LR: 3.871e-04  Data: 0.012 (0.018)
Train: 172 [ 500/1251 ( 40%)]  Loss: 3.350 (3.26)  Time: 0.657s, 1557.84/s  (0.666s, 1538.34/s)  LR: 3.869e-04  Data: 0.014 (0.017)
Train: 172 [ 550/1251 ( 44%)]  Loss: 3.447 (3.28)  Time: 0.666s, 1536.61/s  (0.666s, 1538.07/s)  LR: 3.867e-04  Data: 0.013 (0.017)
Train: 172 [ 600/1251 ( 48%)]  Loss: 3.517 (3.30)  Time: 0.664s, 1542.52/s  (0.666s, 1537.77/s)  LR: 3.865e-04  Data: 0.013 (0.017)
Train: 172 [ 650/1251 ( 52%)]  Loss: 3.392 (3.30)  Time: 0.669s, 1530.42/s  (0.666s, 1537.93/s)  LR: 3.863e-04  Data: 0.014 (0.016)
Train: 172 [ 700/1251 ( 56%)]  Loss: 3.498 (3.32)  Time: 0.665s, 1539.13/s  (0.666s, 1537.85/s)  LR: 3.861e-04  Data: 0.014 (0.016)
Train: 172 [ 750/1251 ( 60%)]  Loss: 3.368 (3.32)  Time: 0.667s, 1534.47/s  (0.666s, 1537.41/s)  LR: 3.859e-04  Data: 0.013 (0.016)
Train: 172 [ 800/1251 ( 64%)]  Loss: 3.420 (3.33)  Time: 0.666s, 1538.28/s  (0.666s, 1537.17/s)  LR: 3.857e-04  Data: 0.014 (0.016)
Train: 172 [ 850/1251 ( 68%)]  Loss: 3.449 (3.33)  Time: 0.664s, 1542.48/s  (0.666s, 1537.06/s)  LR: 3.854e-04  Data: 0.013 (0.016)
Train: 172 [ 900/1251 ( 72%)]  Loss: 3.233 (3.33)  Time: 0.660s, 1552.56/s  (0.666s, 1536.67/s)  LR: 3.852e-04  Data: 0.013 (0.016)
Train: 172 [ 950/1251 ( 76%)]  Loss: 3.550 (3.34)  Time: 0.673s, 1522.60/s  (0.666s, 1536.64/s)  LR: 3.850e-04  Data: 0.012 (0.015)
Train: 172 [1000/1251 ( 80%)]  Loss: 3.768 (3.36)  Time: 0.673s, 1521.67/s  (0.667s, 1536.32/s)  LR: 3.848e-04  Data: 0.014 (0.015)
Train: 172 [1050/1251 ( 84%)]  Loss: 3.295 (3.36)  Time: 0.668s, 1531.84/s  (0.667s, 1536.35/s)  LR: 3.846e-04  Data: 0.014 (0.015)
Train: 172 [1100/1251 ( 88%)]  Loss: 3.601 (3.37)  Time: 0.669s, 1530.69/s  (0.667s, 1536.18/s)  LR: 3.844e-04  Data: 0.013 (0.015)
Train: 172 [1150/1251 ( 92%)]  Loss: 3.349 (3.37)  Time: 0.667s, 1534.14/s  (0.667s, 1536.08/s)  LR: 3.842e-04  Data: 0.012 (0.015)
Train: 172 [1200/1251 ( 96%)]  Loss: 3.560 (3.37)  Time: 0.663s, 1545.58/s  (0.667s, 1535.92/s)  LR: 3.840e-04  Data: 0.014 (0.015)
Train: 172 [1250/1251 (100%)]  Loss: 3.475 (3.38)  Time: 0.661s, 1548.64/s  (0.667s, 1536.10/s)  LR: 3.838e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.892 (2.892)  Loss:  0.4497 (0.4497)  Acc@1: 90.9180 (90.9180)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.161 (0.323)  Loss:  0.5776 (0.9603)  Acc@1: 86.2028 (77.5480)  Acc@5: 97.5236 (94.0180)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-171.pth.tar', 77.93599997558594)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-172.pth.tar', 77.54799995361329)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-167.pth.tar', 77.49600010986327)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-168.pth.tar', 77.47999992919922)

Train: 173 [   0/1251 (  0%)]  Loss: 3.118 (3.12)  Time: 3.341s,  306.53/s  (3.341s,  306.53/s)  LR: 3.838e-04  Data: 1.613 (1.613)
Train: 173 [  50/1251 (  4%)]  Loss: 3.687 (3.40)  Time: 0.652s, 1571.25/s  (0.680s, 1506.48/s)  LR: 3.836e-04  Data: 0.013 (0.045)
Train: 173 [ 100/1251 (  8%)]  Loss: 3.730 (3.51)  Time: 0.663s, 1543.55/s  (0.667s, 1534.15/s)  LR: 3.834e-04  Data: 0.013 (0.029)
Train: 173 [ 150/1251 ( 12%)]  Loss: 3.356 (3.47)  Time: 0.658s, 1555.24/s  (0.665s, 1540.19/s)  LR: 3.832e-04  Data: 0.013 (0.024)
Train: 173 [ 200/1251 ( 16%)]  Loss: 3.255 (3.43)  Time: 0.667s, 1534.64/s  (0.664s, 1542.20/s)  LR: 3.830e-04  Data: 0.017 (0.022)
Train: 173 [ 250/1251 ( 20%)]  Loss: 3.420 (3.43)  Time: 0.662s, 1545.67/s  (0.664s, 1542.93/s)  LR: 3.828e-04  Data: 0.014 (0.020)
Train: 173 [ 300/1251 ( 24%)]  Loss: 3.443 (3.43)  Time: 0.664s, 1542.33/s  (0.664s, 1543.21/s)  LR: 3.826e-04  Data: 0.014 (0.019)
Train: 173 [ 350/1251 ( 28%)]  Loss: 2.983 (3.37)  Time: 0.676s, 1514.78/s  (0.664s, 1542.50/s)  LR: 3.824e-04  Data: 0.014 (0.018)
Train: 173 [ 400/1251 ( 32%)]  Loss: 3.571 (3.40)  Time: 0.670s, 1529.17/s  (0.664s, 1541.57/s)  LR: 3.822e-04  Data: 0.013 (0.018)
Train: 173 [ 450/1251 ( 36%)]  Loss: 3.306 (3.39)  Time: 0.671s, 1525.57/s  (0.664s, 1541.27/s)  LR: 3.820e-04  Data: 0.013 (0.017)
Train: 173 [ 500/1251 ( 40%)]  Loss: 3.539 (3.40)  Time: 0.664s, 1543.17/s  (0.664s, 1541.17/s)  LR: 3.818e-04  Data: 0.013 (0.017)
Train: 173 [ 550/1251 ( 44%)]  Loss: 3.386 (3.40)  Time: 0.673s, 1521.37/s  (0.665s, 1540.67/s)  LR: 3.816e-04  Data: 0.013 (0.017)
Train: 173 [ 600/1251 ( 48%)]  Loss: 3.196 (3.38)  Time: 0.671s, 1527.21/s  (0.665s, 1540.19/s)  LR: 3.814e-04  Data: 0.013 (0.016)
Train: 173 [ 650/1251 ( 52%)]  Loss: 3.025 (3.36)  Time: 0.669s, 1530.28/s  (0.665s, 1539.93/s)  LR: 3.812e-04  Data: 0.013 (0.016)
Train: 173 [ 700/1251 ( 56%)]  Loss: 3.393 (3.36)  Time: 0.661s, 1548.37/s  (0.665s, 1539.40/s)  LR: 3.810e-04  Data: 0.014 (0.016)
Train: 173 [ 750/1251 ( 60%)]  Loss: 3.616 (3.38)  Time: 0.669s, 1531.15/s  (0.665s, 1538.98/s)  LR: 3.808e-04  Data: 0.015 (0.016)
Train: 173 [ 800/1251 ( 64%)]  Loss: 3.368 (3.38)  Time: 0.665s, 1540.22/s  (0.665s, 1538.93/s)  LR: 3.806e-04  Data: 0.014 (0.016)
Train: 173 [ 850/1251 ( 68%)]  Loss: 3.408 (3.38)  Time: 0.670s, 1529.32/s  (0.666s, 1538.63/s)  LR: 3.804e-04  Data: 0.014 (0.016)
Train: 173 [ 900/1251 ( 72%)]  Loss: 3.176 (3.37)  Time: 0.667s, 1534.89/s  (0.666s, 1538.17/s)  LR: 3.802e-04  Data: 0.013 (0.015)
Train: 173 [ 950/1251 ( 76%)]  Loss: 3.610 (3.38)  Time: 0.657s, 1557.51/s  (0.666s, 1537.89/s)  LR: 3.800e-04  Data: 0.013 (0.015)
Train: 173 [1000/1251 ( 80%)]  Loss: 3.429 (3.38)  Time: 0.664s, 1541.24/s  (0.666s, 1537.79/s)  LR: 3.798e-04  Data: 0.016 (0.015)
Train: 173 [1050/1251 ( 84%)]  Loss: 3.372 (3.38)  Time: 0.670s, 1527.81/s  (0.666s, 1537.58/s)  LR: 3.796e-04  Data: 0.015 (0.015)
Train: 173 [1100/1251 ( 88%)]  Loss: 3.409 (3.38)  Time: 0.665s, 1538.74/s  (0.666s, 1537.47/s)  LR: 3.794e-04  Data: 0.014 (0.015)
Train: 173 [1150/1251 ( 92%)]  Loss: 3.468 (3.39)  Time: 0.670s, 1528.97/s  (0.666s, 1537.47/s)  LR: 3.792e-04  Data: 0.012 (0.015)
Train: 173 [1200/1251 ( 96%)]  Loss: 3.623 (3.40)  Time: 0.670s, 1528.00/s  (0.666s, 1537.38/s)  LR: 3.790e-04  Data: 0.014 (0.015)
Train: 173 [1250/1251 (100%)]  Loss: 3.674 (3.41)  Time: 0.651s, 1571.95/s  (0.666s, 1537.45/s)  LR: 3.788e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.838 (2.838)  Loss:  0.4561 (0.4561)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.162 (0.326)  Loss:  0.5181 (0.9466)  Acc@1: 86.7924 (77.7760)  Acc@5: 98.1132 (94.2200)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-171.pth.tar', 77.93599997558594)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-173.pth.tar', 77.77599995117187)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-172.pth.tar', 77.54799995361329)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-167.pth.tar', 77.49600010986327)

Train: 174 [   0/1251 (  0%)]  Loss: 3.592 (3.59)  Time: 3.543s,  288.99/s  (3.543s,  288.99/s)  LR: 3.788e-04  Data: 1.662 (1.662)
Train: 174 [  50/1251 (  4%)]  Loss: 3.181 (3.39)  Time: 0.660s, 1551.18/s  (0.692s, 1479.62/s)  LR: 3.786e-04  Data: 0.013 (0.047)
Train: 174 [ 100/1251 (  8%)]  Loss: 3.420 (3.40)  Time: 0.665s, 1540.69/s  (0.676s, 1513.85/s)  LR: 3.784e-04  Data: 0.018 (0.030)
Train: 174 [ 150/1251 ( 12%)]  Loss: 3.343 (3.38)  Time: 0.661s, 1548.97/s  (0.671s, 1525.49/s)  LR: 3.782e-04  Data: 0.013 (0.025)
Train: 174 [ 200/1251 ( 16%)]  Loss: 3.128 (3.33)  Time: 0.664s, 1542.20/s  (0.669s, 1530.18/s)  LR: 3.780e-04  Data: 0.016 (0.022)
Train: 174 [ 250/1251 ( 20%)]  Loss: 3.465 (3.35)  Time: 0.672s, 1524.15/s  (0.668s, 1532.68/s)  LR: 3.778e-04  Data: 0.014 (0.021)
Train: 174 [ 300/1251 ( 24%)]  Loss: 3.228 (3.34)  Time: 0.668s, 1533.61/s  (0.668s, 1533.77/s)  LR: 3.776e-04  Data: 0.013 (0.020)
Train: 174 [ 350/1251 ( 28%)]  Loss: 3.470 (3.35)  Time: 0.666s, 1537.33/s  (0.668s, 1534.00/s)  LR: 3.774e-04  Data: 0.014 (0.019)
Train: 174 [ 400/1251 ( 32%)]  Loss: 3.673 (3.39)  Time: 0.661s, 1548.92/s  (0.667s, 1534.45/s)  LR: 3.772e-04  Data: 0.013 (0.018)
Train: 174 [ 450/1251 ( 36%)]  Loss: 3.188 (3.37)  Time: 0.657s, 1557.62/s  (0.667s, 1534.74/s)  LR: 3.770e-04  Data: 0.012 (0.018)
Train: 174 [ 500/1251 ( 40%)]  Loss: 3.366 (3.37)  Time: 0.666s, 1536.39/s  (0.667s, 1534.52/s)  LR: 3.768e-04  Data: 0.014 (0.017)
Train: 174 [ 550/1251 ( 44%)]  Loss: 3.504 (3.38)  Time: 0.665s, 1540.76/s  (0.667s, 1534.83/s)  LR: 3.766e-04  Data: 0.012 (0.017)
Train: 174 [ 600/1251 ( 48%)]  Loss: 3.531 (3.39)  Time: 0.665s, 1540.06/s  (0.667s, 1535.27/s)  LR: 3.764e-04  Data: 0.014 (0.017)
Train: 174 [ 650/1251 ( 52%)]  Loss: 3.236 (3.38)  Time: 0.669s, 1530.79/s  (0.667s, 1535.26/s)  LR: 3.762e-04  Data: 0.013 (0.016)
Train: 174 [ 700/1251 ( 56%)]  Loss: 3.150 (3.36)  Time: 0.662s, 1545.94/s  (0.667s, 1534.85/s)  LR: 3.760e-04  Data: 0.013 (0.016)
Train: 174 [ 750/1251 ( 60%)]  Loss: 3.301 (3.36)  Time: 0.676s, 1514.69/s  (0.667s, 1534.52/s)  LR: 3.757e-04  Data: 0.015 (0.016)
Train: 174 [ 800/1251 ( 64%)]  Loss: 3.501 (3.37)  Time: 0.671s, 1526.69/s  (0.667s, 1534.17/s)  LR: 3.755e-04  Data: 0.013 (0.016)
Train: 174 [ 850/1251 ( 68%)]  Loss: 3.451 (3.37)  Time: 0.676s, 1514.34/s  (0.668s, 1533.79/s)  LR: 3.753e-04  Data: 0.013 (0.016)
Train: 174 [ 900/1251 ( 72%)]  Loss: 2.990 (3.35)  Time: 0.666s, 1537.90/s  (0.668s, 1533.67/s)  LR: 3.751e-04  Data: 0.014 (0.016)
Train: 174 [ 950/1251 ( 76%)]  Loss: 3.285 (3.35)  Time: 0.664s, 1541.54/s  (0.668s, 1533.55/s)  LR: 3.749e-04  Data: 0.013 (0.016)
Train: 174 [1000/1251 ( 80%)]  Loss: 3.142 (3.34)  Time: 0.666s, 1537.31/s  (0.668s, 1533.34/s)  LR: 3.747e-04  Data: 0.015 (0.016)
Train: 174 [1050/1251 ( 84%)]  Loss: 3.342 (3.34)  Time: 0.673s, 1521.82/s  (0.668s, 1533.15/s)  LR: 3.745e-04  Data: 0.014 (0.015)
Train: 174 [1100/1251 ( 88%)]  Loss: 3.480 (3.35)  Time: 0.658s, 1556.13/s  (0.668s, 1533.23/s)  LR: 3.743e-04  Data: 0.013 (0.015)
Train: 174 [1150/1251 ( 92%)]  Loss: 3.288 (3.34)  Time: 0.666s, 1538.49/s  (0.668s, 1533.43/s)  LR: 3.741e-04  Data: 0.014 (0.015)
Train: 174 [1200/1251 ( 96%)]  Loss: 3.394 (3.35)  Time: 0.657s, 1559.33/s  (0.668s, 1533.89/s)  LR: 3.739e-04  Data: 0.013 (0.015)
Train: 174 [1250/1251 (100%)]  Loss: 3.521 (3.35)  Time: 0.651s, 1573.67/s  (0.667s, 1534.31/s)  LR: 3.737e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.965 (2.965)  Loss:  0.4578 (0.4578)  Acc@1: 90.8203 (90.8203)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.163 (0.323)  Loss:  0.5449 (0.9406)  Acc@1: 86.4387 (77.7840)  Acc@5: 97.5236 (94.0700)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-171.pth.tar', 77.93599997558594)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-174.pth.tar', 77.78399992675782)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-173.pth.tar', 77.77599995117187)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-172.pth.tar', 77.54799995361329)

Train: 175 [   0/1251 (  0%)]  Loss: 3.468 (3.47)  Time: 3.296s,  310.71/s  (3.296s,  310.71/s)  LR: 3.737e-04  Data: 1.731 (1.731)
Train: 175 [  50/1251 (  4%)]  Loss: 3.323 (3.40)  Time: 0.649s, 1577.69/s  (0.676s, 1514.95/s)  LR: 3.735e-04  Data: 0.016 (0.048)
Train: 175 [ 100/1251 (  8%)]  Loss: 3.643 (3.48)  Time: 0.663s, 1543.87/s  (0.666s, 1538.66/s)  LR: 3.733e-04  Data: 0.013 (0.031)
Train: 175 [ 150/1251 ( 12%)]  Loss: 3.311 (3.44)  Time: 0.667s, 1535.79/s  (0.664s, 1543.09/s)  LR: 3.731e-04  Data: 0.014 (0.026)
Train: 175 [ 200/1251 ( 16%)]  Loss: 3.310 (3.41)  Time: 0.672s, 1523.11/s  (0.663s, 1544.39/s)  LR: 3.729e-04  Data: 0.013 (0.023)
Train: 175 [ 250/1251 ( 20%)]  Loss: 3.549 (3.43)  Time: 0.662s, 1547.38/s  (0.663s, 1544.03/s)  LR: 3.727e-04  Data: 0.012 (0.021)
Train: 175 [ 300/1251 ( 24%)]  Loss: 3.554 (3.45)  Time: 0.669s, 1530.69/s  (0.664s, 1543.02/s)  LR: 3.725e-04  Data: 0.012 (0.020)
Train: 175 [ 350/1251 ( 28%)]  Loss: 3.311 (3.43)  Time: 0.662s, 1545.71/s  (0.664s, 1542.54/s)  LR: 3.723e-04  Data: 0.014 (0.019)
Train: 175 [ 400/1251 ( 32%)]  Loss: 3.373 (3.43)  Time: 0.661s, 1548.58/s  (0.664s, 1542.29/s)  LR: 3.721e-04  Data: 0.013 (0.018)
Train: 175 [ 450/1251 ( 36%)]  Loss: 3.443 (3.43)  Time: 0.675s, 1517.42/s  (0.664s, 1541.72/s)  LR: 3.719e-04  Data: 0.013 (0.018)
Train: 175 [ 500/1251 ( 40%)]  Loss: 3.508 (3.44)  Time: 0.672s, 1524.75/s  (0.664s, 1541.22/s)  LR: 3.717e-04  Data: 0.015 (0.017)
Train: 175 [ 550/1251 ( 44%)]  Loss: 3.624 (3.45)  Time: 0.673s, 1521.09/s  (0.665s, 1540.64/s)  LR: 3.715e-04  Data: 0.014 (0.017)
Train: 175 [ 600/1251 ( 48%)]  Loss: 3.309 (3.44)  Time: 0.665s, 1540.89/s  (0.665s, 1540.24/s)  LR: 3.713e-04  Data: 0.013 (0.017)
Train: 175 [ 650/1251 ( 52%)]  Loss: 3.461 (3.44)  Time: 0.666s, 1538.59/s  (0.665s, 1539.81/s)  LR: 3.711e-04  Data: 0.013 (0.016)
Train: 175 [ 700/1251 ( 56%)]  Loss: 3.402 (3.44)  Time: 0.666s, 1537.17/s  (0.665s, 1539.43/s)  LR: 3.709e-04  Data: 0.013 (0.016)
Train: 175 [ 750/1251 ( 60%)]  Loss: 3.358 (3.43)  Time: 0.666s, 1537.91/s  (0.665s, 1539.04/s)  LR: 3.707e-04  Data: 0.013 (0.016)
Train: 175 [ 800/1251 ( 64%)]  Loss: 3.422 (3.43)  Time: 0.665s, 1540.60/s  (0.665s, 1538.71/s)  LR: 3.705e-04  Data: 0.016 (0.016)
Train: 175 [ 850/1251 ( 68%)]  Loss: 3.398 (3.43)  Time: 0.660s, 1551.41/s  (0.666s, 1538.51/s)  LR: 3.703e-04  Data: 0.011 (0.016)
Train: 175 [ 900/1251 ( 72%)]  Loss: 3.357 (3.43)  Time: 0.669s, 1529.79/s  (0.666s, 1538.45/s)  LR: 3.701e-04  Data: 0.013 (0.016)
Train: 175 [ 950/1251 ( 76%)]  Loss: 3.470 (3.43)  Time: 0.659s, 1554.73/s  (0.666s, 1538.48/s)  LR: 3.699e-04  Data: 0.013 (0.016)
Train: 175 [1000/1251 ( 80%)]  Loss: 3.614 (3.44)  Time: 0.669s, 1530.41/s  (0.666s, 1538.58/s)  LR: 3.697e-04  Data: 0.014 (0.015)
Train: 175 [1050/1251 ( 84%)]  Loss: 3.297 (3.43)  Time: 0.664s, 1543.16/s  (0.666s, 1538.40/s)  LR: 3.695e-04  Data: 0.013 (0.015)
Train: 175 [1100/1251 ( 88%)]  Loss: 2.949 (3.41)  Time: 0.659s, 1554.32/s  (0.666s, 1538.04/s)  LR: 3.693e-04  Data: 0.012 (0.015)
Train: 175 [1150/1251 ( 92%)]  Loss: 3.414 (3.41)  Time: 0.660s, 1551.57/s  (0.666s, 1537.96/s)  LR: 3.691e-04  Data: 0.013 (0.015)
Train: 175 [1200/1251 ( 96%)]  Loss: 3.472 (3.41)  Time: 0.661s, 1548.73/s  (0.666s, 1537.94/s)  LR: 3.689e-04  Data: 0.013 (0.015)
Train: 175 [1250/1251 (100%)]  Loss: 3.073 (3.40)  Time: 0.647s, 1581.55/s  (0.666s, 1538.10/s)  LR: 3.687e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.805 (2.805)  Loss:  0.4438 (0.4438)  Acc@1: 90.9180 (90.9180)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.161 (0.329)  Loss:  0.5532 (0.9433)  Acc@1: 86.2028 (77.7580)  Acc@5: 97.7594 (94.1800)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-171.pth.tar', 77.93599997558594)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-174.pth.tar', 77.78399992675782)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-173.pth.tar', 77.77599995117187)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-175.pth.tar', 77.75799995361328)

Train: 176 [   0/1251 (  0%)]  Loss: 3.288 (3.29)  Time: 3.913s,  261.69/s  (3.913s,  261.69/s)  LR: 3.687e-04  Data: 1.655 (1.655)
Train: 176 [  50/1251 (  4%)]  Loss: 3.167 (3.23)  Time: 0.653s, 1569.16/s  (0.695s, 1473.88/s)  LR: 3.685e-04  Data: 0.014 (0.046)
Train: 176 [ 100/1251 (  8%)]  Loss: 3.463 (3.31)  Time: 0.661s, 1549.34/s  (0.676s, 1514.44/s)  LR: 3.683e-04  Data: 0.016 (0.030)
Train: 176 [ 150/1251 ( 12%)]  Loss: 3.145 (3.27)  Time: 0.663s, 1544.26/s  (0.671s, 1525.62/s)  LR: 3.681e-04  Data: 0.013 (0.025)
Train: 176 [ 200/1251 ( 16%)]  Loss: 3.245 (3.26)  Time: 0.663s, 1544.63/s  (0.669s, 1530.18/s)  LR: 3.679e-04  Data: 0.013 (0.022)
Train: 176 [ 250/1251 ( 20%)]  Loss: 3.610 (3.32)  Time: 0.665s, 1538.89/s  (0.668s, 1533.02/s)  LR: 3.677e-04  Data: 0.014 (0.020)
Train: 176 [ 300/1251 ( 24%)]  Loss: 3.732 (3.38)  Time: 0.667s, 1535.07/s  (0.667s, 1534.47/s)  LR: 3.675e-04  Data: 0.013 (0.019)
Train: 176 [ 350/1251 ( 28%)]  Loss: 3.232 (3.36)  Time: 0.672s, 1524.89/s  (0.667s, 1534.82/s)  LR: 3.673e-04  Data: 0.013 (0.018)
Train: 176 [ 400/1251 ( 32%)]  Loss: 3.441 (3.37)  Time: 0.658s, 1557.23/s  (0.667s, 1535.38/s)  LR: 3.671e-04  Data: 0.014 (0.018)
Train: 176 [ 450/1251 ( 36%)]  Loss: 3.528 (3.39)  Time: 0.670s, 1528.26/s  (0.667s, 1535.62/s)  LR: 3.669e-04  Data: 0.013 (0.017)
Train: 176 [ 500/1251 ( 40%)]  Loss: 3.475 (3.39)  Time: 0.673s, 1521.06/s  (0.667s, 1536.06/s)  LR: 3.667e-04  Data: 0.013 (0.017)
Train: 176 [ 550/1251 ( 44%)]  Loss: 3.198 (3.38)  Time: 0.665s, 1540.55/s  (0.667s, 1536.17/s)  LR: 3.665e-04  Data: 0.013 (0.017)
Train: 176 [ 600/1251 ( 48%)]  Loss: 3.447 (3.38)  Time: 0.670s, 1528.29/s  (0.667s, 1535.68/s)  LR: 3.663e-04  Data: 0.013 (0.016)
Train: 176 [ 650/1251 ( 52%)]  Loss: 3.234 (3.37)  Time: 0.670s, 1527.43/s  (0.667s, 1535.41/s)  LR: 3.661e-04  Data: 0.014 (0.016)
Train: 176 [ 700/1251 ( 56%)]  Loss: 3.223 (3.36)  Time: 0.676s, 1513.69/s  (0.667s, 1535.44/s)  LR: 3.659e-04  Data: 0.013 (0.016)
Train: 176 [ 750/1251 ( 60%)]  Loss: 3.377 (3.36)  Time: 0.670s, 1528.49/s  (0.667s, 1534.99/s)  LR: 3.657e-04  Data: 0.015 (0.016)
Train: 176 [ 800/1251 ( 64%)]  Loss: 3.331 (3.36)  Time: 0.660s, 1550.42/s  (0.667s, 1534.91/s)  LR: 3.655e-04  Data: 0.013 (0.016)
Train: 176 [ 850/1251 ( 68%)]  Loss: 3.148 (3.35)  Time: 0.667s, 1535.08/s  (0.667s, 1534.75/s)  LR: 3.653e-04  Data: 0.013 (0.016)
Train: 176 [ 900/1251 ( 72%)]  Loss: 3.091 (3.34)  Time: 0.657s, 1557.55/s  (0.667s, 1534.74/s)  LR: 3.651e-04  Data: 0.016 (0.016)
Train: 176 [ 950/1251 ( 76%)]  Loss: 3.390 (3.34)  Time: 0.669s, 1531.57/s  (0.667s, 1534.74/s)  LR: 3.649e-04  Data: 0.014 (0.015)
Train: 176 [1000/1251 ( 80%)]  Loss: 3.307 (3.34)  Time: 0.664s, 1542.92/s  (0.667s, 1534.78/s)  LR: 3.647e-04  Data: 0.013 (0.015)
Train: 176 [1050/1251 ( 84%)]  Loss: 3.534 (3.35)  Time: 0.668s, 1531.79/s  (0.667s, 1534.97/s)  LR: 3.645e-04  Data: 0.013 (0.015)
Train: 176 [1100/1251 ( 88%)]  Loss: 3.450 (3.35)  Time: 0.659s, 1552.72/s  (0.667s, 1535.15/s)  LR: 3.643e-04  Data: 0.013 (0.015)
Train: 176 [1150/1251 ( 92%)]  Loss: 3.168 (3.34)  Time: 0.658s, 1556.95/s  (0.667s, 1535.46/s)  LR: 3.641e-04  Data: 0.014 (0.015)
Train: 176 [1200/1251 ( 96%)]  Loss: 3.149 (3.33)  Time: 0.663s, 1545.58/s  (0.667s, 1535.72/s)  LR: 3.639e-04  Data: 0.012 (0.015)
Train: 176 [1250/1251 (100%)]  Loss: 3.335 (3.33)  Time: 0.641s, 1598.19/s  (0.667s, 1535.94/s)  LR: 3.637e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.748 (2.748)  Loss:  0.4553 (0.4553)  Acc@1: 90.3320 (90.3320)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.162 (0.328)  Loss:  0.5464 (0.9293)  Acc@1: 85.8491 (77.7000)  Acc@5: 97.7594 (94.1760)
Train: 177 [   0/1251 (  0%)]  Loss: 3.099 (3.10)  Time: 3.473s,  294.86/s  (3.473s,  294.86/s)  LR: 3.637e-04  Data: 1.809 (1.809)
Train: 177 [  50/1251 (  4%)]  Loss: 3.380 (3.24)  Time: 0.650s, 1575.00/s  (0.685s, 1494.02/s)  LR: 3.635e-04  Data: 0.013 (0.049)
Train: 177 [ 100/1251 (  8%)]  Loss: 3.413 (3.30)  Time: 0.661s, 1548.37/s  (0.670s, 1528.23/s)  LR: 3.633e-04  Data: 0.013 (0.032)
Train: 177 [ 150/1251 ( 12%)]  Loss: 3.777 (3.42)  Time: 0.664s, 1542.39/s  (0.667s, 1535.22/s)  LR: 3.631e-04  Data: 0.013 (0.026)
Train: 177 [ 200/1251 ( 16%)]  Loss: 3.190 (3.37)  Time: 0.669s, 1530.52/s  (0.666s, 1537.60/s)  LR: 3.629e-04  Data: 0.013 (0.023)
Train: 177 [ 250/1251 ( 20%)]  Loss: 3.620 (3.41)  Time: 0.672s, 1522.94/s  (0.665s, 1538.92/s)  LR: 3.627e-04  Data: 0.014 (0.021)
Train: 177 [ 300/1251 ( 24%)]  Loss: 3.299 (3.40)  Time: 0.673s, 1521.20/s  (0.665s, 1539.43/s)  LR: 3.625e-04  Data: 0.013 (0.020)
Train: 177 [ 350/1251 ( 28%)]  Loss: 3.219 (3.37)  Time: 0.673s, 1521.17/s  (0.666s, 1538.29/s)  LR: 3.623e-04  Data: 0.013 (0.019)
Train: 177 [ 400/1251 ( 32%)]  Loss: 3.284 (3.36)  Time: 0.664s, 1542.45/s  (0.666s, 1537.33/s)  LR: 3.621e-04  Data: 0.013 (0.018)
Train: 177 [ 450/1251 ( 36%)]  Loss: 3.336 (3.36)  Time: 0.669s, 1531.74/s  (0.666s, 1536.83/s)  LR: 3.619e-04  Data: 0.013 (0.018)
Train: 177 [ 500/1251 ( 40%)]  Loss: 3.581 (3.38)  Time: 0.673s, 1521.72/s  (0.667s, 1536.09/s)  LR: 3.617e-04  Data: 0.014 (0.017)
Train: 177 [ 550/1251 ( 44%)]  Loss: 3.079 (3.36)  Time: 0.655s, 1564.11/s  (0.667s, 1535.84/s)  LR: 3.615e-04  Data: 0.013 (0.017)
Train: 177 [ 600/1251 ( 48%)]  Loss: 3.196 (3.34)  Time: 0.658s, 1555.45/s  (0.667s, 1535.87/s)  LR: 3.613e-04  Data: 0.012 (0.017)
Train: 177 [ 650/1251 ( 52%)]  Loss: 3.395 (3.35)  Time: 0.665s, 1540.17/s  (0.667s, 1535.96/s)  LR: 3.611e-04  Data: 0.015 (0.016)
Train: 177 [ 700/1251 ( 56%)]  Loss: 3.313 (3.35)  Time: 0.665s, 1539.52/s  (0.667s, 1536.23/s)  LR: 3.609e-04  Data: 0.013 (0.016)
Train: 177 [ 750/1251 ( 60%)]  Loss: 3.372 (3.35)  Time: 0.671s, 1526.51/s  (0.667s, 1536.31/s)  LR: 3.607e-04  Data: 0.012 (0.016)
Train: 177 [ 800/1251 ( 64%)]  Loss: 3.210 (3.34)  Time: 0.665s, 1539.89/s  (0.666s, 1536.40/s)  LR: 3.605e-04  Data: 0.013 (0.016)
Train: 177 [ 850/1251 ( 68%)]  Loss: 3.368 (3.34)  Time: 0.661s, 1549.55/s  (0.667s, 1536.15/s)  LR: 3.603e-04  Data: 0.015 (0.016)
Train: 177 [ 900/1251 ( 72%)]  Loss: 3.468 (3.35)  Time: 0.671s, 1525.96/s  (0.667s, 1535.93/s)  LR: 3.601e-04  Data: 0.013 (0.016)
Train: 177 [ 950/1251 ( 76%)]  Loss: 3.201 (3.34)  Time: 0.670s, 1527.28/s  (0.667s, 1535.83/s)  LR: 3.599e-04  Data: 0.013 (0.015)
Train: 177 [1000/1251 ( 80%)]  Loss: 3.327 (3.34)  Time: 0.665s, 1539.03/s  (0.667s, 1535.66/s)  LR: 3.597e-04  Data: 0.017 (0.015)
Train: 177 [1050/1251 ( 84%)]  Loss: 3.406 (3.34)  Time: 0.664s, 1541.38/s  (0.667s, 1535.81/s)  LR: 3.595e-04  Data: 0.013 (0.015)
Train: 177 [1100/1251 ( 88%)]  Loss: 3.140 (3.33)  Time: 0.665s, 1539.13/s  (0.667s, 1535.97/s)  LR: 3.593e-04  Data: 0.014 (0.015)
Train: 177 [1150/1251 ( 92%)]  Loss: 3.171 (3.33)  Time: 0.665s, 1540.75/s  (0.666s, 1536.45/s)  LR: 3.591e-04  Data: 0.019 (0.015)
Train: 177 [1200/1251 ( 96%)]  Loss: 3.210 (3.32)  Time: 0.658s, 1555.05/s  (0.666s, 1536.90/s)  LR: 3.589e-04  Data: 0.014 (0.015)
Train: 177 [1250/1251 (100%)]  Loss: 3.121 (3.31)  Time: 0.651s, 1572.14/s  (0.666s, 1537.55/s)  LR: 3.587e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.817 (2.817)  Loss:  0.4438 (0.4438)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.162 (0.329)  Loss:  0.5640 (0.9392)  Acc@1: 86.2028 (77.7520)  Acc@5: 97.6415 (94.1820)
Train: 178 [   0/1251 (  0%)]  Loss: 3.554 (3.55)  Time: 3.851s,  265.88/s  (3.851s,  265.88/s)  LR: 3.587e-04  Data: 1.642 (1.642)
Train: 178 [  50/1251 (  4%)]  Loss: 3.626 (3.59)  Time: 0.645s, 1588.15/s  (0.690s, 1484.14/s)  LR: 3.585e-04  Data: 0.015 (0.046)
Train: 178 [ 100/1251 (  8%)]  Loss: 3.153 (3.44)  Time: 0.652s, 1570.67/s  (0.673s, 1522.26/s)  LR: 3.583e-04  Data: 0.016 (0.030)
Train: 178 [ 150/1251 ( 12%)]  Loss: 3.201 (3.38)  Time: 0.659s, 1553.27/s  (0.669s, 1530.90/s)  LR: 3.581e-04  Data: 0.014 (0.025)
Train: 178 [ 200/1251 ( 16%)]  Loss: 3.452 (3.40)  Time: 0.661s, 1550.19/s  (0.667s, 1534.32/s)  LR: 3.579e-04  Data: 0.013 (0.022)
Train: 178 [ 250/1251 ( 20%)]  Loss: 3.376 (3.39)  Time: 0.676s, 1515.66/s  (0.667s, 1535.58/s)  LR: 3.577e-04  Data: 0.015 (0.020)
Train: 178 [ 300/1251 ( 24%)]  Loss: 3.278 (3.38)  Time: 0.664s, 1542.79/s  (0.667s, 1535.71/s)  LR: 3.575e-04  Data: 0.013 (0.019)
Train: 178 [ 350/1251 ( 28%)]  Loss: 3.552 (3.40)  Time: 0.672s, 1523.01/s  (0.667s, 1535.50/s)  LR: 3.573e-04  Data: 0.014 (0.018)
Train: 178 [ 400/1251 ( 32%)]  Loss: 3.459 (3.41)  Time: 0.661s, 1548.21/s  (0.667s, 1535.31/s)  LR: 3.571e-04  Data: 0.013 (0.018)
Train: 178 [ 450/1251 ( 36%)]  Loss: 3.531 (3.42)  Time: 0.671s, 1525.20/s  (0.667s, 1535.08/s)  LR: 3.569e-04  Data: 0.015 (0.017)
Train: 178 [ 500/1251 ( 40%)]  Loss: 3.232 (3.40)  Time: 0.660s, 1551.91/s  (0.667s, 1535.05/s)  LR: 3.567e-04  Data: 0.012 (0.017)
Train: 178 [ 550/1251 ( 44%)]  Loss: 3.645 (3.42)  Time: 0.664s, 1542.55/s  (0.667s, 1535.17/s)  LR: 3.565e-04  Data: 0.014 (0.017)
Train: 178 [ 600/1251 ( 48%)]  Loss: 3.303 (3.41)  Time: 0.674s, 1519.36/s  (0.667s, 1535.01/s)  LR: 3.563e-04  Data: 0.013 (0.016)
Train: 178 [ 650/1251 ( 52%)]  Loss: 3.408 (3.41)  Time: 0.663s, 1543.53/s  (0.667s, 1535.22/s)  LR: 3.561e-04  Data: 0.014 (0.016)
Train: 178 [ 700/1251 ( 56%)]  Loss: 3.523 (3.42)  Time: 0.676s, 1515.32/s  (0.667s, 1535.05/s)  LR: 3.559e-04  Data: 0.013 (0.016)
Train: 178 [ 750/1251 ( 60%)]  Loss: 3.192 (3.41)  Time: 0.659s, 1553.00/s  (0.667s, 1534.91/s)  LR: 3.557e-04  Data: 0.013 (0.016)
Train: 178 [ 800/1251 ( 64%)]  Loss: 3.344 (3.40)  Time: 0.676s, 1514.30/s  (0.667s, 1534.71/s)  LR: 3.555e-04  Data: 0.013 (0.016)
Train: 178 [ 850/1251 ( 68%)]  Loss: 3.616 (3.41)  Time: 0.666s, 1538.13/s  (0.667s, 1534.69/s)  LR: 3.553e-04  Data: 0.015 (0.016)
Train: 178 [ 900/1251 ( 72%)]  Loss: 3.465 (3.42)  Time: 0.660s, 1552.33/s  (0.667s, 1534.85/s)  LR: 3.551e-04  Data: 0.016 (0.016)
Train: 178 [ 950/1251 ( 76%)]  Loss: 3.545 (3.42)  Time: 0.658s, 1555.70/s  (0.667s, 1535.03/s)  LR: 3.549e-04  Data: 0.013 (0.015)
Train: 178 [1000/1251 ( 80%)]  Loss: 3.286 (3.42)  Time: 0.664s, 1541.03/s  (0.667s, 1535.15/s)  LR: 3.547e-04  Data: 0.013 (0.015)
Train: 178 [1050/1251 ( 84%)]  Loss: 3.439 (3.42)  Time: 0.664s, 1541.72/s  (0.667s, 1535.41/s)  LR: 3.545e-04  Data: 0.014 (0.015)
Train: 178 [1100/1251 ( 88%)]  Loss: 3.583 (3.42)  Time: 0.660s, 1550.37/s  (0.667s, 1535.67/s)  LR: 3.543e-04  Data: 0.013 (0.015)
Train: 178 [1150/1251 ( 92%)]  Loss: 3.527 (3.43)  Time: 0.674s, 1518.87/s  (0.667s, 1535.82/s)  LR: 3.541e-04  Data: 0.016 (0.015)
Train: 178 [1200/1251 ( 96%)]  Loss: 3.461 (3.43)  Time: 0.661s, 1548.71/s  (0.667s, 1535.93/s)  LR: 3.539e-04  Data: 0.014 (0.015)
Train: 178 [1250/1251 (100%)]  Loss: 3.465 (3.43)  Time: 0.650s, 1576.57/s  (0.667s, 1536.08/s)  LR: 3.537e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.983 (2.983)  Loss:  0.4390 (0.4390)  Acc@1: 90.9180 (90.9180)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.161 (0.324)  Loss:  0.5571 (0.9471)  Acc@1: 86.4387 (77.5760)  Acc@5: 97.5236 (94.2840)
Train: 179 [   0/1251 (  0%)]  Loss: 3.432 (3.43)  Time: 3.491s,  293.36/s  (3.491s,  293.36/s)  LR: 3.537e-04  Data: 1.936 (1.936)
Train: 179 [  50/1251 (  4%)]  Loss: 3.523 (3.48)  Time: 0.645s, 1587.86/s  (0.683s, 1499.48/s)  LR: 3.535e-04  Data: 0.015 (0.051)
Train: 179 [ 100/1251 (  8%)]  Loss: 3.320 (3.42)  Time: 0.657s, 1559.29/s  (0.667s, 1534.83/s)  LR: 3.533e-04  Data: 0.013 (0.033)
Train: 179 [ 150/1251 ( 12%)]  Loss: 3.547 (3.46)  Time: 0.656s, 1561.22/s  (0.664s, 1541.46/s)  LR: 3.531e-04  Data: 0.012 (0.026)
Train: 179 [ 200/1251 ( 16%)]  Loss: 3.510 (3.47)  Time: 0.661s, 1548.46/s  (0.664s, 1542.66/s)  LR: 3.529e-04  Data: 0.014 (0.023)
Train: 179 [ 250/1251 ( 20%)]  Loss: 3.178 (3.42)  Time: 0.668s, 1533.30/s  (0.664s, 1542.54/s)  LR: 3.527e-04  Data: 0.012 (0.021)
Train: 179 [ 300/1251 ( 24%)]  Loss: 3.332 (3.41)  Time: 0.670s, 1528.60/s  (0.664s, 1543.23/s)  LR: 3.525e-04  Data: 0.013 (0.020)
Train: 179 [ 350/1251 ( 28%)]  Loss: 3.616 (3.43)  Time: 0.667s, 1535.13/s  (0.664s, 1543.18/s)  LR: 3.523e-04  Data: 0.013 (0.019)
Train: 179 [ 400/1251 ( 32%)]  Loss: 3.139 (3.40)  Time: 0.650s, 1575.74/s  (0.663s, 1543.59/s)  LR: 3.521e-04  Data: 0.014 (0.018)
Train: 179 [ 450/1251 ( 36%)]  Loss: 3.420 (3.40)  Time: 0.662s, 1547.56/s  (0.663s, 1543.66/s)  LR: 3.519e-04  Data: 0.017 (0.018)
Train: 179 [ 500/1251 ( 40%)]  Loss: 3.215 (3.38)  Time: 0.669s, 1530.70/s  (0.663s, 1543.57/s)  LR: 3.517e-04  Data: 0.013 (0.017)
Train: 179 [ 550/1251 ( 44%)]  Loss: 3.461 (3.39)  Time: 0.662s, 1546.91/s  (0.663s, 1543.51/s)  LR: 3.515e-04  Data: 0.013 (0.017)
Train: 179 [ 600/1251 ( 48%)]  Loss: 3.459 (3.40)  Time: 0.664s, 1542.41/s  (0.663s, 1543.61/s)  LR: 3.513e-04  Data: 0.016 (0.017)
Train: 179 [ 650/1251 ( 52%)]  Loss: 3.441 (3.40)  Time: 0.659s, 1553.04/s  (0.663s, 1543.62/s)  LR: 3.511e-04  Data: 0.013 (0.017)
Train: 179 [ 700/1251 ( 56%)]  Loss: 3.589 (3.41)  Time: 0.665s, 1540.36/s  (0.663s, 1543.34/s)  LR: 3.509e-04  Data: 0.015 (0.016)
Train: 179 [ 750/1251 ( 60%)]  Loss: 3.575 (3.42)  Time: 0.673s, 1520.60/s  (0.664s, 1543.28/s)  LR: 3.507e-04  Data: 0.013 (0.016)
Train: 179 [ 800/1251 ( 64%)]  Loss: 3.359 (3.42)  Time: 0.662s, 1547.41/s  (0.663s, 1543.57/s)  LR: 3.505e-04  Data: 0.013 (0.016)
Train: 179 [ 850/1251 ( 68%)]  Loss: 3.445 (3.42)  Time: 0.666s, 1536.72/s  (0.663s, 1543.50/s)  LR: 3.503e-04  Data: 0.012 (0.016)
Train: 179 [ 900/1251 ( 72%)]  Loss: 3.389 (3.42)  Time: 0.668s, 1532.14/s  (0.663s, 1543.34/s)  LR: 3.502e-04  Data: 0.013 (0.016)
Train: 179 [ 950/1251 ( 76%)]  Loss: 3.433 (3.42)  Time: 0.658s, 1556.30/s  (0.664s, 1543.26/s)  LR: 3.500e-04  Data: 0.013 (0.016)
Train: 179 [1000/1251 ( 80%)]  Loss: 3.361 (3.42)  Time: 0.657s, 1559.62/s  (0.663s, 1543.36/s)  LR: 3.498e-04  Data: 0.012 (0.016)
Train: 179 [1050/1251 ( 84%)]  Loss: 3.424 (3.42)  Time: 0.659s, 1553.73/s  (0.664s, 1542.96/s)  LR: 3.496e-04  Data: 0.013 (0.015)
Train: 179 [1100/1251 ( 88%)]  Loss: 3.340 (3.41)  Time: 0.667s, 1534.51/s  (0.664s, 1542.51/s)  LR: 3.494e-04  Data: 0.013 (0.015)
Train: 179 [1150/1251 ( 92%)]  Loss: 3.255 (3.41)  Time: 0.669s, 1530.46/s  (0.664s, 1542.38/s)  LR: 3.492e-04  Data: 0.017 (0.015)
Train: 179 [1200/1251 ( 96%)]  Loss: 3.088 (3.39)  Time: 0.668s, 1533.88/s  (0.664s, 1542.31/s)  LR: 3.490e-04  Data: 0.013 (0.015)
Train: 179 [1250/1251 (100%)]  Loss: 3.379 (3.39)  Time: 0.660s, 1550.68/s  (0.664s, 1542.18/s)  LR: 3.488e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.896 (2.896)  Loss:  0.4624 (0.4624)  Acc@1: 91.2109 (91.2109)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.160 (0.334)  Loss:  0.5791 (0.9373)  Acc@1: 86.4387 (77.8380)  Acc@5: 97.6415 (94.2640)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-171.pth.tar', 77.93599997558594)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-179.pth.tar', 77.83800005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-174.pth.tar', 77.78399992675782)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-173.pth.tar', 77.77599995117187)

Train: 180 [   0/1251 (  0%)]  Loss: 3.199 (3.20)  Time: 3.494s,  293.10/s  (3.494s,  293.10/s)  LR: 3.488e-04  Data: 1.768 (1.768)
Train: 180 [  50/1251 (  4%)]  Loss: 3.536 (3.37)  Time: 0.644s, 1591.27/s  (0.682s, 1501.71/s)  LR: 3.486e-04  Data: 0.018 (0.048)
Train: 180 [ 100/1251 (  8%)]  Loss: 3.326 (3.35)  Time: 0.659s, 1553.62/s  (0.667s, 1535.25/s)  LR: 3.484e-04  Data: 0.014 (0.031)
Train: 180 [ 150/1251 ( 12%)]  Loss: 3.397 (3.36)  Time: 0.663s, 1544.25/s  (0.664s, 1541.28/s)  LR: 3.482e-04  Data: 0.012 (0.025)
Train: 180 [ 200/1251 ( 16%)]  Loss: 3.492 (3.39)  Time: 0.659s, 1553.93/s  (0.664s, 1542.82/s)  LR: 3.480e-04  Data: 0.013 (0.022)
Train: 180 [ 250/1251 ( 20%)]  Loss: 3.558 (3.42)  Time: 0.650s, 1575.83/s  (0.663s, 1544.51/s)  LR: 3.478e-04  Data: 0.014 (0.021)
Train: 180 [ 300/1251 ( 24%)]  Loss: 3.182 (3.38)  Time: 0.655s, 1563.14/s  (0.663s, 1545.47/s)  LR: 3.476e-04  Data: 0.014 (0.020)
Train: 180 [ 350/1251 ( 28%)]  Loss: 3.393 (3.39)  Time: 0.663s, 1544.16/s  (0.663s, 1545.26/s)  LR: 3.474e-04  Data: 0.014 (0.019)
Train: 180 [ 400/1251 ( 32%)]  Loss: 3.566 (3.41)  Time: 0.664s, 1541.96/s  (0.663s, 1544.44/s)  LR: 3.472e-04  Data: 0.014 (0.018)
Train: 180 [ 450/1251 ( 36%)]  Loss: 3.101 (3.37)  Time: 0.660s, 1551.74/s  (0.663s, 1543.51/s)  LR: 3.470e-04  Data: 0.018 (0.018)
Train: 180 [ 500/1251 ( 40%)]  Loss: 3.180 (3.36)  Time: 0.670s, 1528.70/s  (0.663s, 1543.43/s)  LR: 3.468e-04  Data: 0.014 (0.017)
Train: 180 [ 550/1251 ( 44%)]  Loss: 3.353 (3.36)  Time: 0.664s, 1542.53/s  (0.663s, 1543.41/s)  LR: 3.466e-04  Data: 0.013 (0.017)
Train: 180 [ 600/1251 ( 48%)]  Loss: 3.453 (3.36)  Time: 0.662s, 1546.09/s  (0.663s, 1543.54/s)  LR: 3.464e-04  Data: 0.014 (0.017)
Train: 180 [ 650/1251 ( 52%)]  Loss: 3.590 (3.38)  Time: 0.659s, 1554.76/s  (0.663s, 1543.72/s)  LR: 3.462e-04  Data: 0.014 (0.016)
Train: 180 [ 700/1251 ( 56%)]  Loss: 3.406 (3.38)  Time: 0.657s, 1559.26/s  (0.663s, 1544.12/s)  LR: 3.460e-04  Data: 0.013 (0.016)
Train: 180 [ 750/1251 ( 60%)]  Loss: 3.347 (3.38)  Time: 0.661s, 1548.69/s  (0.663s, 1544.73/s)  LR: 3.458e-04  Data: 0.014 (0.016)
Train: 180 [ 800/1251 ( 64%)]  Loss: 3.193 (3.37)  Time: 0.662s, 1546.93/s  (0.663s, 1544.92/s)  LR: 3.456e-04  Data: 0.013 (0.016)
Train: 180 [ 850/1251 ( 68%)]  Loss: 3.316 (3.37)  Time: 0.671s, 1526.10/s  (0.663s, 1544.97/s)  LR: 3.454e-04  Data: 0.012 (0.016)
Train: 180 [ 900/1251 ( 72%)]  Loss: 3.210 (3.36)  Time: 0.661s, 1549.48/s  (0.663s, 1544.95/s)  LR: 3.452e-04  Data: 0.014 (0.016)
Train: 180 [ 950/1251 ( 76%)]  Loss: 3.679 (3.37)  Time: 0.664s, 1542.83/s  (0.663s, 1545.05/s)  LR: 3.450e-04  Data: 0.013 (0.015)
Train: 180 [1000/1251 ( 80%)]  Loss: 3.521 (3.38)  Time: 0.665s, 1540.53/s  (0.663s, 1544.73/s)  LR: 3.448e-04  Data: 0.013 (0.015)
Train: 180 [1050/1251 ( 84%)]  Loss: 3.377 (3.38)  Time: 0.667s, 1535.57/s  (0.663s, 1544.62/s)  LR: 3.446e-04  Data: 0.013 (0.015)
Train: 180 [1100/1251 ( 88%)]  Loss: 3.133 (3.37)  Time: 0.670s, 1529.08/s  (0.663s, 1544.44/s)  LR: 3.444e-04  Data: 0.014 (0.015)
Train: 180 [1150/1251 ( 92%)]  Loss: 3.006 (3.35)  Time: 0.662s, 1546.63/s  (0.663s, 1544.23/s)  LR: 3.442e-04  Data: 0.014 (0.015)
Train: 180 [1200/1251 ( 96%)]  Loss: 3.212 (3.35)  Time: 0.667s, 1534.99/s  (0.663s, 1543.85/s)  LR: 3.440e-04  Data: 0.013 (0.015)
Train: 180 [1250/1251 (100%)]  Loss: 3.613 (3.36)  Time: 0.663s, 1543.94/s  (0.663s, 1543.77/s)  LR: 3.438e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.811 (2.811)  Loss:  0.4485 (0.4485)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.169 (0.326)  Loss:  0.5083 (0.9313)  Acc@1: 87.6179 (78.0400)  Acc@5: 98.3491 (94.2660)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-180.pth.tar', 78.04000005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-171.pth.tar', 77.93599997558594)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-179.pth.tar', 77.83800005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-174.pth.tar', 77.78399992675782)

Train: 181 [   0/1251 (  0%)]  Loss: 3.419 (3.42)  Time: 3.800s,  269.44/s  (3.800s,  269.44/s)  LR: 3.438e-04  Data: 1.702 (1.702)
Train: 181 [  50/1251 (  4%)]  Loss: 3.058 (3.24)  Time: 0.654s, 1566.33/s  (0.692s, 1479.03/s)  LR: 3.436e-04  Data: 0.013 (0.047)
Train: 181 [ 100/1251 (  8%)]  Loss: 3.388 (3.29)  Time: 0.667s, 1535.10/s  (0.675s, 1516.58/s)  LR: 3.434e-04  Data: 0.013 (0.030)
Train: 181 [ 150/1251 ( 12%)]  Loss: 3.443 (3.33)  Time: 0.662s, 1546.89/s  (0.671s, 1527.08/s)  LR: 3.432e-04  Data: 0.013 (0.025)
Train: 181 [ 200/1251 ( 16%)]  Loss: 3.440 (3.35)  Time: 0.661s, 1549.35/s  (0.669s, 1530.88/s)  LR: 3.430e-04  Data: 0.013 (0.022)
Train: 181 [ 250/1251 ( 20%)]  Loss: 3.217 (3.33)  Time: 0.661s, 1548.13/s  (0.668s, 1533.18/s)  LR: 3.428e-04  Data: 0.014 (0.020)
Train: 181 [ 300/1251 ( 24%)]  Loss: 3.132 (3.30)  Time: 0.661s, 1548.87/s  (0.667s, 1535.02/s)  LR: 3.426e-04  Data: 0.015 (0.019)
Train: 181 [ 350/1251 ( 28%)]  Loss: 2.989 (3.26)  Time: 0.657s, 1558.87/s  (0.666s, 1536.66/s)  LR: 3.424e-04  Data: 0.013 (0.018)
Train: 181 [ 400/1251 ( 32%)]  Loss: 3.239 (3.26)  Time: 0.659s, 1552.72/s  (0.666s, 1537.45/s)  LR: 3.422e-04  Data: 0.013 (0.018)
Train: 181 [ 450/1251 ( 36%)]  Loss: 3.012 (3.23)  Time: 0.665s, 1540.98/s  (0.666s, 1537.93/s)  LR: 3.420e-04  Data: 0.014 (0.017)
Train: 181 [ 500/1251 ( 40%)]  Loss: 3.420 (3.25)  Time: 0.670s, 1528.82/s  (0.666s, 1538.44/s)  LR: 3.418e-04  Data: 0.012 (0.017)
Train: 181 [ 550/1251 ( 44%)]  Loss: 3.171 (3.24)  Time: 0.657s, 1559.53/s  (0.665s, 1539.04/s)  LR: 3.416e-04  Data: 0.014 (0.017)
Train: 181 [ 600/1251 ( 48%)]  Loss: 3.480 (3.26)  Time: 0.668s, 1533.93/s  (0.665s, 1539.23/s)  LR: 3.414e-04  Data: 0.013 (0.016)
Train: 181 [ 650/1251 ( 52%)]  Loss: 3.308 (3.27)  Time: 0.666s, 1536.45/s  (0.665s, 1539.69/s)  LR: 3.413e-04  Data: 0.013 (0.016)
Train: 181 [ 700/1251 ( 56%)]  Loss: 3.351 (3.27)  Time: 0.658s, 1555.86/s  (0.665s, 1539.67/s)  LR: 3.411e-04  Data: 0.013 (0.016)
Train: 181 [ 750/1251 ( 60%)]  Loss: 3.086 (3.26)  Time: 0.664s, 1541.18/s  (0.665s, 1539.43/s)  LR: 3.409e-04  Data: 0.015 (0.016)
Train: 181 [ 800/1251 ( 64%)]  Loss: 3.416 (3.27)  Time: 0.663s, 1544.20/s  (0.665s, 1539.59/s)  LR: 3.407e-04  Data: 0.013 (0.016)
Train: 181 [ 850/1251 ( 68%)]  Loss: 3.385 (3.28)  Time: 0.664s, 1542.33/s  (0.665s, 1539.82/s)  LR: 3.405e-04  Data: 0.013 (0.016)
Train: 181 [ 900/1251 ( 72%)]  Loss: 3.290 (3.28)  Time: 0.673s, 1521.84/s  (0.665s, 1539.88/s)  LR: 3.403e-04  Data: 0.013 (0.016)
Train: 181 [ 950/1251 ( 76%)]  Loss: 3.387 (3.28)  Time: 0.663s, 1545.50/s  (0.665s, 1540.03/s)  LR: 3.401e-04  Data: 0.013 (0.015)
Train: 181 [1000/1251 ( 80%)]  Loss: 3.222 (3.28)  Time: 0.662s, 1546.49/s  (0.665s, 1540.27/s)  LR: 3.399e-04  Data: 0.013 (0.015)
Train: 181 [1050/1251 ( 84%)]  Loss: 3.013 (3.27)  Time: 0.668s, 1533.77/s  (0.665s, 1540.30/s)  LR: 3.397e-04  Data: 0.019 (0.015)
Train: 181 [1100/1251 ( 88%)]  Loss: 3.071 (3.26)  Time: 0.655s, 1562.27/s  (0.665s, 1540.39/s)  LR: 3.395e-04  Data: 0.013 (0.015)
Train: 181 [1150/1251 ( 92%)]  Loss: 3.248 (3.26)  Time: 0.661s, 1549.01/s  (0.665s, 1540.57/s)  LR: 3.393e-04  Data: 0.016 (0.015)
Train: 181 [1200/1251 ( 96%)]  Loss: 3.454 (3.27)  Time: 0.667s, 1535.30/s  (0.665s, 1540.55/s)  LR: 3.391e-04  Data: 0.013 (0.015)
Train: 181 [1250/1251 (100%)]  Loss: 3.352 (3.27)  Time: 0.656s, 1560.46/s  (0.665s, 1540.66/s)  LR: 3.389e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.074 (3.074)  Loss:  0.4688 (0.4688)  Acc@1: 91.0156 (91.0156)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.161 (0.325)  Loss:  0.5347 (0.9303)  Acc@1: 87.0283 (77.9500)  Acc@5: 98.1132 (94.3460)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-180.pth.tar', 78.04000005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-181.pth.tar', 77.95000005371094)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-171.pth.tar', 77.93599997558594)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-179.pth.tar', 77.83800005615234)

Train: 182 [   0/1251 (  0%)]  Loss: 3.113 (3.11)  Time: 3.552s,  288.27/s  (3.552s,  288.27/s)  LR: 3.389e-04  Data: 1.663 (1.663)
Train: 182 [  50/1251 (  4%)]  Loss: 3.163 (3.14)  Time: 0.653s, 1568.98/s  (0.683s, 1499.17/s)  LR: 3.387e-04  Data: 0.014 (0.046)
Train: 182 [ 100/1251 (  8%)]  Loss: 3.498 (3.26)  Time: 0.649s, 1577.89/s  (0.669s, 1530.16/s)  LR: 3.385e-04  Data: 0.013 (0.030)
Train: 182 [ 150/1251 ( 12%)]  Loss: 3.369 (3.29)  Time: 0.665s, 1540.26/s  (0.666s, 1536.89/s)  LR: 3.383e-04  Data: 0.013 (0.024)
Train: 182 [ 200/1251 ( 16%)]  Loss: 3.557 (3.34)  Time: 0.661s, 1548.32/s  (0.666s, 1537.44/s)  LR: 3.381e-04  Data: 0.013 (0.022)
Train: 182 [ 250/1251 ( 20%)]  Loss: 3.552 (3.38)  Time: 0.664s, 1542.43/s  (0.666s, 1537.30/s)  LR: 3.379e-04  Data: 0.013 (0.020)
Train: 182 [ 300/1251 ( 24%)]  Loss: 3.271 (3.36)  Time: 0.665s, 1540.51/s  (0.666s, 1537.30/s)  LR: 3.377e-04  Data: 0.013 (0.019)
Train: 182 [ 350/1251 ( 28%)]  Loss: 3.326 (3.36)  Time: 0.666s, 1536.55/s  (0.666s, 1536.41/s)  LR: 3.375e-04  Data: 0.015 (0.018)
Train: 182 [ 400/1251 ( 32%)]  Loss: 3.062 (3.32)  Time: 0.667s, 1535.49/s  (0.667s, 1536.34/s)  LR: 3.373e-04  Data: 0.017 (0.018)
Train: 182 [ 450/1251 ( 36%)]  Loss: 3.266 (3.32)  Time: 0.667s, 1535.57/s  (0.666s, 1536.44/s)  LR: 3.371e-04  Data: 0.013 (0.017)
Train: 182 [ 500/1251 ( 40%)]  Loss: 3.293 (3.32)  Time: 0.673s, 1521.52/s  (0.666s, 1536.52/s)  LR: 3.369e-04  Data: 0.013 (0.017)
Train: 182 [ 550/1251 ( 44%)]  Loss: 3.624 (3.34)  Time: 0.659s, 1554.46/s  (0.666s, 1536.96/s)  LR: 3.367e-04  Data: 0.014 (0.017)
Train: 182 [ 600/1251 ( 48%)]  Loss: 3.100 (3.32)  Time: 0.667s, 1534.24/s  (0.666s, 1537.01/s)  LR: 3.365e-04  Data: 0.012 (0.016)
Train: 182 [ 650/1251 ( 52%)]  Loss: 3.380 (3.33)  Time: 0.661s, 1549.60/s  (0.666s, 1537.48/s)  LR: 3.363e-04  Data: 0.013 (0.016)
Train: 182 [ 700/1251 ( 56%)]  Loss: 3.325 (3.33)  Time: 0.674s, 1519.47/s  (0.666s, 1537.28/s)  LR: 3.361e-04  Data: 0.012 (0.016)
Train: 182 [ 750/1251 ( 60%)]  Loss: 3.287 (3.32)  Time: 0.664s, 1542.92/s  (0.666s, 1537.36/s)  LR: 3.359e-04  Data: 0.013 (0.016)
Train: 182 [ 800/1251 ( 64%)]  Loss: 3.369 (3.33)  Time: 0.671s, 1525.74/s  (0.666s, 1537.17/s)  LR: 3.357e-04  Data: 0.013 (0.016)
Train: 182 [ 850/1251 ( 68%)]  Loss: 3.158 (3.32)  Time: 0.667s, 1534.92/s  (0.666s, 1537.31/s)  LR: 3.355e-04  Data: 0.016 (0.016)
Train: 182 [ 900/1251 ( 72%)]  Loss: 3.502 (3.33)  Time: 0.671s, 1526.97/s  (0.666s, 1537.41/s)  LR: 3.354e-04  Data: 0.014 (0.015)
Train: 182 [ 950/1251 ( 76%)]  Loss: 3.525 (3.34)  Time: 0.671s, 1525.55/s  (0.666s, 1537.40/s)  LR: 3.352e-04  Data: 0.013 (0.015)
Train: 182 [1000/1251 ( 80%)]  Loss: 3.379 (3.34)  Time: 0.671s, 1525.71/s  (0.666s, 1537.36/s)  LR: 3.350e-04  Data: 0.014 (0.015)
Train: 182 [1050/1251 ( 84%)]  Loss: 3.675 (3.35)  Time: 0.672s, 1523.07/s  (0.666s, 1537.53/s)  LR: 3.348e-04  Data: 0.013 (0.015)
Train: 182 [1100/1251 ( 88%)]  Loss: 3.328 (3.35)  Time: 0.659s, 1554.84/s  (0.666s, 1537.76/s)  LR: 3.346e-04  Data: 0.016 (0.015)
Train: 182 [1150/1251 ( 92%)]  Loss: 2.807 (3.33)  Time: 0.656s, 1561.47/s  (0.666s, 1537.89/s)  LR: 3.344e-04  Data: 0.014 (0.015)
Train: 182 [1200/1251 ( 96%)]  Loss: 3.527 (3.34)  Time: 0.671s, 1524.98/s  (0.666s, 1537.98/s)  LR: 3.342e-04  Data: 0.014 (0.015)
Train: 182 [1250/1251 (100%)]  Loss: 3.315 (3.34)  Time: 0.649s, 1578.71/s  (0.666s, 1538.18/s)  LR: 3.340e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.942 (2.942)  Loss:  0.4424 (0.4424)  Acc@1: 90.5273 (90.5273)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.161 (0.322)  Loss:  0.5591 (0.9144)  Acc@1: 85.9670 (78.1300)  Acc@5: 97.5236 (94.3340)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-182.pth.tar', 78.12999998046875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-180.pth.tar', 78.04000005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-181.pth.tar', 77.95000005371094)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-171.pth.tar', 77.93599997558594)

Train: 183 [   0/1251 (  0%)]  Loss: 3.362 (3.36)  Time: 4.340s,  235.92/s  (4.340s,  235.92/s)  LR: 3.340e-04  Data: 1.817 (1.817)
Train: 183 [  50/1251 (  4%)]  Loss: 3.403 (3.38)  Time: 0.649s, 1577.54/s  (0.693s, 1477.62/s)  LR: 3.338e-04  Data: 0.013 (0.049)
Train: 183 [ 100/1251 (  8%)]  Loss: 3.286 (3.35)  Time: 0.664s, 1542.35/s  (0.676s, 1515.23/s)  LR: 3.336e-04  Data: 0.014 (0.032)
Train: 183 [ 150/1251 ( 12%)]  Loss: 3.464 (3.38)  Time: 0.669s, 1529.78/s  (0.673s, 1522.51/s)  LR: 3.334e-04  Data: 0.013 (0.026)
Train: 183 [ 200/1251 ( 16%)]  Loss: 3.335 (3.37)  Time: 0.669s, 1529.85/s  (0.671s, 1525.89/s)  LR: 3.332e-04  Data: 0.013 (0.023)
Train: 183 [ 250/1251 ( 20%)]  Loss: 3.239 (3.35)  Time: 0.670s, 1528.13/s  (0.671s, 1527.14/s)  LR: 3.330e-04  Data: 0.013 (0.021)
Train: 183 [ 300/1251 ( 24%)]  Loss: 3.470 (3.37)  Time: 0.671s, 1527.13/s  (0.670s, 1527.88/s)  LR: 3.328e-04  Data: 0.016 (0.020)
Train: 183 [ 350/1251 ( 28%)]  Loss: 3.245 (3.35)  Time: 0.666s, 1537.63/s  (0.670s, 1527.65/s)  LR: 3.326e-04  Data: 0.013 (0.019)
Train: 183 [ 400/1251 ( 32%)]  Loss: 3.285 (3.34)  Time: 0.664s, 1542.64/s  (0.670s, 1528.34/s)  LR: 3.324e-04  Data: 0.015 (0.018)
Train: 183 [ 450/1251 ( 36%)]  Loss: 3.435 (3.35)  Time: 0.669s, 1530.39/s  (0.670s, 1529.13/s)  LR: 3.322e-04  Data: 0.013 (0.018)
Train: 183 [ 500/1251 ( 40%)]  Loss: 3.466 (3.36)  Time: 0.666s, 1536.85/s  (0.669s, 1529.75/s)  LR: 3.320e-04  Data: 0.013 (0.017)
Train: 183 [ 550/1251 ( 44%)]  Loss: 3.488 (3.37)  Time: 0.672s, 1523.75/s  (0.669s, 1529.89/s)  LR: 3.318e-04  Data: 0.013 (0.017)
Train: 183 [ 600/1251 ( 48%)]  Loss: 3.688 (3.40)  Time: 0.653s, 1569.24/s  (0.669s, 1530.40/s)  LR: 3.316e-04  Data: 0.013 (0.017)
Train: 183 [ 650/1251 ( 52%)]  Loss: 3.479 (3.40)  Time: 0.668s, 1532.46/s  (0.669s, 1531.40/s)  LR: 3.314e-04  Data: 0.012 (0.016)
Train: 183 [ 700/1251 ( 56%)]  Loss: 3.651 (3.42)  Time: 0.672s, 1524.22/s  (0.668s, 1532.17/s)  LR: 3.312e-04  Data: 0.013 (0.016)
Train: 183 [ 750/1251 ( 60%)]  Loss: 3.406 (3.42)  Time: 0.669s, 1530.01/s  (0.668s, 1532.87/s)  LR: 3.310e-04  Data: 0.013 (0.016)
Train: 183 [ 800/1251 ( 64%)]  Loss: 3.101 (3.40)  Time: 0.668s, 1532.84/s  (0.668s, 1533.53/s)  LR: 3.308e-04  Data: 0.013 (0.016)
Train: 183 [ 850/1251 ( 68%)]  Loss: 3.072 (3.38)  Time: 0.664s, 1542.08/s  (0.667s, 1534.16/s)  LR: 3.306e-04  Data: 0.013 (0.016)
Train: 183 [ 900/1251 ( 72%)]  Loss: 3.163 (3.37)  Time: 0.664s, 1542.16/s  (0.667s, 1534.70/s)  LR: 3.305e-04  Data: 0.013 (0.016)
Train: 183 [ 950/1251 ( 76%)]  Loss: 2.869 (3.35)  Time: 0.665s, 1539.88/s  (0.667s, 1535.18/s)  LR: 3.303e-04  Data: 0.016 (0.016)
Train: 183 [1000/1251 ( 80%)]  Loss: 3.195 (3.34)  Time: 0.659s, 1553.64/s  (0.667s, 1535.48/s)  LR: 3.301e-04  Data: 0.014 (0.015)
Train: 183 [1050/1251 ( 84%)]  Loss: 3.205 (3.33)  Time: 0.661s, 1548.36/s  (0.667s, 1535.87/s)  LR: 3.299e-04  Data: 0.014 (0.015)
Train: 183 [1100/1251 ( 88%)]  Loss: 3.283 (3.33)  Time: 0.675s, 1517.48/s  (0.667s, 1536.02/s)  LR: 3.297e-04  Data: 0.014 (0.015)
Train: 183 [1150/1251 ( 92%)]  Loss: 3.423 (3.33)  Time: 0.674s, 1520.15/s  (0.667s, 1536.21/s)  LR: 3.295e-04  Data: 0.016 (0.015)
Train: 183 [1200/1251 ( 96%)]  Loss: 3.258 (3.33)  Time: 0.663s, 1543.63/s  (0.667s, 1536.27/s)  LR: 3.293e-04  Data: 0.012 (0.015)
Train: 183 [1250/1251 (100%)]  Loss: 3.531 (3.34)  Time: 0.649s, 1577.66/s  (0.666s, 1536.42/s)  LR: 3.291e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.959 (2.959)  Loss:  0.4563 (0.4563)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.162 (0.331)  Loss:  0.5713 (0.9240)  Acc@1: 86.4387 (78.2080)  Acc@5: 97.0519 (94.4040)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-183.pth.tar', 78.20800005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-182.pth.tar', 78.12999998046875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-180.pth.tar', 78.04000005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-181.pth.tar', 77.95000005371094)

Train: 184 [   0/1251 (  0%)]  Loss: 3.176 (3.18)  Time: 3.455s,  296.38/s  (3.455s,  296.38/s)  LR: 3.291e-04  Data: 1.816 (1.816)
Train: 184 [  50/1251 (  4%)]  Loss: 3.220 (3.20)  Time: 0.650s, 1576.48/s  (0.691s, 1482.62/s)  LR: 3.289e-04  Data: 0.016 (0.049)
Train: 184 [ 100/1251 (  8%)]  Loss: 3.256 (3.22)  Time: 0.662s, 1547.10/s  (0.676s, 1515.47/s)  LR: 3.287e-04  Data: 0.016 (0.032)
Train: 184 [ 150/1251 ( 12%)]  Loss: 3.418 (3.27)  Time: 0.669s, 1531.12/s  (0.672s, 1524.61/s)  LR: 3.285e-04  Data: 0.014 (0.026)
Train: 184 [ 200/1251 ( 16%)]  Loss: 3.258 (3.27)  Time: 0.666s, 1538.47/s  (0.670s, 1528.01/s)  LR: 3.283e-04  Data: 0.013 (0.023)
Train: 184 [ 250/1251 ( 20%)]  Loss: 3.421 (3.29)  Time: 0.668s, 1533.71/s  (0.669s, 1529.80/s)  LR: 3.281e-04  Data: 0.013 (0.021)
Train: 184 [ 300/1251 ( 24%)]  Loss: 3.468 (3.32)  Time: 0.663s, 1543.92/s  (0.669s, 1529.96/s)  LR: 3.279e-04  Data: 0.013 (0.020)
Train: 184 [ 350/1251 ( 28%)]  Loss: 3.387 (3.33)  Time: 0.665s, 1539.28/s  (0.669s, 1530.39/s)  LR: 3.277e-04  Data: 0.012 (0.019)
Train: 184 [ 400/1251 ( 32%)]  Loss: 3.372 (3.33)  Time: 0.667s, 1535.17/s  (0.669s, 1530.89/s)  LR: 3.275e-04  Data: 0.012 (0.018)
Train: 184 [ 450/1251 ( 36%)]  Loss: 3.105 (3.31)  Time: 0.675s, 1517.05/s  (0.669s, 1531.31/s)  LR: 3.273e-04  Data: 0.015 (0.018)
Train: 184 [ 500/1251 ( 40%)]  Loss: 3.295 (3.31)  Time: 0.671s, 1525.84/s  (0.669s, 1531.48/s)  LR: 3.271e-04  Data: 0.013 (0.017)
Train: 184 [ 550/1251 ( 44%)]  Loss: 3.535 (3.33)  Time: 0.673s, 1521.46/s  (0.669s, 1531.71/s)  LR: 3.269e-04  Data: 0.013 (0.017)
Train: 184 [ 600/1251 ( 48%)]  Loss: 3.212 (3.32)  Time: 0.671s, 1526.49/s  (0.668s, 1532.01/s)  LR: 3.267e-04  Data: 0.016 (0.017)
Train: 184 [ 650/1251 ( 52%)]  Loss: 3.727 (3.35)  Time: 0.671s, 1527.02/s  (0.668s, 1532.42/s)  LR: 3.265e-04  Data: 0.013 (0.016)
Train: 184 [ 700/1251 ( 56%)]  Loss: 3.246 (3.34)  Time: 0.666s, 1537.18/s  (0.668s, 1532.71/s)  LR: 3.264e-04  Data: 0.014 (0.016)
Train: 184 [ 750/1251 ( 60%)]  Loss: 3.193 (3.33)  Time: 0.662s, 1547.10/s  (0.668s, 1532.98/s)  LR: 3.262e-04  Data: 0.013 (0.016)
Train: 184 [ 800/1251 ( 64%)]  Loss: 3.668 (3.35)  Time: 0.660s, 1551.84/s  (0.668s, 1533.41/s)  LR: 3.260e-04  Data: 0.016 (0.016)
Train: 184 [ 850/1251 ( 68%)]  Loss: 3.331 (3.35)  Time: 0.670s, 1529.12/s  (0.668s, 1533.88/s)  LR: 3.258e-04  Data: 0.012 (0.016)
Train: 184 [ 900/1251 ( 72%)]  Loss: 3.231 (3.34)  Time: 0.669s, 1531.18/s  (0.667s, 1534.53/s)  LR: 3.256e-04  Data: 0.013 (0.016)
Train: 184 [ 950/1251 ( 76%)]  Loss: 3.238 (3.34)  Time: 0.664s, 1541.98/s  (0.667s, 1534.88/s)  LR: 3.254e-04  Data: 0.014 (0.016)
Train: 184 [1000/1251 ( 80%)]  Loss: 3.220 (3.33)  Time: 0.677s, 1511.58/s  (0.667s, 1535.36/s)  LR: 3.252e-04  Data: 0.012 (0.015)
Train: 184 [1050/1251 ( 84%)]  Loss: 3.339 (3.33)  Time: 0.666s, 1536.93/s  (0.667s, 1535.85/s)  LR: 3.250e-04  Data: 0.013 (0.015)
Train: 184 [1100/1251 ( 88%)]  Loss: 3.505 (3.34)  Time: 0.665s, 1540.27/s  (0.667s, 1536.12/s)  LR: 3.248e-04  Data: 0.016 (0.015)
Train: 184 [1150/1251 ( 92%)]  Loss: 3.351 (3.34)  Time: 0.665s, 1539.22/s  (0.666s, 1536.47/s)  LR: 3.246e-04  Data: 0.013 (0.015)
Train: 184 [1200/1251 ( 96%)]  Loss: 3.706 (3.36)  Time: 0.671s, 1525.51/s  (0.666s, 1536.68/s)  LR: 3.244e-04  Data: 0.018 (0.015)
Train: 184 [1250/1251 (100%)]  Loss: 3.452 (3.36)  Time: 0.657s, 1558.03/s  (0.666s, 1536.90/s)  LR: 3.242e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.954 (2.954)  Loss:  0.4487 (0.4487)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.163 (0.330)  Loss:  0.5732 (0.9249)  Acc@1: 86.0849 (77.9600)  Acc@5: 97.6415 (94.4560)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-183.pth.tar', 78.20800005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-182.pth.tar', 78.12999998046875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-180.pth.tar', 78.04000005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-184.pth.tar', 77.96000003173828)

Train: 185 [   0/1251 (  0%)]  Loss: 3.355 (3.35)  Time: 3.356s,  305.12/s  (3.356s,  305.12/s)  LR: 3.242e-04  Data: 1.986 (1.986)
Train: 185 [  50/1251 (  4%)]  Loss: 3.192 (3.27)  Time: 0.667s, 1535.09/s  (0.678s, 1509.30/s)  LR: 3.240e-04  Data: 0.014 (0.053)
Train: 185 [ 100/1251 (  8%)]  Loss: 3.478 (3.34)  Time: 0.664s, 1541.53/s  (0.668s, 1533.72/s)  LR: 3.238e-04  Data: 0.013 (0.033)
Train: 185 [ 150/1251 ( 12%)]  Loss: 3.324 (3.34)  Time: 0.661s, 1548.99/s  (0.665s, 1539.00/s)  LR: 3.236e-04  Data: 0.013 (0.027)
Train: 185 [ 200/1251 ( 16%)]  Loss: 3.280 (3.33)  Time: 0.661s, 1549.42/s  (0.665s, 1540.63/s)  LR: 3.234e-04  Data: 0.013 (0.024)
Train: 185 [ 250/1251 ( 20%)]  Loss: 3.341 (3.33)  Time: 0.660s, 1550.76/s  (0.664s, 1541.85/s)  LR: 3.232e-04  Data: 0.013 (0.022)
Train: 185 [ 300/1251 ( 24%)]  Loss: 3.424 (3.34)  Time: 0.669s, 1530.70/s  (0.664s, 1541.68/s)  LR: 3.230e-04  Data: 0.013 (0.020)
Train: 185 [ 350/1251 ( 28%)]  Loss: 3.238 (3.33)  Time: 0.667s, 1534.21/s  (0.664s, 1541.63/s)  LR: 3.228e-04  Data: 0.013 (0.019)
Train: 185 [ 400/1251 ( 32%)]  Loss: 3.401 (3.34)  Time: 0.666s, 1537.81/s  (0.664s, 1541.25/s)  LR: 3.227e-04  Data: 0.014 (0.019)
Train: 185 [ 450/1251 ( 36%)]  Loss: 3.396 (3.34)  Time: 0.662s, 1545.75/s  (0.665s, 1540.45/s)  LR: 3.225e-04  Data: 0.013 (0.018)
Train: 185 [ 500/1251 ( 40%)]  Loss: 3.210 (3.33)  Time: 0.668s, 1533.61/s  (0.665s, 1540.37/s)  LR: 3.223e-04  Data: 0.013 (0.018)
Train: 185 [ 550/1251 ( 44%)]  Loss: 3.173 (3.32)  Time: 0.668s, 1531.97/s  (0.665s, 1540.32/s)  LR: 3.221e-04  Data: 0.012 (0.017)
Train: 185 [ 600/1251 ( 48%)]  Loss: 3.441 (3.33)  Time: 0.680s, 1506.69/s  (0.665s, 1540.01/s)  LR: 3.219e-04  Data: 0.013 (0.017)
Train: 185 [ 650/1251 ( 52%)]  Loss: 3.469 (3.34)  Time: 0.661s, 1549.02/s  (0.665s, 1540.19/s)  LR: 3.217e-04  Data: 0.013 (0.017)
Train: 185 [ 700/1251 ( 56%)]  Loss: 3.258 (3.33)  Time: 0.663s, 1544.34/s  (0.665s, 1540.27/s)  LR: 3.215e-04  Data: 0.014 (0.016)
Train: 185 [ 750/1251 ( 60%)]  Loss: 3.373 (3.33)  Time: 0.670s, 1527.57/s  (0.665s, 1539.87/s)  LR: 3.213e-04  Data: 0.013 (0.016)
Train: 185 [ 800/1251 ( 64%)]  Loss: 3.496 (3.34)  Time: 0.669s, 1530.08/s  (0.665s, 1540.15/s)  LR: 3.211e-04  Data: 0.013 (0.016)
Train: 185 [ 850/1251 ( 68%)]  Loss: 3.326 (3.34)  Time: 0.669s, 1531.65/s  (0.665s, 1540.27/s)  LR: 3.209e-04  Data: 0.013 (0.016)
Train: 185 [ 900/1251 ( 72%)]  Loss: 3.609 (3.36)  Time: 0.677s, 1511.90/s  (0.665s, 1540.33/s)  LR: 3.207e-04  Data: 0.013 (0.016)
Train: 185 [ 950/1251 ( 76%)]  Loss: 3.042 (3.34)  Time: 0.672s, 1523.85/s  (0.665s, 1540.01/s)  LR: 3.205e-04  Data: 0.013 (0.016)
Train: 185 [1000/1251 ( 80%)]  Loss: 3.244 (3.34)  Time: 0.656s, 1561.02/s  (0.665s, 1540.13/s)  LR: 3.203e-04  Data: 0.013 (0.016)
Train: 185 [1050/1251 ( 84%)]  Loss: 3.129 (3.33)  Time: 0.662s, 1546.01/s  (0.665s, 1539.89/s)  LR: 3.201e-04  Data: 0.015 (0.015)
Train: 185 [1100/1251 ( 88%)]  Loss: 3.282 (3.33)  Time: 0.662s, 1547.98/s  (0.665s, 1539.78/s)  LR: 3.199e-04  Data: 0.016 (0.015)
Train: 185 [1150/1251 ( 92%)]  Loss: 3.317 (3.32)  Time: 0.671s, 1527.17/s  (0.665s, 1539.77/s)  LR: 3.197e-04  Data: 0.013 (0.015)
Train: 185 [1200/1251 ( 96%)]  Loss: 3.558 (3.33)  Time: 0.667s, 1534.08/s  (0.665s, 1539.57/s)  LR: 3.196e-04  Data: 0.014 (0.015)
Train: 185 [1250/1251 (100%)]  Loss: 3.126 (3.33)  Time: 0.667s, 1534.97/s  (0.665s, 1539.46/s)  LR: 3.194e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.067 (3.067)  Loss:  0.4514 (0.4514)  Acc@1: 90.8203 (90.8203)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.161 (0.334)  Loss:  0.5796 (0.9285)  Acc@1: 86.4387 (78.0580)  Acc@5: 97.8774 (94.4960)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-183.pth.tar', 78.20800005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-182.pth.tar', 78.12999998046875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-185.pth.tar', 78.05800005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-180.pth.tar', 78.04000005126953)

Train: 186 [   0/1251 (  0%)]  Loss: 3.285 (3.28)  Time: 3.434s,  298.18/s  (3.434s,  298.18/s)  LR: 3.194e-04  Data: 1.643 (1.643)
Train: 186 [  50/1251 (  4%)]  Loss: 3.283 (3.28)  Time: 0.654s, 1566.81/s  (0.684s, 1497.53/s)  LR: 3.192e-04  Data: 0.014 (0.046)
Train: 186 [ 100/1251 (  8%)]  Loss: 3.354 (3.31)  Time: 0.651s, 1572.60/s  (0.670s, 1527.37/s)  LR: 3.190e-04  Data: 0.013 (0.030)
Train: 186 [ 150/1251 ( 12%)]  Loss: 3.336 (3.31)  Time: 0.665s, 1539.17/s  (0.667s, 1535.91/s)  LR: 3.188e-04  Data: 0.014 (0.024)
Train: 186 [ 200/1251 ( 16%)]  Loss: 3.481 (3.35)  Time: 0.664s, 1542.30/s  (0.665s, 1538.76/s)  LR: 3.186e-04  Data: 0.016 (0.022)
Train: 186 [ 250/1251 ( 20%)]  Loss: 3.243 (3.33)  Time: 0.669s, 1530.27/s  (0.665s, 1539.02/s)  LR: 3.184e-04  Data: 0.013 (0.020)
Train: 186 [ 300/1251 ( 24%)]  Loss: 3.351 (3.33)  Time: 0.667s, 1535.24/s  (0.665s, 1539.84/s)  LR: 3.182e-04  Data: 0.012 (0.019)
Train: 186 [ 350/1251 ( 28%)]  Loss: 3.166 (3.31)  Time: 0.673s, 1521.02/s  (0.665s, 1540.27/s)  LR: 3.180e-04  Data: 0.014 (0.018)
Train: 186 [ 400/1251 ( 32%)]  Loss: 3.295 (3.31)  Time: 0.658s, 1555.63/s  (0.665s, 1540.84/s)  LR: 3.178e-04  Data: 0.014 (0.018)
Train: 186 [ 450/1251 ( 36%)]  Loss: 3.507 (3.33)  Time: 0.661s, 1548.73/s  (0.664s, 1541.15/s)  LR: 3.176e-04  Data: 0.017 (0.017)
Train: 186 [ 500/1251 ( 40%)]  Loss: 3.406 (3.34)  Time: 0.666s, 1538.53/s  (0.664s, 1541.42/s)  LR: 3.174e-04  Data: 0.013 (0.017)
Train: 186 [ 550/1251 ( 44%)]  Loss: 3.271 (3.33)  Time: 0.670s, 1528.98/s  (0.664s, 1541.62/s)  LR: 3.172e-04  Data: 0.013 (0.017)
Train: 186 [ 600/1251 ( 48%)]  Loss: 3.290 (3.33)  Time: 0.657s, 1559.26/s  (0.664s, 1541.78/s)  LR: 3.170e-04  Data: 0.016 (0.016)
Train: 186 [ 650/1251 ( 52%)]  Loss: 3.202 (3.32)  Time: 0.668s, 1533.00/s  (0.664s, 1542.28/s)  LR: 3.168e-04  Data: 0.016 (0.016)
Train: 186 [ 700/1251 ( 56%)]  Loss: 3.123 (3.31)  Time: 0.662s, 1547.44/s  (0.664s, 1542.64/s)  LR: 3.166e-04  Data: 0.014 (0.016)
Train: 186 [ 750/1251 ( 60%)]  Loss: 3.527 (3.32)  Time: 0.669s, 1530.99/s  (0.664s, 1543.10/s)  LR: 3.165e-04  Data: 0.013 (0.016)
Train: 186 [ 800/1251 ( 64%)]  Loss: 3.297 (3.32)  Time: 0.660s, 1550.72/s  (0.663s, 1543.43/s)  LR: 3.163e-04  Data: 0.013 (0.016)
Train: 186 [ 850/1251 ( 68%)]  Loss: 3.251 (3.31)  Time: 0.657s, 1559.52/s  (0.663s, 1543.49/s)  LR: 3.161e-04  Data: 0.014 (0.016)
Train: 186 [ 900/1251 ( 72%)]  Loss: 3.386 (3.32)  Time: 0.663s, 1544.50/s  (0.663s, 1543.61/s)  LR: 3.159e-04  Data: 0.013 (0.016)
Train: 186 [ 950/1251 ( 76%)]  Loss: 3.229 (3.31)  Time: 0.668s, 1532.57/s  (0.663s, 1543.59/s)  LR: 3.157e-04  Data: 0.013 (0.015)
Train: 186 [1000/1251 ( 80%)]  Loss: 3.326 (3.31)  Time: 0.664s, 1541.99/s  (0.663s, 1543.61/s)  LR: 3.155e-04  Data: 0.013 (0.015)
Train: 186 [1050/1251 ( 84%)]  Loss: 3.360 (3.32)  Time: 0.664s, 1542.03/s  (0.663s, 1543.55/s)  LR: 3.153e-04  Data: 0.013 (0.015)
Train: 186 [1100/1251 ( 88%)]  Loss: 3.351 (3.32)  Time: 0.661s, 1550.08/s  (0.663s, 1543.51/s)  LR: 3.151e-04  Data: 0.014 (0.015)
Train: 186 [1150/1251 ( 92%)]  Loss: 2.863 (3.30)  Time: 0.664s, 1541.16/s  (0.663s, 1543.49/s)  LR: 3.149e-04  Data: 0.014 (0.015)
Train: 186 [1200/1251 ( 96%)]  Loss: 3.171 (3.29)  Time: 0.660s, 1552.12/s  (0.663s, 1543.46/s)  LR: 3.147e-04  Data: 0.014 (0.015)
Train: 186 [1250/1251 (100%)]  Loss: 3.336 (3.30)  Time: 0.651s, 1571.89/s  (0.663s, 1543.36/s)  LR: 3.145e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.896 (2.896)  Loss:  0.4324 (0.4324)  Acc@1: 91.0156 (91.0156)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.161 (0.330)  Loss:  0.5537 (0.9047)  Acc@1: 86.6745 (78.2480)  Acc@5: 97.7594 (94.5180)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-186.pth.tar', 78.24800002929688)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-183.pth.tar', 78.20800005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-182.pth.tar', 78.12999998046875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-185.pth.tar', 78.05800005615234)

Train: 187 [   0/1251 (  0%)]  Loss: 3.469 (3.47)  Time: 3.481s,  294.21/s  (3.481s,  294.21/s)  LR: 3.145e-04  Data: 1.706 (1.706)
Train: 187 [  50/1251 (  4%)]  Loss: 3.616 (3.54)  Time: 0.651s, 1573.20/s  (0.679s, 1507.87/s)  LR: 3.143e-04  Data: 0.014 (0.047)
Train: 187 [ 100/1251 (  8%)]  Loss: 3.443 (3.51)  Time: 0.661s, 1549.50/s  (0.669s, 1531.67/s)  LR: 3.141e-04  Data: 0.015 (0.030)
Train: 187 [ 150/1251 ( 12%)]  Loss: 3.624 (3.54)  Time: 0.658s, 1555.73/s  (0.667s, 1535.84/s)  LR: 3.139e-04  Data: 0.016 (0.025)
Train: 187 [ 200/1251 ( 16%)]  Loss: 3.167 (3.46)  Time: 0.662s, 1545.77/s  (0.666s, 1537.33/s)  LR: 3.137e-04  Data: 0.013 (0.022)
Train: 187 [ 250/1251 ( 20%)]  Loss: 3.124 (3.41)  Time: 0.675s, 1516.00/s  (0.666s, 1537.63/s)  LR: 3.136e-04  Data: 0.013 (0.020)
Train: 187 [ 300/1251 ( 24%)]  Loss: 3.190 (3.38)  Time: 0.668s, 1532.55/s  (0.666s, 1537.95/s)  LR: 3.134e-04  Data: 0.014 (0.019)
Train: 187 [ 350/1251 ( 28%)]  Loss: 3.427 (3.38)  Time: 0.661s, 1549.58/s  (0.666s, 1538.08/s)  LR: 3.132e-04  Data: 0.013 (0.018)
Train: 187 [ 400/1251 ( 32%)]  Loss: 3.364 (3.38)  Time: 0.669s, 1530.57/s  (0.666s, 1537.83/s)  LR: 3.130e-04  Data: 0.013 (0.018)
Train: 187 [ 450/1251 ( 36%)]  Loss: 3.274 (3.37)  Time: 0.660s, 1550.80/s  (0.666s, 1537.14/s)  LR: 3.128e-04  Data: 0.016 (0.017)
Train: 187 [ 500/1251 ( 40%)]  Loss: 3.415 (3.37)  Time: 0.654s, 1565.27/s  (0.666s, 1537.13/s)  LR: 3.126e-04  Data: 0.012 (0.017)
Train: 187 [ 550/1251 ( 44%)]  Loss: 3.320 (3.37)  Time: 0.663s, 1544.56/s  (0.666s, 1536.81/s)  LR: 3.124e-04  Data: 0.013 (0.017)
Train: 187 [ 600/1251 ( 48%)]  Loss: 3.283 (3.36)  Time: 0.670s, 1527.26/s  (0.666s, 1536.66/s)  LR: 3.122e-04  Data: 0.013 (0.016)
Train: 187 [ 650/1251 ( 52%)]  Loss: 3.481 (3.37)  Time: 0.667s, 1534.92/s  (0.667s, 1536.26/s)  LR: 3.120e-04  Data: 0.013 (0.016)
Train: 187 [ 700/1251 ( 56%)]  Loss: 3.440 (3.38)  Time: 0.679s, 1507.30/s  (0.667s, 1536.14/s)  LR: 3.118e-04  Data: 0.014 (0.016)
Train: 187 [ 750/1251 ( 60%)]  Loss: 3.374 (3.38)  Time: 0.670s, 1527.38/s  (0.667s, 1536.12/s)  LR: 3.116e-04  Data: 0.013 (0.016)
Train: 187 [ 800/1251 ( 64%)]  Loss: 3.069 (3.36)  Time: 0.666s, 1536.98/s  (0.667s, 1535.72/s)  LR: 3.114e-04  Data: 0.013 (0.016)
Train: 187 [ 850/1251 ( 68%)]  Loss: 3.639 (3.37)  Time: 0.675s, 1517.58/s  (0.667s, 1535.47/s)  LR: 3.112e-04  Data: 0.012 (0.016)
Train: 187 [ 900/1251 ( 72%)]  Loss: 3.631 (3.39)  Time: 0.669s, 1531.16/s  (0.667s, 1535.37/s)  LR: 3.111e-04  Data: 0.013 (0.016)
Train: 187 [ 950/1251 ( 76%)]  Loss: 3.663 (3.40)  Time: 0.665s, 1540.83/s  (0.667s, 1535.55/s)  LR: 3.109e-04  Data: 0.014 (0.015)
Train: 187 [1000/1251 ( 80%)]  Loss: 3.481 (3.40)  Time: 0.662s, 1547.13/s  (0.667s, 1535.68/s)  LR: 3.107e-04  Data: 0.013 (0.015)
Train: 187 [1050/1251 ( 84%)]  Loss: 3.260 (3.40)  Time: 0.667s, 1536.05/s  (0.667s, 1535.95/s)  LR: 3.105e-04  Data: 0.013 (0.015)
Train: 187 [1100/1251 ( 88%)]  Loss: 3.155 (3.39)  Time: 0.668s, 1532.24/s  (0.667s, 1536.00/s)  LR: 3.103e-04  Data: 0.015 (0.015)
Train: 187 [1150/1251 ( 92%)]  Loss: 3.120 (3.38)  Time: 0.664s, 1542.82/s  (0.667s, 1536.04/s)  LR: 3.101e-04  Data: 0.014 (0.015)
Train: 187 [1200/1251 ( 96%)]  Loss: 3.480 (3.38)  Time: 0.660s, 1551.61/s  (0.667s, 1536.02/s)  LR: 3.099e-04  Data: 0.013 (0.015)
Train: 187 [1250/1251 (100%)]  Loss: 3.121 (3.37)  Time: 0.648s, 1580.05/s  (0.667s, 1536.18/s)  LR: 3.097e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.939 (2.939)  Loss:  0.4497 (0.4497)  Acc@1: 91.6016 (91.6016)  Acc@5: 97.7539 (97.7539)
Test: [  48/48]  Time: 0.162 (0.333)  Loss:  0.5850 (0.9234)  Acc@1: 86.6745 (78.3700)  Acc@5: 97.1698 (94.4080)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-187.pth.tar', 78.36999989990234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-186.pth.tar', 78.24800002929688)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-183.pth.tar', 78.20800005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-182.pth.tar', 78.12999998046875)

Train: 188 [   0/1251 (  0%)]  Loss: 3.490 (3.49)  Time: 3.698s,  276.89/s  (3.698s,  276.89/s)  LR: 3.097e-04  Data: 1.589 (1.589)
Train: 188 [  50/1251 (  4%)]  Loss: 3.667 (3.58)  Time: 0.650s, 1575.85/s  (0.684s, 1496.66/s)  LR: 3.095e-04  Data: 0.017 (0.045)
Train: 188 [ 100/1251 (  8%)]  Loss: 3.436 (3.53)  Time: 0.658s, 1555.12/s  (0.670s, 1527.59/s)  LR: 3.093e-04  Data: 0.014 (0.030)
Train: 188 [ 150/1251 ( 12%)]  Loss: 3.382 (3.49)  Time: 0.669s, 1531.13/s  (0.668s, 1532.77/s)  LR: 3.091e-04  Data: 0.016 (0.025)
Train: 188 [ 200/1251 ( 16%)]  Loss: 3.452 (3.49)  Time: 0.657s, 1558.54/s  (0.667s, 1534.16/s)  LR: 3.089e-04  Data: 0.014 (0.022)
Train: 188 [ 250/1251 ( 20%)]  Loss: 3.074 (3.42)  Time: 0.667s, 1534.42/s  (0.667s, 1534.39/s)  LR: 3.087e-04  Data: 0.015 (0.020)
Train: 188 [ 300/1251 ( 24%)]  Loss: 3.209 (3.39)  Time: 0.671s, 1526.45/s  (0.667s, 1534.30/s)  LR: 3.086e-04  Data: 0.016 (0.019)
Train: 188 [ 350/1251 ( 28%)]  Loss: 3.587 (3.41)  Time: 0.674s, 1518.67/s  (0.668s, 1533.14/s)  LR: 3.084e-04  Data: 0.017 (0.019)
Train: 188 [ 400/1251 ( 32%)]  Loss: 3.148 (3.38)  Time: 0.667s, 1535.61/s  (0.668s, 1532.24/s)  LR: 3.082e-04  Data: 0.014 (0.018)
Train: 188 [ 450/1251 ( 36%)]  Loss: 3.023 (3.35)  Time: 0.665s, 1540.70/s  (0.669s, 1531.64/s)  LR: 3.080e-04  Data: 0.013 (0.018)
Train: 188 [ 500/1251 ( 40%)]  Loss: 3.098 (3.32)  Time: 0.668s, 1532.00/s  (0.669s, 1531.44/s)  LR: 3.078e-04  Data: 0.013 (0.017)
Train: 188 [ 550/1251 ( 44%)]  Loss: 3.299 (3.32)  Time: 0.659s, 1552.94/s  (0.669s, 1530.98/s)  LR: 3.076e-04  Data: 0.014 (0.017)
Train: 188 [ 600/1251 ( 48%)]  Loss: 3.247 (3.32)  Time: 0.670s, 1528.79/s  (0.669s, 1530.55/s)  LR: 3.074e-04  Data: 0.013 (0.017)
Train: 188 [ 650/1251 ( 52%)]  Loss: 3.173 (3.31)  Time: 0.678s, 1509.42/s  (0.669s, 1530.68/s)  LR: 3.072e-04  Data: 0.015 (0.016)
Train: 188 [ 700/1251 ( 56%)]  Loss: 3.363 (3.31)  Time: 0.666s, 1537.82/s  (0.669s, 1531.03/s)  LR: 3.070e-04  Data: 0.016 (0.016)
Train: 188 [ 750/1251 ( 60%)]  Loss: 3.348 (3.31)  Time: 0.666s, 1536.61/s  (0.669s, 1531.14/s)  LR: 3.068e-04  Data: 0.013 (0.016)
Train: 188 [ 800/1251 ( 64%)]  Loss: 3.187 (3.30)  Time: 0.664s, 1541.94/s  (0.669s, 1531.54/s)  LR: 3.066e-04  Data: 0.013 (0.016)
Train: 188 [ 850/1251 ( 68%)]  Loss: 3.416 (3.31)  Time: 0.667s, 1534.53/s  (0.669s, 1531.71/s)  LR: 3.064e-04  Data: 0.013 (0.016)
Train: 188 [ 900/1251 ( 72%)]  Loss: 3.146 (3.30)  Time: 0.669s, 1531.73/s  (0.668s, 1532.09/s)  LR: 3.063e-04  Data: 0.012 (0.016)
Train: 188 [ 950/1251 ( 76%)]  Loss: 3.521 (3.31)  Time: 0.673s, 1522.37/s  (0.668s, 1532.32/s)  LR: 3.061e-04  Data: 0.014 (0.016)
Train: 188 [1000/1251 ( 80%)]  Loss: 3.209 (3.31)  Time: 0.675s, 1516.49/s  (0.668s, 1532.72/s)  LR: 3.059e-04  Data: 0.014 (0.015)
Train: 188 [1050/1251 ( 84%)]  Loss: 3.517 (3.32)  Time: 0.669s, 1530.61/s  (0.668s, 1533.04/s)  LR: 3.057e-04  Data: 0.014 (0.015)
Train: 188 [1100/1251 ( 88%)]  Loss: 3.748 (3.34)  Time: 0.667s, 1534.92/s  (0.668s, 1533.28/s)  LR: 3.055e-04  Data: 0.013 (0.015)
Train: 188 [1150/1251 ( 92%)]  Loss: 3.621 (3.35)  Time: 0.662s, 1545.96/s  (0.668s, 1533.56/s)  LR: 3.053e-04  Data: 0.013 (0.015)
Train: 188 [1200/1251 ( 96%)]  Loss: 3.510 (3.35)  Time: 0.663s, 1543.98/s  (0.668s, 1533.74/s)  LR: 3.051e-04  Data: 0.015 (0.015)
Train: 188 [1250/1251 (100%)]  Loss: 3.708 (3.37)  Time: 0.652s, 1570.80/s  (0.667s, 1534.09/s)  LR: 3.049e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.861 (2.861)  Loss:  0.4329 (0.4329)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.162 (0.324)  Loss:  0.5239 (0.9057)  Acc@1: 87.5000 (78.5260)  Acc@5: 97.9953 (94.4900)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-188.pth.tar', 78.526)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-187.pth.tar', 78.36999989990234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-186.pth.tar', 78.24800002929688)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-183.pth.tar', 78.20800005615234)

Train: 189 [   0/1251 (  0%)]  Loss: 3.368 (3.37)  Time: 3.687s,  277.70/s  (3.687s,  277.70/s)  LR: 3.049e-04  Data: 1.760 (1.760)
Train: 189 [  50/1251 (  4%)]  Loss: 3.104 (3.24)  Time: 0.651s, 1573.15/s  (0.683s, 1499.96/s)  LR: 3.047e-04  Data: 0.013 (0.048)
Train: 189 [ 100/1251 (  8%)]  Loss: 3.489 (3.32)  Time: 0.652s, 1571.33/s  (0.669s, 1531.57/s)  LR: 3.045e-04  Data: 0.012 (0.031)
Train: 189 [ 150/1251 ( 12%)]  Loss: 3.048 (3.25)  Time: 0.663s, 1543.99/s  (0.666s, 1536.99/s)  LR: 3.043e-04  Data: 0.016 (0.026)
Train: 189 [ 200/1251 ( 16%)]  Loss: 3.353 (3.27)  Time: 0.655s, 1563.39/s  (0.665s, 1538.93/s)  LR: 3.042e-04  Data: 0.013 (0.023)
Train: 189 [ 250/1251 ( 20%)]  Loss: 3.225 (3.26)  Time: 0.660s, 1550.51/s  (0.666s, 1538.57/s)  LR: 3.040e-04  Data: 0.013 (0.021)
Train: 189 [ 300/1251 ( 24%)]  Loss: 3.688 (3.32)  Time: 0.672s, 1522.79/s  (0.665s, 1538.79/s)  LR: 3.038e-04  Data: 0.014 (0.020)
Train: 189 [ 350/1251 ( 28%)]  Loss: 3.138 (3.30)  Time: 0.657s, 1559.05/s  (0.665s, 1538.81/s)  LR: 3.036e-04  Data: 0.013 (0.019)
Train: 189 [ 400/1251 ( 32%)]  Loss: 3.209 (3.29)  Time: 0.671s, 1525.29/s  (0.666s, 1538.51/s)  LR: 3.034e-04  Data: 0.016 (0.018)
Train: 189 [ 450/1251 ( 36%)]  Loss: 3.468 (3.31)  Time: 0.670s, 1527.76/s  (0.665s, 1538.80/s)  LR: 3.032e-04  Data: 0.018 (0.018)
Train: 189 [ 500/1251 ( 40%)]  Loss: 3.234 (3.30)  Time: 0.659s, 1553.26/s  (0.665s, 1538.95/s)  LR: 3.030e-04  Data: 0.013 (0.017)
Train: 189 [ 550/1251 ( 44%)]  Loss: 3.367 (3.31)  Time: 0.672s, 1524.57/s  (0.665s, 1538.85/s)  LR: 3.028e-04  Data: 0.013 (0.017)
Train: 189 [ 600/1251 ( 48%)]  Loss: 3.172 (3.30)  Time: 0.664s, 1542.02/s  (0.665s, 1538.74/s)  LR: 3.026e-04  Data: 0.014 (0.017)
Train: 189 [ 650/1251 ( 52%)]  Loss: 3.173 (3.29)  Time: 0.676s, 1515.12/s  (0.666s, 1538.47/s)  LR: 3.024e-04  Data: 0.015 (0.016)
Train: 189 [ 700/1251 ( 56%)]  Loss: 3.161 (3.28)  Time: 0.663s, 1543.89/s  (0.666s, 1537.88/s)  LR: 3.022e-04  Data: 0.013 (0.016)
Train: 189 [ 750/1251 ( 60%)]  Loss: 3.280 (3.28)  Time: 0.667s, 1535.91/s  (0.666s, 1537.57/s)  LR: 3.021e-04  Data: 0.013 (0.016)
Train: 189 [ 800/1251 ( 64%)]  Loss: 3.032 (3.27)  Time: 0.663s, 1544.57/s  (0.666s, 1537.52/s)  LR: 3.019e-04  Data: 0.012 (0.016)
Train: 189 [ 850/1251 ( 68%)]  Loss: 3.639 (3.29)  Time: 0.665s, 1540.98/s  (0.666s, 1537.42/s)  LR: 3.017e-04  Data: 0.014 (0.016)
Train: 189 [ 900/1251 ( 72%)]  Loss: 3.352 (3.29)  Time: 0.666s, 1536.57/s  (0.666s, 1537.49/s)  LR: 3.015e-04  Data: 0.013 (0.016)
Train: 189 [ 950/1251 ( 76%)]  Loss: 3.330 (3.29)  Time: 0.665s, 1540.69/s  (0.666s, 1537.44/s)  LR: 3.013e-04  Data: 0.014 (0.015)
Train: 189 [1000/1251 ( 80%)]  Loss: 3.183 (3.29)  Time: 0.675s, 1517.19/s  (0.666s, 1537.33/s)  LR: 3.011e-04  Data: 0.013 (0.015)
Train: 189 [1050/1251 ( 84%)]  Loss: 3.272 (3.29)  Time: 0.661s, 1548.60/s  (0.666s, 1537.51/s)  LR: 3.009e-04  Data: 0.014 (0.015)
Train: 189 [1100/1251 ( 88%)]  Loss: 3.097 (3.28)  Time: 0.666s, 1537.47/s  (0.666s, 1537.59/s)  LR: 3.007e-04  Data: 0.013 (0.015)
Train: 189 [1150/1251 ( 92%)]  Loss: 3.560 (3.29)  Time: 0.666s, 1537.43/s  (0.666s, 1537.52/s)  LR: 3.005e-04  Data: 0.013 (0.015)
Train: 189 [1200/1251 ( 96%)]  Loss: 3.451 (3.30)  Time: 0.668s, 1533.37/s  (0.666s, 1537.48/s)  LR: 3.003e-04  Data: 0.013 (0.015)
Train: 189 [1250/1251 (100%)]  Loss: 3.370 (3.30)  Time: 0.656s, 1560.92/s  (0.666s, 1537.58/s)  LR: 3.001e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.916 (2.916)  Loss:  0.4502 (0.4502)  Acc@1: 91.2109 (91.2109)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.162 (0.327)  Loss:  0.5410 (0.9240)  Acc@1: 86.0849 (78.3800)  Acc@5: 98.2311 (94.4860)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-188.pth.tar', 78.526)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-189.pth.tar', 78.38000003173828)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-187.pth.tar', 78.36999989990234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-186.pth.tar', 78.24800002929688)

Train: 190 [   0/1251 (  0%)]  Loss: 3.313 (3.31)  Time: 3.453s,  296.54/s  (3.453s,  296.54/s)  LR: 3.001e-04  Data: 2.284 (2.284)
Train: 190 [  50/1251 (  4%)]  Loss: 3.233 (3.27)  Time: 0.657s, 1559.34/s  (0.684s, 1496.64/s)  LR: 3.000e-04  Data: 0.015 (0.058)
Train: 190 [ 100/1251 (  8%)]  Loss: 3.139 (3.23)  Time: 0.666s, 1538.38/s  (0.670s, 1528.43/s)  LR: 2.998e-04  Data: 0.014 (0.036)
Train: 190 [ 150/1251 ( 12%)]  Loss: 3.228 (3.23)  Time: 0.658s, 1555.42/s  (0.667s, 1534.78/s)  LR: 2.996e-04  Data: 0.013 (0.029)
Train: 190 [ 200/1251 ( 16%)]  Loss: 3.310 (3.24)  Time: 0.668s, 1532.71/s  (0.666s, 1537.48/s)  LR: 2.994e-04  Data: 0.013 (0.025)
Train: 190 [ 250/1251 ( 20%)]  Loss: 3.397 (3.27)  Time: 0.668s, 1533.67/s  (0.666s, 1537.37/s)  LR: 2.992e-04  Data: 0.018 (0.023)
Train: 190 [ 300/1251 ( 24%)]  Loss: 3.319 (3.28)  Time: 0.666s, 1536.98/s  (0.666s, 1536.91/s)  LR: 2.990e-04  Data: 0.016 (0.021)
Train: 190 [ 350/1251 ( 28%)]  Loss: 3.456 (3.30)  Time: 0.672s, 1522.73/s  (0.666s, 1537.50/s)  LR: 2.988e-04  Data: 0.015 (0.020)
Train: 190 [ 400/1251 ( 32%)]  Loss: 3.209 (3.29)  Time: 0.669s, 1531.52/s  (0.666s, 1537.34/s)  LR: 2.986e-04  Data: 0.013 (0.019)
Train: 190 [ 450/1251 ( 36%)]  Loss: 3.287 (3.29)  Time: 0.677s, 1513.52/s  (0.666s, 1536.85/s)  LR: 2.984e-04  Data: 0.014 (0.019)
Train: 190 [ 500/1251 ( 40%)]  Loss: 3.307 (3.29)  Time: 0.664s, 1541.67/s  (0.666s, 1536.73/s)  LR: 2.982e-04  Data: 0.014 (0.018)
Train: 190 [ 550/1251 ( 44%)]  Loss: 3.458 (3.30)  Time: 0.669s, 1531.18/s  (0.666s, 1536.53/s)  LR: 2.981e-04  Data: 0.014 (0.018)
Train: 190 [ 600/1251 ( 48%)]  Loss: 2.960 (3.28)  Time: 0.674s, 1519.06/s  (0.667s, 1536.38/s)  LR: 2.979e-04  Data: 0.014 (0.018)
Train: 190 [ 650/1251 ( 52%)]  Loss: 3.192 (3.27)  Time: 0.668s, 1532.78/s  (0.667s, 1536.37/s)  LR: 2.977e-04  Data: 0.014 (0.017)
Train: 190 [ 700/1251 ( 56%)]  Loss: 3.158 (3.26)  Time: 0.666s, 1537.96/s  (0.667s, 1536.23/s)  LR: 2.975e-04  Data: 0.013 (0.017)
Train: 190 [ 750/1251 ( 60%)]  Loss: 3.257 (3.26)  Time: 0.666s, 1536.40/s  (0.666s, 1536.69/s)  LR: 2.973e-04  Data: 0.014 (0.017)
Train: 190 [ 800/1251 ( 64%)]  Loss: 3.274 (3.26)  Time: 0.671s, 1526.78/s  (0.666s, 1536.91/s)  LR: 2.971e-04  Data: 0.013 (0.017)
Train: 190 [ 850/1251 ( 68%)]  Loss: 3.194 (3.26)  Time: 0.653s, 1567.01/s  (0.666s, 1537.06/s)  LR: 2.969e-04  Data: 0.013 (0.016)
Train: 190 [ 900/1251 ( 72%)]  Loss: 3.295 (3.26)  Time: 0.665s, 1539.43/s  (0.666s, 1537.22/s)  LR: 2.967e-04  Data: 0.018 (0.016)
Train: 190 [ 950/1251 ( 76%)]  Loss: 3.202 (3.26)  Time: 0.658s, 1556.94/s  (0.666s, 1537.45/s)  LR: 2.965e-04  Data: 0.017 (0.016)
Train: 190 [1000/1251 ( 80%)]  Loss: 3.505 (3.27)  Time: 0.663s, 1543.57/s  (0.666s, 1537.95/s)  LR: 2.963e-04  Data: 0.013 (0.016)
Train: 190 [1050/1251 ( 84%)]  Loss: 3.626 (3.29)  Time: 0.665s, 1538.75/s  (0.666s, 1538.32/s)  LR: 2.962e-04  Data: 0.014 (0.016)
Train: 190 [1100/1251 ( 88%)]  Loss: 3.511 (3.30)  Time: 0.664s, 1541.33/s  (0.666s, 1538.62/s)  LR: 2.960e-04  Data: 0.013 (0.016)
Train: 190 [1150/1251 ( 92%)]  Loss: 3.080 (3.29)  Time: 0.657s, 1558.10/s  (0.665s, 1538.92/s)  LR: 2.958e-04  Data: 0.013 (0.016)
Train: 190 [1200/1251 ( 96%)]  Loss: 3.406 (3.29)  Time: 0.661s, 1550.10/s  (0.665s, 1539.35/s)  LR: 2.956e-04  Data: 0.013 (0.016)
Train: 190 [1250/1251 (100%)]  Loss: 3.556 (3.30)  Time: 0.648s, 1580.89/s  (0.665s, 1539.59/s)  LR: 2.954e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.872 (2.872)  Loss:  0.4224 (0.4224)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.162 (0.321)  Loss:  0.5620 (0.8946)  Acc@1: 86.3208 (78.5060)  Acc@5: 97.8774 (94.6220)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-188.pth.tar', 78.526)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-190.pth.tar', 78.50600000488281)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-189.pth.tar', 78.38000003173828)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-187.pth.tar', 78.36999989990234)

Train: 191 [   0/1251 (  0%)]  Loss: 3.558 (3.56)  Time: 3.589s,  285.29/s  (3.589s,  285.29/s)  LR: 2.954e-04  Data: 1.880 (1.880)
Train: 191 [  50/1251 (  4%)]  Loss: 3.281 (3.42)  Time: 0.653s, 1567.40/s  (0.680s, 1505.11/s)  LR: 2.952e-04  Data: 0.013 (0.051)
Train: 191 [ 100/1251 (  8%)]  Loss: 3.259 (3.37)  Time: 0.649s, 1578.90/s  (0.667s, 1534.27/s)  LR: 2.950e-04  Data: 0.014 (0.032)
Train: 191 [ 150/1251 ( 12%)]  Loss: 3.506 (3.40)  Time: 0.659s, 1553.49/s  (0.664s, 1541.25/s)  LR: 2.948e-04  Data: 0.013 (0.026)
Train: 191 [ 200/1251 ( 16%)]  Loss: 3.345 (3.39)  Time: 0.660s, 1550.85/s  (0.664s, 1542.11/s)  LR: 2.946e-04  Data: 0.013 (0.023)
Train: 191 [ 250/1251 ( 20%)]  Loss: 3.482 (3.41)  Time: 0.671s, 1526.79/s  (0.664s, 1542.88/s)  LR: 2.945e-04  Data: 0.014 (0.021)
Train: 191 [ 300/1251 ( 24%)]  Loss: 3.373 (3.40)  Time: 0.672s, 1523.10/s  (0.664s, 1542.87/s)  LR: 2.943e-04  Data: 0.013 (0.020)
Train: 191 [ 350/1251 ( 28%)]  Loss: 3.386 (3.40)  Time: 0.665s, 1540.41/s  (0.664s, 1542.71/s)  LR: 2.941e-04  Data: 0.013 (0.019)
Train: 191 [ 400/1251 ( 32%)]  Loss: 3.353 (3.39)  Time: 0.662s, 1547.06/s  (0.664s, 1542.09/s)  LR: 2.939e-04  Data: 0.012 (0.018)
Train: 191 [ 450/1251 ( 36%)]  Loss: 3.627 (3.42)  Time: 0.662s, 1546.15/s  (0.664s, 1541.43/s)  LR: 2.937e-04  Data: 0.014 (0.018)
Train: 191 [ 500/1251 ( 40%)]  Loss: 3.202 (3.40)  Time: 0.669s, 1531.31/s  (0.665s, 1540.54/s)  LR: 2.935e-04  Data: 0.013 (0.017)
Train: 191 [ 550/1251 ( 44%)]  Loss: 3.569 (3.41)  Time: 0.655s, 1562.97/s  (0.665s, 1540.36/s)  LR: 2.933e-04  Data: 0.013 (0.017)
Train: 191 [ 600/1251 ( 48%)]  Loss: 3.128 (3.39)  Time: 0.668s, 1532.63/s  (0.665s, 1540.35/s)  LR: 2.931e-04  Data: 0.012 (0.017)
Train: 191 [ 650/1251 ( 52%)]  Loss: 3.105 (3.37)  Time: 0.665s, 1539.21/s  (0.665s, 1540.12/s)  LR: 2.929e-04  Data: 0.013 (0.017)
Train: 191 [ 700/1251 ( 56%)]  Loss: 3.244 (3.36)  Time: 0.666s, 1537.92/s  (0.665s, 1540.07/s)  LR: 2.927e-04  Data: 0.013 (0.016)
Train: 191 [ 750/1251 ( 60%)]  Loss: 3.241 (3.35)  Time: 0.666s, 1536.85/s  (0.665s, 1539.62/s)  LR: 2.926e-04  Data: 0.016 (0.016)
Train: 191 [ 800/1251 ( 64%)]  Loss: 3.227 (3.35)  Time: 0.675s, 1517.24/s  (0.665s, 1539.05/s)  LR: 2.924e-04  Data: 0.013 (0.016)
Train: 191 [ 850/1251 ( 68%)]  Loss: 3.397 (3.35)  Time: 0.670s, 1529.17/s  (0.665s, 1539.02/s)  LR: 2.922e-04  Data: 0.014 (0.016)
Train: 191 [ 900/1251 ( 72%)]  Loss: 3.135 (3.34)  Time: 0.669s, 1531.60/s  (0.665s, 1538.88/s)  LR: 2.920e-04  Data: 0.018 (0.016)
Train: 191 [ 950/1251 ( 76%)]  Loss: 3.547 (3.35)  Time: 0.667s, 1534.12/s  (0.665s, 1538.90/s)  LR: 2.918e-04  Data: 0.013 (0.016)
Train: 191 [1000/1251 ( 80%)]  Loss: 3.278 (3.35)  Time: 0.668s, 1533.03/s  (0.665s, 1538.74/s)  LR: 2.916e-04  Data: 0.013 (0.015)
Train: 191 [1050/1251 ( 84%)]  Loss: 3.546 (3.35)  Time: 0.666s, 1537.99/s  (0.666s, 1538.63/s)  LR: 2.914e-04  Data: 0.016 (0.015)
Train: 191 [1100/1251 ( 88%)]  Loss: 3.071 (3.34)  Time: 0.659s, 1553.13/s  (0.665s, 1538.79/s)  LR: 2.912e-04  Data: 0.013 (0.015)
Train: 191 [1150/1251 ( 92%)]  Loss: 3.480 (3.35)  Time: 0.665s, 1541.00/s  (0.665s, 1538.91/s)  LR: 2.911e-04  Data: 0.013 (0.015)
Train: 191 [1200/1251 ( 96%)]  Loss: 3.055 (3.34)  Time: 0.668s, 1533.97/s  (0.665s, 1539.05/s)  LR: 2.909e-04  Data: 0.013 (0.015)
Train: 191 [1250/1251 (100%)]  Loss: 3.465 (3.34)  Time: 0.651s, 1572.83/s  (0.665s, 1539.12/s)  LR: 2.907e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.830 (2.830)  Loss:  0.4541 (0.4541)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.164 (0.318)  Loss:  0.5918 (0.9154)  Acc@1: 86.3208 (78.5080)  Acc@5: 97.7594 (94.5580)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-188.pth.tar', 78.526)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-191.pth.tar', 78.50800000488282)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-190.pth.tar', 78.50600000488281)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-189.pth.tar', 78.38000003173828)

Train: 192 [   0/1251 (  0%)]  Loss: 2.931 (2.93)  Time: 3.968s,  258.05/s  (3.968s,  258.05/s)  LR: 2.907e-04  Data: 2.234 (2.234)
Train: 192 [  50/1251 (  4%)]  Loss: 3.537 (3.23)  Time: 0.659s, 1554.04/s  (0.698s, 1467.91/s)  LR: 2.905e-04  Data: 0.013 (0.058)
Train: 192 [ 100/1251 (  8%)]  Loss: 3.122 (3.20)  Time: 0.663s, 1543.65/s  (0.679s, 1508.45/s)  LR: 2.903e-04  Data: 0.017 (0.036)
Train: 192 [ 150/1251 ( 12%)]  Loss: 3.254 (3.21)  Time: 0.673s, 1520.51/s  (0.674s, 1520.18/s)  LR: 2.901e-04  Data: 0.014 (0.029)
Train: 192 [ 200/1251 ( 16%)]  Loss: 3.199 (3.21)  Time: 0.661s, 1549.01/s  (0.671s, 1524.97/s)  LR: 2.899e-04  Data: 0.013 (0.025)
Train: 192 [ 250/1251 ( 20%)]  Loss: 3.105 (3.19)  Time: 0.671s, 1526.06/s  (0.671s, 1527.10/s)  LR: 2.897e-04  Data: 0.014 (0.023)
Train: 192 [ 300/1251 ( 24%)]  Loss: 3.346 (3.21)  Time: 0.660s, 1551.77/s  (0.670s, 1528.41/s)  LR: 2.895e-04  Data: 0.014 (0.021)
Train: 192 [ 350/1251 ( 28%)]  Loss: 3.589 (3.26)  Time: 0.672s, 1524.46/s  (0.669s, 1529.71/s)  LR: 2.894e-04  Data: 0.014 (0.020)
Train: 192 [ 400/1251 ( 32%)]  Loss: 3.501 (3.29)  Time: 0.672s, 1524.94/s  (0.669s, 1530.63/s)  LR: 2.892e-04  Data: 0.014 (0.019)
Train: 192 [ 450/1251 ( 36%)]  Loss: 3.083 (3.27)  Time: 0.657s, 1557.99/s  (0.669s, 1531.33/s)  LR: 2.890e-04  Data: 0.013 (0.018)
Train: 192 [ 500/1251 ( 40%)]  Loss: 3.256 (3.27)  Time: 0.664s, 1541.12/s  (0.668s, 1532.07/s)  LR: 2.888e-04  Data: 0.013 (0.018)
Train: 192 [ 550/1251 ( 44%)]  Loss: 3.427 (3.28)  Time: 0.665s, 1538.77/s  (0.668s, 1532.89/s)  LR: 2.886e-04  Data: 0.013 (0.018)
Train: 192 [ 600/1251 ( 48%)]  Loss: 3.277 (3.28)  Time: 0.669s, 1531.36/s  (0.668s, 1533.17/s)  LR: 2.884e-04  Data: 0.016 (0.017)
Train: 192 [ 650/1251 ( 52%)]  Loss: 3.413 (3.29)  Time: 0.664s, 1541.13/s  (0.668s, 1533.51/s)  LR: 2.882e-04  Data: 0.013 (0.017)
Train: 192 [ 700/1251 ( 56%)]  Loss: 3.581 (3.31)  Time: 0.673s, 1521.37/s  (0.668s, 1533.66/s)  LR: 2.880e-04  Data: 0.013 (0.017)
Train: 192 [ 750/1251 ( 60%)]  Loss: 3.463 (3.32)  Time: 0.664s, 1542.20/s  (0.668s, 1534.04/s)  LR: 2.878e-04  Data: 0.013 (0.017)
Train: 192 [ 800/1251 ( 64%)]  Loss: 3.174 (3.31)  Time: 0.660s, 1552.31/s  (0.667s, 1534.12/s)  LR: 2.877e-04  Data: 0.013 (0.016)
Train: 192 [ 850/1251 ( 68%)]  Loss: 3.442 (3.32)  Time: 0.670s, 1528.88/s  (0.667s, 1534.23/s)  LR: 2.875e-04  Data: 0.014 (0.016)
Train: 192 [ 900/1251 ( 72%)]  Loss: 3.427 (3.32)  Time: 0.664s, 1543.12/s  (0.667s, 1534.42/s)  LR: 2.873e-04  Data: 0.013 (0.016)
Train: 192 [ 950/1251 ( 76%)]  Loss: 3.453 (3.33)  Time: 0.667s, 1534.23/s  (0.667s, 1534.37/s)  LR: 2.871e-04  Data: 0.012 (0.016)
Train: 192 [1000/1251 ( 80%)]  Loss: 3.291 (3.33)  Time: 0.658s, 1555.38/s  (0.667s, 1534.65/s)  LR: 2.869e-04  Data: 0.016 (0.016)
Train: 192 [1050/1251 ( 84%)]  Loss: 3.259 (3.32)  Time: 0.661s, 1550.31/s  (0.667s, 1535.11/s)  LR: 2.867e-04  Data: 0.017 (0.016)
Train: 192 [1100/1251 ( 88%)]  Loss: 3.454 (3.33)  Time: 0.668s, 1533.91/s  (0.667s, 1535.22/s)  LR: 2.865e-04  Data: 0.015 (0.016)
Train: 192 [1150/1251 ( 92%)]  Loss: 3.312 (3.33)  Time: 0.666s, 1538.22/s  (0.667s, 1535.57/s)  LR: 2.863e-04  Data: 0.013 (0.016)
Train: 192 [1200/1251 ( 96%)]  Loss: 3.325 (3.33)  Time: 0.672s, 1524.81/s  (0.667s, 1535.97/s)  LR: 2.862e-04  Data: 0.014 (0.015)
Train: 192 [1250/1251 (100%)]  Loss: 3.214 (3.32)  Time: 0.646s, 1584.35/s  (0.666s, 1536.39/s)  LR: 2.860e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.907 (2.907)  Loss:  0.4475 (0.4475)  Acc@1: 91.1133 (91.1133)  Acc@5: 97.5586 (97.5586)
Test: [  48/48]  Time: 0.165 (0.329)  Loss:  0.5698 (0.9022)  Acc@1: 85.3774 (78.5900)  Acc@5: 97.4057 (94.6040)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-192.pth.tar', 78.58999998291016)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-188.pth.tar', 78.526)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-191.pth.tar', 78.50800000488282)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-190.pth.tar', 78.50600000488281)

Train: 193 [   0/1251 (  0%)]  Loss: 3.525 (3.52)  Time: 3.414s,  299.95/s  (3.414s,  299.95/s)  LR: 2.860e-04  Data: 1.722 (1.722)
Train: 193 [  50/1251 (  4%)]  Loss: 3.211 (3.37)  Time: 0.658s, 1557.12/s  (0.684s, 1497.71/s)  LR: 2.858e-04  Data: 0.013 (0.048)
Train: 193 [ 100/1251 (  8%)]  Loss: 3.112 (3.28)  Time: 0.664s, 1541.84/s  (0.670s, 1528.24/s)  LR: 2.856e-04  Data: 0.013 (0.031)
Train: 193 [ 150/1251 ( 12%)]  Loss: 3.176 (3.26)  Time: 0.665s, 1540.98/s  (0.667s, 1535.18/s)  LR: 2.854e-04  Data: 0.014 (0.025)
Train: 193 [ 200/1251 ( 16%)]  Loss: 3.047 (3.21)  Time: 0.664s, 1541.16/s  (0.666s, 1537.46/s)  LR: 2.852e-04  Data: 0.013 (0.023)
Train: 193 [ 250/1251 ( 20%)]  Loss: 3.024 (3.18)  Time: 0.665s, 1539.80/s  (0.666s, 1538.02/s)  LR: 2.850e-04  Data: 0.013 (0.021)
Train: 193 [ 300/1251 ( 24%)]  Loss: 3.402 (3.21)  Time: 0.660s, 1552.33/s  (0.666s, 1537.97/s)  LR: 2.848e-04  Data: 0.012 (0.020)
Train: 193 [ 350/1251 ( 28%)]  Loss: 2.981 (3.18)  Time: 0.667s, 1534.19/s  (0.666s, 1537.86/s)  LR: 2.847e-04  Data: 0.013 (0.019)
Train: 193 [ 400/1251 ( 32%)]  Loss: 3.126 (3.18)  Time: 0.665s, 1539.55/s  (0.666s, 1537.53/s)  LR: 2.845e-04  Data: 0.013 (0.018)
Train: 193 [ 450/1251 ( 36%)]  Loss: 3.138 (3.17)  Time: 0.674s, 1518.99/s  (0.666s, 1536.89/s)  LR: 2.843e-04  Data: 0.014 (0.017)
Train: 193 [ 500/1251 ( 40%)]  Loss: 2.983 (3.16)  Time: 0.660s, 1552.29/s  (0.666s, 1536.76/s)  LR: 2.841e-04  Data: 0.014 (0.017)
Train: 193 [ 550/1251 ( 44%)]  Loss: 3.494 (3.18)  Time: 0.671s, 1524.96/s  (0.667s, 1536.33/s)  LR: 2.839e-04  Data: 0.013 (0.017)
Train: 193 [ 600/1251 ( 48%)]  Loss: 3.433 (3.20)  Time: 0.669s, 1531.41/s  (0.667s, 1536.10/s)  LR: 2.837e-04  Data: 0.013 (0.017)
Train: 193 [ 650/1251 ( 52%)]  Loss: 3.583 (3.23)  Time: 0.665s, 1540.42/s  (0.666s, 1536.47/s)  LR: 2.835e-04  Data: 0.013 (0.016)
Train: 193 [ 700/1251 ( 56%)]  Loss: 2.825 (3.20)  Time: 0.664s, 1541.35/s  (0.666s, 1536.77/s)  LR: 2.833e-04  Data: 0.016 (0.016)
Train: 193 [ 750/1251 ( 60%)]  Loss: 3.275 (3.21)  Time: 0.663s, 1544.87/s  (0.666s, 1536.80/s)  LR: 2.832e-04  Data: 0.014 (0.016)
Train: 193 [ 800/1251 ( 64%)]  Loss: 3.206 (3.21)  Time: 0.661s, 1549.26/s  (0.666s, 1536.86/s)  LR: 2.830e-04  Data: 0.014 (0.016)
Train: 193 [ 850/1251 ( 68%)]  Loss: 3.371 (3.22)  Time: 0.667s, 1534.89/s  (0.666s, 1537.13/s)  LR: 2.828e-04  Data: 0.014 (0.016)
Train: 193 [ 900/1251 ( 72%)]  Loss: 3.086 (3.21)  Time: 0.664s, 1541.50/s  (0.666s, 1537.34/s)  LR: 2.826e-04  Data: 0.016 (0.016)
Train: 193 [ 950/1251 ( 76%)]  Loss: 3.604 (3.23)  Time: 0.676s, 1515.67/s  (0.666s, 1537.47/s)  LR: 2.824e-04  Data: 0.014 (0.016)
Train: 193 [1000/1251 ( 80%)]  Loss: 3.092 (3.22)  Time: 0.666s, 1538.26/s  (0.666s, 1537.61/s)  LR: 2.822e-04  Data: 0.013 (0.015)
Train: 193 [1050/1251 ( 84%)]  Loss: 3.409 (3.23)  Time: 0.664s, 1543.12/s  (0.666s, 1537.73/s)  LR: 2.820e-04  Data: 0.013 (0.015)
Train: 193 [1100/1251 ( 88%)]  Loss: 3.421 (3.24)  Time: 0.659s, 1553.08/s  (0.666s, 1537.91/s)  LR: 2.819e-04  Data: 0.014 (0.015)
Train: 193 [1150/1251 ( 92%)]  Loss: 3.314 (3.24)  Time: 0.652s, 1571.45/s  (0.666s, 1538.19/s)  LR: 2.817e-04  Data: 0.013 (0.015)
Train: 193 [1200/1251 ( 96%)]  Loss: 3.036 (3.23)  Time: 0.654s, 1565.36/s  (0.666s, 1538.47/s)  LR: 2.815e-04  Data: 0.013 (0.015)
Train: 193 [1250/1251 (100%)]  Loss: 3.365 (3.24)  Time: 0.650s, 1575.97/s  (0.665s, 1538.71/s)  LR: 2.813e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.896 (2.896)  Loss:  0.4526 (0.4526)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.161 (0.321)  Loss:  0.5752 (0.9175)  Acc@1: 85.9670 (78.6160)  Acc@5: 97.4057 (94.5840)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-193.pth.tar', 78.61600010986328)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-192.pth.tar', 78.58999998291016)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-188.pth.tar', 78.526)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-191.pth.tar', 78.50800000488282)

Train: 194 [   0/1251 (  0%)]  Loss: 3.092 (3.09)  Time: 3.355s,  305.24/s  (3.355s,  305.24/s)  LR: 2.813e-04  Data: 1.920 (1.920)
Train: 194 [  50/1251 (  4%)]  Loss: 3.528 (3.31)  Time: 0.650s, 1576.16/s  (0.681s, 1504.60/s)  LR: 2.811e-04  Data: 0.016 (0.052)
Train: 194 [ 100/1251 (  8%)]  Loss: 3.492 (3.37)  Time: 0.673s, 1522.40/s  (0.669s, 1530.38/s)  LR: 2.809e-04  Data: 0.017 (0.033)
Train: 194 [ 150/1251 ( 12%)]  Loss: 3.350 (3.37)  Time: 0.660s, 1551.14/s  (0.667s, 1536.33/s)  LR: 2.807e-04  Data: 0.013 (0.027)
Train: 194 [ 200/1251 ( 16%)]  Loss: 3.246 (3.34)  Time: 0.664s, 1543.10/s  (0.665s, 1539.24/s)  LR: 2.805e-04  Data: 0.013 (0.023)
Train: 194 [ 250/1251 ( 20%)]  Loss: 3.043 (3.29)  Time: 0.671s, 1525.11/s  (0.665s, 1539.34/s)  LR: 2.804e-04  Data: 0.014 (0.022)
Train: 194 [ 300/1251 ( 24%)]  Loss: 3.011 (3.25)  Time: 0.656s, 1561.39/s  (0.665s, 1540.12/s)  LR: 2.802e-04  Data: 0.013 (0.020)
Train: 194 [ 350/1251 ( 28%)]  Loss: 3.309 (3.26)  Time: 0.667s, 1536.08/s  (0.665s, 1539.98/s)  LR: 2.800e-04  Data: 0.013 (0.019)
Train: 194 [ 400/1251 ( 32%)]  Loss: 3.274 (3.26)  Time: 0.666s, 1536.52/s  (0.665s, 1539.41/s)  LR: 2.798e-04  Data: 0.014 (0.019)
Train: 194 [ 450/1251 ( 36%)]  Loss: 3.255 (3.26)  Time: 0.663s, 1544.43/s  (0.665s, 1539.31/s)  LR: 2.796e-04  Data: 0.017 (0.018)
Train: 194 [ 500/1251 ( 40%)]  Loss: 3.397 (3.27)  Time: 0.648s, 1579.84/s  (0.665s, 1539.58/s)  LR: 2.794e-04  Data: 0.014 (0.018)
Train: 194 [ 550/1251 ( 44%)]  Loss: 3.333 (3.28)  Time: 0.666s, 1538.53/s  (0.665s, 1539.97/s)  LR: 2.792e-04  Data: 0.014 (0.017)
Train: 194 [ 600/1251 ( 48%)]  Loss: 3.433 (3.29)  Time: 0.662s, 1546.93/s  (0.665s, 1540.42/s)  LR: 2.791e-04  Data: 0.014 (0.017)
Train: 194 [ 650/1251 ( 52%)]  Loss: 2.878 (3.26)  Time: 0.665s, 1539.95/s  (0.665s, 1540.80/s)  LR: 2.789e-04  Data: 0.014 (0.017)
Train: 194 [ 700/1251 ( 56%)]  Loss: 3.183 (3.26)  Time: 0.657s, 1557.91/s  (0.665s, 1540.73/s)  LR: 2.787e-04  Data: 0.016 (0.017)
Train: 194 [ 750/1251 ( 60%)]  Loss: 3.348 (3.26)  Time: 0.675s, 1517.23/s  (0.665s, 1540.71/s)  LR: 2.785e-04  Data: 0.014 (0.016)
Train: 194 [ 800/1251 ( 64%)]  Loss: 3.549 (3.28)  Time: 0.673s, 1522.38/s  (0.665s, 1540.47/s)  LR: 2.783e-04  Data: 0.013 (0.016)
Train: 194 [ 850/1251 ( 68%)]  Loss: 3.571 (3.29)  Time: 0.666s, 1538.13/s  (0.665s, 1540.09/s)  LR: 2.781e-04  Data: 0.014 (0.016)
Train: 194 [ 900/1251 ( 72%)]  Loss: 3.494 (3.30)  Time: 0.667s, 1534.59/s  (0.665s, 1540.25/s)  LR: 2.779e-04  Data: 0.013 (0.016)
Train: 194 [ 950/1251 ( 76%)]  Loss: 3.100 (3.29)  Time: 0.664s, 1542.99/s  (0.665s, 1540.18/s)  LR: 2.778e-04  Data: 0.013 (0.016)
Train: 194 [1000/1251 ( 80%)]  Loss: 3.499 (3.30)  Time: 0.669s, 1530.93/s  (0.665s, 1540.36/s)  LR: 2.776e-04  Data: 0.013 (0.016)
Train: 194 [1050/1251 ( 84%)]  Loss: 3.473 (3.31)  Time: 0.666s, 1537.18/s  (0.665s, 1540.39/s)  LR: 2.774e-04  Data: 0.013 (0.016)
Train: 194 [1100/1251 ( 88%)]  Loss: 3.454 (3.32)  Time: 0.666s, 1537.77/s  (0.665s, 1540.32/s)  LR: 2.772e-04  Data: 0.014 (0.015)
Train: 194 [1150/1251 ( 92%)]  Loss: 3.379 (3.32)  Time: 0.673s, 1521.77/s  (0.665s, 1540.31/s)  LR: 2.770e-04  Data: 0.016 (0.015)
Train: 194 [1200/1251 ( 96%)]  Loss: 3.005 (3.31)  Time: 0.659s, 1553.35/s  (0.665s, 1540.22/s)  LR: 2.768e-04  Data: 0.013 (0.015)
Train: 194 [1250/1251 (100%)]  Loss: 3.040 (3.30)  Time: 0.652s, 1569.89/s  (0.665s, 1540.24/s)  LR: 2.766e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.940 (2.940)  Loss:  0.4207 (0.4207)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.162 (0.318)  Loss:  0.5723 (0.9053)  Acc@1: 86.0849 (78.6260)  Acc@5: 97.6415 (94.6300)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-194.pth.tar', 78.62600003173829)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-193.pth.tar', 78.61600010986328)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-192.pth.tar', 78.58999998291016)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-188.pth.tar', 78.526)

Train: 195 [   0/1251 (  0%)]  Loss: 3.207 (3.21)  Time: 3.283s,  311.93/s  (3.283s,  311.93/s)  LR: 2.766e-04  Data: 1.650 (1.650)
Train: 195 [  50/1251 (  4%)]  Loss: 2.838 (3.02)  Time: 0.658s, 1556.53/s  (0.684s, 1497.18/s)  LR: 2.765e-04  Data: 0.015 (0.046)
Train: 195 [ 100/1251 (  8%)]  Loss: 3.433 (3.16)  Time: 0.666s, 1536.88/s  (0.672s, 1524.74/s)  LR: 2.763e-04  Data: 0.013 (0.030)
Train: 195 [ 150/1251 ( 12%)]  Loss: 2.967 (3.11)  Time: 0.666s, 1536.45/s  (0.669s, 1530.88/s)  LR: 2.761e-04  Data: 0.017 (0.025)
Train: 195 [ 200/1251 ( 16%)]  Loss: 3.381 (3.17)  Time: 0.665s, 1540.33/s  (0.668s, 1533.46/s)  LR: 2.759e-04  Data: 0.014 (0.022)
Train: 195 [ 250/1251 ( 20%)]  Loss: 3.304 (3.19)  Time: 0.669s, 1531.34/s  (0.667s, 1534.60/s)  LR: 2.757e-04  Data: 0.014 (0.021)
Train: 195 [ 300/1251 ( 24%)]  Loss: 3.279 (3.20)  Time: 0.676s, 1514.33/s  (0.667s, 1534.67/s)  LR: 2.755e-04  Data: 0.012 (0.019)
Train: 195 [ 350/1251 ( 28%)]  Loss: 3.137 (3.19)  Time: 0.667s, 1534.79/s  (0.667s, 1535.47/s)  LR: 2.753e-04  Data: 0.013 (0.019)
Train: 195 [ 400/1251 ( 32%)]  Loss: 3.173 (3.19)  Time: 0.666s, 1537.70/s  (0.667s, 1536.20/s)  LR: 2.752e-04  Data: 0.013 (0.018)
Train: 195 [ 450/1251 ( 36%)]  Loss: 3.414 (3.21)  Time: 0.660s, 1552.25/s  (0.666s, 1536.84/s)  LR: 2.750e-04  Data: 0.013 (0.018)
Train: 195 [ 500/1251 ( 40%)]  Loss: 3.286 (3.22)  Time: 0.658s, 1555.73/s  (0.666s, 1537.27/s)  LR: 2.748e-04  Data: 0.013 (0.017)
Train: 195 [ 550/1251 ( 44%)]  Loss: 3.359 (3.23)  Time: 0.666s, 1537.65/s  (0.666s, 1537.58/s)  LR: 2.746e-04  Data: 0.014 (0.017)
Train: 195 [ 600/1251 ( 48%)]  Loss: 3.497 (3.25)  Time: 0.652s, 1570.83/s  (0.666s, 1537.33/s)  LR: 2.744e-04  Data: 0.012 (0.017)
Train: 195 [ 650/1251 ( 52%)]  Loss: 3.362 (3.26)  Time: 0.668s, 1532.68/s  (0.666s, 1537.20/s)  LR: 2.742e-04  Data: 0.016 (0.017)
Train: 195 [ 700/1251 ( 56%)]  Loss: 3.577 (3.28)  Time: 0.659s, 1553.20/s  (0.666s, 1537.29/s)  LR: 2.740e-04  Data: 0.014 (0.016)
Train: 195 [ 750/1251 ( 60%)]  Loss: 3.413 (3.29)  Time: 0.665s, 1540.26/s  (0.666s, 1537.48/s)  LR: 2.739e-04  Data: 0.013 (0.016)
Train: 195 [ 800/1251 ( 64%)]  Loss: 3.006 (3.27)  Time: 0.669s, 1530.07/s  (0.666s, 1537.48/s)  LR: 2.737e-04  Data: 0.016 (0.016)
Train: 195 [ 850/1251 ( 68%)]  Loss: 3.324 (3.28)  Time: 0.656s, 1559.79/s  (0.666s, 1537.46/s)  LR: 2.735e-04  Data: 0.015 (0.016)
Train: 195 [ 900/1251 ( 72%)]  Loss: 2.921 (3.26)  Time: 0.671s, 1525.97/s  (0.666s, 1537.32/s)  LR: 2.733e-04  Data: 0.013 (0.016)
Train: 195 [ 950/1251 ( 76%)]  Loss: 3.186 (3.25)  Time: 0.665s, 1539.65/s  (0.666s, 1537.10/s)  LR: 2.731e-04  Data: 0.014 (0.016)
Train: 195 [1000/1251 ( 80%)]  Loss: 3.406 (3.26)  Time: 0.663s, 1545.53/s  (0.666s, 1537.05/s)  LR: 2.729e-04  Data: 0.014 (0.016)
Train: 195 [1050/1251 ( 84%)]  Loss: 3.212 (3.26)  Time: 0.666s, 1537.11/s  (0.666s, 1536.97/s)  LR: 2.727e-04  Data: 0.014 (0.016)
Train: 195 [1100/1251 ( 88%)]  Loss: 3.136 (3.25)  Time: 0.665s, 1540.02/s  (0.666s, 1536.79/s)  LR: 2.726e-04  Data: 0.014 (0.015)
Train: 195 [1150/1251 ( 92%)]  Loss: 3.464 (3.26)  Time: 0.672s, 1523.39/s  (0.666s, 1536.81/s)  LR: 2.724e-04  Data: 0.016 (0.015)
Train: 195 [1200/1251 ( 96%)]  Loss: 3.435 (3.27)  Time: 0.672s, 1523.81/s  (0.666s, 1536.52/s)  LR: 2.722e-04  Data: 0.014 (0.015)
Train: 195 [1250/1251 (100%)]  Loss: 3.455 (3.28)  Time: 0.663s, 1545.61/s  (0.666s, 1536.43/s)  LR: 2.720e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.898 (2.898)  Loss:  0.4451 (0.4451)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.161 (0.319)  Loss:  0.5596 (0.9112)  Acc@1: 87.1462 (78.5740)  Acc@5: 97.8774 (94.5980)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-194.pth.tar', 78.62600003173829)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-193.pth.tar', 78.61600010986328)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-192.pth.tar', 78.58999998291016)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-195.pth.tar', 78.57400010498047)

Train: 196 [   0/1251 (  0%)]  Loss: 2.919 (2.92)  Time: 3.140s,  326.12/s  (3.140s,  326.12/s)  LR: 2.720e-04  Data: 1.769 (1.769)
Train: 196 [  50/1251 (  4%)]  Loss: 3.385 (3.15)  Time: 0.650s, 1576.53/s  (0.678s, 1510.85/s)  LR: 2.718e-04  Data: 0.014 (0.048)
Train: 196 [ 100/1251 (  8%)]  Loss: 3.207 (3.17)  Time: 0.662s, 1545.72/s  (0.667s, 1535.38/s)  LR: 2.716e-04  Data: 0.013 (0.032)
Train: 196 [ 150/1251 ( 12%)]  Loss: 3.341 (3.21)  Time: 0.667s, 1535.15/s  (0.666s, 1537.39/s)  LR: 2.715e-04  Data: 0.013 (0.026)
Train: 196 [ 200/1251 ( 16%)]  Loss: 3.063 (3.18)  Time: 0.666s, 1536.73/s  (0.666s, 1538.67/s)  LR: 2.713e-04  Data: 0.014 (0.023)
Train: 196 [ 250/1251 ( 20%)]  Loss: 3.397 (3.22)  Time: 0.672s, 1523.98/s  (0.666s, 1538.46/s)  LR: 2.711e-04  Data: 0.016 (0.021)
Train: 196 [ 300/1251 ( 24%)]  Loss: 3.322 (3.23)  Time: 0.662s, 1547.75/s  (0.666s, 1537.78/s)  LR: 2.709e-04  Data: 0.013 (0.020)
Train: 196 [ 350/1251 ( 28%)]  Loss: 3.422 (3.26)  Time: 0.670s, 1528.53/s  (0.666s, 1537.55/s)  LR: 2.707e-04  Data: 0.015 (0.019)
Train: 196 [ 400/1251 ( 32%)]  Loss: 3.557 (3.29)  Time: 0.671s, 1527.03/s  (0.666s, 1537.31/s)  LR: 2.705e-04  Data: 0.012 (0.018)
Train: 196 [ 450/1251 ( 36%)]  Loss: 3.484 (3.31)  Time: 0.662s, 1546.26/s  (0.666s, 1537.28/s)  LR: 2.703e-04  Data: 0.013 (0.018)
Train: 196 [ 500/1251 ( 40%)]  Loss: 3.554 (3.33)  Time: 0.667s, 1535.61/s  (0.666s, 1537.44/s)  LR: 2.702e-04  Data: 0.013 (0.017)
Train: 196 [ 550/1251 ( 44%)]  Loss: 3.258 (3.33)  Time: 0.664s, 1541.95/s  (0.666s, 1537.32/s)  LR: 2.700e-04  Data: 0.013 (0.017)
Train: 196 [ 600/1251 ( 48%)]  Loss: 3.254 (3.32)  Time: 0.666s, 1536.68/s  (0.666s, 1537.86/s)  LR: 2.698e-04  Data: 0.013 (0.017)
Train: 196 [ 650/1251 ( 52%)]  Loss: 3.202 (3.31)  Time: 0.664s, 1541.51/s  (0.666s, 1537.93/s)  LR: 2.696e-04  Data: 0.013 (0.017)
Train: 196 [ 700/1251 ( 56%)]  Loss: 3.455 (3.32)  Time: 0.658s, 1557.05/s  (0.666s, 1538.04/s)  LR: 2.694e-04  Data: 0.013 (0.017)
Train: 196 [ 750/1251 ( 60%)]  Loss: 3.116 (3.31)  Time: 0.663s, 1543.36/s  (0.666s, 1538.05/s)  LR: 2.692e-04  Data: 0.014 (0.016)
Train: 196 [ 800/1251 ( 64%)]  Loss: 3.020 (3.29)  Time: 0.665s, 1540.00/s  (0.666s, 1537.72/s)  LR: 2.691e-04  Data: 0.012 (0.016)
Train: 196 [ 850/1251 ( 68%)]  Loss: 3.395 (3.30)  Time: 0.665s, 1539.02/s  (0.666s, 1537.43/s)  LR: 2.689e-04  Data: 0.014 (0.016)
Train: 196 [ 900/1251 ( 72%)]  Loss: 3.323 (3.30)  Time: 0.664s, 1542.67/s  (0.666s, 1537.26/s)  LR: 2.687e-04  Data: 0.014 (0.016)
Train: 196 [ 950/1251 ( 76%)]  Loss: 3.253 (3.30)  Time: 0.676s, 1515.91/s  (0.666s, 1537.27/s)  LR: 2.685e-04  Data: 0.013 (0.016)
Train: 196 [1000/1251 ( 80%)]  Loss: 3.240 (3.29)  Time: 0.660s, 1552.08/s  (0.666s, 1536.94/s)  LR: 2.683e-04  Data: 0.013 (0.016)
Train: 196 [1050/1251 ( 84%)]  Loss: 3.448 (3.30)  Time: 0.665s, 1539.20/s  (0.666s, 1536.66/s)  LR: 2.681e-04  Data: 0.018 (0.016)
Train: 196 [1100/1251 ( 88%)]  Loss: 3.229 (3.30)  Time: 0.670s, 1529.35/s  (0.666s, 1536.54/s)  LR: 2.680e-04  Data: 0.012 (0.016)
Train: 196 [1150/1251 ( 92%)]  Loss: 3.066 (3.29)  Time: 0.669s, 1530.02/s  (0.667s, 1536.25/s)  LR: 2.678e-04  Data: 0.013 (0.016)
Train: 196 [1200/1251 ( 96%)]  Loss: 3.591 (3.30)  Time: 0.669s, 1531.12/s  (0.667s, 1536.28/s)  LR: 2.676e-04  Data: 0.013 (0.015)
Train: 196 [1250/1251 (100%)]  Loss: 3.187 (3.30)  Time: 0.658s, 1556.81/s  (0.667s, 1536.32/s)  LR: 2.674e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.855 (2.855)  Loss:  0.4441 (0.4441)  Acc@1: 91.3086 (91.3086)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.168 (0.323)  Loss:  0.5542 (0.8985)  Acc@1: 86.5566 (78.7500)  Acc@5: 97.2877 (94.6760)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-196.pth.tar', 78.74999997802735)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-194.pth.tar', 78.62600003173829)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-193.pth.tar', 78.61600010986328)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-192.pth.tar', 78.58999998291016)

Train: 197 [   0/1251 (  0%)]  Loss: 3.219 (3.22)  Time: 3.108s,  329.51/s  (3.108s,  329.51/s)  LR: 2.674e-04  Data: 1.811 (1.811)
Train: 197 [  50/1251 (  4%)]  Loss: 3.605 (3.41)  Time: 0.653s, 1569.19/s  (0.675s, 1516.60/s)  LR: 2.672e-04  Data: 0.017 (0.049)
Train: 197 [ 100/1251 (  8%)]  Loss: 3.378 (3.40)  Time: 0.663s, 1543.89/s  (0.666s, 1538.42/s)  LR: 2.670e-04  Data: 0.014 (0.032)
Train: 197 [ 150/1251 ( 12%)]  Loss: 3.422 (3.41)  Time: 0.662s, 1545.86/s  (0.664s, 1542.57/s)  LR: 2.669e-04  Data: 0.014 (0.026)
Train: 197 [ 200/1251 ( 16%)]  Loss: 3.205 (3.37)  Time: 0.665s, 1540.79/s  (0.664s, 1542.77/s)  LR: 2.667e-04  Data: 0.014 (0.023)
Train: 197 [ 250/1251 ( 20%)]  Loss: 3.207 (3.34)  Time: 0.660s, 1551.19/s  (0.664s, 1542.82/s)  LR: 2.665e-04  Data: 0.014 (0.021)
Train: 197 [ 300/1251 ( 24%)]  Loss: 3.039 (3.30)  Time: 0.667s, 1535.94/s  (0.664s, 1542.44/s)  LR: 2.663e-04  Data: 0.014 (0.020)
Train: 197 [ 350/1251 ( 28%)]  Loss: 3.059 (3.27)  Time: 0.664s, 1542.54/s  (0.664s, 1541.70/s)  LR: 2.661e-04  Data: 0.013 (0.019)
Train: 197 [ 400/1251 ( 32%)]  Loss: 3.309 (3.27)  Time: 0.663s, 1544.28/s  (0.664s, 1541.48/s)  LR: 2.659e-04  Data: 0.017 (0.019)
Train: 197 [ 450/1251 ( 36%)]  Loss: 3.483 (3.29)  Time: 0.657s, 1557.50/s  (0.664s, 1541.53/s)  LR: 2.658e-04  Data: 0.014 (0.018)
Train: 197 [ 500/1251 ( 40%)]  Loss: 3.279 (3.29)  Time: 0.661s, 1549.07/s  (0.664s, 1541.71/s)  LR: 2.656e-04  Data: 0.015 (0.018)
Train: 197 [ 550/1251 ( 44%)]  Loss: 3.579 (3.32)  Time: 0.660s, 1551.58/s  (0.664s, 1541.69/s)  LR: 2.654e-04  Data: 0.013 (0.017)
Train: 197 [ 600/1251 ( 48%)]  Loss: 3.525 (3.33)  Time: 0.675s, 1517.00/s  (0.664s, 1541.23/s)  LR: 2.652e-04  Data: 0.013 (0.017)
Train: 197 [ 650/1251 ( 52%)]  Loss: 3.468 (3.34)  Time: 0.652s, 1569.36/s  (0.665s, 1540.99/s)  LR: 2.650e-04  Data: 0.019 (0.017)
Train: 197 [ 700/1251 ( 56%)]  Loss: 3.310 (3.34)  Time: 0.660s, 1550.78/s  (0.665s, 1540.66/s)  LR: 2.648e-04  Data: 0.014 (0.017)
Train: 197 [ 750/1251 ( 60%)]  Loss: 3.511 (3.35)  Time: 0.671s, 1526.98/s  (0.665s, 1540.25/s)  LR: 2.647e-04  Data: 0.014 (0.016)
Train: 197 [ 800/1251 ( 64%)]  Loss: 3.495 (3.36)  Time: 0.670s, 1527.84/s  (0.665s, 1539.79/s)  LR: 2.645e-04  Data: 0.013 (0.016)
Train: 197 [ 850/1251 ( 68%)]  Loss: 3.072 (3.34)  Time: 0.672s, 1524.75/s  (0.665s, 1539.57/s)  LR: 2.643e-04  Data: 0.016 (0.016)
Train: 197 [ 900/1251 ( 72%)]  Loss: 3.430 (3.35)  Time: 0.665s, 1540.20/s  (0.665s, 1539.39/s)  LR: 2.641e-04  Data: 0.013 (0.016)
Train: 197 [ 950/1251 ( 76%)]  Loss: 3.575 (3.36)  Time: 0.660s, 1551.10/s  (0.665s, 1539.35/s)  LR: 2.639e-04  Data: 0.013 (0.016)
Train: 197 [1000/1251 ( 80%)]  Loss: 3.219 (3.35)  Time: 0.682s, 1502.42/s  (0.665s, 1539.21/s)  LR: 2.637e-04  Data: 0.014 (0.016)
Train: 197 [1050/1251 ( 84%)]  Loss: 3.212 (3.35)  Time: 0.657s, 1559.65/s  (0.665s, 1539.42/s)  LR: 2.636e-04  Data: 0.015 (0.016)
Train: 197 [1100/1251 ( 88%)]  Loss: 3.177 (3.34)  Time: 0.666s, 1537.51/s  (0.665s, 1539.46/s)  LR: 2.634e-04  Data: 0.013 (0.016)
Train: 197 [1150/1251 ( 92%)]  Loss: 2.990 (3.32)  Time: 0.666s, 1537.83/s  (0.665s, 1539.55/s)  LR: 2.632e-04  Data: 0.013 (0.016)
Train: 197 [1200/1251 ( 96%)]  Loss: 3.129 (3.32)  Time: 0.664s, 1542.76/s  (0.665s, 1539.59/s)  LR: 2.630e-04  Data: 0.013 (0.016)
Train: 197 [1250/1251 (100%)]  Loss: 3.389 (3.32)  Time: 0.659s, 1554.99/s  (0.665s, 1539.60/s)  LR: 2.628e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.854 (2.854)  Loss:  0.4736 (0.4736)  Acc@1: 90.7227 (90.7227)  Acc@5: 97.9492 (97.9492)
Test: [  48/48]  Time: 0.163 (0.326)  Loss:  0.5747 (0.9071)  Acc@1: 85.6132 (78.7040)  Acc@5: 97.6415 (94.6740)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-196.pth.tar', 78.74999997802735)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-197.pth.tar', 78.70400008544922)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-194.pth.tar', 78.62600003173829)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-193.pth.tar', 78.61600010986328)

Train: 198 [   0/1251 (  0%)]  Loss: 3.157 (3.16)  Time: 3.295s,  310.76/s  (3.295s,  310.76/s)  LR: 2.628e-04  Data: 1.828 (1.828)
Train: 198 [  50/1251 (  4%)]  Loss: 3.234 (3.20)  Time: 0.656s, 1562.01/s  (0.684s, 1496.08/s)  LR: 2.626e-04  Data: 0.013 (0.049)
Train: 198 [ 100/1251 (  8%)]  Loss: 3.351 (3.25)  Time: 0.665s, 1539.10/s  (0.673s, 1521.23/s)  LR: 2.625e-04  Data: 0.012 (0.032)
Train: 198 [ 150/1251 ( 12%)]  Loss: 3.371 (3.28)  Time: 0.677s, 1513.35/s  (0.670s, 1527.74/s)  LR: 2.623e-04  Data: 0.013 (0.026)
Train: 198 [ 200/1251 ( 16%)]  Loss: 2.977 (3.22)  Time: 0.671s, 1525.61/s  (0.669s, 1530.58/s)  LR: 2.621e-04  Data: 0.015 (0.023)
Train: 198 [ 250/1251 ( 20%)]  Loss: 3.204 (3.22)  Time: 0.667s, 1534.20/s  (0.669s, 1531.36/s)  LR: 2.619e-04  Data: 0.012 (0.021)
Train: 198 [ 300/1251 ( 24%)]  Loss: 3.191 (3.21)  Time: 0.666s, 1537.31/s  (0.669s, 1531.13/s)  LR: 2.617e-04  Data: 0.014 (0.020)
Train: 198 [ 350/1251 ( 28%)]  Loss: 3.255 (3.22)  Time: 0.671s, 1526.85/s  (0.669s, 1530.99/s)  LR: 2.615e-04  Data: 0.017 (0.019)
Train: 198 [ 400/1251 ( 32%)]  Loss: 3.125 (3.21)  Time: 0.668s, 1532.81/s  (0.669s, 1531.64/s)  LR: 2.614e-04  Data: 0.013 (0.018)
Train: 198 [ 450/1251 ( 36%)]  Loss: 3.382 (3.22)  Time: 0.665s, 1540.88/s  (0.668s, 1531.98/s)  LR: 2.612e-04  Data: 0.014 (0.018)
Train: 198 [ 500/1251 ( 40%)]  Loss: 3.520 (3.25)  Time: 0.671s, 1525.45/s  (0.668s, 1532.72/s)  LR: 2.610e-04  Data: 0.013 (0.017)
Train: 198 [ 550/1251 ( 44%)]  Loss: 3.119 (3.24)  Time: 0.673s, 1521.87/s  (0.668s, 1532.68/s)  LR: 2.608e-04  Data: 0.013 (0.017)
Train: 198 [ 600/1251 ( 48%)]  Loss: 3.297 (3.24)  Time: 0.672s, 1524.67/s  (0.668s, 1532.86/s)  LR: 2.606e-04  Data: 0.014 (0.017)
Train: 198 [ 650/1251 ( 52%)]  Loss: 3.317 (3.25)  Time: 0.661s, 1548.99/s  (0.668s, 1533.43/s)  LR: 2.605e-04  Data: 0.013 (0.016)
Train: 198 [ 700/1251 ( 56%)]  Loss: 3.215 (3.25)  Time: 0.664s, 1541.28/s  (0.668s, 1533.96/s)  LR: 2.603e-04  Data: 0.013 (0.016)
Train: 198 [ 750/1251 ( 60%)]  Loss: 3.172 (3.24)  Time: 0.668s, 1532.10/s  (0.668s, 1533.96/s)  LR: 2.601e-04  Data: 0.013 (0.016)
Train: 198 [ 800/1251 ( 64%)]  Loss: 3.192 (3.24)  Time: 0.670s, 1528.39/s  (0.668s, 1534.01/s)  LR: 2.599e-04  Data: 0.012 (0.016)
Train: 198 [ 850/1251 ( 68%)]  Loss: 3.524 (3.26)  Time: 0.672s, 1524.58/s  (0.667s, 1534.30/s)  LR: 2.597e-04  Data: 0.013 (0.016)
Train: 198 [ 900/1251 ( 72%)]  Loss: 3.100 (3.25)  Time: 0.663s, 1543.44/s  (0.667s, 1534.64/s)  LR: 2.595e-04  Data: 0.016 (0.016)
Train: 198 [ 950/1251 ( 76%)]  Loss: 3.507 (3.26)  Time: 0.665s, 1539.97/s  (0.667s, 1534.79/s)  LR: 2.594e-04  Data: 0.013 (0.015)
Train: 198 [1000/1251 ( 80%)]  Loss: 3.350 (3.26)  Time: 0.668s, 1532.82/s  (0.667s, 1535.03/s)  LR: 2.592e-04  Data: 0.013 (0.015)
Train: 198 [1050/1251 ( 84%)]  Loss: 2.958 (3.25)  Time: 0.664s, 1543.22/s  (0.667s, 1535.35/s)  LR: 2.590e-04  Data: 0.013 (0.015)
Train: 198 [1100/1251 ( 88%)]  Loss: 3.252 (3.25)  Time: 0.658s, 1555.07/s  (0.667s, 1535.59/s)  LR: 2.588e-04  Data: 0.015 (0.015)
Train: 198 [1150/1251 ( 92%)]  Loss: 3.139 (3.25)  Time: 0.667s, 1535.01/s  (0.667s, 1535.65/s)  LR: 2.586e-04  Data: 0.013 (0.015)
Train: 198 [1200/1251 ( 96%)]  Loss: 3.333 (3.25)  Time: 0.677s, 1512.28/s  (0.667s, 1535.59/s)  LR: 2.585e-04  Data: 0.015 (0.015)
Train: 198 [1250/1251 (100%)]  Loss: 3.007 (3.24)  Time: 0.649s, 1576.91/s  (0.667s, 1535.58/s)  LR: 2.583e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.838 (2.838)  Loss:  0.4307 (0.4307)  Acc@1: 91.2109 (91.2109)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.166 (0.316)  Loss:  0.5361 (0.8992)  Acc@1: 86.6745 (78.6400)  Acc@5: 97.7594 (94.7040)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-196.pth.tar', 78.74999997802735)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-197.pth.tar', 78.70400008544922)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-198.pth.tar', 78.64000015869141)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-194.pth.tar', 78.62600003173829)

Train: 199 [   0/1251 (  0%)]  Loss: 2.838 (2.84)  Time: 3.341s,  306.50/s  (3.341s,  306.50/s)  LR: 2.583e-04  Data: 1.803 (1.803)
Train: 199 [  50/1251 (  4%)]  Loss: 3.223 (3.03)  Time: 0.654s, 1565.93/s  (0.684s, 1496.82/s)  LR: 2.581e-04  Data: 0.015 (0.049)
Train: 199 [ 100/1251 (  8%)]  Loss: 3.495 (3.19)  Time: 0.653s, 1568.57/s  (0.672s, 1524.84/s)  LR: 2.579e-04  Data: 0.013 (0.031)
Train: 199 [ 150/1251 ( 12%)]  Loss: 3.107 (3.17)  Time: 0.663s, 1543.72/s  (0.669s, 1530.52/s)  LR: 2.577e-04  Data: 0.013 (0.025)
Train: 199 [ 200/1251 ( 16%)]  Loss: 3.373 (3.21)  Time: 0.667s, 1535.13/s  (0.668s, 1533.49/s)  LR: 2.575e-04  Data: 0.013 (0.022)
Train: 199 [ 250/1251 ( 20%)]  Loss: 3.316 (3.23)  Time: 0.666s, 1537.97/s  (0.667s, 1534.17/s)  LR: 2.574e-04  Data: 0.013 (0.021)
Train: 199 [ 300/1251 ( 24%)]  Loss: 3.087 (3.21)  Time: 0.670s, 1527.61/s  (0.667s, 1534.36/s)  LR: 2.572e-04  Data: 0.014 (0.019)
Train: 199 [ 350/1251 ( 28%)]  Loss: 3.205 (3.21)  Time: 0.669s, 1529.62/s  (0.667s, 1535.00/s)  LR: 2.570e-04  Data: 0.013 (0.019)
Train: 199 [ 400/1251 ( 32%)]  Loss: 3.101 (3.19)  Time: 0.664s, 1543.19/s  (0.667s, 1535.62/s)  LR: 2.568e-04  Data: 0.016 (0.018)
Train: 199 [ 450/1251 ( 36%)]  Loss: 2.921 (3.17)  Time: 0.669s, 1530.30/s  (0.667s, 1535.79/s)  LR: 2.566e-04  Data: 0.013 (0.017)
Train: 199 [ 500/1251 ( 40%)]  Loss: 3.324 (3.18)  Time: 0.675s, 1517.51/s  (0.667s, 1535.96/s)  LR: 2.565e-04  Data: 0.014 (0.017)
Train: 199 [ 550/1251 ( 44%)]  Loss: 3.105 (3.17)  Time: 0.662s, 1547.59/s  (0.667s, 1536.15/s)  LR: 2.563e-04  Data: 0.014 (0.017)
Train: 199 [ 600/1251 ( 48%)]  Loss: 3.078 (3.17)  Time: 0.668s, 1532.80/s  (0.666s, 1536.62/s)  LR: 2.561e-04  Data: 0.013 (0.017)
Train: 199 [ 650/1251 ( 52%)]  Loss: 3.277 (3.18)  Time: 0.656s, 1559.90/s  (0.666s, 1536.92/s)  LR: 2.559e-04  Data: 0.014 (0.016)
Train: 199 [ 700/1251 ( 56%)]  Loss: 2.924 (3.16)  Time: 0.671s, 1526.40/s  (0.666s, 1537.34/s)  LR: 2.557e-04  Data: 0.013 (0.016)
Train: 199 [ 750/1251 ( 60%)]  Loss: 3.282 (3.17)  Time: 0.663s, 1544.60/s  (0.666s, 1537.46/s)  LR: 2.556e-04  Data: 0.013 (0.016)
Train: 199 [ 800/1251 ( 64%)]  Loss: 3.315 (3.17)  Time: 0.664s, 1542.08/s  (0.666s, 1537.67/s)  LR: 2.554e-04  Data: 0.013 (0.016)
Train: 199 [ 850/1251 ( 68%)]  Loss: 3.405 (3.19)  Time: 0.672s, 1524.71/s  (0.666s, 1538.12/s)  LR: 2.552e-04  Data: 0.013 (0.016)
Train: 199 [ 900/1251 ( 72%)]  Loss: 3.127 (3.18)  Time: 0.667s, 1536.33/s  (0.666s, 1538.61/s)  LR: 2.550e-04  Data: 0.013 (0.016)
Train: 199 [ 950/1251 ( 76%)]  Loss: 3.213 (3.19)  Time: 0.667s, 1536.06/s  (0.665s, 1538.90/s)  LR: 2.548e-04  Data: 0.012 (0.015)
Train: 199 [1000/1251 ( 80%)]  Loss: 3.357 (3.19)  Time: 0.671s, 1526.83/s  (0.665s, 1538.99/s)  LR: 2.547e-04  Data: 0.013 (0.015)
Train: 199 [1050/1251 ( 84%)]  Loss: 3.428 (3.20)  Time: 0.663s, 1544.01/s  (0.665s, 1539.28/s)  LR: 2.545e-04  Data: 0.014 (0.015)
Train: 199 [1100/1251 ( 88%)]  Loss: 3.157 (3.20)  Time: 0.661s, 1548.84/s  (0.665s, 1539.32/s)  LR: 2.543e-04  Data: 0.012 (0.015)
Train: 199 [1150/1251 ( 92%)]  Loss: 3.219 (3.20)  Time: 0.662s, 1546.39/s  (0.665s, 1539.28/s)  LR: 2.541e-04  Data: 0.013 (0.015)
Train: 199 [1200/1251 ( 96%)]  Loss: 3.212 (3.20)  Time: 0.666s, 1538.41/s  (0.665s, 1539.31/s)  LR: 2.539e-04  Data: 0.013 (0.015)
Train: 199 [1250/1251 (100%)]  Loss: 3.220 (3.20)  Time: 0.654s, 1566.37/s  (0.665s, 1539.46/s)  LR: 2.538e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.870 (2.870)  Loss:  0.4211 (0.4211)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.162 (0.326)  Loss:  0.5244 (0.8855)  Acc@1: 86.9104 (78.8880)  Acc@5: 97.5236 (94.7500)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-199.pth.tar', 78.8880000024414)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-196.pth.tar', 78.74999997802735)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-197.pth.tar', 78.70400008544922)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-198.pth.tar', 78.64000015869141)

Train: 200 [   0/1251 (  0%)]  Loss: 3.275 (3.27)  Time: 3.302s,  310.10/s  (3.302s,  310.10/s)  LR: 2.537e-04  Data: 1.594 (1.594)
Train: 200 [  50/1251 (  4%)]  Loss: 3.156 (3.22)  Time: 0.651s, 1574.15/s  (0.680s, 1506.76/s)  LR: 2.536e-04  Data: 0.016 (0.045)
Train: 200 [ 100/1251 (  8%)]  Loss: 3.275 (3.24)  Time: 0.670s, 1527.92/s  (0.669s, 1530.51/s)  LR: 2.534e-04  Data: 0.013 (0.029)
Train: 200 [ 150/1251 ( 12%)]  Loss: 3.257 (3.24)  Time: 0.665s, 1539.31/s  (0.667s, 1535.66/s)  LR: 2.532e-04  Data: 0.013 (0.024)
Train: 200 [ 200/1251 ( 16%)]  Loss: 3.502 (3.29)  Time: 0.663s, 1544.37/s  (0.667s, 1536.16/s)  LR: 2.530e-04  Data: 0.013 (0.021)
Train: 200 [ 250/1251 ( 20%)]  Loss: 3.367 (3.31)  Time: 0.667s, 1535.15/s  (0.666s, 1536.91/s)  LR: 2.528e-04  Data: 0.013 (0.020)
Train: 200 [ 300/1251 ( 24%)]  Loss: 3.339 (3.31)  Time: 0.655s, 1563.66/s  (0.666s, 1537.65/s)  LR: 2.527e-04  Data: 0.012 (0.019)
Train: 200 [ 350/1251 ( 28%)]  Loss: 3.322 (3.31)  Time: 0.673s, 1521.43/s  (0.666s, 1538.26/s)  LR: 2.525e-04  Data: 0.014 (0.018)
Train: 200 [ 400/1251 ( 32%)]  Loss: 3.212 (3.30)  Time: 0.660s, 1552.64/s  (0.666s, 1538.38/s)  LR: 2.523e-04  Data: 0.013 (0.018)
Train: 200 [ 450/1251 ( 36%)]  Loss: 3.315 (3.30)  Time: 0.667s, 1536.03/s  (0.665s, 1538.75/s)  LR: 2.521e-04  Data: 0.015 (0.017)
Train: 200 [ 500/1251 ( 40%)]  Loss: 3.156 (3.29)  Time: 0.671s, 1525.19/s  (0.665s, 1538.76/s)  LR: 2.519e-04  Data: 0.013 (0.017)
Train: 200 [ 550/1251 ( 44%)]  Loss: 3.354 (3.29)  Time: 0.663s, 1543.55/s  (0.665s, 1539.27/s)  LR: 2.518e-04  Data: 0.014 (0.017)
Train: 200 [ 600/1251 ( 48%)]  Loss: 3.205 (3.29)  Time: 0.663s, 1544.45/s  (0.665s, 1539.06/s)  LR: 2.516e-04  Data: 0.013 (0.016)
Train: 200 [ 650/1251 ( 52%)]  Loss: 3.468 (3.30)  Time: 0.662s, 1547.99/s  (0.665s, 1538.91/s)  LR: 2.514e-04  Data: 0.013 (0.016)
Train: 200 [ 700/1251 ( 56%)]  Loss: 3.145 (3.29)  Time: 0.660s, 1550.37/s  (0.665s, 1539.23/s)  LR: 2.512e-04  Data: 0.012 (0.016)
Train: 200 [ 750/1251 ( 60%)]  Loss: 3.292 (3.29)  Time: 0.662s, 1546.02/s  (0.665s, 1539.62/s)  LR: 2.510e-04  Data: 0.015 (0.016)
Train: 200 [ 800/1251 ( 64%)]  Loss: 2.973 (3.27)  Time: 0.665s, 1539.47/s  (0.665s, 1540.12/s)  LR: 2.509e-04  Data: 0.013 (0.016)
Train: 200 [ 850/1251 ( 68%)]  Loss: 3.316 (3.27)  Time: 0.651s, 1572.90/s  (0.665s, 1540.53/s)  LR: 2.507e-04  Data: 0.014 (0.015)
Train: 200 [ 900/1251 ( 72%)]  Loss: 3.418 (3.28)  Time: 0.665s, 1539.85/s  (0.665s, 1540.77/s)  LR: 2.505e-04  Data: 0.014 (0.015)
Train: 200 [ 950/1251 ( 76%)]  Loss: 3.663 (3.30)  Time: 0.671s, 1526.99/s  (0.665s, 1540.97/s)  LR: 2.503e-04  Data: 0.012 (0.015)
Train: 200 [1000/1251 ( 80%)]  Loss: 3.388 (3.30)  Time: 0.659s, 1555.03/s  (0.664s, 1541.28/s)  LR: 2.501e-04  Data: 0.013 (0.015)
Train: 200 [1050/1251 ( 84%)]  Loss: 3.216 (3.30)  Time: 0.662s, 1546.28/s  (0.664s, 1541.45/s)  LR: 2.500e-04  Data: 0.016 (0.015)
Train: 200 [1100/1251 ( 88%)]  Loss: 3.279 (3.30)  Time: 0.668s, 1532.57/s  (0.664s, 1541.62/s)  LR: 2.498e-04  Data: 0.015 (0.015)
Train: 200 [1150/1251 ( 92%)]  Loss: 3.124 (3.29)  Time: 0.663s, 1544.28/s  (0.664s, 1541.82/s)  LR: 2.496e-04  Data: 0.013 (0.015)
Train: 200 [1200/1251 ( 96%)]  Loss: 3.423 (3.30)  Time: 0.657s, 1559.63/s  (0.664s, 1542.00/s)  LR: 2.494e-04  Data: 0.015 (0.015)
Train: 200 [1250/1251 (100%)]  Loss: 3.108 (3.29)  Time: 0.646s, 1585.56/s  (0.664s, 1542.27/s)  LR: 2.493e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.848 (2.848)  Loss:  0.4321 (0.4321)  Acc@1: 91.0156 (91.0156)  Acc@5: 98.0469 (98.0469)
Test: [  48/48]  Time: 0.162 (0.323)  Loss:  0.5410 (0.8868)  Acc@1: 87.2642 (78.9680)  Acc@5: 97.8774 (94.8000)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-200.pth.tar', 78.96800002685546)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-199.pth.tar', 78.8880000024414)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-196.pth.tar', 78.74999997802735)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-197.pth.tar', 78.70400008544922)

Train: 201 [   0/1251 (  0%)]  Loss: 3.571 (3.57)  Time: 3.577s,  286.27/s  (3.577s,  286.27/s)  LR: 2.492e-04  Data: 1.798 (1.798)
Train: 201 [  50/1251 (  4%)]  Loss: 3.002 (3.29)  Time: 0.649s, 1578.81/s  (0.682s, 1500.63/s)  LR: 2.491e-04  Data: 0.016 (0.049)
Train: 201 [ 100/1251 (  8%)]  Loss: 3.324 (3.30)  Time: 0.651s, 1573.74/s  (0.668s, 1532.65/s)  LR: 2.489e-04  Data: 0.018 (0.032)
Train: 201 [ 150/1251 ( 12%)]  Loss: 3.009 (3.23)  Time: 0.660s, 1550.79/s  (0.665s, 1540.54/s)  LR: 2.487e-04  Data: 0.017 (0.026)
Train: 201 [ 200/1251 ( 16%)]  Loss: 3.157 (3.21)  Time: 0.649s, 1578.27/s  (0.664s, 1543.31/s)  LR: 2.485e-04  Data: 0.013 (0.023)
Train: 201 [ 250/1251 ( 20%)]  Loss: 3.117 (3.20)  Time: 0.676s, 1515.16/s  (0.663s, 1543.78/s)  LR: 2.484e-04  Data: 0.014 (0.021)
Train: 201 [ 300/1251 ( 24%)]  Loss: 3.426 (3.23)  Time: 0.670s, 1529.49/s  (0.663s, 1543.46/s)  LR: 2.482e-04  Data: 0.014 (0.020)
Train: 201 [ 350/1251 ( 28%)]  Loss: 3.588 (3.27)  Time: 0.663s, 1545.58/s  (0.664s, 1542.71/s)  LR: 2.480e-04  Data: 0.013 (0.019)
Train: 201 [ 400/1251 ( 32%)]  Loss: 3.538 (3.30)  Time: 0.655s, 1564.33/s  (0.664s, 1542.83/s)  LR: 2.478e-04  Data: 0.013 (0.018)
Train: 201 [ 450/1251 ( 36%)]  Loss: 3.360 (3.31)  Time: 0.661s, 1549.72/s  (0.664s, 1542.14/s)  LR: 2.476e-04  Data: 0.013 (0.018)
Train: 201 [ 500/1251 ( 40%)]  Loss: 3.277 (3.31)  Time: 0.666s, 1538.20/s  (0.664s, 1542.13/s)  LR: 2.475e-04  Data: 0.016 (0.017)
Train: 201 [ 550/1251 ( 44%)]  Loss: 3.257 (3.30)  Time: 0.659s, 1554.44/s  (0.664s, 1542.45/s)  LR: 2.473e-04  Data: 0.013 (0.017)
Train: 201 [ 600/1251 ( 48%)]  Loss: 3.209 (3.29)  Time: 0.664s, 1541.38/s  (0.664s, 1542.63/s)  LR: 2.471e-04  Data: 0.012 (0.017)
Train: 201 [ 650/1251 ( 52%)]  Loss: 3.303 (3.30)  Time: 0.657s, 1558.35/s  (0.664s, 1542.90/s)  LR: 2.469e-04  Data: 0.013 (0.017)
Train: 201 [ 700/1251 ( 56%)]  Loss: 3.383 (3.30)  Time: 0.673s, 1521.82/s  (0.664s, 1543.21/s)  LR: 2.467e-04  Data: 0.014 (0.016)
Train: 201 [ 750/1251 ( 60%)]  Loss: 3.407 (3.31)  Time: 0.657s, 1559.14/s  (0.663s, 1543.39/s)  LR: 2.466e-04  Data: 0.014 (0.016)
Train: 201 [ 800/1251 ( 64%)]  Loss: 3.378 (3.31)  Time: 0.660s, 1551.70/s  (0.663s, 1543.42/s)  LR: 2.464e-04  Data: 0.013 (0.016)
Train: 201 [ 850/1251 ( 68%)]  Loss: 3.479 (3.32)  Time: 0.667s, 1535.81/s  (0.663s, 1543.57/s)  LR: 2.462e-04  Data: 0.014 (0.016)
Train: 201 [ 900/1251 ( 72%)]  Loss: 3.136 (3.31)  Time: 0.663s, 1545.64/s  (0.663s, 1543.56/s)  LR: 2.460e-04  Data: 0.015 (0.016)
Train: 201 [ 950/1251 ( 76%)]  Loss: 3.151 (3.30)  Time: 0.647s, 1581.78/s  (0.663s, 1543.57/s)  LR: 2.459e-04  Data: 0.014 (0.016)
Train: 201 [1000/1251 ( 80%)]  Loss: 3.243 (3.30)  Time: 0.671s, 1526.06/s  (0.663s, 1543.69/s)  LR: 2.457e-04  Data: 0.013 (0.016)
Train: 201 [1050/1251 ( 84%)]  Loss: 3.441 (3.31)  Time: 0.663s, 1544.37/s  (0.663s, 1543.56/s)  LR: 2.455e-04  Data: 0.014 (0.016)
Train: 201 [1100/1251 ( 88%)]  Loss: 3.547 (3.32)  Time: 0.662s, 1547.12/s  (0.663s, 1543.77/s)  LR: 2.453e-04  Data: 0.014 (0.016)
Train: 201 [1150/1251 ( 92%)]  Loss: 2.973 (3.30)  Time: 0.660s, 1550.48/s  (0.663s, 1543.92/s)  LR: 2.451e-04  Data: 0.013 (0.015)
Train: 201 [1200/1251 ( 96%)]  Loss: 3.175 (3.30)  Time: 0.653s, 1569.14/s  (0.663s, 1543.99/s)  LR: 2.450e-04  Data: 0.013 (0.015)
Train: 201 [1250/1251 (100%)]  Loss: 3.141 (3.29)  Time: 0.645s, 1587.03/s  (0.663s, 1544.04/s)  LR: 2.448e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.849 (2.849)  Loss:  0.4363 (0.4363)  Acc@1: 91.5039 (91.5039)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.163 (0.321)  Loss:  0.5205 (0.9114)  Acc@1: 87.3821 (78.9620)  Acc@5: 97.6415 (94.6200)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-200.pth.tar', 78.96800002685546)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-201.pth.tar', 78.96199994873047)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-199.pth.tar', 78.8880000024414)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-196.pth.tar', 78.74999997802735)

Train: 202 [   0/1251 (  0%)]  Loss: 3.433 (3.43)  Time: 2.994s,  342.00/s  (2.994s,  342.00/s)  LR: 2.448e-04  Data: 1.837 (1.837)
Train: 202 [  50/1251 (  4%)]  Loss: 3.600 (3.52)  Time: 0.651s, 1572.98/s  (0.680s, 1505.43/s)  LR: 2.446e-04  Data: 0.013 (0.049)
Train: 202 [ 100/1251 (  8%)]  Loss: 3.395 (3.48)  Time: 0.654s, 1565.80/s  (0.670s, 1527.53/s)  LR: 2.444e-04  Data: 0.015 (0.032)
Train: 202 [ 150/1251 ( 12%)]  Loss: 3.400 (3.46)  Time: 0.665s, 1539.87/s  (0.668s, 1533.78/s)  LR: 2.442e-04  Data: 0.015 (0.026)
Train: 202 [ 200/1251 ( 16%)]  Loss: 3.350 (3.44)  Time: 0.666s, 1538.31/s  (0.667s, 1536.00/s)  LR: 2.441e-04  Data: 0.014 (0.023)
Train: 202 [ 250/1251 ( 20%)]  Loss: 3.037 (3.37)  Time: 0.666s, 1538.32/s  (0.666s, 1536.78/s)  LR: 2.439e-04  Data: 0.013 (0.021)
Train: 202 [ 300/1251 ( 24%)]  Loss: 3.136 (3.34)  Time: 0.656s, 1560.74/s  (0.666s, 1537.58/s)  LR: 2.437e-04  Data: 0.016 (0.020)
Train: 202 [ 350/1251 ( 28%)]  Loss: 3.157 (3.31)  Time: 0.658s, 1555.76/s  (0.666s, 1538.59/s)  LR: 2.435e-04  Data: 0.017 (0.019)
Train: 202 [ 400/1251 ( 32%)]  Loss: 3.109 (3.29)  Time: 0.668s, 1532.50/s  (0.666s, 1538.58/s)  LR: 2.434e-04  Data: 0.016 (0.018)
Train: 202 [ 450/1251 ( 36%)]  Loss: 3.536 (3.32)  Time: 0.659s, 1554.33/s  (0.665s, 1539.74/s)  LR: 2.432e-04  Data: 0.012 (0.018)
Train: 202 [ 500/1251 ( 40%)]  Loss: 3.427 (3.33)  Time: 0.658s, 1555.94/s  (0.665s, 1540.47/s)  LR: 2.430e-04  Data: 0.013 (0.018)
Train: 202 [ 550/1251 ( 44%)]  Loss: 3.154 (3.31)  Time: 0.667s, 1536.19/s  (0.665s, 1540.72/s)  LR: 2.428e-04  Data: 0.013 (0.017)
Train: 202 [ 600/1251 ( 48%)]  Loss: 3.237 (3.31)  Time: 0.666s, 1538.18/s  (0.665s, 1540.85/s)  LR: 2.426e-04  Data: 0.012 (0.017)
Train: 202 [ 650/1251 ( 52%)]  Loss: 3.218 (3.30)  Time: 0.660s, 1552.31/s  (0.665s, 1540.84/s)  LR: 2.425e-04  Data: 0.016 (0.017)
Train: 202 [ 700/1251 ( 56%)]  Loss: 3.275 (3.30)  Time: 0.659s, 1554.47/s  (0.664s, 1541.18/s)  LR: 2.423e-04  Data: 0.013 (0.017)
Train: 202 [ 750/1251 ( 60%)]  Loss: 3.174 (3.29)  Time: 0.664s, 1542.18/s  (0.664s, 1541.40/s)  LR: 2.421e-04  Data: 0.015 (0.016)
Train: 202 [ 800/1251 ( 64%)]  Loss: 3.183 (3.28)  Time: 0.661s, 1549.98/s  (0.664s, 1541.21/s)  LR: 2.419e-04  Data: 0.014 (0.016)
Train: 202 [ 850/1251 ( 68%)]  Loss: 3.303 (3.28)  Time: 0.658s, 1555.80/s  (0.664s, 1541.24/s)  LR: 2.418e-04  Data: 0.017 (0.016)
Train: 202 [ 900/1251 ( 72%)]  Loss: 3.398 (3.29)  Time: 0.661s, 1548.95/s  (0.664s, 1541.51/s)  LR: 2.416e-04  Data: 0.013 (0.016)
Train: 202 [ 950/1251 ( 76%)]  Loss: 3.180 (3.29)  Time: 0.663s, 1544.64/s  (0.664s, 1541.63/s)  LR: 2.414e-04  Data: 0.014 (0.016)
Train: 202 [1000/1251 ( 80%)]  Loss: 2.931 (3.27)  Time: 0.669s, 1530.61/s  (0.664s, 1541.76/s)  LR: 2.412e-04  Data: 0.013 (0.016)
Train: 202 [1050/1251 ( 84%)]  Loss: 3.279 (3.27)  Time: 0.665s, 1538.90/s  (0.664s, 1541.84/s)  LR: 2.410e-04  Data: 0.013 (0.016)
Train: 202 [1100/1251 ( 88%)]  Loss: 3.135 (3.26)  Time: 0.660s, 1552.30/s  (0.664s, 1542.07/s)  LR: 2.409e-04  Data: 0.013 (0.015)
Train: 202 [1150/1251 ( 92%)]  Loss: 3.017 (3.25)  Time: 0.662s, 1547.35/s  (0.664s, 1541.92/s)  LR: 2.407e-04  Data: 0.013 (0.015)
Train: 202 [1200/1251 ( 96%)]  Loss: 3.556 (3.26)  Time: 0.663s, 1544.79/s  (0.664s, 1541.70/s)  LR: 2.405e-04  Data: 0.018 (0.015)
Train: 202 [1250/1251 (100%)]  Loss: 3.236 (3.26)  Time: 0.646s, 1585.32/s  (0.664s, 1541.59/s)  LR: 2.403e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.852 (2.852)  Loss:  0.3958 (0.3958)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.326)  Loss:  0.5645 (0.8951)  Acc@1: 87.5000 (79.0460)  Acc@5: 97.4057 (94.7760)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-202.pth.tar', 79.04600012939453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-200.pth.tar', 78.96800002685546)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-201.pth.tar', 78.96199994873047)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-199.pth.tar', 78.8880000024414)

Train: 203 [   0/1251 (  0%)]  Loss: 3.441 (3.44)  Time: 3.027s,  338.31/s  (3.027s,  338.31/s)  LR: 2.403e-04  Data: 1.911 (1.911)
Train: 203 [  50/1251 (  4%)]  Loss: 3.271 (3.36)  Time: 0.643s, 1592.26/s  (0.674s, 1518.76/s)  LR: 2.402e-04  Data: 0.014 (0.051)
Train: 203 [ 100/1251 (  8%)]  Loss: 3.265 (3.33)  Time: 0.658s, 1556.93/s  (0.665s, 1540.63/s)  LR: 2.400e-04  Data: 0.013 (0.033)
Train: 203 [ 150/1251 ( 12%)]  Loss: 3.525 (3.38)  Time: 0.660s, 1550.98/s  (0.662s, 1546.33/s)  LR: 2.398e-04  Data: 0.014 (0.026)
Train: 203 [ 200/1251 ( 16%)]  Loss: 3.355 (3.37)  Time: 0.659s, 1553.41/s  (0.662s, 1546.75/s)  LR: 2.396e-04  Data: 0.013 (0.023)
Train: 203 [ 250/1251 ( 20%)]  Loss: 3.187 (3.34)  Time: 0.670s, 1529.49/s  (0.662s, 1546.32/s)  LR: 2.395e-04  Data: 0.013 (0.021)
Train: 203 [ 300/1251 ( 24%)]  Loss: 3.050 (3.30)  Time: 0.655s, 1562.92/s  (0.663s, 1545.13/s)  LR: 2.393e-04  Data: 0.013 (0.020)
Train: 203 [ 350/1251 ( 28%)]  Loss: 3.181 (3.28)  Time: 0.656s, 1561.99/s  (0.662s, 1546.18/s)  LR: 2.391e-04  Data: 0.014 (0.019)
Train: 203 [ 400/1251 ( 32%)]  Loss: 3.399 (3.30)  Time: 0.662s, 1547.17/s  (0.662s, 1546.73/s)  LR: 2.389e-04  Data: 0.013 (0.018)
Train: 203 [ 450/1251 ( 36%)]  Loss: 3.267 (3.29)  Time: 0.661s, 1548.31/s  (0.662s, 1547.44/s)  LR: 2.387e-04  Data: 0.013 (0.018)
Train: 203 [ 500/1251 ( 40%)]  Loss: 3.195 (3.29)  Time: 0.664s, 1542.26/s  (0.661s, 1548.08/s)  LR: 2.386e-04  Data: 0.014 (0.017)
Train: 203 [ 550/1251 ( 44%)]  Loss: 3.212 (3.28)  Time: 0.657s, 1558.13/s  (0.661s, 1548.07/s)  LR: 2.384e-04  Data: 0.013 (0.017)
Train: 203 [ 600/1251 ( 48%)]  Loss: 3.067 (3.26)  Time: 0.670s, 1528.09/s  (0.662s, 1547.86/s)  LR: 2.382e-04  Data: 0.014 (0.017)
Train: 203 [ 650/1251 ( 52%)]  Loss: 3.376 (3.27)  Time: 0.662s, 1547.44/s  (0.662s, 1547.38/s)  LR: 2.380e-04  Data: 0.012 (0.016)
Train: 203 [ 700/1251 ( 56%)]  Loss: 3.280 (3.27)  Time: 0.661s, 1548.53/s  (0.662s, 1547.21/s)  LR: 2.379e-04  Data: 0.012 (0.016)
Train: 203 [ 750/1251 ( 60%)]  Loss: 3.252 (3.27)  Time: 0.661s, 1549.06/s  (0.662s, 1547.13/s)  LR: 2.377e-04  Data: 0.013 (0.016)
Train: 203 [ 800/1251 ( 64%)]  Loss: 3.214 (3.27)  Time: 0.651s, 1573.98/s  (0.662s, 1546.69/s)  LR: 2.375e-04  Data: 0.013 (0.016)
Train: 203 [ 850/1251 ( 68%)]  Loss: 3.387 (3.27)  Time: 0.663s, 1544.84/s  (0.662s, 1545.97/s)  LR: 2.373e-04  Data: 0.014 (0.016)
Train: 203 [ 900/1251 ( 72%)]  Loss: 3.323 (3.28)  Time: 0.667s, 1534.83/s  (0.663s, 1545.56/s)  LR: 2.372e-04  Data: 0.013 (0.016)
Train: 203 [ 950/1251 ( 76%)]  Loss: 3.234 (3.27)  Time: 0.664s, 1542.63/s  (0.663s, 1545.16/s)  LR: 2.370e-04  Data: 0.013 (0.016)
Train: 203 [1000/1251 ( 80%)]  Loss: 3.078 (3.26)  Time: 0.661s, 1549.16/s  (0.663s, 1544.81/s)  LR: 2.368e-04  Data: 0.013 (0.016)
Train: 203 [1050/1251 ( 84%)]  Loss: 3.425 (3.27)  Time: 0.665s, 1539.41/s  (0.663s, 1544.59/s)  LR: 2.366e-04  Data: 0.014 (0.015)
Train: 203 [1100/1251 ( 88%)]  Loss: 3.231 (3.27)  Time: 0.657s, 1557.63/s  (0.663s, 1544.32/s)  LR: 2.365e-04  Data: 0.013 (0.015)
Train: 203 [1150/1251 ( 92%)]  Loss: 3.360 (3.27)  Time: 0.666s, 1537.86/s  (0.663s, 1543.98/s)  LR: 2.363e-04  Data: 0.013 (0.015)
Train: 203 [1200/1251 ( 96%)]  Loss: 3.313 (3.28)  Time: 0.668s, 1532.56/s  (0.663s, 1543.57/s)  LR: 2.361e-04  Data: 0.013 (0.015)
Train: 203 [1250/1251 (100%)]  Loss: 3.375 (3.28)  Time: 0.646s, 1584.68/s  (0.664s, 1543.30/s)  LR: 2.359e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.972 (2.972)  Loss:  0.4373 (0.4373)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.164 (0.325)  Loss:  0.5244 (0.8819)  Acc@1: 86.0849 (79.0480)  Acc@5: 97.7594 (94.7960)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-203.pth.tar', 79.04800003173828)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-202.pth.tar', 79.04600012939453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-200.pth.tar', 78.96800002685546)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-201.pth.tar', 78.96199994873047)

Train: 204 [   0/1251 (  0%)]  Loss: 3.342 (3.34)  Time: 3.403s,  300.93/s  (3.403s,  300.93/s)  LR: 2.359e-04  Data: 1.819 (1.819)
Train: 204 [  50/1251 (  4%)]  Loss: 3.116 (3.23)  Time: 0.657s, 1559.69/s  (0.692s, 1480.59/s)  LR: 2.357e-04  Data: 0.014 (0.049)
Train: 204 [ 100/1251 (  8%)]  Loss: 3.243 (3.23)  Time: 0.660s, 1552.29/s  (0.676s, 1514.60/s)  LR: 2.356e-04  Data: 0.013 (0.032)
Train: 204 [ 150/1251 ( 12%)]  Loss: 3.146 (3.21)  Time: 0.661s, 1548.22/s  (0.672s, 1522.89/s)  LR: 2.354e-04  Data: 0.013 (0.026)
Train: 204 [ 200/1251 ( 16%)]  Loss: 3.391 (3.25)  Time: 0.667s, 1536.18/s  (0.670s, 1527.86/s)  LR: 2.352e-04  Data: 0.013 (0.023)
Train: 204 [ 250/1251 ( 20%)]  Loss: 3.493 (3.29)  Time: 0.662s, 1547.93/s  (0.669s, 1529.80/s)  LR: 2.350e-04  Data: 0.013 (0.021)
Train: 204 [ 300/1251 ( 24%)]  Loss: 3.248 (3.28)  Time: 0.671s, 1526.76/s  (0.669s, 1530.61/s)  LR: 2.349e-04  Data: 0.013 (0.020)
Train: 204 [ 350/1251 ( 28%)]  Loss: 3.364 (3.29)  Time: 0.679s, 1507.99/s  (0.669s, 1531.14/s)  LR: 2.347e-04  Data: 0.014 (0.019)
Train: 204 [ 400/1251 ( 32%)]  Loss: 3.223 (3.29)  Time: 0.677s, 1511.86/s  (0.669s, 1531.30/s)  LR: 2.345e-04  Data: 0.013 (0.018)
Train: 204 [ 450/1251 ( 36%)]  Loss: 3.348 (3.29)  Time: 0.667s, 1534.19/s  (0.669s, 1531.46/s)  LR: 2.343e-04  Data: 0.017 (0.018)
Train: 204 [ 500/1251 ( 40%)]  Loss: 3.132 (3.28)  Time: 0.671s, 1526.96/s  (0.668s, 1532.46/s)  LR: 2.342e-04  Data: 0.017 (0.017)
Train: 204 [ 550/1251 ( 44%)]  Loss: 3.265 (3.28)  Time: 0.665s, 1539.08/s  (0.668s, 1532.94/s)  LR: 2.340e-04  Data: 0.020 (0.017)
Train: 204 [ 600/1251 ( 48%)]  Loss: 3.264 (3.28)  Time: 0.671s, 1526.84/s  (0.668s, 1533.32/s)  LR: 2.338e-04  Data: 0.016 (0.017)
Train: 204 [ 650/1251 ( 52%)]  Loss: 3.476 (3.29)  Time: 0.658s, 1556.28/s  (0.668s, 1534.00/s)  LR: 2.336e-04  Data: 0.012 (0.017)
Train: 204 [ 700/1251 ( 56%)]  Loss: 3.010 (3.27)  Time: 0.658s, 1555.15/s  (0.667s, 1534.28/s)  LR: 2.335e-04  Data: 0.013 (0.016)
Train: 204 [ 750/1251 ( 60%)]  Loss: 3.168 (3.26)  Time: 0.657s, 1557.83/s  (0.667s, 1534.67/s)  LR: 2.333e-04  Data: 0.014 (0.016)
Train: 204 [ 800/1251 ( 64%)]  Loss: 3.235 (3.26)  Time: 0.670s, 1528.21/s  (0.667s, 1534.87/s)  LR: 2.331e-04  Data: 0.013 (0.016)
Train: 204 [ 850/1251 ( 68%)]  Loss: 3.003 (3.25)  Time: 0.673s, 1520.94/s  (0.667s, 1535.12/s)  LR: 2.329e-04  Data: 0.013 (0.016)
Train: 204 [ 900/1251 ( 72%)]  Loss: 3.303 (3.25)  Time: 0.667s, 1534.59/s  (0.667s, 1535.84/s)  LR: 2.328e-04  Data: 0.015 (0.016)
Train: 204 [ 950/1251 ( 76%)]  Loss: 3.243 (3.25)  Time: 0.666s, 1538.51/s  (0.666s, 1536.43/s)  LR: 2.326e-04  Data: 0.012 (0.016)
Train: 204 [1000/1251 ( 80%)]  Loss: 3.371 (3.26)  Time: 0.668s, 1533.06/s  (0.666s, 1536.87/s)  LR: 2.324e-04  Data: 0.023 (0.016)
Train: 204 [1050/1251 ( 84%)]  Loss: 3.275 (3.26)  Time: 0.665s, 1539.17/s  (0.666s, 1537.54/s)  LR: 2.322e-04  Data: 0.013 (0.016)
Train: 204 [1100/1251 ( 88%)]  Loss: 3.587 (3.27)  Time: 0.674s, 1520.08/s  (0.666s, 1537.84/s)  LR: 2.321e-04  Data: 0.015 (0.015)
Train: 204 [1150/1251 ( 92%)]  Loss: 3.144 (3.27)  Time: 0.657s, 1557.53/s  (0.666s, 1537.95/s)  LR: 2.319e-04  Data: 0.014 (0.015)
Train: 204 [1200/1251 ( 96%)]  Loss: 2.906 (3.25)  Time: 0.669s, 1530.91/s  (0.666s, 1538.06/s)  LR: 2.317e-04  Data: 0.014 (0.015)
Train: 204 [1250/1251 (100%)]  Loss: 3.141 (3.25)  Time: 0.653s, 1568.28/s  (0.666s, 1538.35/s)  LR: 2.315e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.019 (3.019)  Loss:  0.4116 (0.4116)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.162 (0.323)  Loss:  0.5400 (0.8834)  Acc@1: 86.6745 (78.9580)  Acc@5: 97.6415 (94.8380)
Train: 205 [   0/1251 (  0%)]  Loss: 2.944 (2.94)  Time: 3.268s,  313.30/s  (3.268s,  313.30/s)  LR: 2.315e-04  Data: 1.642 (1.642)
Train: 205 [  50/1251 (  4%)]  Loss: 3.277 (3.11)  Time: 0.644s, 1589.59/s  (0.684s, 1496.77/s)  LR: 2.314e-04  Data: 0.014 (0.046)
Train: 205 [ 100/1251 (  8%)]  Loss: 3.349 (3.19)  Time: 0.660s, 1552.08/s  (0.672s, 1523.47/s)  LR: 2.312e-04  Data: 0.015 (0.030)
Train: 205 [ 150/1251 ( 12%)]  Loss: 3.152 (3.18)  Time: 0.663s, 1544.09/s  (0.668s, 1531.82/s)  LR: 2.310e-04  Data: 0.016 (0.025)
Train: 205 [ 200/1251 ( 16%)]  Loss: 3.116 (3.17)  Time: 0.660s, 1551.24/s  (0.667s, 1534.82/s)  LR: 2.308e-04  Data: 0.015 (0.022)
Train: 205 [ 250/1251 ( 20%)]  Loss: 3.241 (3.18)  Time: 0.656s, 1560.73/s  (0.666s, 1536.49/s)  LR: 2.307e-04  Data: 0.013 (0.020)
Train: 205 [ 300/1251 ( 24%)]  Loss: 3.183 (3.18)  Time: 0.662s, 1546.69/s  (0.666s, 1537.33/s)  LR: 2.305e-04  Data: 0.013 (0.019)
Train: 205 [ 350/1251 ( 28%)]  Loss: 3.512 (3.22)  Time: 0.661s, 1548.73/s  (0.666s, 1538.26/s)  LR: 2.303e-04  Data: 0.013 (0.018)
Train: 205 [ 400/1251 ( 32%)]  Loss: 3.175 (3.22)  Time: 0.670s, 1528.43/s  (0.665s, 1538.84/s)  LR: 2.301e-04  Data: 0.018 (0.018)
Train: 205 [ 450/1251 ( 36%)]  Loss: 3.259 (3.22)  Time: 0.665s, 1539.64/s  (0.665s, 1539.47/s)  LR: 2.300e-04  Data: 0.013 (0.017)
Train: 205 [ 500/1251 ( 40%)]  Loss: 3.239 (3.22)  Time: 0.653s, 1567.12/s  (0.665s, 1540.78/s)  LR: 2.298e-04  Data: 0.016 (0.017)
Train: 205 [ 550/1251 ( 44%)]  Loss: 2.926 (3.20)  Time: 0.664s, 1541.65/s  (0.664s, 1541.67/s)  LR: 2.296e-04  Data: 0.013 (0.017)
Train: 205 [ 600/1251 ( 48%)]  Loss: 3.465 (3.22)  Time: 0.657s, 1559.17/s  (0.664s, 1542.29/s)  LR: 2.294e-04  Data: 0.013 (0.016)
Train: 205 [ 650/1251 ( 52%)]  Loss: 3.054 (3.21)  Time: 0.660s, 1551.53/s  (0.664s, 1542.88/s)  LR: 2.293e-04  Data: 0.013 (0.016)
Train: 205 [ 700/1251 ( 56%)]  Loss: 3.306 (3.21)  Time: 0.673s, 1520.52/s  (0.664s, 1543.18/s)  LR: 2.291e-04  Data: 0.013 (0.016)
Train: 205 [ 750/1251 ( 60%)]  Loss: 3.467 (3.23)  Time: 0.659s, 1552.88/s  (0.663s, 1543.42/s)  LR: 2.289e-04  Data: 0.013 (0.016)
Train: 205 [ 800/1251 ( 64%)]  Loss: 2.952 (3.21)  Time: 0.657s, 1559.47/s  (0.663s, 1543.74/s)  LR: 2.288e-04  Data: 0.013 (0.016)
Train: 205 [ 850/1251 ( 68%)]  Loss: 3.458 (3.23)  Time: 0.658s, 1555.69/s  (0.663s, 1544.07/s)  LR: 2.286e-04  Data: 0.013 (0.016)
Train: 205 [ 900/1251 ( 72%)]  Loss: 3.347 (3.23)  Time: 0.658s, 1555.78/s  (0.663s, 1544.15/s)  LR: 2.284e-04  Data: 0.013 (0.015)
Train: 205 [ 950/1251 ( 76%)]  Loss: 3.349 (3.24)  Time: 0.655s, 1564.46/s  (0.663s, 1544.67/s)  LR: 2.282e-04  Data: 0.013 (0.015)
Train: 205 [1000/1251 ( 80%)]  Loss: 3.401 (3.25)  Time: 0.667s, 1536.05/s  (0.663s, 1544.95/s)  LR: 2.281e-04  Data: 0.013 (0.015)
Train: 205 [1050/1251 ( 84%)]  Loss: 3.223 (3.25)  Time: 0.656s, 1560.78/s  (0.663s, 1545.07/s)  LR: 2.279e-04  Data: 0.016 (0.015)
Train: 205 [1100/1251 ( 88%)]  Loss: 3.365 (3.25)  Time: 0.661s, 1548.81/s  (0.663s, 1545.46/s)  LR: 2.277e-04  Data: 0.013 (0.015)
Train: 205 [1150/1251 ( 92%)]  Loss: 3.018 (3.24)  Time: 0.656s, 1560.96/s  (0.662s, 1546.02/s)  LR: 2.275e-04  Data: 0.013 (0.015)
Train: 205 [1200/1251 ( 96%)]  Loss: 3.117 (3.24)  Time: 0.657s, 1559.30/s  (0.662s, 1546.37/s)  LR: 2.274e-04  Data: 0.015 (0.015)
Train: 205 [1250/1251 (100%)]  Loss: 3.334 (3.24)  Time: 0.646s, 1584.60/s  (0.662s, 1546.78/s)  LR: 2.272e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.842 (2.842)  Loss:  0.4155 (0.4155)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.162 (0.325)  Loss:  0.5332 (0.8943)  Acc@1: 87.0283 (78.9000)  Acc@5: 97.9953 (94.8420)
Train: 206 [   0/1251 (  0%)]  Loss: 3.094 (3.09)  Time: 3.328s,  307.68/s  (3.328s,  307.68/s)  LR: 2.272e-04  Data: 1.966 (1.966)
Train: 206 [  50/1251 (  4%)]  Loss: 2.979 (3.04)  Time: 0.645s, 1586.97/s  (0.680s, 1506.31/s)  LR: 2.270e-04  Data: 0.014 (0.052)
Train: 206 [ 100/1251 (  8%)]  Loss: 3.201 (3.09)  Time: 0.664s, 1541.42/s  (0.666s, 1538.10/s)  LR: 2.268e-04  Data: 0.013 (0.033)
Train: 206 [ 150/1251 ( 12%)]  Loss: 3.316 (3.15)  Time: 0.658s, 1556.00/s  (0.663s, 1544.84/s)  LR: 2.267e-04  Data: 0.014 (0.027)
Train: 206 [ 200/1251 ( 16%)]  Loss: 3.193 (3.16)  Time: 0.654s, 1566.93/s  (0.662s, 1546.56/s)  LR: 2.265e-04  Data: 0.013 (0.024)
Train: 206 [ 250/1251 ( 20%)]  Loss: 3.287 (3.18)  Time: 0.671s, 1525.74/s  (0.662s, 1547.41/s)  LR: 2.263e-04  Data: 0.016 (0.022)
Train: 206 [ 300/1251 ( 24%)]  Loss: 3.166 (3.18)  Time: 0.663s, 1545.08/s  (0.662s, 1547.60/s)  LR: 2.261e-04  Data: 0.013 (0.020)
Train: 206 [ 350/1251 ( 28%)]  Loss: 3.105 (3.17)  Time: 0.665s, 1539.13/s  (0.662s, 1546.95/s)  LR: 2.260e-04  Data: 0.017 (0.019)
Train: 206 [ 400/1251 ( 32%)]  Loss: 3.402 (3.19)  Time: 0.656s, 1560.11/s  (0.662s, 1546.59/s)  LR: 2.258e-04  Data: 0.014 (0.019)
Train: 206 [ 450/1251 ( 36%)]  Loss: 2.950 (3.17)  Time: 0.657s, 1558.08/s  (0.662s, 1546.43/s)  LR: 2.256e-04  Data: 0.013 (0.018)
Train: 206 [ 500/1251 ( 40%)]  Loss: 3.180 (3.17)  Time: 0.671s, 1526.32/s  (0.662s, 1546.45/s)  LR: 2.255e-04  Data: 0.013 (0.018)
Train: 206 [ 550/1251 ( 44%)]  Loss: 3.408 (3.19)  Time: 0.668s, 1532.63/s  (0.662s, 1546.06/s)  LR: 2.253e-04  Data: 0.013 (0.017)
Train: 206 [ 600/1251 ( 48%)]  Loss: 3.291 (3.20)  Time: 0.658s, 1555.84/s  (0.663s, 1545.58/s)  LR: 2.251e-04  Data: 0.014 (0.017)
Train: 206 [ 650/1251 ( 52%)]  Loss: 3.008 (3.18)  Time: 0.664s, 1541.37/s  (0.663s, 1545.27/s)  LR: 2.249e-04  Data: 0.014 (0.017)
Train: 206 [ 700/1251 ( 56%)]  Loss: 3.225 (3.19)  Time: 0.665s, 1539.42/s  (0.663s, 1545.07/s)  LR: 2.248e-04  Data: 0.013 (0.017)
Train: 206 [ 750/1251 ( 60%)]  Loss: 3.230 (3.19)  Time: 0.663s, 1544.99/s  (0.663s, 1544.98/s)  LR: 2.246e-04  Data: 0.013 (0.016)
Train: 206 [ 800/1251 ( 64%)]  Loss: 3.556 (3.21)  Time: 0.661s, 1548.10/s  (0.663s, 1544.99/s)  LR: 2.244e-04  Data: 0.013 (0.016)
Train: 206 [ 850/1251 ( 68%)]  Loss: 2.783 (3.19)  Time: 0.665s, 1540.34/s  (0.663s, 1544.92/s)  LR: 2.242e-04  Data: 0.014 (0.016)
Train: 206 [ 900/1251 ( 72%)]  Loss: 3.360 (3.20)  Time: 0.658s, 1556.32/s  (0.663s, 1545.00/s)  LR: 2.241e-04  Data: 0.014 (0.016)
Train: 206 [ 950/1251 ( 76%)]  Loss: 3.161 (3.19)  Time: 0.665s, 1539.30/s  (0.663s, 1545.02/s)  LR: 2.239e-04  Data: 0.014 (0.016)
Train: 206 [1000/1251 ( 80%)]  Loss: 3.306 (3.20)  Time: 0.665s, 1539.41/s  (0.663s, 1545.09/s)  LR: 2.237e-04  Data: 0.014 (0.016)
Train: 206 [1050/1251 ( 84%)]  Loss: 3.239 (3.20)  Time: 0.663s, 1545.05/s  (0.663s, 1544.87/s)  LR: 2.236e-04  Data: 0.012 (0.016)
Train: 206 [1100/1251 ( 88%)]  Loss: 3.336 (3.21)  Time: 0.655s, 1563.88/s  (0.663s, 1544.98/s)  LR: 2.234e-04  Data: 0.014 (0.015)
Train: 206 [1150/1251 ( 92%)]  Loss: 2.984 (3.20)  Time: 0.671s, 1526.74/s  (0.663s, 1544.88/s)  LR: 2.232e-04  Data: 0.012 (0.015)
Train: 206 [1200/1251 ( 96%)]  Loss: 3.121 (3.20)  Time: 0.665s, 1539.28/s  (0.663s, 1544.73/s)  LR: 2.230e-04  Data: 0.012 (0.015)
Train: 206 [1250/1251 (100%)]  Loss: 3.323 (3.20)  Time: 0.656s, 1561.56/s  (0.663s, 1544.70/s)  LR: 2.229e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.093 (3.093)  Loss:  0.4250 (0.4250)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.161 (0.329)  Loss:  0.5420 (0.8917)  Acc@1: 86.7924 (79.0320)  Acc@5: 98.1132 (94.9340)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-203.pth.tar', 79.04800003173828)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-202.pth.tar', 79.04600012939453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-206.pth.tar', 79.03199995117187)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-200.pth.tar', 78.96800002685546)

Train: 207 [   0/1251 (  0%)]  Loss: 3.262 (3.26)  Time: 3.062s,  334.47/s  (3.062s,  334.47/s)  LR: 2.229e-04  Data: 1.709 (1.709)
Train: 207 [  50/1251 (  4%)]  Loss: 3.453 (3.36)  Time: 0.661s, 1548.34/s  (0.673s, 1520.63/s)  LR: 2.227e-04  Data: 0.014 (0.047)
Train: 207 [ 100/1251 (  8%)]  Loss: 3.307 (3.34)  Time: 0.667s, 1535.78/s  (0.664s, 1542.64/s)  LR: 2.225e-04  Data: 0.013 (0.030)
Train: 207 [ 150/1251 ( 12%)]  Loss: 3.430 (3.36)  Time: 0.661s, 1549.57/s  (0.662s, 1545.73/s)  LR: 2.223e-04  Data: 0.014 (0.025)
Train: 207 [ 200/1251 ( 16%)]  Loss: 3.407 (3.37)  Time: 0.653s, 1568.99/s  (0.662s, 1545.97/s)  LR: 2.222e-04  Data: 0.016 (0.022)
Train: 207 [ 250/1251 ( 20%)]  Loss: 3.421 (3.38)  Time: 0.668s, 1532.59/s  (0.663s, 1545.29/s)  LR: 2.220e-04  Data: 0.013 (0.020)
Train: 207 [ 300/1251 ( 24%)]  Loss: 3.369 (3.38)  Time: 0.677s, 1512.87/s  (0.663s, 1543.98/s)  LR: 2.218e-04  Data: 0.012 (0.019)
Train: 207 [ 350/1251 ( 28%)]  Loss: 3.207 (3.36)  Time: 0.653s, 1567.54/s  (0.663s, 1543.58/s)  LR: 2.217e-04  Data: 0.014 (0.019)
Train: 207 [ 400/1251 ( 32%)]  Loss: 3.240 (3.34)  Time: 0.659s, 1552.93/s  (0.663s, 1543.43/s)  LR: 2.215e-04  Data: 0.014 (0.018)
Train: 207 [ 450/1251 ( 36%)]  Loss: 3.297 (3.34)  Time: 0.664s, 1543.19/s  (0.663s, 1543.52/s)  LR: 2.213e-04  Data: 0.016 (0.018)
Train: 207 [ 500/1251 ( 40%)]  Loss: 3.129 (3.32)  Time: 0.665s, 1538.92/s  (0.663s, 1543.75/s)  LR: 2.211e-04  Data: 0.016 (0.017)
Train: 207 [ 550/1251 ( 44%)]  Loss: 2.986 (3.29)  Time: 0.670s, 1529.02/s  (0.663s, 1544.16/s)  LR: 2.210e-04  Data: 0.014 (0.017)
Train: 207 [ 600/1251 ( 48%)]  Loss: 3.325 (3.29)  Time: 0.657s, 1558.33/s  (0.663s, 1544.15/s)  LR: 2.208e-04  Data: 0.013 (0.017)
Train: 207 [ 650/1251 ( 52%)]  Loss: 3.232 (3.29)  Time: 0.658s, 1555.42/s  (0.663s, 1544.57/s)  LR: 2.206e-04  Data: 0.013 (0.016)
Train: 207 [ 700/1251 ( 56%)]  Loss: 3.249 (3.29)  Time: 0.662s, 1545.75/s  (0.663s, 1544.50/s)  LR: 2.205e-04  Data: 0.014 (0.016)
Train: 207 [ 750/1251 ( 60%)]  Loss: 3.328 (3.29)  Time: 0.662s, 1545.76/s  (0.663s, 1544.68/s)  LR: 2.203e-04  Data: 0.013 (0.016)
Train: 207 [ 800/1251 ( 64%)]  Loss: 3.045 (3.28)  Time: 0.663s, 1545.12/s  (0.663s, 1544.94/s)  LR: 2.201e-04  Data: 0.013 (0.016)
Train: 207 [ 850/1251 ( 68%)]  Loss: 3.416 (3.28)  Time: 0.655s, 1562.22/s  (0.663s, 1545.23/s)  LR: 2.199e-04  Data: 0.013 (0.016)
Train: 207 [ 900/1251 ( 72%)]  Loss: 3.128 (3.28)  Time: 0.668s, 1533.08/s  (0.663s, 1545.46/s)  LR: 2.198e-04  Data: 0.013 (0.016)
Train: 207 [ 950/1251 ( 76%)]  Loss: 3.389 (3.28)  Time: 0.664s, 1543.29/s  (0.663s, 1545.53/s)  LR: 2.196e-04  Data: 0.012 (0.016)
Train: 207 [1000/1251 ( 80%)]  Loss: 3.045 (3.27)  Time: 0.665s, 1540.75/s  (0.663s, 1545.65/s)  LR: 2.194e-04  Data: 0.013 (0.015)
Train: 207 [1050/1251 ( 84%)]  Loss: 3.248 (3.27)  Time: 0.660s, 1551.38/s  (0.663s, 1545.62/s)  LR: 2.193e-04  Data: 0.014 (0.015)
Train: 207 [1100/1251 ( 88%)]  Loss: 3.189 (3.27)  Time: 0.663s, 1545.03/s  (0.663s, 1545.38/s)  LR: 2.191e-04  Data: 0.014 (0.015)
Train: 207 [1150/1251 ( 92%)]  Loss: 3.085 (3.26)  Time: 0.661s, 1549.35/s  (0.663s, 1545.27/s)  LR: 2.189e-04  Data: 0.012 (0.015)
Train: 207 [1200/1251 ( 96%)]  Loss: 3.234 (3.26)  Time: 0.672s, 1523.73/s  (0.663s, 1545.01/s)  LR: 2.187e-04  Data: 0.017 (0.015)
Train: 207 [1250/1251 (100%)]  Loss: 3.404 (3.26)  Time: 0.649s, 1578.23/s  (0.663s, 1544.95/s)  LR: 2.186e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.906 (2.906)  Loss:  0.4382 (0.4382)  Acc@1: 90.7227 (90.7227)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.162 (0.325)  Loss:  0.5610 (0.8813)  Acc@1: 86.6745 (79.1340)  Acc@5: 97.4057 (94.8920)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-207.pth.tar', 79.13400002929687)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-203.pth.tar', 79.04800003173828)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-202.pth.tar', 79.04600012939453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-206.pth.tar', 79.03199995117187)

Train: 208 [   0/1251 (  0%)]  Loss: 3.125 (3.12)  Time: 3.296s,  310.65/s  (3.296s,  310.65/s)  LR: 2.186e-04  Data: 1.692 (1.692)
Train: 208 [  50/1251 (  4%)]  Loss: 3.326 (3.23)  Time: 0.650s, 1574.58/s  (0.681s, 1503.99/s)  LR: 2.184e-04  Data: 0.013 (0.047)
Train: 208 [ 100/1251 (  8%)]  Loss: 3.041 (3.16)  Time: 0.654s, 1564.62/s  (0.667s, 1534.58/s)  LR: 2.182e-04  Data: 0.013 (0.030)
Train: 208 [ 150/1251 ( 12%)]  Loss: 3.208 (3.17)  Time: 0.657s, 1557.84/s  (0.664s, 1542.68/s)  LR: 2.181e-04  Data: 0.013 (0.025)
Train: 208 [ 200/1251 ( 16%)]  Loss: 3.209 (3.18)  Time: 0.665s, 1539.95/s  (0.663s, 1545.04/s)  LR: 2.179e-04  Data: 0.013 (0.022)
Train: 208 [ 250/1251 ( 20%)]  Loss: 3.464 (3.23)  Time: 0.651s, 1572.13/s  (0.662s, 1546.60/s)  LR: 2.177e-04  Data: 0.014 (0.020)
Train: 208 [ 300/1251 ( 24%)]  Loss: 3.176 (3.22)  Time: 0.659s, 1555.02/s  (0.661s, 1548.09/s)  LR: 2.175e-04  Data: 0.013 (0.019)
Train: 208 [ 350/1251 ( 28%)]  Loss: 3.436 (3.25)  Time: 0.657s, 1558.45/s  (0.662s, 1547.66/s)  LR: 2.174e-04  Data: 0.013 (0.018)
Train: 208 [ 400/1251 ( 32%)]  Loss: 3.263 (3.25)  Time: 0.668s, 1533.01/s  (0.661s, 1548.27/s)  LR: 2.172e-04  Data: 0.018 (0.018)
Train: 208 [ 450/1251 ( 36%)]  Loss: 3.189 (3.24)  Time: 0.667s, 1534.21/s  (0.661s, 1548.62/s)  LR: 2.170e-04  Data: 0.013 (0.017)
Train: 208 [ 500/1251 ( 40%)]  Loss: 3.366 (3.25)  Time: 0.659s, 1553.25/s  (0.662s, 1547.81/s)  LR: 2.169e-04  Data: 0.013 (0.017)
Train: 208 [ 550/1251 ( 44%)]  Loss: 3.361 (3.26)  Time: 0.668s, 1533.69/s  (0.662s, 1546.96/s)  LR: 2.167e-04  Data: 0.013 (0.017)
Train: 208 [ 600/1251 ( 48%)]  Loss: 3.354 (3.27)  Time: 0.667s, 1535.84/s  (0.662s, 1546.50/s)  LR: 2.165e-04  Data: 0.013 (0.016)
Train: 208 [ 650/1251 ( 52%)]  Loss: 3.095 (3.26)  Time: 0.670s, 1529.39/s  (0.662s, 1545.76/s)  LR: 2.163e-04  Data: 0.014 (0.016)
Train: 208 [ 700/1251 ( 56%)]  Loss: 2.890 (3.23)  Time: 0.665s, 1540.52/s  (0.663s, 1545.40/s)  LR: 2.162e-04  Data: 0.014 (0.016)
Train: 208 [ 750/1251 ( 60%)]  Loss: 3.227 (3.23)  Time: 0.672s, 1523.61/s  (0.663s, 1545.15/s)  LR: 2.160e-04  Data: 0.014 (0.016)
Train: 208 [ 800/1251 ( 64%)]  Loss: 3.147 (3.23)  Time: 0.672s, 1524.73/s  (0.663s, 1545.17/s)  LR: 2.158e-04  Data: 0.014 (0.016)
Train: 208 [ 850/1251 ( 68%)]  Loss: 3.189 (3.23)  Time: 0.673s, 1520.90/s  (0.663s, 1545.42/s)  LR: 2.157e-04  Data: 0.014 (0.015)
Train: 208 [ 900/1251 ( 72%)]  Loss: 3.257 (3.23)  Time: 0.663s, 1545.39/s  (0.663s, 1545.40/s)  LR: 2.155e-04  Data: 0.014 (0.015)
Train: 208 [ 950/1251 ( 76%)]  Loss: 2.899 (3.21)  Time: 0.664s, 1543.15/s  (0.663s, 1545.02/s)  LR: 2.153e-04  Data: 0.014 (0.015)
Train: 208 [1000/1251 ( 80%)]  Loss: 3.208 (3.21)  Time: 0.676s, 1515.28/s  (0.663s, 1544.47/s)  LR: 2.152e-04  Data: 0.015 (0.015)
Train: 208 [1050/1251 ( 84%)]  Loss: 3.188 (3.21)  Time: 0.669s, 1531.10/s  (0.663s, 1543.99/s)  LR: 2.150e-04  Data: 0.013 (0.015)
Train: 208 [1100/1251 ( 88%)]  Loss: 3.213 (3.21)  Time: 0.661s, 1548.69/s  (0.663s, 1543.37/s)  LR: 2.148e-04  Data: 0.013 (0.015)
Train: 208 [1150/1251 ( 92%)]  Loss: 3.388 (3.22)  Time: 0.665s, 1539.61/s  (0.664s, 1543.16/s)  LR: 2.146e-04  Data: 0.012 (0.015)
Train: 208 [1200/1251 ( 96%)]  Loss: 3.192 (3.22)  Time: 0.675s, 1516.81/s  (0.664s, 1542.59/s)  LR: 2.145e-04  Data: 0.014 (0.015)
Train: 208 [1250/1251 (100%)]  Loss: 3.149 (3.21)  Time: 0.661s, 1549.78/s  (0.664s, 1542.47/s)  LR: 2.143e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.889 (2.889)  Loss:  0.4036 (0.4036)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.162 (0.319)  Loss:  0.5112 (0.8723)  Acc@1: 87.5000 (79.3560)  Acc@5: 98.3491 (94.8200)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-208.pth.tar', 79.356)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-207.pth.tar', 79.13400002929687)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-203.pth.tar', 79.04800003173828)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-202.pth.tar', 79.04600012939453)

Train: 209 [   0/1251 (  0%)]  Loss: 3.308 (3.31)  Time: 3.197s,  320.34/s  (3.197s,  320.34/s)  LR: 2.143e-04  Data: 1.647 (1.647)
Train: 209 [  50/1251 (  4%)]  Loss: 3.426 (3.37)  Time: 0.651s, 1573.70/s  (0.684s, 1496.35/s)  LR: 2.141e-04  Data: 0.014 (0.046)
Train: 209 [ 100/1251 (  8%)]  Loss: 3.105 (3.28)  Time: 0.658s, 1556.41/s  (0.671s, 1526.45/s)  LR: 2.140e-04  Data: 0.022 (0.030)
Train: 209 [ 150/1251 ( 12%)]  Loss: 3.386 (3.31)  Time: 0.664s, 1541.20/s  (0.668s, 1532.96/s)  LR: 2.138e-04  Data: 0.016 (0.025)
Train: 209 [ 200/1251 ( 16%)]  Loss: 3.097 (3.26)  Time: 0.668s, 1532.89/s  (0.667s, 1535.77/s)  LR: 2.136e-04  Data: 0.013 (0.022)
Train: 209 [ 250/1251 ( 20%)]  Loss: 3.148 (3.24)  Time: 0.660s, 1551.39/s  (0.666s, 1536.87/s)  LR: 2.135e-04  Data: 0.012 (0.020)
Train: 209 [ 300/1251 ( 24%)]  Loss: 3.189 (3.24)  Time: 0.666s, 1537.32/s  (0.666s, 1536.74/s)  LR: 2.133e-04  Data: 0.013 (0.019)
Train: 209 [ 350/1251 ( 28%)]  Loss: 3.284 (3.24)  Time: 0.663s, 1543.84/s  (0.666s, 1537.45/s)  LR: 2.131e-04  Data: 0.013 (0.018)
Train: 209 [ 400/1251 ( 32%)]  Loss: 2.781 (3.19)  Time: 0.664s, 1543.26/s  (0.666s, 1537.93/s)  LR: 2.129e-04  Data: 0.013 (0.018)
Train: 209 [ 450/1251 ( 36%)]  Loss: 2.859 (3.16)  Time: 0.671s, 1525.05/s  (0.666s, 1537.94/s)  LR: 2.128e-04  Data: 0.013 (0.017)
Train: 209 [ 500/1251 ( 40%)]  Loss: 3.412 (3.18)  Time: 0.658s, 1556.92/s  (0.666s, 1538.50/s)  LR: 2.126e-04  Data: 0.014 (0.017)
Train: 209 [ 550/1251 ( 44%)]  Loss: 3.346 (3.19)  Time: 0.657s, 1558.45/s  (0.665s, 1539.05/s)  LR: 2.124e-04  Data: 0.013 (0.017)
Train: 209 [ 600/1251 ( 48%)]  Loss: 3.071 (3.19)  Time: 0.677s, 1512.96/s  (0.665s, 1539.21/s)  LR: 2.123e-04  Data: 0.013 (0.016)
Train: 209 [ 650/1251 ( 52%)]  Loss: 3.142 (3.18)  Time: 0.650s, 1574.98/s  (0.665s, 1539.17/s)  LR: 2.121e-04  Data: 0.014 (0.016)
Train: 209 [ 700/1251 ( 56%)]  Loss: 3.312 (3.19)  Time: 0.656s, 1560.80/s  (0.665s, 1539.55/s)  LR: 2.119e-04  Data: 0.013 (0.016)
Train: 209 [ 750/1251 ( 60%)]  Loss: 3.085 (3.18)  Time: 0.665s, 1539.77/s  (0.665s, 1539.66/s)  LR: 2.118e-04  Data: 0.013 (0.016)
Train: 209 [ 800/1251 ( 64%)]  Loss: 3.360 (3.19)  Time: 0.669s, 1529.71/s  (0.665s, 1539.41/s)  LR: 2.116e-04  Data: 0.014 (0.016)
Train: 209 [ 850/1251 ( 68%)]  Loss: 3.012 (3.18)  Time: 0.662s, 1546.94/s  (0.665s, 1539.44/s)  LR: 2.114e-04  Data: 0.013 (0.016)
Train: 209 [ 900/1251 ( 72%)]  Loss: 3.381 (3.19)  Time: 0.656s, 1560.70/s  (0.665s, 1539.51/s)  LR: 2.113e-04  Data: 0.014 (0.016)
Train: 209 [ 950/1251 ( 76%)]  Loss: 3.310 (3.20)  Time: 0.669s, 1529.66/s  (0.665s, 1539.87/s)  LR: 2.111e-04  Data: 0.014 (0.015)
Train: 209 [1000/1251 ( 80%)]  Loss: 3.369 (3.21)  Time: 0.668s, 1533.01/s  (0.665s, 1540.15/s)  LR: 2.109e-04  Data: 0.016 (0.015)
Train: 209 [1050/1251 ( 84%)]  Loss: 3.100 (3.20)  Time: 0.667s, 1535.10/s  (0.665s, 1540.26/s)  LR: 2.108e-04  Data: 0.014 (0.015)
Train: 209 [1100/1251 ( 88%)]  Loss: 3.557 (3.22)  Time: 0.659s, 1553.79/s  (0.665s, 1540.51/s)  LR: 2.106e-04  Data: 0.013 (0.015)
Train: 209 [1150/1251 ( 92%)]  Loss: 3.182 (3.22)  Time: 0.666s, 1537.54/s  (0.665s, 1540.73/s)  LR: 2.104e-04  Data: 0.013 (0.015)
Train: 209 [1200/1251 ( 96%)]  Loss: 3.412 (3.23)  Time: 0.660s, 1550.45/s  (0.664s, 1541.10/s)  LR: 2.102e-04  Data: 0.013 (0.015)
Train: 209 [1250/1251 (100%)]  Loss: 3.106 (3.22)  Time: 0.651s, 1573.44/s  (0.664s, 1541.34/s)  LR: 2.101e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.882 (2.882)  Loss:  0.4167 (0.4167)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.162 (0.317)  Loss:  0.5537 (0.8841)  Acc@1: 85.6132 (78.9960)  Acc@5: 97.5236 (94.8860)
Train: 210 [   0/1251 (  0%)]  Loss: 3.276 (3.28)  Time: 3.609s,  283.72/s  (3.609s,  283.72/s)  LR: 2.101e-04  Data: 1.595 (1.595)
Train: 210 [  50/1251 (  4%)]  Loss: 3.205 (3.24)  Time: 0.652s, 1570.41/s  (0.691s, 1482.72/s)  LR: 2.099e-04  Data: 0.013 (0.045)
Train: 210 [ 100/1251 (  8%)]  Loss: 3.323 (3.27)  Time: 0.665s, 1540.38/s  (0.675s, 1516.29/s)  LR: 2.097e-04  Data: 0.013 (0.029)
Train: 210 [ 150/1251 ( 12%)]  Loss: 3.315 (3.28)  Time: 0.661s, 1549.88/s  (0.671s, 1525.97/s)  LR: 2.096e-04  Data: 0.015 (0.024)
Train: 210 [ 200/1251 ( 16%)]  Loss: 3.100 (3.24)  Time: 0.659s, 1554.81/s  (0.669s, 1530.22/s)  LR: 2.094e-04  Data: 0.013 (0.022)
Train: 210 [ 250/1251 ( 20%)]  Loss: 3.285 (3.25)  Time: 0.660s, 1550.53/s  (0.668s, 1533.68/s)  LR: 2.092e-04  Data: 0.016 (0.020)
Train: 210 [ 300/1251 ( 24%)]  Loss: 3.284 (3.26)  Time: 0.659s, 1554.20/s  (0.667s, 1534.90/s)  LR: 2.091e-04  Data: 0.013 (0.019)
Train: 210 [ 350/1251 ( 28%)]  Loss: 3.404 (3.27)  Time: 0.658s, 1557.41/s  (0.667s, 1535.70/s)  LR: 2.089e-04  Data: 0.016 (0.018)
Train: 210 [ 400/1251 ( 32%)]  Loss: 3.056 (3.25)  Time: 0.672s, 1523.94/s  (0.667s, 1536.33/s)  LR: 2.087e-04  Data: 0.013 (0.018)
Train: 210 [ 450/1251 ( 36%)]  Loss: 2.995 (3.22)  Time: 0.661s, 1550.18/s  (0.667s, 1536.14/s)  LR: 2.086e-04  Data: 0.016 (0.017)
Train: 210 [ 500/1251 ( 40%)]  Loss: 3.352 (3.24)  Time: 0.666s, 1537.77/s  (0.667s, 1536.32/s)  LR: 2.084e-04  Data: 0.012 (0.017)
Train: 210 [ 550/1251 ( 44%)]  Loss: 3.158 (3.23)  Time: 0.659s, 1553.61/s  (0.666s, 1536.82/s)  LR: 2.082e-04  Data: 0.013 (0.016)
Train: 210 [ 600/1251 ( 48%)]  Loss: 3.374 (3.24)  Time: 0.671s, 1525.76/s  (0.666s, 1536.86/s)  LR: 2.081e-04  Data: 0.013 (0.016)
Train: 210 [ 650/1251 ( 52%)]  Loss: 3.532 (3.26)  Time: 0.668s, 1532.84/s  (0.666s, 1537.00/s)  LR: 2.079e-04  Data: 0.015 (0.016)
Train: 210 [ 700/1251 ( 56%)]  Loss: 3.462 (3.27)  Time: 0.674s, 1518.99/s  (0.666s, 1536.79/s)  LR: 2.077e-04  Data: 0.018 (0.016)
Train: 210 [ 750/1251 ( 60%)]  Loss: 3.160 (3.27)  Time: 0.661s, 1549.78/s  (0.666s, 1537.02/s)  LR: 2.076e-04  Data: 0.016 (0.016)
Train: 210 [ 800/1251 ( 64%)]  Loss: 3.248 (3.27)  Time: 0.659s, 1553.12/s  (0.666s, 1537.23/s)  LR: 2.074e-04  Data: 0.014 (0.016)
Train: 210 [ 850/1251 ( 68%)]  Loss: 3.526 (3.28)  Time: 0.658s, 1555.69/s  (0.666s, 1537.53/s)  LR: 2.072e-04  Data: 0.013 (0.015)
Train: 210 [ 900/1251 ( 72%)]  Loss: 3.278 (3.28)  Time: 0.661s, 1549.99/s  (0.666s, 1537.72/s)  LR: 2.070e-04  Data: 0.014 (0.015)
Train: 210 [ 950/1251 ( 76%)]  Loss: 3.075 (3.27)  Time: 0.670s, 1527.48/s  (0.666s, 1537.96/s)  LR: 2.069e-04  Data: 0.013 (0.015)
Train: 210 [1000/1251 ( 80%)]  Loss: 3.293 (3.27)  Time: 0.669s, 1531.65/s  (0.666s, 1537.75/s)  LR: 2.067e-04  Data: 0.014 (0.015)
Train: 210 [1050/1251 ( 84%)]  Loss: 3.083 (3.26)  Time: 0.663s, 1544.26/s  (0.666s, 1537.92/s)  LR: 2.065e-04  Data: 0.016 (0.015)
Train: 210 [1100/1251 ( 88%)]  Loss: 2.988 (3.25)  Time: 0.669s, 1530.41/s  (0.666s, 1538.22/s)  LR: 2.064e-04  Data: 0.017 (0.015)
Train: 210 [1150/1251 ( 92%)]  Loss: 2.951 (3.24)  Time: 0.661s, 1548.83/s  (0.666s, 1538.15/s)  LR: 2.062e-04  Data: 0.013 (0.015)
Train: 210 [1200/1251 ( 96%)]  Loss: 3.453 (3.25)  Time: 0.666s, 1538.09/s  (0.666s, 1538.25/s)  LR: 2.060e-04  Data: 0.014 (0.015)
Train: 210 [1250/1251 (100%)]  Loss: 3.370 (3.25)  Time: 0.644s, 1589.15/s  (0.666s, 1538.47/s)  LR: 2.059e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.944 (2.944)  Loss:  0.4175 (0.4175)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.162 (0.318)  Loss:  0.5269 (0.8610)  Acc@1: 86.2028 (79.2680)  Acc@5: 98.2311 (94.9440)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-208.pth.tar', 79.356)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-210.pth.tar', 79.26799995361328)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-207.pth.tar', 79.13400002929687)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-203.pth.tar', 79.04800003173828)

Train: 211 [   0/1251 (  0%)]  Loss: 3.416 (3.42)  Time: 3.084s,  332.01/s  (3.084s,  332.01/s)  LR: 2.059e-04  Data: 2.077 (2.077)
Train: 211 [  50/1251 (  4%)]  Loss: 3.206 (3.31)  Time: 0.654s, 1565.55/s  (0.674s, 1519.18/s)  LR: 2.057e-04  Data: 0.013 (0.054)
Train: 211 [ 100/1251 (  8%)]  Loss: 3.048 (3.22)  Time: 0.653s, 1569.31/s  (0.665s, 1539.27/s)  LR: 2.055e-04  Data: 0.014 (0.034)
Train: 211 [ 150/1251 ( 12%)]  Loss: 3.149 (3.20)  Time: 0.662s, 1546.54/s  (0.664s, 1542.68/s)  LR: 2.054e-04  Data: 0.014 (0.027)
Train: 211 [ 200/1251 ( 16%)]  Loss: 3.530 (3.27)  Time: 0.656s, 1561.11/s  (0.663s, 1543.87/s)  LR: 2.052e-04  Data: 0.015 (0.024)
Train: 211 [ 250/1251 ( 20%)]  Loss: 3.115 (3.24)  Time: 0.664s, 1541.18/s  (0.663s, 1543.58/s)  LR: 2.050e-04  Data: 0.014 (0.022)
Train: 211 [ 300/1251 ( 24%)]  Loss: 2.996 (3.21)  Time: 0.665s, 1538.72/s  (0.663s, 1543.89/s)  LR: 2.049e-04  Data: 0.013 (0.021)
Train: 211 [ 350/1251 ( 28%)]  Loss: 3.293 (3.22)  Time: 0.665s, 1539.58/s  (0.663s, 1543.88/s)  LR: 2.047e-04  Data: 0.013 (0.020)
Train: 211 [ 400/1251 ( 32%)]  Loss: 3.538 (3.25)  Time: 0.663s, 1544.52/s  (0.663s, 1544.29/s)  LR: 2.045e-04  Data: 0.014 (0.019)
Train: 211 [ 450/1251 ( 36%)]  Loss: 3.043 (3.23)  Time: 0.663s, 1544.59/s  (0.663s, 1543.96/s)  LR: 2.044e-04  Data: 0.014 (0.018)
Train: 211 [ 500/1251 ( 40%)]  Loss: 3.258 (3.24)  Time: 0.662s, 1547.29/s  (0.664s, 1543.26/s)  LR: 2.042e-04  Data: 0.014 (0.018)
Train: 211 [ 550/1251 ( 44%)]  Loss: 2.982 (3.21)  Time: 0.667s, 1535.65/s  (0.664s, 1542.52/s)  LR: 2.040e-04  Data: 0.014 (0.017)
Train: 211 [ 600/1251 ( 48%)]  Loss: 3.246 (3.22)  Time: 0.668s, 1533.98/s  (0.664s, 1542.52/s)  LR: 2.039e-04  Data: 0.013 (0.017)
Train: 211 [ 650/1251 ( 52%)]  Loss: 3.284 (3.22)  Time: 0.663s, 1545.06/s  (0.664s, 1542.07/s)  LR: 2.037e-04  Data: 0.012 (0.017)
Train: 211 [ 700/1251 ( 56%)]  Loss: 3.127 (3.22)  Time: 0.658s, 1555.94/s  (0.664s, 1541.90/s)  LR: 2.035e-04  Data: 0.013 (0.017)
Train: 211 [ 750/1251 ( 60%)]  Loss: 3.198 (3.21)  Time: 0.669s, 1529.56/s  (0.664s, 1541.75/s)  LR: 2.034e-04  Data: 0.012 (0.016)
Train: 211 [ 800/1251 ( 64%)]  Loss: 3.174 (3.21)  Time: 0.662s, 1546.79/s  (0.664s, 1541.75/s)  LR: 2.032e-04  Data: 0.014 (0.016)
Train: 211 [ 850/1251 ( 68%)]  Loss: 2.948 (3.20)  Time: 0.666s, 1537.81/s  (0.664s, 1541.68/s)  LR: 2.030e-04  Data: 0.013 (0.016)
Train: 211 [ 900/1251 ( 72%)]  Loss: 3.316 (3.20)  Time: 0.653s, 1566.95/s  (0.664s, 1541.99/s)  LR: 2.029e-04  Data: 0.013 (0.016)
Train: 211 [ 950/1251 ( 76%)]  Loss: 3.461 (3.22)  Time: 0.654s, 1566.63/s  (0.664s, 1542.15/s)  LR: 2.027e-04  Data: 0.015 (0.016)
Train: 211 [1000/1251 ( 80%)]  Loss: 3.472 (3.23)  Time: 0.660s, 1550.81/s  (0.664s, 1542.49/s)  LR: 2.025e-04  Data: 0.016 (0.016)
Train: 211 [1050/1251 ( 84%)]  Loss: 3.260 (3.23)  Time: 0.661s, 1550.16/s  (0.664s, 1542.66/s)  LR: 2.024e-04  Data: 0.014 (0.016)
Train: 211 [1100/1251 ( 88%)]  Loss: 3.304 (3.23)  Time: 0.664s, 1541.19/s  (0.664s, 1543.10/s)  LR: 2.022e-04  Data: 0.015 (0.016)
Train: 211 [1150/1251 ( 92%)]  Loss: 3.311 (3.24)  Time: 0.650s, 1575.63/s  (0.663s, 1543.36/s)  LR: 2.020e-04  Data: 0.013 (0.015)
Train: 211 [1200/1251 ( 96%)]  Loss: 2.818 (3.22)  Time: 0.651s, 1572.32/s  (0.663s, 1543.50/s)  LR: 2.019e-04  Data: 0.013 (0.015)
Train: 211 [1250/1251 (100%)]  Loss: 2.648 (3.20)  Time: 0.648s, 1579.53/s  (0.663s, 1543.98/s)  LR: 2.017e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.874 (2.874)  Loss:  0.4209 (0.4209)  Acc@1: 91.6992 (91.6992)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.319)  Loss:  0.5303 (0.8914)  Acc@1: 87.5000 (79.2980)  Acc@5: 97.2877 (94.8940)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-208.pth.tar', 79.356)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-211.pth.tar', 79.29799987060547)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-210.pth.tar', 79.26799995361328)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-207.pth.tar', 79.13400002929687)

Train: 212 [   0/1251 (  0%)]  Loss: 3.258 (3.26)  Time: 3.809s,  268.80/s  (3.809s,  268.80/s)  LR: 2.017e-04  Data: 1.837 (1.837)
Train: 212 [  50/1251 (  4%)]  Loss: 3.036 (3.15)  Time: 0.645s, 1588.78/s  (0.683s, 1499.66/s)  LR: 2.015e-04  Data: 0.013 (0.050)
Train: 212 [ 100/1251 (  8%)]  Loss: 3.252 (3.18)  Time: 0.641s, 1596.75/s  (0.667s, 1536.25/s)  LR: 2.014e-04  Data: 0.016 (0.032)
Train: 212 [ 150/1251 ( 12%)]  Loss: 3.277 (3.21)  Time: 0.658s, 1556.14/s  (0.663s, 1545.01/s)  LR: 2.012e-04  Data: 0.015 (0.026)
Train: 212 [ 200/1251 ( 16%)]  Loss: 3.142 (3.19)  Time: 0.659s, 1553.41/s  (0.661s, 1549.44/s)  LR: 2.010e-04  Data: 0.015 (0.023)
Train: 212 [ 250/1251 ( 20%)]  Loss: 3.385 (3.22)  Time: 0.664s, 1542.91/s  (0.660s, 1551.75/s)  LR: 2.009e-04  Data: 0.014 (0.021)
Train: 212 [ 300/1251 ( 24%)]  Loss: 3.144 (3.21)  Time: 0.653s, 1567.73/s  (0.660s, 1552.04/s)  LR: 2.007e-04  Data: 0.013 (0.020)
Train: 212 [ 350/1251 ( 28%)]  Loss: 3.341 (3.23)  Time: 0.667s, 1535.54/s  (0.660s, 1552.28/s)  LR: 2.005e-04  Data: 0.016 (0.019)
Train: 212 [ 400/1251 ( 32%)]  Loss: 3.202 (3.23)  Time: 0.663s, 1544.41/s  (0.660s, 1551.54/s)  LR: 2.004e-04  Data: 0.013 (0.018)
Train: 212 [ 450/1251 ( 36%)]  Loss: 3.178 (3.22)  Time: 0.667s, 1535.72/s  (0.660s, 1551.01/s)  LR: 2.002e-04  Data: 0.016 (0.018)
Train: 212 [ 500/1251 ( 40%)]  Loss: 3.015 (3.20)  Time: 0.658s, 1556.52/s  (0.660s, 1550.39/s)  LR: 2.001e-04  Data: 0.013 (0.017)
Train: 212 [ 550/1251 ( 44%)]  Loss: 3.025 (3.19)  Time: 0.665s, 1539.56/s  (0.660s, 1550.42/s)  LR: 1.999e-04  Data: 0.013 (0.017)
Train: 212 [ 600/1251 ( 48%)]  Loss: 3.365 (3.20)  Time: 0.656s, 1560.47/s  (0.660s, 1550.80/s)  LR: 1.997e-04  Data: 0.013 (0.017)
Train: 212 [ 650/1251 ( 52%)]  Loss: 2.850 (3.18)  Time: 0.662s, 1546.91/s  (0.660s, 1550.90/s)  LR: 1.996e-04  Data: 0.016 (0.016)
Train: 212 [ 700/1251 ( 56%)]  Loss: 3.216 (3.18)  Time: 0.662s, 1547.13/s  (0.660s, 1550.56/s)  LR: 1.994e-04  Data: 0.012 (0.016)
Train: 212 [ 750/1251 ( 60%)]  Loss: 3.302 (3.19)  Time: 0.660s, 1550.60/s  (0.661s, 1550.27/s)  LR: 1.992e-04  Data: 0.016 (0.016)
Train: 212 [ 800/1251 ( 64%)]  Loss: 3.398 (3.20)  Time: 0.664s, 1542.47/s  (0.661s, 1550.21/s)  LR: 1.991e-04  Data: 0.014 (0.016)
Train: 212 [ 850/1251 ( 68%)]  Loss: 3.060 (3.19)  Time: 0.665s, 1540.85/s  (0.661s, 1550.23/s)  LR: 1.989e-04  Data: 0.013 (0.016)
Train: 212 [ 900/1251 ( 72%)]  Loss: 3.291 (3.20)  Time: 0.660s, 1551.45/s  (0.661s, 1550.22/s)  LR: 1.987e-04  Data: 0.014 (0.016)
Train: 212 [ 950/1251 ( 76%)]  Loss: 3.345 (3.20)  Time: 0.654s, 1566.69/s  (0.661s, 1550.31/s)  LR: 1.986e-04  Data: 0.017 (0.016)
Train: 212 [1000/1251 ( 80%)]  Loss: 3.507 (3.22)  Time: 0.668s, 1531.96/s  (0.661s, 1549.98/s)  LR: 1.984e-04  Data: 0.013 (0.016)
Train: 212 [1050/1251 ( 84%)]  Loss: 3.049 (3.21)  Time: 0.659s, 1554.07/s  (0.661s, 1549.85/s)  LR: 1.982e-04  Data: 0.014 (0.015)
Train: 212 [1100/1251 ( 88%)]  Loss: 3.263 (3.21)  Time: 0.669s, 1530.72/s  (0.661s, 1549.44/s)  LR: 1.981e-04  Data: 0.015 (0.015)
Train: 212 [1150/1251 ( 92%)]  Loss: 3.001 (3.20)  Time: 0.652s, 1570.32/s  (0.661s, 1549.33/s)  LR: 1.979e-04  Data: 0.013 (0.015)
Train: 212 [1200/1251 ( 96%)]  Loss: 2.973 (3.19)  Time: 0.661s, 1548.98/s  (0.661s, 1549.28/s)  LR: 1.977e-04  Data: 0.014 (0.015)
Train: 212 [1250/1251 (100%)]  Loss: 3.207 (3.20)  Time: 0.643s, 1591.78/s  (0.661s, 1549.26/s)  LR: 1.976e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.846 (2.846)  Loss:  0.3972 (0.3972)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.161 (0.315)  Loss:  0.5166 (0.8645)  Acc@1: 87.6179 (79.3560)  Acc@5: 97.8774 (94.9740)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-208.pth.tar', 79.356)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-212.pth.tar', 79.355999921875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-211.pth.tar', 79.29799987060547)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-210.pth.tar', 79.26799995361328)

Train: 213 [   0/1251 (  0%)]  Loss: 3.253 (3.25)  Time: 3.347s,  305.95/s  (3.347s,  305.95/s)  LR: 1.976e-04  Data: 1.569 (1.569)
Train: 213 [  50/1251 (  4%)]  Loss: 3.435 (3.34)  Time: 0.648s, 1580.28/s  (0.681s, 1502.84/s)  LR: 1.974e-04  Data: 0.014 (0.045)
Train: 213 [ 100/1251 (  8%)]  Loss: 3.261 (3.32)  Time: 0.653s, 1568.79/s  (0.668s, 1532.03/s)  LR: 1.972e-04  Data: 0.013 (0.029)
Train: 213 [ 150/1251 ( 12%)]  Loss: 3.477 (3.36)  Time: 0.662s, 1545.85/s  (0.665s, 1539.64/s)  LR: 1.971e-04  Data: 0.014 (0.024)
Train: 213 [ 200/1251 ( 16%)]  Loss: 3.401 (3.37)  Time: 0.669s, 1531.30/s  (0.663s, 1543.90/s)  LR: 1.969e-04  Data: 0.013 (0.022)
Train: 213 [ 250/1251 ( 20%)]  Loss: 3.033 (3.31)  Time: 0.673s, 1522.15/s  (0.663s, 1545.30/s)  LR: 1.968e-04  Data: 0.014 (0.020)
Train: 213 [ 300/1251 ( 24%)]  Loss: 2.997 (3.27)  Time: 0.661s, 1548.45/s  (0.662s, 1546.76/s)  LR: 1.966e-04  Data: 0.014 (0.019)
Train: 213 [ 350/1251 ( 28%)]  Loss: 3.083 (3.24)  Time: 0.653s, 1568.91/s  (0.662s, 1547.22/s)  LR: 1.964e-04  Data: 0.013 (0.018)
Train: 213 [ 400/1251 ( 32%)]  Loss: 3.171 (3.23)  Time: 0.654s, 1564.75/s  (0.662s, 1547.69/s)  LR: 1.963e-04  Data: 0.013 (0.018)
Train: 213 [ 450/1251 ( 36%)]  Loss: 3.036 (3.21)  Time: 0.661s, 1548.15/s  (0.661s, 1548.27/s)  LR: 1.961e-04  Data: 0.013 (0.017)
Train: 213 [ 500/1251 ( 40%)]  Loss: 3.098 (3.20)  Time: 0.657s, 1558.07/s  (0.661s, 1548.90/s)  LR: 1.959e-04  Data: 0.013 (0.017)
Train: 213 [ 550/1251 ( 44%)]  Loss: 3.233 (3.21)  Time: 0.662s, 1547.88/s  (0.661s, 1549.36/s)  LR: 1.958e-04  Data: 0.013 (0.017)
Train: 213 [ 600/1251 ( 48%)]  Loss: 3.369 (3.22)  Time: 0.658s, 1556.12/s  (0.661s, 1549.47/s)  LR: 1.956e-04  Data: 0.013 (0.016)
Train: 213 [ 650/1251 ( 52%)]  Loss: 3.077 (3.21)  Time: 0.664s, 1542.34/s  (0.661s, 1549.45/s)  LR: 1.954e-04  Data: 0.014 (0.016)
Train: 213 [ 700/1251 ( 56%)]  Loss: 2.979 (3.19)  Time: 0.662s, 1545.99/s  (0.661s, 1549.76/s)  LR: 1.953e-04  Data: 0.013 (0.016)
Train: 213 [ 750/1251 ( 60%)]  Loss: 3.160 (3.19)  Time: 0.663s, 1544.92/s  (0.661s, 1550.21/s)  LR: 1.951e-04  Data: 0.013 (0.016)
Train: 213 [ 800/1251 ( 64%)]  Loss: 3.406 (3.20)  Time: 0.657s, 1557.67/s  (0.661s, 1550.29/s)  LR: 1.949e-04  Data: 0.013 (0.016)
Train: 213 [ 850/1251 ( 68%)]  Loss: 3.358 (3.21)  Time: 0.657s, 1558.07/s  (0.660s, 1550.40/s)  LR: 1.948e-04  Data: 0.013 (0.016)
Train: 213 [ 900/1251 ( 72%)]  Loss: 3.399 (3.22)  Time: 0.655s, 1562.18/s  (0.660s, 1550.53/s)  LR: 1.946e-04  Data: 0.013 (0.015)
Train: 213 [ 950/1251 ( 76%)]  Loss: 3.190 (3.22)  Time: 0.666s, 1537.12/s  (0.660s, 1550.58/s)  LR: 1.945e-04  Data: 0.014 (0.015)
Train: 213 [1000/1251 ( 80%)]  Loss: 3.147 (3.22)  Time: 0.661s, 1548.87/s  (0.660s, 1550.84/s)  LR: 1.943e-04  Data: 0.013 (0.015)
Train: 213 [1050/1251 ( 84%)]  Loss: 3.455 (3.23)  Time: 0.664s, 1543.21/s  (0.660s, 1550.95/s)  LR: 1.941e-04  Data: 0.013 (0.015)
Train: 213 [1100/1251 ( 88%)]  Loss: 3.256 (3.23)  Time: 0.667s, 1534.35/s  (0.660s, 1550.77/s)  LR: 1.940e-04  Data: 0.013 (0.015)
Train: 213 [1150/1251 ( 92%)]  Loss: 2.993 (3.22)  Time: 0.661s, 1548.54/s  (0.660s, 1550.87/s)  LR: 1.938e-04  Data: 0.013 (0.015)
Train: 213 [1200/1251 ( 96%)]  Loss: 2.843 (3.20)  Time: 0.661s, 1548.40/s  (0.660s, 1550.85/s)  LR: 1.936e-04  Data: 0.014 (0.015)
Train: 213 [1250/1251 (100%)]  Loss: 3.255 (3.21)  Time: 0.647s, 1581.83/s  (0.660s, 1550.81/s)  LR: 1.935e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.876 (2.876)  Loss:  0.4092 (0.4092)  Acc@1: 91.8945 (91.8945)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.161 (0.319)  Loss:  0.5239 (0.8664)  Acc@1: 87.0283 (79.4560)  Acc@5: 97.6415 (94.9440)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-213.pth.tar', 79.4559999243164)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-208.pth.tar', 79.356)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-212.pth.tar', 79.355999921875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-211.pth.tar', 79.29799987060547)

Train: 214 [   0/1251 (  0%)]  Loss: 3.011 (3.01)  Time: 3.289s,  311.30/s  (3.289s,  311.30/s)  LR: 1.935e-04  Data: 1.611 (1.611)
Train: 214 [  50/1251 (  4%)]  Loss: 3.156 (3.08)  Time: 0.644s, 1589.76/s  (0.673s, 1522.05/s)  LR: 1.933e-04  Data: 0.014 (0.045)
Train: 214 [ 100/1251 (  8%)]  Loss: 3.474 (3.21)  Time: 0.661s, 1548.96/s  (0.662s, 1546.69/s)  LR: 1.931e-04  Data: 0.013 (0.030)
Train: 214 [ 150/1251 ( 12%)]  Loss: 3.037 (3.17)  Time: 0.656s, 1560.89/s  (0.661s, 1549.53/s)  LR: 1.930e-04  Data: 0.012 (0.024)
Train: 214 [ 200/1251 ( 16%)]  Loss: 3.050 (3.15)  Time: 0.656s, 1560.91/s  (0.661s, 1549.64/s)  LR: 1.928e-04  Data: 0.013 (0.022)
Train: 214 [ 250/1251 ( 20%)]  Loss: 3.448 (3.20)  Time: 0.659s, 1554.28/s  (0.661s, 1549.42/s)  LR: 1.927e-04  Data: 0.014 (0.020)
Train: 214 [ 300/1251 ( 24%)]  Loss: 3.063 (3.18)  Time: 0.661s, 1549.21/s  (0.661s, 1549.96/s)  LR: 1.925e-04  Data: 0.013 (0.019)
Train: 214 [ 350/1251 ( 28%)]  Loss: 3.259 (3.19)  Time: 0.661s, 1549.09/s  (0.660s, 1550.51/s)  LR: 1.923e-04  Data: 0.014 (0.018)
Train: 214 [ 400/1251 ( 32%)]  Loss: 3.127 (3.18)  Time: 0.663s, 1545.00/s  (0.660s, 1550.42/s)  LR: 1.922e-04  Data: 0.013 (0.018)
Train: 214 [ 450/1251 ( 36%)]  Loss: 2.996 (3.16)  Time: 0.650s, 1575.67/s  (0.660s, 1550.64/s)  LR: 1.920e-04  Data: 0.013 (0.017)
Train: 214 [ 500/1251 ( 40%)]  Loss: 2.893 (3.14)  Time: 0.663s, 1545.57/s  (0.660s, 1550.80/s)  LR: 1.918e-04  Data: 0.013 (0.017)
Train: 214 [ 550/1251 ( 44%)]  Loss: 3.093 (3.13)  Time: 0.668s, 1533.80/s  (0.660s, 1550.43/s)  LR: 1.917e-04  Data: 0.016 (0.017)
Train: 214 [ 600/1251 ( 48%)]  Loss: 3.112 (3.13)  Time: 0.651s, 1572.53/s  (0.661s, 1550.13/s)  LR: 1.915e-04  Data: 0.013 (0.016)
Train: 214 [ 650/1251 ( 52%)]  Loss: 3.382 (3.15)  Time: 0.658s, 1555.07/s  (0.661s, 1549.96/s)  LR: 1.914e-04  Data: 0.014 (0.016)
Train: 214 [ 700/1251 ( 56%)]  Loss: 3.071 (3.14)  Time: 0.664s, 1541.95/s  (0.661s, 1549.65/s)  LR: 1.912e-04  Data: 0.015 (0.016)
Train: 214 [ 750/1251 ( 60%)]  Loss: 3.020 (3.14)  Time: 0.655s, 1563.48/s  (0.661s, 1549.40/s)  LR: 1.910e-04  Data: 0.013 (0.016)
Train: 214 [ 800/1251 ( 64%)]  Loss: 3.224 (3.14)  Time: 0.669s, 1530.43/s  (0.661s, 1549.17/s)  LR: 1.909e-04  Data: 0.013 (0.016)
Train: 214 [ 850/1251 ( 68%)]  Loss: 3.166 (3.14)  Time: 0.663s, 1543.41/s  (0.661s, 1548.76/s)  LR: 1.907e-04  Data: 0.014 (0.015)
Train: 214 [ 900/1251 ( 72%)]  Loss: 3.145 (3.14)  Time: 0.652s, 1571.55/s  (0.661s, 1548.44/s)  LR: 1.905e-04  Data: 0.014 (0.015)
Train: 214 [ 950/1251 ( 76%)]  Loss: 3.187 (3.15)  Time: 0.665s, 1538.82/s  (0.661s, 1548.16/s)  LR: 1.904e-04  Data: 0.014 (0.015)
Train: 214 [1000/1251 ( 80%)]  Loss: 3.300 (3.15)  Time: 0.660s, 1551.93/s  (0.662s, 1547.90/s)  LR: 1.902e-04  Data: 0.013 (0.015)
Train: 214 [1050/1251 ( 84%)]  Loss: 3.014 (3.15)  Time: 0.656s, 1561.02/s  (0.662s, 1547.76/s)  LR: 1.901e-04  Data: 0.013 (0.015)
Train: 214 [1100/1251 ( 88%)]  Loss: 3.261 (3.15)  Time: 0.658s, 1556.68/s  (0.661s, 1548.00/s)  LR: 1.899e-04  Data: 0.015 (0.015)
Train: 214 [1150/1251 ( 92%)]  Loss: 3.184 (3.15)  Time: 0.668s, 1533.17/s  (0.661s, 1548.21/s)  LR: 1.897e-04  Data: 0.013 (0.015)
Train: 214 [1200/1251 ( 96%)]  Loss: 3.049 (3.15)  Time: 0.662s, 1546.34/s  (0.661s, 1548.32/s)  LR: 1.896e-04  Data: 0.016 (0.015)
Train: 214 [1250/1251 (100%)]  Loss: 3.365 (3.16)  Time: 0.644s, 1589.86/s  (0.661s, 1548.42/s)  LR: 1.894e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.166 (3.166)  Loss:  0.4041 (0.4041)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.162 (0.324)  Loss:  0.5283 (0.8679)  Acc@1: 86.7925 (79.5680)  Acc@5: 98.1132 (95.0240)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-214.pth.tar', 79.5680000805664)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-213.pth.tar', 79.4559999243164)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-208.pth.tar', 79.356)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-212.pth.tar', 79.355999921875)

Train: 215 [   0/1251 (  0%)]  Loss: 3.404 (3.40)  Time: 3.189s,  321.13/s  (3.189s,  321.13/s)  LR: 1.894e-04  Data: 1.785 (1.785)
Train: 215 [  50/1251 (  4%)]  Loss: 3.315 (3.36)  Time: 0.640s, 1599.73/s  (0.673s, 1520.45/s)  LR: 1.892e-04  Data: 0.014 (0.049)
Train: 215 [ 100/1251 (  8%)]  Loss: 3.436 (3.39)  Time: 0.656s, 1561.56/s  (0.662s, 1546.29/s)  LR: 1.891e-04  Data: 0.013 (0.031)
Train: 215 [ 150/1251 ( 12%)]  Loss: 3.208 (3.34)  Time: 0.652s, 1570.48/s  (0.660s, 1551.85/s)  LR: 1.889e-04  Data: 0.013 (0.025)
Train: 215 [ 200/1251 ( 16%)]  Loss: 3.337 (3.34)  Time: 0.662s, 1547.79/s  (0.660s, 1552.37/s)  LR: 1.888e-04  Data: 0.014 (0.022)
Train: 215 [ 250/1251 ( 20%)]  Loss: 3.239 (3.32)  Time: 0.666s, 1537.96/s  (0.660s, 1551.05/s)  LR: 1.886e-04  Data: 0.016 (0.021)
Train: 215 [ 300/1251 ( 24%)]  Loss: 3.455 (3.34)  Time: 0.662s, 1547.60/s  (0.660s, 1550.77/s)  LR: 1.884e-04  Data: 0.015 (0.019)
Train: 215 [ 350/1251 ( 28%)]  Loss: 3.529 (3.37)  Time: 0.675s, 1517.13/s  (0.661s, 1549.30/s)  LR: 1.883e-04  Data: 0.013 (0.019)
Train: 215 [ 400/1251 ( 32%)]  Loss: 3.100 (3.34)  Time: 0.654s, 1564.95/s  (0.661s, 1548.87/s)  LR: 1.881e-04  Data: 0.013 (0.018)
Train: 215 [ 450/1251 ( 36%)]  Loss: 3.485 (3.35)  Time: 0.661s, 1548.13/s  (0.661s, 1548.31/s)  LR: 1.880e-04  Data: 0.013 (0.018)
Train: 215 [ 500/1251 ( 40%)]  Loss: 3.250 (3.34)  Time: 0.658s, 1556.72/s  (0.662s, 1547.92/s)  LR: 1.878e-04  Data: 0.013 (0.017)
Train: 215 [ 550/1251 ( 44%)]  Loss: 3.274 (3.34)  Time: 0.667s, 1534.40/s  (0.662s, 1547.67/s)  LR: 1.876e-04  Data: 0.013 (0.017)
Train: 215 [ 600/1251 ( 48%)]  Loss: 3.233 (3.33)  Time: 0.653s, 1567.53/s  (0.662s, 1547.70/s)  LR: 1.875e-04  Data: 0.014 (0.016)
Train: 215 [ 650/1251 ( 52%)]  Loss: 3.210 (3.32)  Time: 0.674s, 1519.77/s  (0.662s, 1547.06/s)  LR: 1.873e-04  Data: 0.014 (0.016)
Train: 215 [ 700/1251 ( 56%)]  Loss: 2.980 (3.30)  Time: 0.669s, 1531.18/s  (0.662s, 1546.74/s)  LR: 1.871e-04  Data: 0.013 (0.016)
Train: 215 [ 750/1251 ( 60%)]  Loss: 3.294 (3.30)  Time: 0.661s, 1548.04/s  (0.662s, 1546.40/s)  LR: 1.870e-04  Data: 0.014 (0.016)
Train: 215 [ 800/1251 ( 64%)]  Loss: 3.341 (3.30)  Time: 0.665s, 1540.93/s  (0.662s, 1546.09/s)  LR: 1.868e-04  Data: 0.013 (0.016)
Train: 215 [ 850/1251 ( 68%)]  Loss: 3.273 (3.30)  Time: 0.663s, 1545.46/s  (0.662s, 1546.15/s)  LR: 1.867e-04  Data: 0.013 (0.016)
Train: 215 [ 900/1251 ( 72%)]  Loss: 3.326 (3.30)  Time: 0.658s, 1555.50/s  (0.662s, 1546.14/s)  LR: 1.865e-04  Data: 0.013 (0.015)
Train: 215 [ 950/1251 ( 76%)]  Loss: 3.204 (3.29)  Time: 0.662s, 1547.49/s  (0.662s, 1546.35/s)  LR: 1.863e-04  Data: 0.013 (0.015)
Train: 215 [1000/1251 ( 80%)]  Loss: 3.023 (3.28)  Time: 0.664s, 1542.32/s  (0.662s, 1546.59/s)  LR: 1.862e-04  Data: 0.013 (0.015)
Train: 215 [1050/1251 ( 84%)]  Loss: 3.276 (3.28)  Time: 0.657s, 1559.18/s  (0.662s, 1546.77/s)  LR: 1.860e-04  Data: 0.013 (0.015)
Train: 215 [1100/1251 ( 88%)]  Loss: 3.010 (3.27)  Time: 0.660s, 1551.14/s  (0.662s, 1546.99/s)  LR: 1.859e-04  Data: 0.013 (0.015)
Train: 215 [1150/1251 ( 92%)]  Loss: 3.110 (3.26)  Time: 0.669s, 1530.48/s  (0.662s, 1547.25/s)  LR: 1.857e-04  Data: 0.014 (0.015)
Train: 215 [1200/1251 ( 96%)]  Loss: 3.062 (3.26)  Time: 0.662s, 1545.66/s  (0.662s, 1547.61/s)  LR: 1.855e-04  Data: 0.016 (0.015)
Train: 215 [1250/1251 (100%)]  Loss: 3.493 (3.26)  Time: 0.636s, 1609.75/s  (0.661s, 1548.12/s)  LR: 1.854e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.809 (2.809)  Loss:  0.4292 (0.4292)  Acc@1: 91.4062 (91.4062)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.161 (0.317)  Loss:  0.5044 (0.8533)  Acc@1: 87.5000 (79.6500)  Acc@5: 98.1132 (95.0320)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-215.pth.tar', 79.65)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-214.pth.tar', 79.5680000805664)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-213.pth.tar', 79.4559999243164)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-208.pth.tar', 79.356)

Train: 216 [   0/1251 (  0%)]  Loss: 3.270 (3.27)  Time: 3.496s,  292.93/s  (3.496s,  292.93/s)  LR: 1.854e-04  Data: 1.735 (1.735)
Train: 216 [  50/1251 (  4%)]  Loss: 2.920 (3.09)  Time: 0.639s, 1603.09/s  (0.679s, 1507.15/s)  LR: 1.852e-04  Data: 0.014 (0.047)
Train: 216 [ 100/1251 (  8%)]  Loss: 3.362 (3.18)  Time: 0.663s, 1544.70/s  (0.665s, 1540.99/s)  LR: 1.851e-04  Data: 0.013 (0.031)
Train: 216 [ 150/1251 ( 12%)]  Loss: 3.132 (3.17)  Time: 0.648s, 1580.66/s  (0.661s, 1549.66/s)  LR: 1.849e-04  Data: 0.016 (0.025)
Train: 216 [ 200/1251 ( 16%)]  Loss: 3.099 (3.16)  Time: 0.648s, 1579.47/s  (0.659s, 1553.12/s)  LR: 1.847e-04  Data: 0.013 (0.022)
Train: 216 [ 250/1251 ( 20%)]  Loss: 2.917 (3.12)  Time: 0.660s, 1551.01/s  (0.658s, 1555.11/s)  LR: 1.846e-04  Data: 0.014 (0.021)
Train: 216 [ 300/1251 ( 24%)]  Loss: 3.476 (3.17)  Time: 0.652s, 1571.60/s  (0.658s, 1555.21/s)  LR: 1.844e-04  Data: 0.013 (0.019)
Train: 216 [ 350/1251 ( 28%)]  Loss: 2.901 (3.13)  Time: 0.663s, 1544.43/s  (0.659s, 1554.74/s)  LR: 1.843e-04  Data: 0.013 (0.019)
Train: 216 [ 400/1251 ( 32%)]  Loss: 3.198 (3.14)  Time: 0.657s, 1558.74/s  (0.659s, 1553.91/s)  LR: 1.841e-04  Data: 0.015 (0.018)
Train: 216 [ 450/1251 ( 36%)]  Loss: 3.325 (3.16)  Time: 0.653s, 1569.15/s  (0.659s, 1553.12/s)  LR: 1.839e-04  Data: 0.012 (0.017)
Train: 216 [ 500/1251 ( 40%)]  Loss: 3.341 (3.18)  Time: 0.669s, 1530.38/s  (0.660s, 1552.46/s)  LR: 1.838e-04  Data: 0.013 (0.017)
Train: 216 [ 550/1251 ( 44%)]  Loss: 3.018 (3.16)  Time: 0.673s, 1520.67/s  (0.660s, 1551.50/s)  LR: 1.836e-04  Data: 0.013 (0.017)
Train: 216 [ 600/1251 ( 48%)]  Loss: 3.295 (3.17)  Time: 0.662s, 1547.94/s  (0.660s, 1550.86/s)  LR: 1.835e-04  Data: 0.016 (0.016)
Train: 216 [ 650/1251 ( 52%)]  Loss: 3.135 (3.17)  Time: 0.660s, 1551.23/s  (0.661s, 1550.16/s)  LR: 1.833e-04  Data: 0.014 (0.016)
Train: 216 [ 700/1251 ( 56%)]  Loss: 3.080 (3.16)  Time: 0.661s, 1549.28/s  (0.661s, 1549.65/s)  LR: 1.831e-04  Data: 0.013 (0.016)
Train: 216 [ 750/1251 ( 60%)]  Loss: 3.467 (3.18)  Time: 0.665s, 1539.02/s  (0.661s, 1549.39/s)  LR: 1.830e-04  Data: 0.014 (0.016)
Train: 216 [ 800/1251 ( 64%)]  Loss: 3.113 (3.18)  Time: 0.657s, 1557.59/s  (0.661s, 1549.06/s)  LR: 1.828e-04  Data: 0.013 (0.016)
Train: 216 [ 850/1251 ( 68%)]  Loss: 3.055 (3.17)  Time: 0.661s, 1549.38/s  (0.661s, 1548.89/s)  LR: 1.827e-04  Data: 0.013 (0.016)
Train: 216 [ 900/1251 ( 72%)]  Loss: 3.306 (3.18)  Time: 0.652s, 1571.14/s  (0.661s, 1548.62/s)  LR: 1.825e-04  Data: 0.015 (0.016)
Train: 216 [ 950/1251 ( 76%)]  Loss: 3.493 (3.20)  Time: 0.661s, 1550.29/s  (0.661s, 1548.65/s)  LR: 1.823e-04  Data: 0.013 (0.015)
Train: 216 [1000/1251 ( 80%)]  Loss: 3.305 (3.20)  Time: 0.660s, 1552.39/s  (0.661s, 1548.52/s)  LR: 1.822e-04  Data: 0.013 (0.015)
Train: 216 [1050/1251 ( 84%)]  Loss: 3.090 (3.20)  Time: 0.661s, 1548.19/s  (0.661s, 1548.30/s)  LR: 1.820e-04  Data: 0.013 (0.015)
Train: 216 [1100/1251 ( 88%)]  Loss: 2.927 (3.18)  Time: 0.664s, 1543.16/s  (0.661s, 1548.54/s)  LR: 1.819e-04  Data: 0.014 (0.015)
Train: 216 [1150/1251 ( 92%)]  Loss: 3.277 (3.19)  Time: 0.665s, 1539.18/s  (0.661s, 1548.39/s)  LR: 1.817e-04  Data: 0.016 (0.015)
Train: 216 [1200/1251 ( 96%)]  Loss: 3.083 (3.18)  Time: 0.669s, 1531.62/s  (0.661s, 1548.14/s)  LR: 1.815e-04  Data: 0.013 (0.015)
Train: 216 [1250/1251 (100%)]  Loss: 3.191 (3.18)  Time: 0.652s, 1569.56/s  (0.661s, 1548.17/s)  LR: 1.814e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.877 (2.877)  Loss:  0.4055 (0.4055)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.326)  Loss:  0.5005 (0.8717)  Acc@1: 86.9104 (79.6420)  Acc@5: 98.1132 (95.0700)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-215.pth.tar', 79.65)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-216.pth.tar', 79.64199987304687)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-214.pth.tar', 79.5680000805664)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-213.pth.tar', 79.4559999243164)

Train: 217 [   0/1251 (  0%)]  Loss: 3.045 (3.04)  Time: 3.400s,  301.15/s  (3.400s,  301.15/s)  LR: 1.814e-04  Data: 1.643 (1.643)
Train: 217 [  50/1251 (  4%)]  Loss: 2.869 (2.96)  Time: 0.644s, 1590.17/s  (0.676s, 1514.18/s)  LR: 1.812e-04  Data: 0.014 (0.045)
Train: 217 [ 100/1251 (  8%)]  Loss: 3.472 (3.13)  Time: 0.668s, 1533.78/s  (0.666s, 1538.44/s)  LR: 1.811e-04  Data: 0.014 (0.030)
Train: 217 [ 150/1251 ( 12%)]  Loss: 2.925 (3.08)  Time: 0.665s, 1540.47/s  (0.663s, 1543.39/s)  LR: 1.809e-04  Data: 0.013 (0.025)
Train: 217 [ 200/1251 ( 16%)]  Loss: 3.287 (3.12)  Time: 0.656s, 1561.74/s  (0.663s, 1545.26/s)  LR: 1.807e-04  Data: 0.013 (0.022)
Train: 217 [ 250/1251 ( 20%)]  Loss: 3.239 (3.14)  Time: 0.662s, 1546.03/s  (0.663s, 1545.45/s)  LR: 1.806e-04  Data: 0.013 (0.020)
Train: 217 [ 300/1251 ( 24%)]  Loss: 2.962 (3.11)  Time: 0.655s, 1563.90/s  (0.662s, 1546.12/s)  LR: 1.804e-04  Data: 0.013 (0.019)
Train: 217 [ 350/1251 ( 28%)]  Loss: 3.251 (3.13)  Time: 0.661s, 1548.42/s  (0.662s, 1546.10/s)  LR: 1.803e-04  Data: 0.015 (0.019)
Train: 217 [ 400/1251 ( 32%)]  Loss: 3.161 (3.13)  Time: 0.667s, 1535.69/s  (0.662s, 1546.04/s)  LR: 1.801e-04  Data: 0.012 (0.018)
Train: 217 [ 450/1251 ( 36%)]  Loss: 3.113 (3.13)  Time: 0.662s, 1546.23/s  (0.662s, 1545.95/s)  LR: 1.800e-04  Data: 0.013 (0.017)
Train: 217 [ 500/1251 ( 40%)]  Loss: 3.258 (3.14)  Time: 0.665s, 1538.80/s  (0.662s, 1546.12/s)  LR: 1.798e-04  Data: 0.013 (0.017)
Train: 217 [ 550/1251 ( 44%)]  Loss: 3.380 (3.16)  Time: 0.665s, 1540.33/s  (0.662s, 1546.16/s)  LR: 1.796e-04  Data: 0.016 (0.017)
Train: 217 [ 600/1251 ( 48%)]  Loss: 3.264 (3.17)  Time: 0.656s, 1560.56/s  (0.662s, 1546.41/s)  LR: 1.795e-04  Data: 0.013 (0.017)
Train: 217 [ 650/1251 ( 52%)]  Loss: 3.233 (3.18)  Time: 0.653s, 1567.52/s  (0.662s, 1547.18/s)  LR: 1.793e-04  Data: 0.014 (0.016)
Train: 217 [ 700/1251 ( 56%)]  Loss: 3.169 (3.18)  Time: 0.652s, 1570.32/s  (0.662s, 1547.76/s)  LR: 1.792e-04  Data: 0.016 (0.016)
Train: 217 [ 750/1251 ( 60%)]  Loss: 3.301 (3.18)  Time: 0.645s, 1586.70/s  (0.661s, 1548.12/s)  LR: 1.790e-04  Data: 0.016 (0.016)
Train: 217 [ 800/1251 ( 64%)]  Loss: 3.419 (3.20)  Time: 0.661s, 1548.78/s  (0.661s, 1548.80/s)  LR: 1.788e-04  Data: 0.013 (0.016)
Train: 217 [ 850/1251 ( 68%)]  Loss: 3.080 (3.19)  Time: 0.655s, 1562.19/s  (0.661s, 1548.97/s)  LR: 1.787e-04  Data: 0.014 (0.016)
Train: 217 [ 900/1251 ( 72%)]  Loss: 3.091 (3.19)  Time: 0.654s, 1564.83/s  (0.661s, 1549.14/s)  LR: 1.785e-04  Data: 0.014 (0.016)
Train: 217 [ 950/1251 ( 76%)]  Loss: 3.327 (3.19)  Time: 0.659s, 1552.80/s  (0.661s, 1549.54/s)  LR: 1.784e-04  Data: 0.017 (0.016)
Train: 217 [1000/1251 ( 80%)]  Loss: 3.041 (3.19)  Time: 0.663s, 1545.58/s  (0.661s, 1549.73/s)  LR: 1.782e-04  Data: 0.013 (0.016)
Train: 217 [1050/1251 ( 84%)]  Loss: 3.103 (3.18)  Time: 0.661s, 1550.26/s  (0.661s, 1549.82/s)  LR: 1.781e-04  Data: 0.013 (0.015)
Train: 217 [1100/1251 ( 88%)]  Loss: 3.163 (3.18)  Time: 0.660s, 1550.53/s  (0.661s, 1549.73/s)  LR: 1.779e-04  Data: 0.013 (0.015)
Train: 217 [1150/1251 ( 92%)]  Loss: 3.284 (3.18)  Time: 0.664s, 1541.60/s  (0.661s, 1549.53/s)  LR: 1.777e-04  Data: 0.015 (0.015)
Train: 217 [1200/1251 ( 96%)]  Loss: 3.197 (3.19)  Time: 0.657s, 1559.15/s  (0.661s, 1549.38/s)  LR: 1.776e-04  Data: 0.013 (0.015)
Train: 217 [1250/1251 (100%)]  Loss: 2.979 (3.18)  Time: 0.660s, 1552.24/s  (0.661s, 1549.30/s)  LR: 1.774e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.925 (2.925)  Loss:  0.4001 (0.4001)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.162 (0.317)  Loss:  0.5244 (0.8662)  Acc@1: 87.5000 (79.7120)  Acc@5: 97.6415 (95.0320)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-217.pth.tar', 79.712)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-215.pth.tar', 79.65)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-216.pth.tar', 79.64199987304687)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-214.pth.tar', 79.5680000805664)

Train: 218 [   0/1251 (  0%)]  Loss: 3.099 (3.10)  Time: 3.179s,  322.10/s  (3.179s,  322.10/s)  LR: 1.774e-04  Data: 1.835 (1.835)
Train: 218 [  50/1251 (  4%)]  Loss: 3.238 (3.17)  Time: 0.650s, 1574.82/s  (0.670s, 1528.74/s)  LR: 1.773e-04  Data: 0.016 (0.049)
Train: 218 [ 100/1251 (  8%)]  Loss: 3.380 (3.24)  Time: 0.659s, 1553.88/s  (0.661s, 1549.02/s)  LR: 1.771e-04  Data: 0.015 (0.032)
Train: 218 [ 150/1251 ( 12%)]  Loss: 3.179 (3.22)  Time: 0.664s, 1542.52/s  (0.660s, 1552.21/s)  LR: 1.769e-04  Data: 0.016 (0.026)
Train: 218 [ 200/1251 ( 16%)]  Loss: 3.467 (3.27)  Time: 0.650s, 1575.04/s  (0.659s, 1553.39/s)  LR: 1.768e-04  Data: 0.014 (0.023)
Train: 218 [ 250/1251 ( 20%)]  Loss: 3.184 (3.26)  Time: 0.661s, 1550.04/s  (0.659s, 1553.15/s)  LR: 1.766e-04  Data: 0.014 (0.021)
Train: 218 [ 300/1251 ( 24%)]  Loss: 2.906 (3.21)  Time: 0.654s, 1565.84/s  (0.660s, 1552.47/s)  LR: 1.765e-04  Data: 0.015 (0.020)
Train: 218 [ 350/1251 ( 28%)]  Loss: 3.442 (3.24)  Time: 0.664s, 1543.17/s  (0.660s, 1552.08/s)  LR: 1.763e-04  Data: 0.015 (0.019)
Train: 218 [ 400/1251 ( 32%)]  Loss: 3.320 (3.25)  Time: 0.665s, 1539.45/s  (0.660s, 1551.79/s)  LR: 1.762e-04  Data: 0.014 (0.019)
Train: 218 [ 450/1251 ( 36%)]  Loss: 3.071 (3.23)  Time: 0.666s, 1537.82/s  (0.660s, 1551.38/s)  LR: 1.760e-04  Data: 0.013 (0.018)
Train: 218 [ 500/1251 ( 40%)]  Loss: 3.052 (3.21)  Time: 0.661s, 1549.36/s  (0.660s, 1550.94/s)  LR: 1.758e-04  Data: 0.014 (0.018)
Train: 218 [ 550/1251 ( 44%)]  Loss: 3.183 (3.21)  Time: 0.664s, 1542.58/s  (0.661s, 1550.29/s)  LR: 1.757e-04  Data: 0.015 (0.017)
Train: 218 [ 600/1251 ( 48%)]  Loss: 3.192 (3.21)  Time: 0.655s, 1564.02/s  (0.661s, 1549.80/s)  LR: 1.755e-04  Data: 0.015 (0.017)
Train: 218 [ 650/1251 ( 52%)]  Loss: 3.006 (3.19)  Time: 0.653s, 1567.06/s  (0.661s, 1549.78/s)  LR: 1.754e-04  Data: 0.015 (0.017)
Train: 218 [ 700/1251 ( 56%)]  Loss: 3.195 (3.19)  Time: 0.656s, 1560.72/s  (0.661s, 1549.79/s)  LR: 1.752e-04  Data: 0.019 (0.017)
Train: 218 [ 750/1251 ( 60%)]  Loss: 3.304 (3.20)  Time: 0.662s, 1546.18/s  (0.661s, 1549.96/s)  LR: 1.751e-04  Data: 0.014 (0.016)
Train: 218 [ 800/1251 ( 64%)]  Loss: 3.247 (3.20)  Time: 0.665s, 1538.77/s  (0.661s, 1550.10/s)  LR: 1.749e-04  Data: 0.012 (0.016)
Train: 218 [ 850/1251 ( 68%)]  Loss: 2.804 (3.18)  Time: 0.659s, 1554.09/s  (0.661s, 1550.05/s)  LR: 1.747e-04  Data: 0.013 (0.016)
Train: 218 [ 900/1251 ( 72%)]  Loss: 3.395 (3.19)  Time: 0.656s, 1560.65/s  (0.661s, 1550.04/s)  LR: 1.746e-04  Data: 0.013 (0.016)
Train: 218 [ 950/1251 ( 76%)]  Loss: 3.291 (3.20)  Time: 0.664s, 1543.11/s  (0.661s, 1549.81/s)  LR: 1.744e-04  Data: 0.012 (0.016)
Train: 218 [1000/1251 ( 80%)]  Loss: 3.321 (3.20)  Time: 0.655s, 1562.38/s  (0.661s, 1549.53/s)  LR: 1.743e-04  Data: 0.015 (0.016)
Train: 218 [1050/1251 ( 84%)]  Loss: 3.486 (3.22)  Time: 0.666s, 1538.00/s  (0.661s, 1549.40/s)  LR: 1.741e-04  Data: 0.016 (0.016)
Train: 218 [1100/1251 ( 88%)]  Loss: 3.293 (3.22)  Time: 0.663s, 1543.74/s  (0.661s, 1549.03/s)  LR: 1.740e-04  Data: 0.015 (0.016)
Train: 218 [1150/1251 ( 92%)]  Loss: 3.301 (3.22)  Time: 0.656s, 1561.58/s  (0.661s, 1549.02/s)  LR: 1.738e-04  Data: 0.013 (0.016)
Train: 218 [1200/1251 ( 96%)]  Loss: 3.073 (3.22)  Time: 0.657s, 1558.16/s  (0.661s, 1549.14/s)  LR: 1.737e-04  Data: 0.013 (0.016)
Train: 218 [1250/1251 (100%)]  Loss: 3.302 (3.22)  Time: 0.652s, 1570.09/s  (0.661s, 1549.30/s)  LR: 1.735e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.796 (2.796)  Loss:  0.4082 (0.4082)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.162 (0.318)  Loss:  0.5254 (0.8593)  Acc@1: 87.2642 (79.8380)  Acc@5: 97.8774 (95.0760)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-218.pth.tar', 79.83800002685547)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-217.pth.tar', 79.712)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-215.pth.tar', 79.65)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-216.pth.tar', 79.64199987304687)

Train: 219 [   0/1251 (  0%)]  Loss: 3.345 (3.34)  Time: 3.552s,  288.32/s  (3.552s,  288.32/s)  LR: 1.735e-04  Data: 1.752 (1.752)
Train: 219 [  50/1251 (  4%)]  Loss: 3.166 (3.26)  Time: 0.645s, 1587.50/s  (0.675s, 1518.01/s)  LR: 1.733e-04  Data: 0.013 (0.048)
Train: 219 [ 100/1251 (  8%)]  Loss: 3.273 (3.26)  Time: 0.652s, 1571.34/s  (0.663s, 1544.05/s)  LR: 1.732e-04  Data: 0.016 (0.031)
Train: 219 [ 150/1251 ( 12%)]  Loss: 3.353 (3.28)  Time: 0.660s, 1551.95/s  (0.661s, 1548.27/s)  LR: 1.730e-04  Data: 0.013 (0.025)
Train: 219 [ 200/1251 ( 16%)]  Loss: 3.150 (3.26)  Time: 0.653s, 1567.32/s  (0.661s, 1549.05/s)  LR: 1.729e-04  Data: 0.013 (0.022)
Train: 219 [ 250/1251 ( 20%)]  Loss: 3.135 (3.24)  Time: 0.653s, 1569.26/s  (0.661s, 1548.65/s)  LR: 1.727e-04  Data: 0.013 (0.020)
Train: 219 [ 300/1251 ( 24%)]  Loss: 3.114 (3.22)  Time: 0.660s, 1550.84/s  (0.661s, 1548.05/s)  LR: 1.726e-04  Data: 0.013 (0.019)
Train: 219 [ 350/1251 ( 28%)]  Loss: 3.290 (3.23)  Time: 0.655s, 1563.73/s  (0.661s, 1548.05/s)  LR: 1.724e-04  Data: 0.013 (0.018)
Train: 219 [ 400/1251 ( 32%)]  Loss: 3.188 (3.22)  Time: 0.655s, 1562.33/s  (0.661s, 1548.20/s)  LR: 1.722e-04  Data: 0.013 (0.018)
Train: 219 [ 450/1251 ( 36%)]  Loss: 3.314 (3.23)  Time: 0.660s, 1552.37/s  (0.661s, 1548.28/s)  LR: 1.721e-04  Data: 0.013 (0.017)
Train: 219 [ 500/1251 ( 40%)]  Loss: 3.494 (3.26)  Time: 0.665s, 1539.81/s  (0.661s, 1548.25/s)  LR: 1.719e-04  Data: 0.014 (0.017)
Train: 219 [ 550/1251 ( 44%)]  Loss: 3.233 (3.25)  Time: 0.666s, 1537.45/s  (0.661s, 1548.28/s)  LR: 1.718e-04  Data: 0.013 (0.017)
Train: 219 [ 600/1251 ( 48%)]  Loss: 3.250 (3.25)  Time: 0.665s, 1539.89/s  (0.661s, 1548.39/s)  LR: 1.716e-04  Data: 0.013 (0.016)
Train: 219 [ 650/1251 ( 52%)]  Loss: 3.042 (3.24)  Time: 0.662s, 1547.78/s  (0.661s, 1548.61/s)  LR: 1.715e-04  Data: 0.014 (0.016)
Train: 219 [ 700/1251 ( 56%)]  Loss: 3.035 (3.23)  Time: 0.670s, 1527.74/s  (0.661s, 1548.33/s)  LR: 1.713e-04  Data: 0.013 (0.016)
Train: 219 [ 750/1251 ( 60%)]  Loss: 3.476 (3.24)  Time: 0.654s, 1566.62/s  (0.661s, 1548.42/s)  LR: 1.712e-04  Data: 0.012 (0.016)
Train: 219 [ 800/1251 ( 64%)]  Loss: 3.207 (3.24)  Time: 0.666s, 1536.41/s  (0.661s, 1548.75/s)  LR: 1.710e-04  Data: 0.016 (0.016)
Train: 219 [ 850/1251 ( 68%)]  Loss: 3.171 (3.24)  Time: 0.665s, 1539.75/s  (0.661s, 1548.90/s)  LR: 1.708e-04  Data: 0.013 (0.016)
Train: 219 [ 900/1251 ( 72%)]  Loss: 3.001 (3.22)  Time: 0.658s, 1555.69/s  (0.661s, 1549.20/s)  LR: 1.707e-04  Data: 0.014 (0.016)
Train: 219 [ 950/1251 ( 76%)]  Loss: 3.285 (3.23)  Time: 0.663s, 1544.45/s  (0.661s, 1549.23/s)  LR: 1.705e-04  Data: 0.013 (0.015)
Train: 219 [1000/1251 ( 80%)]  Loss: 3.172 (3.22)  Time: 0.655s, 1562.86/s  (0.661s, 1549.32/s)  LR: 1.704e-04  Data: 0.013 (0.015)
Train: 219 [1050/1251 ( 84%)]  Loss: 3.028 (3.21)  Time: 0.652s, 1570.56/s  (0.661s, 1549.69/s)  LR: 1.702e-04  Data: 0.016 (0.015)
Train: 219 [1100/1251 ( 88%)]  Loss: 3.405 (3.22)  Time: 0.653s, 1567.51/s  (0.661s, 1550.20/s)  LR: 1.701e-04  Data: 0.012 (0.015)
Train: 219 [1150/1251 ( 92%)]  Loss: 3.462 (3.23)  Time: 0.656s, 1561.05/s  (0.660s, 1550.53/s)  LR: 1.699e-04  Data: 0.012 (0.015)
Train: 219 [1200/1251 ( 96%)]  Loss: 3.413 (3.24)  Time: 0.659s, 1553.37/s  (0.660s, 1550.87/s)  LR: 1.698e-04  Data: 0.015 (0.015)
Train: 219 [1250/1251 (100%)]  Loss: 3.352 (3.24)  Time: 0.648s, 1580.23/s  (0.660s, 1551.20/s)  LR: 1.696e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.824 (2.824)  Loss:  0.4192 (0.4192)  Acc@1: 91.6016 (91.6016)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.162 (0.321)  Loss:  0.5078 (0.8600)  Acc@1: 87.0283 (79.7360)  Acc@5: 98.5849 (95.0240)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-218.pth.tar', 79.83800002685547)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-219.pth.tar', 79.73600005371094)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-217.pth.tar', 79.712)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-215.pth.tar', 79.65)

Train: 220 [   0/1251 (  0%)]  Loss: 3.006 (3.01)  Time: 3.421s,  299.36/s  (3.421s,  299.36/s)  LR: 1.696e-04  Data: 1.515 (1.515)
Train: 220 [  50/1251 (  4%)]  Loss: 2.922 (2.96)  Time: 0.641s, 1596.34/s  (0.670s, 1529.32/s)  LR: 1.694e-04  Data: 0.014 (0.043)
Train: 220 [ 100/1251 (  8%)]  Loss: 3.219 (3.05)  Time: 0.653s, 1569.19/s  (0.659s, 1553.55/s)  LR: 1.693e-04  Data: 0.013 (0.029)
Train: 220 [ 150/1251 ( 12%)]  Loss: 3.273 (3.10)  Time: 0.658s, 1556.64/s  (0.658s, 1555.79/s)  LR: 1.691e-04  Data: 0.015 (0.024)
Train: 220 [ 200/1251 ( 16%)]  Loss: 3.264 (3.14)  Time: 0.655s, 1563.08/s  (0.658s, 1555.91/s)  LR: 1.690e-04  Data: 0.013 (0.021)
Train: 220 [ 250/1251 ( 20%)]  Loss: 3.180 (3.14)  Time: 0.658s, 1556.59/s  (0.658s, 1555.51/s)  LR: 1.688e-04  Data: 0.013 (0.020)
Train: 220 [ 300/1251 ( 24%)]  Loss: 3.090 (3.14)  Time: 0.654s, 1564.82/s  (0.659s, 1554.63/s)  LR: 1.687e-04  Data: 0.017 (0.019)
Train: 220 [ 350/1251 ( 28%)]  Loss: 3.288 (3.16)  Time: 0.666s, 1536.49/s  (0.659s, 1553.85/s)  LR: 1.685e-04  Data: 0.013 (0.018)
Train: 220 [ 400/1251 ( 32%)]  Loss: 3.010 (3.14)  Time: 0.663s, 1543.33/s  (0.659s, 1553.06/s)  LR: 1.684e-04  Data: 0.013 (0.017)
Train: 220 [ 450/1251 ( 36%)]  Loss: 3.301 (3.16)  Time: 0.657s, 1558.00/s  (0.659s, 1552.99/s)  LR: 1.682e-04  Data: 0.012 (0.017)
Train: 220 [ 500/1251 ( 40%)]  Loss: 3.082 (3.15)  Time: 0.662s, 1547.64/s  (0.660s, 1552.56/s)  LR: 1.681e-04  Data: 0.013 (0.017)
Train: 220 [ 550/1251 ( 44%)]  Loss: 2.945 (3.13)  Time: 0.666s, 1538.52/s  (0.660s, 1551.98/s)  LR: 1.679e-04  Data: 0.014 (0.016)
Train: 220 [ 600/1251 ( 48%)]  Loss: 2.932 (3.12)  Time: 0.662s, 1546.47/s  (0.660s, 1551.86/s)  LR: 1.678e-04  Data: 0.013 (0.016)
Train: 220 [ 650/1251 ( 52%)]  Loss: 3.097 (3.11)  Time: 0.654s, 1566.63/s  (0.660s, 1551.53/s)  LR: 1.676e-04  Data: 0.014 (0.016)
Train: 220 [ 700/1251 ( 56%)]  Loss: 2.787 (3.09)  Time: 0.659s, 1554.39/s  (0.660s, 1551.45/s)  LR: 1.674e-04  Data: 0.014 (0.016)
Train: 220 [ 750/1251 ( 60%)]  Loss: 2.911 (3.08)  Time: 0.664s, 1541.55/s  (0.660s, 1551.63/s)  LR: 1.673e-04  Data: 0.016 (0.016)
Train: 220 [ 800/1251 ( 64%)]  Loss: 3.089 (3.08)  Time: 0.665s, 1538.71/s  (0.660s, 1551.93/s)  LR: 1.671e-04  Data: 0.013 (0.015)
Train: 220 [ 850/1251 ( 68%)]  Loss: 3.187 (3.09)  Time: 0.656s, 1560.64/s  (0.660s, 1552.12/s)  LR: 1.670e-04  Data: 0.012 (0.015)
Train: 220 [ 900/1251 ( 72%)]  Loss: 3.084 (3.09)  Time: 0.656s, 1560.15/s  (0.660s, 1552.37/s)  LR: 1.668e-04  Data: 0.013 (0.015)
Train: 220 [ 950/1251 ( 76%)]  Loss: 3.331 (3.10)  Time: 0.654s, 1565.78/s  (0.659s, 1552.70/s)  LR: 1.667e-04  Data: 0.017 (0.015)
Train: 220 [1000/1251 ( 80%)]  Loss: 3.213 (3.11)  Time: 0.665s, 1540.50/s  (0.659s, 1552.70/s)  LR: 1.665e-04  Data: 0.013 (0.015)
Train: 220 [1050/1251 ( 84%)]  Loss: 3.040 (3.10)  Time: 0.652s, 1569.72/s  (0.660s, 1552.56/s)  LR: 1.664e-04  Data: 0.016 (0.015)
Train: 220 [1100/1251 ( 88%)]  Loss: 3.033 (3.10)  Time: 0.660s, 1550.36/s  (0.660s, 1552.56/s)  LR: 1.662e-04  Data: 0.012 (0.015)
Train: 220 [1150/1251 ( 92%)]  Loss: 3.099 (3.10)  Time: 0.661s, 1549.85/s  (0.660s, 1552.63/s)  LR: 1.661e-04  Data: 0.013 (0.015)
Train: 220 [1200/1251 ( 96%)]  Loss: 3.378 (3.11)  Time: 0.655s, 1563.03/s  (0.660s, 1552.67/s)  LR: 1.659e-04  Data: 0.013 (0.015)
Train: 220 [1250/1251 (100%)]  Loss: 3.267 (3.12)  Time: 0.654s, 1566.17/s  (0.659s, 1552.72/s)  LR: 1.658e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.199 (3.199)  Loss:  0.4136 (0.4136)  Acc@1: 91.7969 (91.7969)  Acc@5: 98.2422 (98.2422)
Test: [  48/48]  Time: 0.161 (0.319)  Loss:  0.5259 (0.8651)  Acc@1: 87.0283 (79.6360)  Acc@5: 98.1132 (94.9420)
Train: 221 [   0/1251 (  0%)]  Loss: 3.108 (3.11)  Time: 3.658s,  279.96/s  (3.658s,  279.96/s)  LR: 1.658e-04  Data: 1.938 (1.938)
Train: 221 [  50/1251 (  4%)]  Loss: 3.212 (3.16)  Time: 0.652s, 1571.08/s  (0.679s, 1507.38/s)  LR: 1.656e-04  Data: 0.014 (0.052)
Train: 221 [ 100/1251 (  8%)]  Loss: 3.112 (3.14)  Time: 0.652s, 1570.50/s  (0.665s, 1540.24/s)  LR: 1.654e-04  Data: 0.013 (0.033)
Train: 221 [ 150/1251 ( 12%)]  Loss: 3.259 (3.17)  Time: 0.658s, 1556.28/s  (0.661s, 1550.31/s)  LR: 1.653e-04  Data: 0.015 (0.027)
Train: 221 [ 200/1251 ( 16%)]  Loss: 3.422 (3.22)  Time: 0.649s, 1578.92/s  (0.660s, 1552.49/s)  LR: 1.651e-04  Data: 0.016 (0.023)
Train: 221 [ 250/1251 ( 20%)]  Loss: 3.312 (3.24)  Time: 0.656s, 1560.98/s  (0.659s, 1553.70/s)  LR: 1.650e-04  Data: 0.012 (0.021)
Train: 221 [ 300/1251 ( 24%)]  Loss: 3.354 (3.25)  Time: 0.658s, 1555.21/s  (0.659s, 1554.86/s)  LR: 1.648e-04  Data: 0.012 (0.020)
Train: 221 [ 350/1251 ( 28%)]  Loss: 3.032 (3.23)  Time: 0.658s, 1557.19/s  (0.659s, 1554.72/s)  LR: 1.647e-04  Data: 0.014 (0.019)
Train: 221 [ 400/1251 ( 32%)]  Loss: 3.315 (3.24)  Time: 0.663s, 1545.23/s  (0.659s, 1554.89/s)  LR: 1.645e-04  Data: 0.014 (0.019)
Train: 221 [ 450/1251 ( 36%)]  Loss: 3.050 (3.22)  Time: 0.654s, 1566.85/s  (0.659s, 1554.91/s)  LR: 1.644e-04  Data: 0.015 (0.018)
Train: 221 [ 500/1251 ( 40%)]  Loss: 3.173 (3.21)  Time: 0.669s, 1530.93/s  (0.659s, 1554.63/s)  LR: 1.642e-04  Data: 0.013 (0.017)
Train: 221 [ 550/1251 ( 44%)]  Loss: 3.310 (3.22)  Time: 0.655s, 1563.25/s  (0.659s, 1554.54/s)  LR: 1.641e-04  Data: 0.012 (0.017)
Train: 221 [ 600/1251 ( 48%)]  Loss: 3.144 (3.22)  Time: 0.657s, 1558.77/s  (0.659s, 1554.70/s)  LR: 1.639e-04  Data: 0.013 (0.017)
Train: 221 [ 650/1251 ( 52%)]  Loss: 2.880 (3.19)  Time: 0.660s, 1552.22/s  (0.659s, 1554.60/s)  LR: 1.638e-04  Data: 0.012 (0.016)
Train: 221 [ 700/1251 ( 56%)]  Loss: 3.229 (3.19)  Time: 0.660s, 1550.47/s  (0.659s, 1554.65/s)  LR: 1.636e-04  Data: 0.013 (0.016)
Train: 221 [ 750/1251 ( 60%)]  Loss: 3.066 (3.19)  Time: 0.665s, 1539.15/s  (0.659s, 1554.48/s)  LR: 1.635e-04  Data: 0.013 (0.016)
Train: 221 [ 800/1251 ( 64%)]  Loss: 3.236 (3.19)  Time: 0.663s, 1545.57/s  (0.659s, 1554.36/s)  LR: 1.633e-04  Data: 0.015 (0.016)
Train: 221 [ 850/1251 ( 68%)]  Loss: 3.358 (3.20)  Time: 0.658s, 1555.06/s  (0.659s, 1554.36/s)  LR: 1.632e-04  Data: 0.014 (0.016)
Train: 221 [ 900/1251 ( 72%)]  Loss: 2.904 (3.18)  Time: 0.661s, 1550.21/s  (0.659s, 1554.06/s)  LR: 1.630e-04  Data: 0.013 (0.016)
Train: 221 [ 950/1251 ( 76%)]  Loss: 3.107 (3.18)  Time: 0.660s, 1551.91/s  (0.659s, 1553.85/s)  LR: 1.628e-04  Data: 0.013 (0.016)
Train: 221 [1000/1251 ( 80%)]  Loss: 3.460 (3.19)  Time: 0.662s, 1546.34/s  (0.659s, 1553.75/s)  LR: 1.627e-04  Data: 0.012 (0.015)
Train: 221 [1050/1251 ( 84%)]  Loss: 3.028 (3.19)  Time: 0.659s, 1554.97/s  (0.659s, 1553.77/s)  LR: 1.625e-04  Data: 0.012 (0.015)
Train: 221 [1100/1251 ( 88%)]  Loss: 3.077 (3.18)  Time: 0.650s, 1574.28/s  (0.659s, 1553.79/s)  LR: 1.624e-04  Data: 0.011 (0.015)
Train: 221 [1150/1251 ( 92%)]  Loss: 3.172 (3.18)  Time: 0.671s, 1526.03/s  (0.659s, 1553.80/s)  LR: 1.622e-04  Data: 0.014 (0.015)
Train: 221 [1200/1251 ( 96%)]  Loss: 2.877 (3.17)  Time: 0.666s, 1536.63/s  (0.659s, 1553.69/s)  LR: 1.621e-04  Data: 0.013 (0.015)
Train: 221 [1250/1251 (100%)]  Loss: 3.097 (3.17)  Time: 0.661s, 1549.56/s  (0.659s, 1553.60/s)  LR: 1.619e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.850 (2.850)  Loss:  0.3933 (0.3933)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.162 (0.316)  Loss:  0.5215 (0.8566)  Acc@1: 87.0283 (79.6760)  Acc@5: 98.4670 (95.1100)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-218.pth.tar', 79.83800002685547)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-219.pth.tar', 79.73600005371094)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-217.pth.tar', 79.712)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-221.pth.tar', 79.6759999243164)

Train: 222 [   0/1251 (  0%)]  Loss: 3.036 (3.04)  Time: 3.071s,  333.42/s  (3.071s,  333.42/s)  LR: 1.619e-04  Data: 1.802 (1.802)
Train: 222 [  50/1251 (  4%)]  Loss: 3.004 (3.02)  Time: 0.647s, 1583.47/s  (0.671s, 1526.69/s)  LR: 1.618e-04  Data: 0.014 (0.049)
Train: 222 [ 100/1251 (  8%)]  Loss: 3.238 (3.09)  Time: 0.664s, 1543.19/s  (0.661s, 1548.44/s)  LR: 1.616e-04  Data: 0.012 (0.031)
Train: 222 [ 150/1251 ( 12%)]  Loss: 3.051 (3.08)  Time: 0.646s, 1584.45/s  (0.659s, 1554.04/s)  LR: 1.615e-04  Data: 0.014 (0.025)
Train: 222 [ 200/1251 ( 16%)]  Loss: 3.073 (3.08)  Time: 0.659s, 1554.99/s  (0.658s, 1555.42/s)  LR: 1.613e-04  Data: 0.017 (0.023)
Train: 222 [ 250/1251 ( 20%)]  Loss: 3.164 (3.09)  Time: 0.661s, 1548.48/s  (0.659s, 1554.83/s)  LR: 1.612e-04  Data: 0.013 (0.021)
Train: 222 [ 300/1251 ( 24%)]  Loss: 3.060 (3.09)  Time: 0.660s, 1550.74/s  (0.659s, 1554.88/s)  LR: 1.610e-04  Data: 0.014 (0.020)
Train: 222 [ 350/1251 ( 28%)]  Loss: 3.350 (3.12)  Time: 0.660s, 1550.60/s  (0.659s, 1554.87/s)  LR: 1.609e-04  Data: 0.012 (0.019)
Train: 222 [ 400/1251 ( 32%)]  Loss: 3.080 (3.12)  Time: 0.651s, 1573.64/s  (0.659s, 1554.19/s)  LR: 1.607e-04  Data: 0.013 (0.018)
Train: 222 [ 450/1251 ( 36%)]  Loss: 3.373 (3.14)  Time: 0.652s, 1571.04/s  (0.659s, 1554.29/s)  LR: 1.606e-04  Data: 0.013 (0.018)
Train: 222 [ 500/1251 ( 40%)]  Loss: 3.238 (3.15)  Time: 0.655s, 1562.53/s  (0.659s, 1554.70/s)  LR: 1.604e-04  Data: 0.016 (0.017)
Train: 222 [ 550/1251 ( 44%)]  Loss: 2.886 (3.13)  Time: 0.646s, 1585.50/s  (0.659s, 1554.47/s)  LR: 1.603e-04  Data: 0.013 (0.017)
Train: 222 [ 600/1251 ( 48%)]  Loss: 3.473 (3.16)  Time: 0.656s, 1560.82/s  (0.659s, 1554.49/s)  LR: 1.601e-04  Data: 0.013 (0.017)
Train: 222 [ 650/1251 ( 52%)]  Loss: 3.264 (3.16)  Time: 0.660s, 1552.33/s  (0.659s, 1553.95/s)  LR: 1.600e-04  Data: 0.013 (0.016)
Train: 222 [ 700/1251 ( 56%)]  Loss: 3.322 (3.17)  Time: 0.656s, 1561.07/s  (0.659s, 1553.67/s)  LR: 1.598e-04  Data: 0.014 (0.016)
Train: 222 [ 750/1251 ( 60%)]  Loss: 3.154 (3.17)  Time: 0.671s, 1526.27/s  (0.659s, 1553.12/s)  LR: 1.597e-04  Data: 0.013 (0.016)
Train: 222 [ 800/1251 ( 64%)]  Loss: 3.116 (3.17)  Time: 0.665s, 1539.56/s  (0.660s, 1552.58/s)  LR: 1.595e-04  Data: 0.014 (0.016)
Train: 222 [ 850/1251 ( 68%)]  Loss: 2.952 (3.16)  Time: 0.655s, 1563.95/s  (0.660s, 1552.30/s)  LR: 1.594e-04  Data: 0.013 (0.016)
Train: 222 [ 900/1251 ( 72%)]  Loss: 3.272 (3.16)  Time: 0.650s, 1575.95/s  (0.660s, 1552.05/s)  LR: 1.592e-04  Data: 0.013 (0.016)
Train: 222 [ 950/1251 ( 76%)]  Loss: 3.095 (3.16)  Time: 0.659s, 1553.39/s  (0.660s, 1552.04/s)  LR: 1.591e-04  Data: 0.015 (0.016)
Train: 222 [1000/1251 ( 80%)]  Loss: 3.122 (3.16)  Time: 0.657s, 1558.99/s  (0.660s, 1551.84/s)  LR: 1.589e-04  Data: 0.013 (0.016)
Train: 222 [1050/1251 ( 84%)]  Loss: 3.033 (3.15)  Time: 0.658s, 1556.49/s  (0.660s, 1551.86/s)  LR: 1.588e-04  Data: 0.014 (0.015)
Train: 222 [1100/1251 ( 88%)]  Loss: 3.193 (3.15)  Time: 0.668s, 1533.81/s  (0.660s, 1551.83/s)  LR: 1.586e-04  Data: 0.014 (0.015)
Train: 222 [1150/1251 ( 92%)]  Loss: 3.173 (3.16)  Time: 0.652s, 1570.47/s  (0.660s, 1551.71/s)  LR: 1.585e-04  Data: 0.014 (0.015)
Train: 222 [1200/1251 ( 96%)]  Loss: 3.285 (3.16)  Time: 0.659s, 1554.98/s  (0.660s, 1551.73/s)  LR: 1.583e-04  Data: 0.012 (0.015)
Train: 222 [1250/1251 (100%)]  Loss: 3.136 (3.16)  Time: 0.654s, 1566.26/s  (0.660s, 1551.52/s)  LR: 1.582e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.930 (2.930)  Loss:  0.4299 (0.4299)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.161 (0.319)  Loss:  0.5332 (0.8595)  Acc@1: 87.3821 (79.9100)  Acc@5: 97.9953 (95.1640)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-222.pth.tar', 79.910000078125)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-218.pth.tar', 79.83800002685547)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-219.pth.tar', 79.73600005371094)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-217.pth.tar', 79.712)

Train: 223 [   0/1251 (  0%)]  Loss: 3.356 (3.36)  Time: 3.287s,  311.58/s  (3.287s,  311.58/s)  LR: 1.582e-04  Data: 1.679 (1.679)
Train: 223 [  50/1251 (  4%)]  Loss: 2.948 (3.15)  Time: 0.644s, 1590.75/s  (0.674s, 1520.06/s)  LR: 1.580e-04  Data: 0.013 (0.046)
Train: 223 [ 100/1251 (  8%)]  Loss: 2.949 (3.08)  Time: 0.660s, 1551.34/s  (0.662s, 1545.85/s)  LR: 1.579e-04  Data: 0.013 (0.030)
Train: 223 [ 150/1251 ( 12%)]  Loss: 3.227 (3.12)  Time: 0.656s, 1560.40/s  (0.660s, 1550.64/s)  LR: 1.577e-04  Data: 0.014 (0.025)
Train: 223 [ 200/1251 ( 16%)]  Loss: 3.260 (3.15)  Time: 0.658s, 1556.18/s  (0.660s, 1551.58/s)  LR: 1.576e-04  Data: 0.012 (0.022)
Train: 223 [ 250/1251 ( 20%)]  Loss: 3.128 (3.14)  Time: 0.659s, 1554.76/s  (0.660s, 1552.42/s)  LR: 1.574e-04  Data: 0.013 (0.020)
Train: 223 [ 300/1251 ( 24%)]  Loss: 3.213 (3.15)  Time: 0.656s, 1560.94/s  (0.660s, 1551.53/s)  LR: 1.573e-04  Data: 0.013 (0.019)
Train: 223 [ 350/1251 ( 28%)]  Loss: 3.188 (3.16)  Time: 0.667s, 1535.87/s  (0.660s, 1551.37/s)  LR: 1.571e-04  Data: 0.013 (0.019)
Train: 223 [ 400/1251 ( 32%)]  Loss: 3.174 (3.16)  Time: 0.660s, 1551.18/s  (0.660s, 1551.25/s)  LR: 1.570e-04  Data: 0.013 (0.018)
Train: 223 [ 450/1251 ( 36%)]  Loss: 3.342 (3.18)  Time: 0.659s, 1552.96/s  (0.660s, 1551.30/s)  LR: 1.568e-04  Data: 0.014 (0.018)
Train: 223 [ 500/1251 ( 40%)]  Loss: 3.043 (3.17)  Time: 0.658s, 1555.16/s  (0.660s, 1551.28/s)  LR: 1.567e-04  Data: 0.014 (0.017)
Train: 223 [ 550/1251 ( 44%)]  Loss: 3.385 (3.18)  Time: 0.664s, 1542.55/s  (0.660s, 1551.05/s)  LR: 1.565e-04  Data: 0.014 (0.017)
Train: 223 [ 600/1251 ( 48%)]  Loss: 2.994 (3.17)  Time: 0.660s, 1551.77/s  (0.660s, 1550.73/s)  LR: 1.564e-04  Data: 0.013 (0.017)
Train: 223 [ 650/1251 ( 52%)]  Loss: 3.020 (3.16)  Time: 0.659s, 1553.62/s  (0.660s, 1550.35/s)  LR: 1.562e-04  Data: 0.014 (0.016)
Train: 223 [ 700/1251 ( 56%)]  Loss: 3.071 (3.15)  Time: 0.659s, 1553.28/s  (0.661s, 1550.09/s)  LR: 1.561e-04  Data: 0.013 (0.016)
Train: 223 [ 750/1251 ( 60%)]  Loss: 3.279 (3.16)  Time: 0.670s, 1527.59/s  (0.661s, 1549.60/s)  LR: 1.559e-04  Data: 0.015 (0.016)
Train: 223 [ 800/1251 ( 64%)]  Loss: 3.089 (3.16)  Time: 0.664s, 1541.26/s  (0.661s, 1549.12/s)  LR: 1.558e-04  Data: 0.013 (0.016)
Train: 223 [ 850/1251 ( 68%)]  Loss: 3.163 (3.16)  Time: 0.667s, 1536.34/s  (0.661s, 1548.85/s)  LR: 1.556e-04  Data: 0.013 (0.016)
Train: 223 [ 900/1251 ( 72%)]  Loss: 3.392 (3.17)  Time: 0.660s, 1551.69/s  (0.661s, 1548.39/s)  LR: 1.555e-04  Data: 0.018 (0.016)
Train: 223 [ 950/1251 ( 76%)]  Loss: 3.074 (3.16)  Time: 0.666s, 1537.30/s  (0.661s, 1548.01/s)  LR: 1.553e-04  Data: 0.013 (0.016)
Train: 223 [1000/1251 ( 80%)]  Loss: 2.918 (3.15)  Time: 0.671s, 1526.42/s  (0.662s, 1547.34/s)  LR: 1.552e-04  Data: 0.013 (0.015)
Train: 223 [1050/1251 ( 84%)]  Loss: 2.998 (3.15)  Time: 0.659s, 1554.76/s  (0.662s, 1547.23/s)  LR: 1.550e-04  Data: 0.013 (0.015)
Train: 223 [1100/1251 ( 88%)]  Loss: 3.208 (3.15)  Time: 0.659s, 1553.37/s  (0.662s, 1547.14/s)  LR: 1.549e-04  Data: 0.012 (0.015)
Train: 223 [1150/1251 ( 92%)]  Loss: 3.221 (3.15)  Time: 0.672s, 1523.07/s  (0.662s, 1546.97/s)  LR: 1.547e-04  Data: 0.013 (0.015)
Train: 223 [1200/1251 ( 96%)]  Loss: 3.127 (3.15)  Time: 0.658s, 1555.19/s  (0.662s, 1546.70/s)  LR: 1.546e-04  Data: 0.014 (0.015)
Train: 223 [1250/1251 (100%)]  Loss: 3.229 (3.15)  Time: 0.653s, 1568.09/s  (0.662s, 1546.77/s)  LR: 1.544e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.985 (2.985)  Loss:  0.4043 (0.4043)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.8281 (98.8281)
Test: [  48/48]  Time: 0.162 (0.321)  Loss:  0.5166 (0.8648)  Acc@1: 87.3821 (79.7660)  Acc@5: 97.6415 (95.0620)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-222.pth.tar', 79.910000078125)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-218.pth.tar', 79.83800002685547)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-223.pth.tar', 79.76599994873047)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-219.pth.tar', 79.73600005371094)

Train: 224 [   0/1251 (  0%)]  Loss: 3.019 (3.02)  Time: 3.317s,  308.73/s  (3.317s,  308.73/s)  LR: 1.544e-04  Data: 1.765 (1.765)
Train: 224 [  50/1251 (  4%)]  Loss: 3.346 (3.18)  Time: 0.651s, 1572.07/s  (0.680s, 1505.98/s)  LR: 1.543e-04  Data: 0.013 (0.049)
Train: 224 [ 100/1251 (  8%)]  Loss: 3.129 (3.16)  Time: 0.654s, 1565.48/s  (0.667s, 1535.17/s)  LR: 1.541e-04  Data: 0.014 (0.032)
Train: 224 [ 150/1251 ( 12%)]  Loss: 3.163 (3.16)  Time: 0.666s, 1537.20/s  (0.664s, 1541.78/s)  LR: 1.540e-04  Data: 0.013 (0.026)
Train: 224 [ 200/1251 ( 16%)]  Loss: 3.428 (3.22)  Time: 0.660s, 1550.51/s  (0.663s, 1543.78/s)  LR: 1.538e-04  Data: 0.013 (0.023)
Train: 224 [ 250/1251 ( 20%)]  Loss: 3.296 (3.23)  Time: 0.655s, 1562.78/s  (0.663s, 1544.11/s)  LR: 1.537e-04  Data: 0.017 (0.021)
Train: 224 [ 300/1251 ( 24%)]  Loss: 3.279 (3.24)  Time: 0.667s, 1536.32/s  (0.663s, 1544.42/s)  LR: 1.535e-04  Data: 0.013 (0.020)
Train: 224 [ 350/1251 ( 28%)]  Loss: 3.368 (3.25)  Time: 0.651s, 1572.55/s  (0.662s, 1545.83/s)  LR: 1.534e-04  Data: 0.013 (0.019)
Train: 224 [ 400/1251 ( 32%)]  Loss: 3.191 (3.25)  Time: 0.657s, 1558.42/s  (0.662s, 1546.25/s)  LR: 1.532e-04  Data: 0.014 (0.018)
Train: 224 [ 450/1251 ( 36%)]  Loss: 3.263 (3.25)  Time: 0.662s, 1547.54/s  (0.662s, 1547.29/s)  LR: 1.531e-04  Data: 0.016 (0.018)
Train: 224 [ 500/1251 ( 40%)]  Loss: 3.383 (3.26)  Time: 0.660s, 1550.92/s  (0.662s, 1547.99/s)  LR: 1.529e-04  Data: 0.014 (0.017)
Train: 224 [ 550/1251 ( 44%)]  Loss: 3.101 (3.25)  Time: 0.649s, 1578.87/s  (0.661s, 1548.24/s)  LR: 1.528e-04  Data: 0.012 (0.017)
Train: 224 [ 600/1251 ( 48%)]  Loss: 3.171 (3.24)  Time: 0.652s, 1569.47/s  (0.661s, 1548.39/s)  LR: 1.526e-04  Data: 0.014 (0.017)
Train: 224 [ 650/1251 ( 52%)]  Loss: 3.213 (3.24)  Time: 0.664s, 1543.29/s  (0.661s, 1548.57/s)  LR: 1.525e-04  Data: 0.013 (0.017)
Train: 224 [ 700/1251 ( 56%)]  Loss: 3.045 (3.23)  Time: 0.667s, 1536.32/s  (0.661s, 1548.62/s)  LR: 1.523e-04  Data: 0.019 (0.016)
Train: 224 [ 750/1251 ( 60%)]  Loss: 2.972 (3.21)  Time: 0.669s, 1531.70/s  (0.661s, 1548.73/s)  LR: 1.522e-04  Data: 0.013 (0.016)
Train: 224 [ 800/1251 ( 64%)]  Loss: 3.070 (3.20)  Time: 0.656s, 1559.90/s  (0.661s, 1548.83/s)  LR: 1.520e-04  Data: 0.016 (0.016)
Train: 224 [ 850/1251 ( 68%)]  Loss: 3.034 (3.19)  Time: 0.663s, 1545.22/s  (0.661s, 1548.62/s)  LR: 1.519e-04  Data: 0.013 (0.016)
Train: 224 [ 900/1251 ( 72%)]  Loss: 2.849 (3.17)  Time: 0.658s, 1556.51/s  (0.661s, 1548.75/s)  LR: 1.517e-04  Data: 0.014 (0.016)
Train: 224 [ 950/1251 ( 76%)]  Loss: 3.353 (3.18)  Time: 0.658s, 1557.35/s  (0.661s, 1548.75/s)  LR: 1.516e-04  Data: 0.017 (0.016)
Train: 224 [1000/1251 ( 80%)]  Loss: 2.956 (3.17)  Time: 0.671s, 1526.35/s  (0.661s, 1549.07/s)  LR: 1.515e-04  Data: 0.013 (0.016)
Train: 224 [1050/1251 ( 84%)]  Loss: 3.161 (3.17)  Time: 0.667s, 1535.25/s  (0.661s, 1549.09/s)  LR: 1.513e-04  Data: 0.013 (0.015)
Train: 224 [1100/1251 ( 88%)]  Loss: 3.228 (3.17)  Time: 0.656s, 1562.02/s  (0.661s, 1549.27/s)  LR: 1.512e-04  Data: 0.014 (0.015)
Train: 224 [1150/1251 ( 92%)]  Loss: 3.226 (3.18)  Time: 0.663s, 1544.05/s  (0.661s, 1549.56/s)  LR: 1.510e-04  Data: 0.015 (0.015)
Train: 224 [1200/1251 ( 96%)]  Loss: 3.216 (3.18)  Time: 0.647s, 1582.56/s  (0.661s, 1549.80/s)  LR: 1.509e-04  Data: 0.014 (0.015)
Train: 224 [1250/1251 (100%)]  Loss: 3.363 (3.19)  Time: 0.653s, 1567.29/s  (0.661s, 1550.12/s)  LR: 1.507e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.822 (2.822)  Loss:  0.4170 (0.4170)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.161 (0.318)  Loss:  0.5156 (0.8683)  Acc@1: 86.9104 (79.8960)  Acc@5: 98.3491 (95.1600)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-222.pth.tar', 79.910000078125)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-224.pth.tar', 79.89600000244141)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-218.pth.tar', 79.83800002685547)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-223.pth.tar', 79.76599994873047)

Train: 225 [   0/1251 (  0%)]  Loss: 3.305 (3.30)  Time: 3.569s,  286.89/s  (3.569s,  286.89/s)  LR: 1.507e-04  Data: 1.837 (1.837)
Train: 225 [  50/1251 (  4%)]  Loss: 3.158 (3.23)  Time: 0.643s, 1591.60/s  (0.677s, 1512.82/s)  LR: 1.506e-04  Data: 0.014 (0.050)
Train: 225 [ 100/1251 (  8%)]  Loss: 3.103 (3.19)  Time: 0.650s, 1575.22/s  (0.664s, 1541.06/s)  LR: 1.504e-04  Data: 0.013 (0.032)
Train: 225 [ 150/1251 ( 12%)]  Loss: 3.030 (3.15)  Time: 0.662s, 1547.82/s  (0.662s, 1546.10/s)  LR: 1.503e-04  Data: 0.014 (0.026)
Train: 225 [ 200/1251 ( 16%)]  Loss: 3.145 (3.15)  Time: 0.657s, 1559.19/s  (0.661s, 1548.51/s)  LR: 1.501e-04  Data: 0.013 (0.023)
Train: 225 [ 250/1251 ( 20%)]  Loss: 3.085 (3.14)  Time: 0.651s, 1571.96/s  (0.661s, 1549.13/s)  LR: 1.500e-04  Data: 0.013 (0.021)
Train: 225 [ 300/1251 ( 24%)]  Loss: 3.222 (3.15)  Time: 0.664s, 1542.77/s  (0.661s, 1550.11/s)  LR: 1.498e-04  Data: 0.013 (0.020)
Train: 225 [ 350/1251 ( 28%)]  Loss: 3.171 (3.15)  Time: 0.668s, 1533.45/s  (0.660s, 1550.66/s)  LR: 1.497e-04  Data: 0.015 (0.019)
Train: 225 [ 400/1251 ( 32%)]  Loss: 3.017 (3.14)  Time: 0.654s, 1565.21/s  (0.660s, 1551.06/s)  LR: 1.495e-04  Data: 0.014 (0.018)
Train: 225 [ 450/1251 ( 36%)]  Loss: 3.437 (3.17)  Time: 0.661s, 1548.39/s  (0.660s, 1551.07/s)  LR: 1.494e-04  Data: 0.012 (0.018)
Train: 225 [ 500/1251 ( 40%)]  Loss: 3.043 (3.16)  Time: 0.660s, 1552.04/s  (0.660s, 1551.15/s)  LR: 1.492e-04  Data: 0.014 (0.017)
Train: 225 [ 550/1251 ( 44%)]  Loss: 3.442 (3.18)  Time: 0.659s, 1554.67/s  (0.660s, 1551.58/s)  LR: 1.491e-04  Data: 0.013 (0.017)
Train: 225 [ 600/1251 ( 48%)]  Loss: 3.118 (3.18)  Time: 0.663s, 1544.82/s  (0.660s, 1551.38/s)  LR: 1.489e-04  Data: 0.013 (0.017)
Train: 225 [ 650/1251 ( 52%)]  Loss: 3.248 (3.18)  Time: 0.656s, 1560.38/s  (0.660s, 1551.15/s)  LR: 1.488e-04  Data: 0.017 (0.017)
Train: 225 [ 700/1251 ( 56%)]  Loss: 3.278 (3.19)  Time: 0.655s, 1564.04/s  (0.660s, 1550.90/s)  LR: 1.487e-04  Data: 0.013 (0.016)
Train: 225 [ 750/1251 ( 60%)]  Loss: 3.193 (3.19)  Time: 0.659s, 1554.21/s  (0.660s, 1550.70/s)  LR: 1.485e-04  Data: 0.013 (0.016)
Train: 225 [ 800/1251 ( 64%)]  Loss: 3.048 (3.18)  Time: 0.663s, 1544.59/s  (0.660s, 1550.56/s)  LR: 1.484e-04  Data: 0.014 (0.016)
Train: 225 [ 850/1251 ( 68%)]  Loss: 3.175 (3.18)  Time: 0.662s, 1546.03/s  (0.660s, 1550.57/s)  LR: 1.482e-04  Data: 0.013 (0.016)
Train: 225 [ 900/1251 ( 72%)]  Loss: 3.130 (3.18)  Time: 0.667s, 1534.37/s  (0.660s, 1550.71/s)  LR: 1.481e-04  Data: 0.013 (0.016)
Train: 225 [ 950/1251 ( 76%)]  Loss: 3.093 (3.17)  Time: 0.662s, 1547.48/s  (0.660s, 1550.80/s)  LR: 1.479e-04  Data: 0.013 (0.016)
Train: 225 [1000/1251 ( 80%)]  Loss: 3.325 (3.18)  Time: 0.660s, 1551.25/s  (0.660s, 1550.62/s)  LR: 1.478e-04  Data: 0.015 (0.016)
Train: 225 [1050/1251 ( 84%)]  Loss: 3.155 (3.18)  Time: 0.658s, 1555.29/s  (0.660s, 1550.47/s)  LR: 1.476e-04  Data: 0.017 (0.016)
Train: 225 [1100/1251 ( 88%)]  Loss: 3.264 (3.18)  Time: 0.663s, 1544.43/s  (0.660s, 1550.58/s)  LR: 1.475e-04  Data: 0.016 (0.015)
Train: 225 [1150/1251 ( 92%)]  Loss: 3.058 (3.18)  Time: 0.655s, 1562.73/s  (0.660s, 1550.80/s)  LR: 1.473e-04  Data: 0.014 (0.015)
Train: 225 [1200/1251 ( 96%)]  Loss: 3.281 (3.18)  Time: 0.654s, 1565.84/s  (0.660s, 1550.95/s)  LR: 1.472e-04  Data: 0.017 (0.015)
Train: 225 [1250/1251 (100%)]  Loss: 3.187 (3.18)  Time: 0.640s, 1600.06/s  (0.660s, 1551.12/s)  LR: 1.470e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.874 (2.874)  Loss:  0.3872 (0.3872)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.162 (0.322)  Loss:  0.5142 (0.8433)  Acc@1: 86.4387 (79.9660)  Acc@5: 98.1132 (95.1240)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-225.pth.tar', 79.96600005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-222.pth.tar', 79.910000078125)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-224.pth.tar', 79.89600000244141)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-218.pth.tar', 79.83800002685547)

Train: 226 [   0/1251 (  0%)]  Loss: 3.402 (3.40)  Time: 3.489s,  293.52/s  (3.489s,  293.52/s)  LR: 1.470e-04  Data: 1.602 (1.602)
Train: 226 [  50/1251 (  4%)]  Loss: 3.275 (3.34)  Time: 0.642s, 1594.34/s  (0.686s, 1493.56/s)  LR: 1.469e-04  Data: 0.014 (0.045)
Train: 226 [ 100/1251 (  8%)]  Loss: 3.071 (3.25)  Time: 0.654s, 1564.90/s  (0.669s, 1530.81/s)  LR: 1.468e-04  Data: 0.014 (0.030)
Train: 226 [ 150/1251 ( 12%)]  Loss: 3.278 (3.26)  Time: 0.648s, 1579.85/s  (0.665s, 1540.90/s)  LR: 1.466e-04  Data: 0.013 (0.024)
Train: 226 [ 200/1251 ( 16%)]  Loss: 2.739 (3.15)  Time: 0.662s, 1547.71/s  (0.662s, 1546.73/s)  LR: 1.465e-04  Data: 0.013 (0.022)
Train: 226 [ 250/1251 ( 20%)]  Loss: 3.396 (3.19)  Time: 0.651s, 1573.69/s  (0.661s, 1549.28/s)  LR: 1.463e-04  Data: 0.013 (0.020)
Train: 226 [ 300/1251 ( 24%)]  Loss: 2.980 (3.16)  Time: 0.659s, 1552.84/s  (0.660s, 1550.51/s)  LR: 1.462e-04  Data: 0.013 (0.019)
Train: 226 [ 350/1251 ( 28%)]  Loss: 3.176 (3.16)  Time: 0.662s, 1546.48/s  (0.660s, 1550.82/s)  LR: 1.460e-04  Data: 0.013 (0.018)
Train: 226 [ 400/1251 ( 32%)]  Loss: 3.180 (3.17)  Time: 0.662s, 1546.79/s  (0.661s, 1550.17/s)  LR: 1.459e-04  Data: 0.014 (0.018)
Train: 226 [ 450/1251 ( 36%)]  Loss: 3.167 (3.17)  Time: 0.650s, 1576.45/s  (0.660s, 1550.44/s)  LR: 1.457e-04  Data: 0.013 (0.017)
Train: 226 [ 500/1251 ( 40%)]  Loss: 3.093 (3.16)  Time: 0.655s, 1563.85/s  (0.660s, 1551.09/s)  LR: 1.456e-04  Data: 0.014 (0.017)
Train: 226 [ 550/1251 ( 44%)]  Loss: 2.992 (3.15)  Time: 0.659s, 1554.82/s  (0.660s, 1551.99/s)  LR: 1.454e-04  Data: 0.014 (0.017)
Train: 226 [ 600/1251 ( 48%)]  Loss: 3.253 (3.15)  Time: 0.662s, 1546.21/s  (0.660s, 1552.48/s)  LR: 1.453e-04  Data: 0.014 (0.017)
Train: 226 [ 650/1251 ( 52%)]  Loss: 3.282 (3.16)  Time: 0.652s, 1569.95/s  (0.659s, 1552.96/s)  LR: 1.452e-04  Data: 0.016 (0.016)
Train: 226 [ 700/1251 ( 56%)]  Loss: 3.032 (3.15)  Time: 0.658s, 1556.31/s  (0.659s, 1553.39/s)  LR: 1.450e-04  Data: 0.016 (0.016)
Train: 226 [ 750/1251 ( 60%)]  Loss: 2.918 (3.14)  Time: 0.664s, 1541.15/s  (0.659s, 1553.67/s)  LR: 1.449e-04  Data: 0.012 (0.016)
Train: 226 [ 800/1251 ( 64%)]  Loss: 2.972 (3.13)  Time: 0.661s, 1548.60/s  (0.659s, 1553.71/s)  LR: 1.447e-04  Data: 0.013 (0.016)
Train: 226 [ 850/1251 ( 68%)]  Loss: 3.095 (3.13)  Time: 0.671s, 1526.04/s  (0.659s, 1553.36/s)  LR: 1.446e-04  Data: 0.013 (0.016)
Train: 226 [ 900/1251 ( 72%)]  Loss: 3.112 (3.13)  Time: 0.661s, 1548.04/s  (0.659s, 1553.29/s)  LR: 1.444e-04  Data: 0.013 (0.016)
Train: 226 [ 950/1251 ( 76%)]  Loss: 3.113 (3.13)  Time: 0.658s, 1555.75/s  (0.659s, 1553.33/s)  LR: 1.443e-04  Data: 0.013 (0.016)
Train: 226 [1000/1251 ( 80%)]  Loss: 3.263 (3.13)  Time: 0.670s, 1528.88/s  (0.659s, 1553.40/s)  LR: 1.441e-04  Data: 0.013 (0.015)
Train: 226 [1050/1251 ( 84%)]  Loss: 3.228 (3.14)  Time: 0.653s, 1568.24/s  (0.659s, 1553.68/s)  LR: 1.440e-04  Data: 0.013 (0.015)
Train: 226 [1100/1251 ( 88%)]  Loss: 2.971 (3.13)  Time: 0.661s, 1550.20/s  (0.659s, 1553.97/s)  LR: 1.439e-04  Data: 0.014 (0.015)
Train: 226 [1150/1251 ( 92%)]  Loss: 3.244 (3.13)  Time: 0.657s, 1559.12/s  (0.659s, 1553.94/s)  LR: 1.437e-04  Data: 0.016 (0.015)
Train: 226 [1200/1251 ( 96%)]  Loss: 3.457 (3.15)  Time: 0.649s, 1578.22/s  (0.659s, 1553.97/s)  LR: 1.436e-04  Data: 0.016 (0.015)
Train: 226 [1250/1251 (100%)]  Loss: 3.248 (3.15)  Time: 0.648s, 1580.24/s  (0.659s, 1554.42/s)  LR: 1.434e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.891 (2.891)  Loss:  0.3853 (0.3853)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.161 (0.319)  Loss:  0.5093 (0.8489)  Acc@1: 86.3208 (79.9820)  Acc@5: 98.4670 (95.1580)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-226.pth.tar', 79.98200013427734)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-225.pth.tar', 79.96600005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-222.pth.tar', 79.910000078125)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-224.pth.tar', 79.89600000244141)

Train: 227 [   0/1251 (  0%)]  Loss: 3.098 (3.10)  Time: 3.552s,  288.32/s  (3.552s,  288.32/s)  LR: 1.434e-04  Data: 1.639 (1.639)
Train: 227 [  50/1251 (  4%)]  Loss: 3.410 (3.25)  Time: 0.650s, 1576.51/s  (0.678s, 1509.64/s)  LR: 1.433e-04  Data: 0.017 (0.046)
Train: 227 [ 100/1251 (  8%)]  Loss: 3.217 (3.24)  Time: 0.664s, 1541.47/s  (0.664s, 1543.05/s)  LR: 1.431e-04  Data: 0.014 (0.030)
Train: 227 [ 150/1251 ( 12%)]  Loss: 3.166 (3.22)  Time: 0.653s, 1567.59/s  (0.660s, 1550.64/s)  LR: 1.430e-04  Data: 0.015 (0.025)
Train: 227 [ 200/1251 ( 16%)]  Loss: 3.089 (3.20)  Time: 0.650s, 1574.95/s  (0.659s, 1554.29/s)  LR: 1.428e-04  Data: 0.013 (0.022)
Train: 227 [ 250/1251 ( 20%)]  Loss: 2.965 (3.16)  Time: 0.648s, 1579.66/s  (0.658s, 1556.57/s)  LR: 1.427e-04  Data: 0.019 (0.020)
Train: 227 [ 300/1251 ( 24%)]  Loss: 3.529 (3.21)  Time: 0.657s, 1557.54/s  (0.658s, 1556.73/s)  LR: 1.426e-04  Data: 0.016 (0.019)
Train: 227 [ 350/1251 ( 28%)]  Loss: 3.024 (3.19)  Time: 0.665s, 1539.87/s  (0.658s, 1555.88/s)  LR: 1.424e-04  Data: 0.014 (0.019)
Train: 227 [ 400/1251 ( 32%)]  Loss: 3.142 (3.18)  Time: 0.664s, 1541.98/s  (0.659s, 1554.97/s)  LR: 1.423e-04  Data: 0.012 (0.018)
Train: 227 [ 450/1251 ( 36%)]  Loss: 3.150 (3.18)  Time: 0.659s, 1554.59/s  (0.659s, 1554.29/s)  LR: 1.421e-04  Data: 0.013 (0.017)
Train: 227 [ 500/1251 ( 40%)]  Loss: 2.984 (3.16)  Time: 0.665s, 1538.72/s  (0.659s, 1553.85/s)  LR: 1.420e-04  Data: 0.013 (0.017)
Train: 227 [ 550/1251 ( 44%)]  Loss: 3.191 (3.16)  Time: 0.658s, 1555.23/s  (0.659s, 1553.16/s)  LR: 1.418e-04  Data: 0.013 (0.017)
Train: 227 [ 600/1251 ( 48%)]  Loss: 3.192 (3.17)  Time: 0.669s, 1531.48/s  (0.660s, 1552.40/s)  LR: 1.417e-04  Data: 0.013 (0.017)
Train: 227 [ 650/1251 ( 52%)]  Loss: 3.067 (3.16)  Time: 0.661s, 1548.09/s  (0.660s, 1551.63/s)  LR: 1.416e-04  Data: 0.014 (0.016)
Train: 227 [ 700/1251 ( 56%)]  Loss: 3.397 (3.17)  Time: 0.665s, 1538.90/s  (0.660s, 1550.93/s)  LR: 1.414e-04  Data: 0.012 (0.016)
Train: 227 [ 750/1251 ( 60%)]  Loss: 3.209 (3.18)  Time: 0.663s, 1545.37/s  (0.660s, 1550.65/s)  LR: 1.413e-04  Data: 0.014 (0.016)
Train: 227 [ 800/1251 ( 64%)]  Loss: 2.848 (3.16)  Time: 0.658s, 1555.63/s  (0.660s, 1550.34/s)  LR: 1.411e-04  Data: 0.014 (0.016)
Train: 227 [ 850/1251 ( 68%)]  Loss: 3.178 (3.16)  Time: 0.664s, 1541.27/s  (0.661s, 1550.15/s)  LR: 1.410e-04  Data: 0.013 (0.016)
Train: 227 [ 900/1251 ( 72%)]  Loss: 2.994 (3.15)  Time: 0.662s, 1545.70/s  (0.661s, 1550.18/s)  LR: 1.408e-04  Data: 0.017 (0.016)
Train: 227 [ 950/1251 ( 76%)]  Loss: 3.398 (3.16)  Time: 0.655s, 1563.56/s  (0.661s, 1549.93/s)  LR: 1.407e-04  Data: 0.013 (0.015)
Train: 227 [1000/1251 ( 80%)]  Loss: 3.301 (3.17)  Time: 0.663s, 1544.44/s  (0.661s, 1549.72/s)  LR: 1.406e-04  Data: 0.013 (0.015)
Train: 227 [1050/1251 ( 84%)]  Loss: 3.180 (3.17)  Time: 0.665s, 1541.00/s  (0.661s, 1549.48/s)  LR: 1.404e-04  Data: 0.013 (0.015)
Train: 227 [1100/1251 ( 88%)]  Loss: 3.143 (3.17)  Time: 0.652s, 1570.16/s  (0.661s, 1549.35/s)  LR: 1.403e-04  Data: 0.013 (0.015)
Train: 227 [1150/1251 ( 92%)]  Loss: 3.225 (3.17)  Time: 0.661s, 1549.81/s  (0.661s, 1549.15/s)  LR: 1.401e-04  Data: 0.013 (0.015)
Train: 227 [1200/1251 ( 96%)]  Loss: 3.160 (3.17)  Time: 0.656s, 1561.73/s  (0.661s, 1549.09/s)  LR: 1.400e-04  Data: 0.017 (0.015)
Train: 227 [1250/1251 (100%)]  Loss: 3.016 (3.16)  Time: 0.654s, 1565.97/s  (0.661s, 1549.24/s)  LR: 1.398e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.829 (2.829)  Loss:  0.3950 (0.3950)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.162 (0.324)  Loss:  0.5020 (0.8463)  Acc@1: 87.0283 (80.0860)  Acc@5: 98.1132 (95.1040)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-227.pth.tar', 80.08600005371093)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-226.pth.tar', 79.98200013427734)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-225.pth.tar', 79.96600005615234)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-222.pth.tar', 79.910000078125)

Train: 228 [   0/1251 (  0%)]  Loss: 3.204 (3.20)  Time: 3.237s,  316.30/s  (3.237s,  316.30/s)  LR: 1.398e-04  Data: 1.724 (1.724)
Train: 228 [  50/1251 (  4%)]  Loss: 3.345 (3.27)  Time: 0.644s, 1590.65/s  (0.678s, 1510.36/s)  LR: 1.397e-04  Data: 0.015 (0.047)
Train: 228 [ 100/1251 (  8%)]  Loss: 3.281 (3.28)  Time: 0.652s, 1569.39/s  (0.666s, 1537.84/s)  LR: 1.396e-04  Data: 0.013 (0.031)
Train: 228 [ 150/1251 ( 12%)]  Loss: 3.054 (3.22)  Time: 0.660s, 1550.46/s  (0.664s, 1542.30/s)  LR: 1.394e-04  Data: 0.012 (0.025)
Train: 228 [ 200/1251 ( 16%)]  Loss: 3.063 (3.19)  Time: 0.650s, 1576.39/s  (0.662s, 1546.16/s)  LR: 1.393e-04  Data: 0.013 (0.022)
Train: 228 [ 250/1251 ( 20%)]  Loss: 3.236 (3.20)  Time: 0.661s, 1548.75/s  (0.661s, 1548.04/s)  LR: 1.391e-04  Data: 0.013 (0.021)
Train: 228 [ 300/1251 ( 24%)]  Loss: 2.827 (3.14)  Time: 0.657s, 1557.91/s  (0.661s, 1548.67/s)  LR: 1.390e-04  Data: 0.013 (0.020)
Train: 228 [ 350/1251 ( 28%)]  Loss: 3.309 (3.16)  Time: 0.662s, 1547.93/s  (0.661s, 1548.66/s)  LR: 1.388e-04  Data: 0.013 (0.019)
Train: 228 [ 400/1251 ( 32%)]  Loss: 3.062 (3.15)  Time: 0.662s, 1546.28/s  (0.661s, 1549.00/s)  LR: 1.387e-04  Data: 0.013 (0.018)
Train: 228 [ 450/1251 ( 36%)]  Loss: 3.270 (3.16)  Time: 0.666s, 1537.71/s  (0.661s, 1549.03/s)  LR: 1.386e-04  Data: 0.013 (0.018)
Train: 228 [ 500/1251 ( 40%)]  Loss: 2.997 (3.15)  Time: 0.671s, 1526.17/s  (0.661s, 1548.29/s)  LR: 1.384e-04  Data: 0.015 (0.017)
Train: 228 [ 550/1251 ( 44%)]  Loss: 3.033 (3.14)  Time: 0.662s, 1546.34/s  (0.662s, 1547.87/s)  LR: 1.383e-04  Data: 0.013 (0.017)
Train: 228 [ 600/1251 ( 48%)]  Loss: 3.073 (3.13)  Time: 0.668s, 1533.61/s  (0.662s, 1547.56/s)  LR: 1.381e-04  Data: 0.013 (0.017)
Train: 228 [ 650/1251 ( 52%)]  Loss: 3.010 (3.13)  Time: 0.666s, 1537.76/s  (0.662s, 1547.38/s)  LR: 1.380e-04  Data: 0.016 (0.016)
Train: 228 [ 700/1251 ( 56%)]  Loss: 3.240 (3.13)  Time: 0.664s, 1541.13/s  (0.662s, 1546.83/s)  LR: 1.378e-04  Data: 0.013 (0.016)
Train: 228 [ 750/1251 ( 60%)]  Loss: 3.257 (3.14)  Time: 0.660s, 1551.37/s  (0.662s, 1546.69/s)  LR: 1.377e-04  Data: 0.013 (0.016)
Train: 228 [ 800/1251 ( 64%)]  Loss: 3.417 (3.16)  Time: 0.666s, 1538.25/s  (0.662s, 1546.50/s)  LR: 1.376e-04  Data: 0.015 (0.016)
Train: 228 [ 850/1251 ( 68%)]  Loss: 3.175 (3.16)  Time: 0.664s, 1541.61/s  (0.662s, 1546.19/s)  LR: 1.374e-04  Data: 0.013 (0.016)
Train: 228 [ 900/1251 ( 72%)]  Loss: 2.979 (3.15)  Time: 0.662s, 1546.20/s  (0.662s, 1545.89/s)  LR: 1.373e-04  Data: 0.013 (0.016)
Train: 228 [ 950/1251 ( 76%)]  Loss: 2.754 (3.13)  Time: 0.667s, 1534.49/s  (0.662s, 1545.71/s)  LR: 1.371e-04  Data: 0.013 (0.016)
Train: 228 [1000/1251 ( 80%)]  Loss: 3.177 (3.13)  Time: 0.667s, 1536.08/s  (0.663s, 1545.47/s)  LR: 1.370e-04  Data: 0.016 (0.015)
Train: 228 [1050/1251 ( 84%)]  Loss: 3.360 (3.14)  Time: 0.669s, 1530.45/s  (0.663s, 1545.35/s)  LR: 1.369e-04  Data: 0.014 (0.015)
Train: 228 [1100/1251 ( 88%)]  Loss: 3.257 (3.15)  Time: 0.658s, 1556.98/s  (0.663s, 1545.11/s)  LR: 1.367e-04  Data: 0.013 (0.015)
Train: 228 [1150/1251 ( 92%)]  Loss: 3.018 (3.14)  Time: 0.666s, 1537.94/s  (0.663s, 1545.16/s)  LR: 1.366e-04  Data: 0.013 (0.015)
Train: 228 [1200/1251 ( 96%)]  Loss: 3.095 (3.14)  Time: 0.654s, 1565.85/s  (0.663s, 1545.15/s)  LR: 1.364e-04  Data: 0.014 (0.015)
Train: 228 [1250/1251 (100%)]  Loss: 3.212 (3.14)  Time: 0.651s, 1573.33/s  (0.663s, 1545.12/s)  LR: 1.363e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.817 (2.817)  Loss:  0.3965 (0.3965)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.162 (0.319)  Loss:  0.5156 (0.8427)  Acc@1: 86.6745 (80.0320)  Acc@5: 97.9953 (95.2080)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-227.pth.tar', 80.08600005371093)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-228.pth.tar', 80.0320001586914)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-226.pth.tar', 79.98200013427734)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-225.pth.tar', 79.96600005615234)

Train: 229 [   0/1251 (  0%)]  Loss: 3.056 (3.06)  Time: 3.242s,  315.82/s  (3.242s,  315.82/s)  LR: 1.363e-04  Data: 1.677 (1.677)
Train: 229 [  50/1251 (  4%)]  Loss: 3.067 (3.06)  Time: 0.652s, 1570.00/s  (0.674s, 1519.10/s)  LR: 1.361e-04  Data: 0.016 (0.047)
Train: 229 [ 100/1251 (  8%)]  Loss: 3.186 (3.10)  Time: 0.661s, 1550.08/s  (0.665s, 1540.84/s)  LR: 1.360e-04  Data: 0.014 (0.030)
Train: 229 [ 150/1251 ( 12%)]  Loss: 2.872 (3.05)  Time: 0.666s, 1537.02/s  (0.662s, 1545.99/s)  LR: 1.359e-04  Data: 0.013 (0.025)
Train: 229 [ 200/1251 ( 16%)]  Loss: 3.063 (3.05)  Time: 0.660s, 1551.99/s  (0.662s, 1547.63/s)  LR: 1.357e-04  Data: 0.018 (0.022)
Train: 229 [ 250/1251 ( 20%)]  Loss: 2.844 (3.01)  Time: 0.662s, 1547.09/s  (0.661s, 1548.55/s)  LR: 1.356e-04  Data: 0.013 (0.020)
Train: 229 [ 300/1251 ( 24%)]  Loss: 3.468 (3.08)  Time: 0.664s, 1543.21/s  (0.661s, 1548.98/s)  LR: 1.354e-04  Data: 0.015 (0.019)
Train: 229 [ 350/1251 ( 28%)]  Loss: 3.037 (3.07)  Time: 0.657s, 1559.11/s  (0.661s, 1549.61/s)  LR: 1.353e-04  Data: 0.016 (0.019)
Train: 229 [ 400/1251 ( 32%)]  Loss: 3.211 (3.09)  Time: 0.660s, 1550.78/s  (0.660s, 1550.43/s)  LR: 1.352e-04  Data: 0.014 (0.018)
Train: 229 [ 450/1251 ( 36%)]  Loss: 3.190 (3.10)  Time: 0.663s, 1544.93/s  (0.660s, 1550.93/s)  LR: 1.350e-04  Data: 0.017 (0.018)
Train: 229 [ 500/1251 ( 40%)]  Loss: 3.077 (3.10)  Time: 0.659s, 1553.54/s  (0.660s, 1551.25/s)  LR: 1.349e-04  Data: 0.014 (0.017)
Train: 229 [ 550/1251 ( 44%)]  Loss: 3.076 (3.10)  Time: 0.666s, 1538.23/s  (0.660s, 1551.37/s)  LR: 1.347e-04  Data: 0.014 (0.017)
Train: 229 [ 600/1251 ( 48%)]  Loss: 3.005 (3.09)  Time: 0.659s, 1553.19/s  (0.660s, 1551.54/s)  LR: 1.346e-04  Data: 0.014 (0.017)
Train: 229 [ 650/1251 ( 52%)]  Loss: 2.840 (3.07)  Time: 0.654s, 1565.61/s  (0.660s, 1551.52/s)  LR: 1.345e-04  Data: 0.013 (0.016)
Train: 229 [ 700/1251 ( 56%)]  Loss: 3.290 (3.09)  Time: 0.649s, 1576.65/s  (0.660s, 1552.00/s)  LR: 1.343e-04  Data: 0.014 (0.016)
Train: 229 [ 750/1251 ( 60%)]  Loss: 2.672 (3.06)  Time: 0.659s, 1554.70/s  (0.660s, 1552.58/s)  LR: 1.342e-04  Data: 0.015 (0.016)
Train: 229 [ 800/1251 ( 64%)]  Loss: 3.030 (3.06)  Time: 0.669s, 1529.59/s  (0.660s, 1552.48/s)  LR: 1.340e-04  Data: 0.013 (0.016)
Train: 229 [ 850/1251 ( 68%)]  Loss: 3.209 (3.07)  Time: 0.660s, 1552.38/s  (0.660s, 1552.56/s)  LR: 1.339e-04  Data: 0.012 (0.016)
Train: 229 [ 900/1251 ( 72%)]  Loss: 2.876 (3.06)  Time: 0.672s, 1524.46/s  (0.660s, 1552.12/s)  LR: 1.338e-04  Data: 0.013 (0.016)
Train: 229 [ 950/1251 ( 76%)]  Loss: 3.187 (3.06)  Time: 0.661s, 1548.61/s  (0.660s, 1552.22/s)  LR: 1.336e-04  Data: 0.013 (0.016)
Train: 229 [1000/1251 ( 80%)]  Loss: 3.099 (3.06)  Time: 0.673s, 1521.19/s  (0.660s, 1552.39/s)  LR: 1.335e-04  Data: 0.012 (0.015)
Train: 229 [1050/1251 ( 84%)]  Loss: 2.797 (3.05)  Time: 0.651s, 1573.26/s  (0.660s, 1552.57/s)  LR: 1.333e-04  Data: 0.015 (0.015)
Train: 229 [1100/1251 ( 88%)]  Loss: 2.820 (3.04)  Time: 0.656s, 1561.97/s  (0.659s, 1552.80/s)  LR: 1.332e-04  Data: 0.013 (0.015)
Train: 229 [1150/1251 ( 92%)]  Loss: 2.704 (3.03)  Time: 0.658s, 1555.87/s  (0.659s, 1553.03/s)  LR: 1.331e-04  Data: 0.013 (0.015)
Train: 229 [1200/1251 ( 96%)]  Loss: 2.923 (3.02)  Time: 0.658s, 1555.18/s  (0.659s, 1553.20/s)  LR: 1.329e-04  Data: 0.012 (0.015)
Train: 229 [1250/1251 (100%)]  Loss: 3.113 (3.03)  Time: 0.651s, 1573.55/s  (0.659s, 1553.39/s)  LR: 1.328e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.038 (3.038)  Loss:  0.3811 (0.3811)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.161 (0.326)  Loss:  0.4858 (0.8297)  Acc@1: 87.7358 (80.3120)  Acc@5: 98.4670 (95.2560)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-229.pth.tar', 80.31199997314454)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-227.pth.tar', 80.08600005371093)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-228.pth.tar', 80.0320001586914)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-226.pth.tar', 79.98200013427734)

Train: 230 [   0/1251 (  0%)]  Loss: 3.021 (3.02)  Time: 3.344s,  306.19/s  (3.344s,  306.19/s)  LR: 1.328e-04  Data: 1.915 (1.915)
Train: 230 [  50/1251 (  4%)]  Loss: 3.267 (3.14)  Time: 0.640s, 1599.20/s  (0.673s, 1522.18/s)  LR: 1.326e-04  Data: 0.013 (0.051)
Train: 230 [ 100/1251 (  8%)]  Loss: 3.195 (3.16)  Time: 0.658s, 1556.79/s  (0.661s, 1550.13/s)  LR: 1.325e-04  Data: 0.013 (0.033)
Train: 230 [ 150/1251 ( 12%)]  Loss: 3.254 (3.18)  Time: 0.650s, 1575.20/s  (0.658s, 1555.95/s)  LR: 1.324e-04  Data: 0.014 (0.027)
Train: 230 [ 200/1251 ( 16%)]  Loss: 3.169 (3.18)  Time: 0.660s, 1552.07/s  (0.658s, 1557.15/s)  LR: 1.322e-04  Data: 0.013 (0.023)
Train: 230 [ 250/1251 ( 20%)]  Loss: 3.069 (3.16)  Time: 0.648s, 1579.68/s  (0.657s, 1557.51/s)  LR: 1.321e-04  Data: 0.014 (0.021)
Train: 230 [ 300/1251 ( 24%)]  Loss: 2.863 (3.12)  Time: 0.649s, 1577.39/s  (0.657s, 1558.32/s)  LR: 1.319e-04  Data: 0.013 (0.020)
Train: 230 [ 350/1251 ( 28%)]  Loss: 3.294 (3.14)  Time: 0.649s, 1578.39/s  (0.657s, 1558.41/s)  LR: 1.318e-04  Data: 0.014 (0.019)
Train: 230 [ 400/1251 ( 32%)]  Loss: 3.259 (3.15)  Time: 0.664s, 1541.38/s  (0.657s, 1558.67/s)  LR: 1.317e-04  Data: 0.013 (0.018)
Train: 230 [ 450/1251 ( 36%)]  Loss: 3.292 (3.17)  Time: 0.670s, 1528.64/s  (0.657s, 1558.44/s)  LR: 1.315e-04  Data: 0.013 (0.018)
Train: 230 [ 500/1251 ( 40%)]  Loss: 3.245 (3.18)  Time: 0.668s, 1533.42/s  (0.657s, 1558.30/s)  LR: 1.314e-04  Data: 0.013 (0.017)
Train: 230 [ 550/1251 ( 44%)]  Loss: 3.294 (3.19)  Time: 0.653s, 1569.04/s  (0.657s, 1557.68/s)  LR: 1.313e-04  Data: 0.013 (0.017)
Train: 230 [ 600/1251 ( 48%)]  Loss: 3.232 (3.19)  Time: 0.658s, 1555.88/s  (0.658s, 1556.90/s)  LR: 1.311e-04  Data: 0.012 (0.017)
Train: 230 [ 650/1251 ( 52%)]  Loss: 3.255 (3.19)  Time: 0.665s, 1540.92/s  (0.658s, 1556.46/s)  LR: 1.310e-04  Data: 0.017 (0.017)
Train: 230 [ 700/1251 ( 56%)]  Loss: 3.081 (3.19)  Time: 0.674s, 1519.59/s  (0.658s, 1556.01/s)  LR: 1.308e-04  Data: 0.014 (0.016)
Train: 230 [ 750/1251 ( 60%)]  Loss: 3.072 (3.18)  Time: 0.658s, 1557.19/s  (0.658s, 1555.66/s)  LR: 1.307e-04  Data: 0.014 (0.016)
Train: 230 [ 800/1251 ( 64%)]  Loss: 3.112 (3.17)  Time: 0.665s, 1538.71/s  (0.658s, 1555.32/s)  LR: 1.306e-04  Data: 0.014 (0.016)
Train: 230 [ 850/1251 ( 68%)]  Loss: 3.122 (3.17)  Time: 0.662s, 1547.56/s  (0.658s, 1555.10/s)  LR: 1.304e-04  Data: 0.016 (0.016)
Train: 230 [ 900/1251 ( 72%)]  Loss: 3.103 (3.17)  Time: 0.657s, 1559.59/s  (0.659s, 1554.98/s)  LR: 1.303e-04  Data: 0.014 (0.016)
Train: 230 [ 950/1251 ( 76%)]  Loss: 3.359 (3.18)  Time: 0.659s, 1553.88/s  (0.659s, 1554.66/s)  LR: 1.301e-04  Data: 0.013 (0.016)
Train: 230 [1000/1251 ( 80%)]  Loss: 3.094 (3.17)  Time: 0.660s, 1552.66/s  (0.659s, 1554.20/s)  LR: 1.300e-04  Data: 0.013 (0.016)
Train: 230 [1050/1251 ( 84%)]  Loss: 3.282 (3.18)  Time: 0.666s, 1538.51/s  (0.659s, 1553.78/s)  LR: 1.299e-04  Data: 0.013 (0.016)
Train: 230 [1100/1251 ( 88%)]  Loss: 3.086 (3.17)  Time: 0.659s, 1554.82/s  (0.659s, 1553.48/s)  LR: 1.297e-04  Data: 0.017 (0.015)
Train: 230 [1150/1251 ( 92%)]  Loss: 3.042 (3.17)  Time: 0.661s, 1548.78/s  (0.659s, 1553.23/s)  LR: 1.296e-04  Data: 0.014 (0.015)
Train: 230 [1200/1251 ( 96%)]  Loss: 3.158 (3.17)  Time: 0.653s, 1568.07/s  (0.659s, 1553.42/s)  LR: 1.295e-04  Data: 0.013 (0.015)
Train: 230 [1250/1251 (100%)]  Loss: 3.125 (3.17)  Time: 0.644s, 1589.09/s  (0.659s, 1553.33/s)  LR: 1.293e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.847 (2.847)  Loss:  0.4141 (0.4141)  Acc@1: 91.9922 (91.9922)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.162 (0.319)  Loss:  0.5181 (0.8440)  Acc@1: 86.6745 (80.1740)  Acc@5: 98.2311 (95.2160)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-229.pth.tar', 80.31199997314454)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-230.pth.tar', 80.17400015869141)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-227.pth.tar', 80.08600005371093)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-228.pth.tar', 80.0320001586914)

Train: 231 [   0/1251 (  0%)]  Loss: 3.172 (3.17)  Time: 3.127s,  327.47/s  (3.127s,  327.47/s)  LR: 1.293e-04  Data: 1.746 (1.746)
Train: 231 [  50/1251 (  4%)]  Loss: 3.281 (3.23)  Time: 0.638s, 1604.65/s  (0.673s, 1521.16/s)  LR: 1.292e-04  Data: 0.013 (0.048)
Train: 231 [ 100/1251 (  8%)]  Loss: 3.250 (3.23)  Time: 0.650s, 1576.58/s  (0.661s, 1548.73/s)  LR: 1.290e-04  Data: 0.016 (0.031)
Train: 231 [ 150/1251 ( 12%)]  Loss: 2.958 (3.17)  Time: 0.653s, 1567.86/s  (0.659s, 1554.97/s)  LR: 1.289e-04  Data: 0.014 (0.025)
Train: 231 [ 200/1251 ( 16%)]  Loss: 2.862 (3.10)  Time: 0.662s, 1546.03/s  (0.659s, 1554.38/s)  LR: 1.288e-04  Data: 0.018 (0.022)
Train: 231 [ 250/1251 ( 20%)]  Loss: 3.247 (3.13)  Time: 0.658s, 1556.08/s  (0.659s, 1554.47/s)  LR: 1.286e-04  Data: 0.014 (0.021)
Train: 231 [ 300/1251 ( 24%)]  Loss: 3.120 (3.13)  Time: 0.660s, 1551.04/s  (0.659s, 1553.80/s)  LR: 1.285e-04  Data: 0.013 (0.020)
Train: 231 [ 350/1251 ( 28%)]  Loss: 3.045 (3.12)  Time: 0.669s, 1530.60/s  (0.659s, 1553.15/s)  LR: 1.284e-04  Data: 0.012 (0.019)
Train: 231 [ 400/1251 ( 32%)]  Loss: 3.160 (3.12)  Time: 0.660s, 1551.97/s  (0.660s, 1552.31/s)  LR: 1.282e-04  Data: 0.013 (0.018)
Train: 231 [ 450/1251 ( 36%)]  Loss: 2.996 (3.11)  Time: 0.671s, 1526.75/s  (0.660s, 1551.53/s)  LR: 1.281e-04  Data: 0.017 (0.018)
Train: 231 [ 500/1251 ( 40%)]  Loss: 3.204 (3.12)  Time: 0.670s, 1528.57/s  (0.660s, 1550.76/s)  LR: 1.279e-04  Data: 0.013 (0.017)
Train: 231 [ 550/1251 ( 44%)]  Loss: 3.096 (3.12)  Time: 0.671s, 1525.53/s  (0.661s, 1549.91/s)  LR: 1.278e-04  Data: 0.013 (0.017)
Train: 231 [ 600/1251 ( 48%)]  Loss: 3.238 (3.13)  Time: 0.657s, 1559.64/s  (0.661s, 1549.32/s)  LR: 1.277e-04  Data: 0.013 (0.016)
Train: 231 [ 650/1251 ( 52%)]  Loss: 2.927 (3.11)  Time: 0.657s, 1559.44/s  (0.661s, 1548.62/s)  LR: 1.275e-04  Data: 0.013 (0.016)
Train: 231 [ 700/1251 ( 56%)]  Loss: 3.343 (3.13)  Time: 0.669s, 1531.53/s  (0.661s, 1548.04/s)  LR: 1.274e-04  Data: 0.014 (0.016)
Train: 231 [ 750/1251 ( 60%)]  Loss: 3.100 (3.12)  Time: 0.655s, 1562.85/s  (0.662s, 1547.85/s)  LR: 1.273e-04  Data: 0.013 (0.016)
Train: 231 [ 800/1251 ( 64%)]  Loss: 3.363 (3.14)  Time: 0.658s, 1556.81/s  (0.662s, 1547.77/s)  LR: 1.271e-04  Data: 0.014 (0.016)
Train: 231 [ 850/1251 ( 68%)]  Loss: 3.240 (3.14)  Time: 0.656s, 1562.12/s  (0.662s, 1547.60/s)  LR: 1.270e-04  Data: 0.014 (0.016)
Train: 231 [ 900/1251 ( 72%)]  Loss: 3.043 (3.14)  Time: 0.669s, 1530.36/s  (0.662s, 1547.38/s)  LR: 1.268e-04  Data: 0.013 (0.016)
Train: 231 [ 950/1251 ( 76%)]  Loss: 3.277 (3.15)  Time: 0.664s, 1541.10/s  (0.662s, 1547.00/s)  LR: 1.267e-04  Data: 0.014 (0.015)
Train: 231 [1000/1251 ( 80%)]  Loss: 2.997 (3.14)  Time: 0.667s, 1534.98/s  (0.662s, 1546.94/s)  LR: 1.266e-04  Data: 0.014 (0.015)
Train: 231 [1050/1251 ( 84%)]  Loss: 3.442 (3.15)  Time: 0.666s, 1536.78/s  (0.662s, 1547.05/s)  LR: 1.264e-04  Data: 0.013 (0.015)
Train: 231 [1100/1251 ( 88%)]  Loss: 3.296 (3.16)  Time: 0.660s, 1552.25/s  (0.662s, 1546.85/s)  LR: 1.263e-04  Data: 0.013 (0.015)
Train: 231 [1150/1251 ( 92%)]  Loss: 3.053 (3.15)  Time: 0.663s, 1544.97/s  (0.662s, 1546.80/s)  LR: 1.262e-04  Data: 0.013 (0.015)
Train: 231 [1200/1251 ( 96%)]  Loss: 3.004 (3.15)  Time: 0.658s, 1556.37/s  (0.662s, 1546.65/s)  LR: 1.260e-04  Data: 0.013 (0.015)
Train: 231 [1250/1251 (100%)]  Loss: 2.942 (3.14)  Time: 0.652s, 1571.41/s  (0.662s, 1546.78/s)  LR: 1.259e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.901 (2.901)  Loss:  0.3984 (0.3984)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.162 (0.318)  Loss:  0.5107 (0.8390)  Acc@1: 87.1462 (80.1620)  Acc@5: 98.3491 (95.2260)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-229.pth.tar', 80.31199997314454)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-230.pth.tar', 80.17400015869141)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-231.pth.tar', 80.16199997558594)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-227.pth.tar', 80.08600005371093)

Train: 232 [   0/1251 (  0%)]  Loss: 3.001 (3.00)  Time: 3.362s,  304.57/s  (3.362s,  304.57/s)  LR: 1.259e-04  Data: 1.728 (1.728)
Train: 232 [  50/1251 (  4%)]  Loss: 3.356 (3.18)  Time: 0.641s, 1597.60/s  (0.682s, 1500.42/s)  LR: 1.258e-04  Data: 0.014 (0.048)
Train: 232 [ 100/1251 (  8%)]  Loss: 3.472 (3.28)  Time: 0.657s, 1557.94/s  (0.667s, 1535.93/s)  LR: 1.256e-04  Data: 0.016 (0.031)
Train: 232 [ 150/1251 ( 12%)]  Loss: 3.386 (3.30)  Time: 0.658s, 1557.12/s  (0.662s, 1546.10/s)  LR: 1.255e-04  Data: 0.013 (0.025)
Train: 232 [ 200/1251 ( 16%)]  Loss: 3.436 (3.33)  Time: 0.652s, 1570.80/s  (0.661s, 1549.96/s)  LR: 1.253e-04  Data: 0.014 (0.022)
Train: 232 [ 250/1251 ( 20%)]  Loss: 3.432 (3.35)  Time: 0.660s, 1551.27/s  (0.660s, 1550.61/s)  LR: 1.252e-04  Data: 0.013 (0.021)
Train: 232 [ 300/1251 ( 24%)]  Loss: 3.003 (3.30)  Time: 0.663s, 1545.30/s  (0.660s, 1550.77/s)  LR: 1.251e-04  Data: 0.016 (0.020)
Train: 232 [ 350/1251 ( 28%)]  Loss: 3.528 (3.33)  Time: 0.665s, 1538.80/s  (0.661s, 1550.04/s)  LR: 1.249e-04  Data: 0.014 (0.019)
Train: 232 [ 400/1251 ( 32%)]  Loss: 3.274 (3.32)  Time: 0.669s, 1530.29/s  (0.661s, 1549.65/s)  LR: 1.248e-04  Data: 0.013 (0.018)
Train: 232 [ 450/1251 ( 36%)]  Loss: 2.876 (3.28)  Time: 0.662s, 1545.72/s  (0.661s, 1549.37/s)  LR: 1.247e-04  Data: 0.014 (0.018)
Train: 232 [ 500/1251 ( 40%)]  Loss: 3.080 (3.26)  Time: 0.661s, 1548.17/s  (0.661s, 1549.47/s)  LR: 1.245e-04  Data: 0.013 (0.017)
Train: 232 [ 550/1251 ( 44%)]  Loss: 2.834 (3.22)  Time: 0.662s, 1547.17/s  (0.661s, 1549.41/s)  LR: 1.244e-04  Data: 0.013 (0.017)
Train: 232 [ 600/1251 ( 48%)]  Loss: 3.302 (3.23)  Time: 0.661s, 1548.52/s  (0.661s, 1549.23/s)  LR: 1.243e-04  Data: 0.013 (0.017)
Train: 232 [ 650/1251 ( 52%)]  Loss: 3.015 (3.21)  Time: 0.670s, 1527.85/s  (0.661s, 1548.76/s)  LR: 1.241e-04  Data: 0.013 (0.016)
Train: 232 [ 700/1251 ( 56%)]  Loss: 3.399 (3.23)  Time: 0.670s, 1529.35/s  (0.661s, 1549.01/s)  LR: 1.240e-04  Data: 0.015 (0.016)
Train: 232 [ 750/1251 ( 60%)]  Loss: 3.089 (3.22)  Time: 0.667s, 1534.77/s  (0.661s, 1548.85/s)  LR: 1.239e-04  Data: 0.013 (0.016)
Train: 232 [ 800/1251 ( 64%)]  Loss: 3.128 (3.21)  Time: 0.655s, 1562.62/s  (0.661s, 1548.58/s)  LR: 1.237e-04  Data: 0.013 (0.016)
Train: 232 [ 850/1251 ( 68%)]  Loss: 2.998 (3.20)  Time: 0.659s, 1554.66/s  (0.661s, 1548.56/s)  LR: 1.236e-04  Data: 0.013 (0.016)
Train: 232 [ 900/1251 ( 72%)]  Loss: 2.922 (3.19)  Time: 0.666s, 1536.43/s  (0.661s, 1548.00/s)  LR: 1.235e-04  Data: 0.015 (0.016)
Train: 232 [ 950/1251 ( 76%)]  Loss: 3.324 (3.19)  Time: 0.665s, 1540.92/s  (0.662s, 1547.88/s)  LR: 1.233e-04  Data: 0.012 (0.016)
Train: 232 [1000/1251 ( 80%)]  Loss: 2.947 (3.18)  Time: 0.660s, 1551.59/s  (0.661s, 1548.00/s)  LR: 1.232e-04  Data: 0.013 (0.015)
Train: 232 [1050/1251 ( 84%)]  Loss: 3.084 (3.18)  Time: 0.663s, 1545.61/s  (0.662s, 1547.84/s)  LR: 1.230e-04  Data: 0.013 (0.015)
Train: 232 [1100/1251 ( 88%)]  Loss: 3.075 (3.17)  Time: 0.665s, 1539.27/s  (0.662s, 1547.92/s)  LR: 1.229e-04  Data: 0.015 (0.015)
Train: 232 [1150/1251 ( 92%)]  Loss: 3.040 (3.17)  Time: 0.666s, 1537.18/s  (0.662s, 1547.82/s)  LR: 1.228e-04  Data: 0.013 (0.015)
Train: 232 [1200/1251 ( 96%)]  Loss: 2.888 (3.16)  Time: 0.661s, 1548.30/s  (0.662s, 1547.83/s)  LR: 1.226e-04  Data: 0.013 (0.015)
Train: 232 [1250/1251 (100%)]  Loss: 3.099 (3.15)  Time: 0.644s, 1590.93/s  (0.661s, 1548.04/s)  LR: 1.225e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.830 (2.830)  Loss:  0.4116 (0.4116)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.1445 (98.1445)
Test: [  48/48]  Time: 0.162 (0.324)  Loss:  0.5151 (0.8403)  Acc@1: 87.1462 (80.2060)  Acc@5: 98.2311 (95.3640)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-229.pth.tar', 80.31199997314454)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-232.pth.tar', 80.20600010498048)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-230.pth.tar', 80.17400015869141)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-231.pth.tar', 80.16199997558594)

Train: 233 [   0/1251 (  0%)]  Loss: 3.226 (3.23)  Time: 3.273s,  312.85/s  (3.273s,  312.85/s)  LR: 1.225e-04  Data: 1.775 (1.775)
Train: 233 [  50/1251 (  4%)]  Loss: 3.145 (3.19)  Time: 0.645s, 1587.82/s  (0.676s, 1515.57/s)  LR: 1.224e-04  Data: 0.014 (0.049)
Train: 233 [ 100/1251 (  8%)]  Loss: 2.918 (3.10)  Time: 0.657s, 1559.61/s  (0.664s, 1542.70/s)  LR: 1.222e-04  Data: 0.016 (0.031)
Train: 233 [ 150/1251 ( 12%)]  Loss: 3.415 (3.18)  Time: 0.670s, 1529.04/s  (0.662s, 1546.13/s)  LR: 1.221e-04  Data: 0.013 (0.026)
Train: 233 [ 200/1251 ( 16%)]  Loss: 3.205 (3.18)  Time: 0.656s, 1560.70/s  (0.662s, 1547.71/s)  LR: 1.220e-04  Data: 0.014 (0.023)
Train: 233 [ 250/1251 ( 20%)]  Loss: 3.269 (3.20)  Time: 0.656s, 1561.61/s  (0.662s, 1547.84/s)  LR: 1.218e-04  Data: 0.013 (0.021)
Train: 233 [ 300/1251 ( 24%)]  Loss: 2.948 (3.16)  Time: 0.663s, 1543.43/s  (0.662s, 1547.77/s)  LR: 1.217e-04  Data: 0.013 (0.019)
Train: 233 [ 350/1251 ( 28%)]  Loss: 2.826 (3.12)  Time: 0.660s, 1551.19/s  (0.662s, 1547.80/s)  LR: 1.216e-04  Data: 0.013 (0.019)
Train: 233 [ 400/1251 ( 32%)]  Loss: 3.124 (3.12)  Time: 0.667s, 1534.28/s  (0.662s, 1547.91/s)  LR: 1.214e-04  Data: 0.013 (0.018)
Train: 233 [ 450/1251 ( 36%)]  Loss: 3.004 (3.11)  Time: 0.662s, 1547.62/s  (0.661s, 1548.34/s)  LR: 1.213e-04  Data: 0.013 (0.017)
Train: 233 [ 500/1251 ( 40%)]  Loss: 3.277 (3.12)  Time: 0.663s, 1543.39/s  (0.661s, 1548.03/s)  LR: 1.212e-04  Data: 0.013 (0.017)
Train: 233 [ 550/1251 ( 44%)]  Loss: 2.864 (3.10)  Time: 0.668s, 1532.14/s  (0.661s, 1548.13/s)  LR: 1.210e-04  Data: 0.013 (0.017)
Train: 233 [ 600/1251 ( 48%)]  Loss: 3.044 (3.10)  Time: 0.654s, 1564.92/s  (0.662s, 1547.96/s)  LR: 1.209e-04  Data: 0.013 (0.017)
Train: 233 [ 650/1251 ( 52%)]  Loss: 3.024 (3.09)  Time: 0.668s, 1532.58/s  (0.662s, 1547.74/s)  LR: 1.208e-04  Data: 0.013 (0.016)
Train: 233 [ 700/1251 ( 56%)]  Loss: 3.114 (3.09)  Time: 0.676s, 1514.50/s  (0.662s, 1547.86/s)  LR: 1.206e-04  Data: 0.013 (0.016)
Train: 233 [ 750/1251 ( 60%)]  Loss: 3.087 (3.09)  Time: 0.665s, 1539.58/s  (0.662s, 1547.75/s)  LR: 1.205e-04  Data: 0.014 (0.016)
Train: 233 [ 800/1251 ( 64%)]  Loss: 3.313 (3.11)  Time: 0.665s, 1539.41/s  (0.662s, 1547.63/s)  LR: 1.204e-04  Data: 0.013 (0.016)
Train: 233 [ 850/1251 ( 68%)]  Loss: 2.973 (3.10)  Time: 0.660s, 1552.07/s  (0.662s, 1547.38/s)  LR: 1.202e-04  Data: 0.017 (0.016)
Train: 233 [ 900/1251 ( 72%)]  Loss: 3.121 (3.10)  Time: 0.673s, 1521.24/s  (0.662s, 1547.42/s)  LR: 1.201e-04  Data: 0.013 (0.016)
Train: 233 [ 950/1251 ( 76%)]  Loss: 3.222 (3.11)  Time: 0.667s, 1535.68/s  (0.662s, 1547.31/s)  LR: 1.200e-04  Data: 0.013 (0.016)
Train: 233 [1000/1251 ( 80%)]  Loss: 2.996 (3.10)  Time: 0.661s, 1549.07/s  (0.662s, 1547.28/s)  LR: 1.198e-04  Data: 0.017 (0.015)
Train: 233 [1050/1251 ( 84%)]  Loss: 3.079 (3.10)  Time: 0.651s, 1573.96/s  (0.662s, 1547.25/s)  LR: 1.197e-04  Data: 0.013 (0.015)
Train: 233 [1100/1251 ( 88%)]  Loss: 3.320 (3.11)  Time: 0.657s, 1558.60/s  (0.662s, 1547.53/s)  LR: 1.196e-04  Data: 0.014 (0.015)
Train: 233 [1150/1251 ( 92%)]  Loss: 3.252 (3.12)  Time: 0.651s, 1573.27/s  (0.662s, 1547.79/s)  LR: 1.194e-04  Data: 0.013 (0.015)
Train: 233 [1200/1251 ( 96%)]  Loss: 3.147 (3.12)  Time: 0.663s, 1544.13/s  (0.662s, 1547.95/s)  LR: 1.193e-04  Data: 0.014 (0.015)
Train: 233 [1250/1251 (100%)]  Loss: 3.343 (3.13)  Time: 0.639s, 1602.11/s  (0.661s, 1548.12/s)  LR: 1.192e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.049 (3.049)  Loss:  0.3984 (0.3984)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.161 (0.325)  Loss:  0.5122 (0.8409)  Acc@1: 86.7924 (80.3060)  Acc@5: 97.7594 (95.2980)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-229.pth.tar', 80.31199997314454)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-233.pth.tar', 80.30599995117187)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-232.pth.tar', 80.20600010498048)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-230.pth.tar', 80.17400015869141)

Train: 234 [   0/1251 (  0%)]  Loss: 3.318 (3.32)  Time: 3.111s,  329.18/s  (3.111s,  329.18/s)  LR: 1.192e-04  Data: 1.819 (1.819)
Train: 234 [  50/1251 (  4%)]  Loss: 3.389 (3.35)  Time: 0.643s, 1591.64/s  (0.668s, 1532.52/s)  LR: 1.190e-04  Data: 0.017 (0.049)
Train: 234 [ 100/1251 (  8%)]  Loss: 3.144 (3.28)  Time: 0.648s, 1580.25/s  (0.658s, 1555.24/s)  LR: 1.189e-04  Data: 0.013 (0.032)
Train: 234 [ 150/1251 ( 12%)]  Loss: 3.185 (3.26)  Time: 0.661s, 1549.20/s  (0.657s, 1559.09/s)  LR: 1.188e-04  Data: 0.015 (0.026)
Train: 234 [ 200/1251 ( 16%)]  Loss: 2.937 (3.19)  Time: 0.664s, 1541.50/s  (0.657s, 1557.78/s)  LR: 1.186e-04  Data: 0.013 (0.023)
Train: 234 [ 250/1251 ( 20%)]  Loss: 3.064 (3.17)  Time: 0.656s, 1561.26/s  (0.658s, 1557.09/s)  LR: 1.185e-04  Data: 0.013 (0.021)
Train: 234 [ 300/1251 ( 24%)]  Loss: 3.148 (3.17)  Time: 0.661s, 1548.07/s  (0.658s, 1556.68/s)  LR: 1.184e-04  Data: 0.013 (0.020)
Train: 234 [ 350/1251 ( 28%)]  Loss: 3.374 (3.19)  Time: 0.667s, 1534.56/s  (0.658s, 1556.24/s)  LR: 1.182e-04  Data: 0.016 (0.019)
Train: 234 [ 400/1251 ( 32%)]  Loss: 3.141 (3.19)  Time: 0.665s, 1540.56/s  (0.658s, 1555.89/s)  LR: 1.181e-04  Data: 0.013 (0.018)
Train: 234 [ 450/1251 ( 36%)]  Loss: 2.904 (3.16)  Time: 0.652s, 1570.29/s  (0.658s, 1555.27/s)  LR: 1.180e-04  Data: 0.014 (0.018)
Train: 234 [ 500/1251 ( 40%)]  Loss: 3.202 (3.16)  Time: 0.659s, 1552.79/s  (0.659s, 1555.01/s)  LR: 1.178e-04  Data: 0.013 (0.017)
Train: 234 [ 550/1251 ( 44%)]  Loss: 3.177 (3.17)  Time: 0.655s, 1562.63/s  (0.658s, 1555.24/s)  LR: 1.177e-04  Data: 0.013 (0.017)
Train: 234 [ 600/1251 ( 48%)]  Loss: 3.247 (3.17)  Time: 0.665s, 1539.54/s  (0.658s, 1555.41/s)  LR: 1.176e-04  Data: 0.013 (0.017)
Train: 234 [ 650/1251 ( 52%)]  Loss: 2.937 (3.15)  Time: 0.663s, 1543.85/s  (0.658s, 1555.55/s)  LR: 1.174e-04  Data: 0.013 (0.016)
Train: 234 [ 700/1251 ( 56%)]  Loss: 2.927 (3.14)  Time: 0.655s, 1563.49/s  (0.658s, 1555.14/s)  LR: 1.173e-04  Data: 0.018 (0.016)
Train: 234 [ 750/1251 ( 60%)]  Loss: 2.583 (3.10)  Time: 0.661s, 1549.59/s  (0.658s, 1555.05/s)  LR: 1.172e-04  Data: 0.013 (0.016)
Train: 234 [ 800/1251 ( 64%)]  Loss: 3.142 (3.11)  Time: 0.660s, 1551.33/s  (0.658s, 1555.22/s)  LR: 1.171e-04  Data: 0.015 (0.016)
Train: 234 [ 850/1251 ( 68%)]  Loss: 3.052 (3.10)  Time: 0.656s, 1561.28/s  (0.658s, 1555.25/s)  LR: 1.169e-04  Data: 0.013 (0.016)
Train: 234 [ 900/1251 ( 72%)]  Loss: 2.940 (3.10)  Time: 0.662s, 1546.05/s  (0.658s, 1555.26/s)  LR: 1.168e-04  Data: 0.013 (0.016)
Train: 234 [ 950/1251 ( 76%)]  Loss: 3.261 (3.10)  Time: 0.660s, 1551.67/s  (0.658s, 1555.08/s)  LR: 1.167e-04  Data: 0.013 (0.016)
Train: 234 [1000/1251 ( 80%)]  Loss: 3.080 (3.10)  Time: 0.658s, 1557.25/s  (0.659s, 1554.94/s)  LR: 1.165e-04  Data: 0.012 (0.015)
Train: 234 [1050/1251 ( 84%)]  Loss: 2.872 (3.09)  Time: 0.670s, 1529.10/s  (0.659s, 1554.73/s)  LR: 1.164e-04  Data: 0.013 (0.015)
Train: 234 [1100/1251 ( 88%)]  Loss: 2.651 (3.07)  Time: 0.656s, 1561.49/s  (0.659s, 1554.42/s)  LR: 1.163e-04  Data: 0.015 (0.015)
Train: 234 [1150/1251 ( 92%)]  Loss: 3.091 (3.07)  Time: 0.663s, 1543.50/s  (0.659s, 1554.24/s)  LR: 1.161e-04  Data: 0.013 (0.015)
Train: 234 [1200/1251 ( 96%)]  Loss: 3.339 (3.08)  Time: 0.656s, 1562.06/s  (0.659s, 1553.84/s)  LR: 1.160e-04  Data: 0.017 (0.015)
Train: 234 [1250/1251 (100%)]  Loss: 3.198 (3.09)  Time: 0.639s, 1603.22/s  (0.659s, 1553.68/s)  LR: 1.159e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.808 (2.808)  Loss:  0.3823 (0.3823)  Acc@1: 92.1875 (92.1875)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.162 (0.317)  Loss:  0.5127 (0.8323)  Acc@1: 87.2642 (80.3340)  Acc@5: 97.8774 (95.2980)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-234.pth.tar', 80.33400015625)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-229.pth.tar', 80.31199997314454)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-233.pth.tar', 80.30599995117187)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-232.pth.tar', 80.20600010498048)

Train: 235 [   0/1251 (  0%)]  Loss: 2.933 (2.93)  Time: 3.403s,  300.93/s  (3.403s,  300.93/s)  LR: 1.159e-04  Data: 1.715 (1.715)
Train: 235 [  50/1251 (  4%)]  Loss: 2.908 (2.92)  Time: 0.655s, 1562.31/s  (0.686s, 1493.49/s)  LR: 1.157e-04  Data: 0.013 (0.047)
Train: 235 [ 100/1251 (  8%)]  Loss: 3.008 (2.95)  Time: 0.655s, 1563.83/s  (0.669s, 1530.84/s)  LR: 1.156e-04  Data: 0.013 (0.031)
Train: 235 [ 150/1251 ( 12%)]  Loss: 3.194 (3.01)  Time: 0.659s, 1554.89/s  (0.664s, 1541.35/s)  LR: 1.155e-04  Data: 0.013 (0.025)
Train: 235 [ 200/1251 ( 16%)]  Loss: 3.436 (3.10)  Time: 0.650s, 1574.43/s  (0.662s, 1546.86/s)  LR: 1.153e-04  Data: 0.014 (0.022)
Train: 235 [ 250/1251 ( 20%)]  Loss: 3.008 (3.08)  Time: 0.666s, 1537.56/s  (0.661s, 1549.05/s)  LR: 1.152e-04  Data: 0.013 (0.021)
Train: 235 [ 300/1251 ( 24%)]  Loss: 3.288 (3.11)  Time: 0.655s, 1564.14/s  (0.661s, 1549.28/s)  LR: 1.151e-04  Data: 0.013 (0.019)
Train: 235 [ 350/1251 ( 28%)]  Loss: 3.343 (3.14)  Time: 0.661s, 1548.99/s  (0.661s, 1550.01/s)  LR: 1.150e-04  Data: 0.014 (0.019)
Train: 235 [ 400/1251 ( 32%)]  Loss: 3.076 (3.13)  Time: 0.663s, 1545.54/s  (0.660s, 1550.64/s)  LR: 1.148e-04  Data: 0.015 (0.018)
Train: 235 [ 450/1251 ( 36%)]  Loss: 2.919 (3.11)  Time: 0.656s, 1560.69/s  (0.660s, 1550.53/s)  LR: 1.147e-04  Data: 0.013 (0.018)
Train: 235 [ 500/1251 ( 40%)]  Loss: 3.088 (3.11)  Time: 0.653s, 1568.07/s  (0.660s, 1551.09/s)  LR: 1.146e-04  Data: 0.013 (0.017)
Train: 235 [ 550/1251 ( 44%)]  Loss: 3.180 (3.12)  Time: 0.662s, 1547.32/s  (0.660s, 1551.22/s)  LR: 1.144e-04  Data: 0.015 (0.017)
Train: 235 [ 600/1251 ( 48%)]  Loss: 3.424 (3.14)  Time: 0.648s, 1581.32/s  (0.660s, 1551.44/s)  LR: 1.143e-04  Data: 0.012 (0.017)
Train: 235 [ 650/1251 ( 52%)]  Loss: 3.267 (3.15)  Time: 0.658s, 1556.95/s  (0.660s, 1551.41/s)  LR: 1.142e-04  Data: 0.013 (0.016)
Train: 235 [ 700/1251 ( 56%)]  Loss: 2.653 (3.12)  Time: 0.660s, 1551.77/s  (0.660s, 1551.27/s)  LR: 1.140e-04  Data: 0.013 (0.016)
Train: 235 [ 750/1251 ( 60%)]  Loss: 3.290 (3.13)  Time: 0.666s, 1537.82/s  (0.660s, 1550.83/s)  LR: 1.139e-04  Data: 0.013 (0.016)
Train: 235 [ 800/1251 ( 64%)]  Loss: 2.847 (3.11)  Time: 0.658s, 1556.00/s  (0.660s, 1550.55/s)  LR: 1.138e-04  Data: 0.016 (0.016)
Train: 235 [ 850/1251 ( 68%)]  Loss: 3.096 (3.11)  Time: 0.664s, 1541.99/s  (0.661s, 1550.22/s)  LR: 1.136e-04  Data: 0.013 (0.016)
Train: 235 [ 900/1251 ( 72%)]  Loss: 3.025 (3.10)  Time: 0.665s, 1539.92/s  (0.661s, 1549.79/s)  LR: 1.135e-04  Data: 0.013 (0.016)
Train: 235 [ 950/1251 ( 76%)]  Loss: 3.068 (3.10)  Time: 0.662s, 1547.21/s  (0.661s, 1549.72/s)  LR: 1.134e-04  Data: 0.013 (0.016)
Train: 235 [1000/1251 ( 80%)]  Loss: 2.984 (3.10)  Time: 0.666s, 1538.36/s  (0.661s, 1549.60/s)  LR: 1.133e-04  Data: 0.016 (0.015)
Train: 235 [1050/1251 ( 84%)]  Loss: 3.142 (3.10)  Time: 0.655s, 1563.97/s  (0.661s, 1549.32/s)  LR: 1.131e-04  Data: 0.013 (0.015)
Train: 235 [1100/1251 ( 88%)]  Loss: 3.103 (3.10)  Time: 0.666s, 1538.23/s  (0.661s, 1549.24/s)  LR: 1.130e-04  Data: 0.013 (0.015)
Train: 235 [1150/1251 ( 92%)]  Loss: 3.096 (3.10)  Time: 0.666s, 1538.21/s  (0.661s, 1548.85/s)  LR: 1.129e-04  Data: 0.013 (0.015)
Train: 235 [1200/1251 ( 96%)]  Loss: 3.215 (3.10)  Time: 0.667s, 1535.54/s  (0.661s, 1548.49/s)  LR: 1.127e-04  Data: 0.013 (0.015)
Train: 235 [1250/1251 (100%)]  Loss: 3.051 (3.10)  Time: 0.641s, 1597.31/s  (0.661s, 1548.44/s)  LR: 1.126e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.965 (2.965)  Loss:  0.4050 (0.4050)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.162 (0.322)  Loss:  0.5107 (0.8377)  Acc@1: 87.7358 (80.4160)  Acc@5: 98.2311 (95.4180)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-235.pth.tar', 80.41599997314454)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-234.pth.tar', 80.33400015625)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-229.pth.tar', 80.31199997314454)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-233.pth.tar', 80.30599995117187)

Train: 236 [   0/1251 (  0%)]  Loss: 3.339 (3.34)  Time: 3.292s,  311.06/s  (3.292s,  311.06/s)  LR: 1.126e-04  Data: 1.639 (1.639)
Train: 236 [  50/1251 (  4%)]  Loss: 3.183 (3.26)  Time: 0.648s, 1579.22/s  (0.687s, 1491.54/s)  LR: 1.125e-04  Data: 0.014 (0.046)
Train: 236 [ 100/1251 (  8%)]  Loss: 3.353 (3.29)  Time: 0.661s, 1548.98/s  (0.670s, 1528.18/s)  LR: 1.124e-04  Data: 0.014 (0.030)
Train: 236 [ 150/1251 ( 12%)]  Loss: 3.234 (3.28)  Time: 0.659s, 1553.24/s  (0.666s, 1538.56/s)  LR: 1.122e-04  Data: 0.013 (0.025)
Train: 236 [ 200/1251 ( 16%)]  Loss: 3.116 (3.25)  Time: 0.663s, 1544.10/s  (0.664s, 1541.65/s)  LR: 1.121e-04  Data: 0.013 (0.022)
Train: 236 [ 250/1251 ( 20%)]  Loss: 3.167 (3.23)  Time: 0.657s, 1558.80/s  (0.663s, 1544.35/s)  LR: 1.120e-04  Data: 0.016 (0.020)
Train: 236 [ 300/1251 ( 24%)]  Loss: 3.175 (3.22)  Time: 0.653s, 1567.64/s  (0.663s, 1545.36/s)  LR: 1.118e-04  Data: 0.014 (0.019)
Train: 236 [ 350/1251 ( 28%)]  Loss: 2.977 (3.19)  Time: 0.656s, 1560.82/s  (0.662s, 1545.98/s)  LR: 1.117e-04  Data: 0.016 (0.018)
Train: 236 [ 400/1251 ( 32%)]  Loss: 2.976 (3.17)  Time: 0.663s, 1544.64/s  (0.662s, 1546.21/s)  LR: 1.116e-04  Data: 0.014 (0.018)
Train: 236 [ 450/1251 ( 36%)]  Loss: 3.135 (3.17)  Time: 0.664s, 1542.38/s  (0.662s, 1546.26/s)  LR: 1.114e-04  Data: 0.013 (0.017)
Train: 236 [ 500/1251 ( 40%)]  Loss: 3.314 (3.18)  Time: 0.664s, 1542.40/s  (0.662s, 1546.78/s)  LR: 1.113e-04  Data: 0.018 (0.017)
Train: 236 [ 550/1251 ( 44%)]  Loss: 3.179 (3.18)  Time: 0.660s, 1552.06/s  (0.662s, 1546.79/s)  LR: 1.112e-04  Data: 0.013 (0.017)
Train: 236 [ 600/1251 ( 48%)]  Loss: 2.955 (3.16)  Time: 0.652s, 1569.60/s  (0.662s, 1546.54/s)  LR: 1.111e-04  Data: 0.012 (0.017)
Train: 236 [ 650/1251 ( 52%)]  Loss: 3.297 (3.17)  Time: 0.664s, 1542.04/s  (0.662s, 1546.54/s)  LR: 1.109e-04  Data: 0.017 (0.016)
Train: 236 [ 700/1251 ( 56%)]  Loss: 3.045 (3.16)  Time: 0.660s, 1550.45/s  (0.662s, 1546.87/s)  LR: 1.108e-04  Data: 0.014 (0.016)
Train: 236 [ 750/1251 ( 60%)]  Loss: 3.254 (3.17)  Time: 0.663s, 1544.46/s  (0.662s, 1546.74/s)  LR: 1.107e-04  Data: 0.013 (0.016)
Train: 236 [ 800/1251 ( 64%)]  Loss: 3.257 (3.17)  Time: 0.659s, 1553.56/s  (0.662s, 1546.19/s)  LR: 1.105e-04  Data: 0.013 (0.016)
Train: 236 [ 850/1251 ( 68%)]  Loss: 3.013 (3.17)  Time: 0.670s, 1527.59/s  (0.662s, 1545.99/s)  LR: 1.104e-04  Data: 0.013 (0.016)
Train: 236 [ 900/1251 ( 72%)]  Loss: 3.220 (3.17)  Time: 0.673s, 1522.26/s  (0.662s, 1545.87/s)  LR: 1.103e-04  Data: 0.014 (0.016)
Train: 236 [ 950/1251 ( 76%)]  Loss: 3.283 (3.17)  Time: 0.669s, 1530.21/s  (0.662s, 1545.97/s)  LR: 1.102e-04  Data: 0.013 (0.015)
Train: 236 [1000/1251 ( 80%)]  Loss: 3.235 (3.18)  Time: 0.669s, 1530.64/s  (0.662s, 1545.67/s)  LR: 1.100e-04  Data: 0.016 (0.015)
Train: 236 [1050/1251 ( 84%)]  Loss: 3.177 (3.18)  Time: 0.665s, 1540.77/s  (0.663s, 1545.47/s)  LR: 1.099e-04  Data: 0.013 (0.015)
Train: 236 [1100/1251 ( 88%)]  Loss: 2.949 (3.17)  Time: 0.664s, 1541.77/s  (0.663s, 1545.36/s)  LR: 1.098e-04  Data: 0.014 (0.015)
Train: 236 [1150/1251 ( 92%)]  Loss: 3.144 (3.17)  Time: 0.671s, 1526.83/s  (0.663s, 1545.37/s)  LR: 1.097e-04  Data: 0.013 (0.015)
Train: 236 [1200/1251 ( 96%)]  Loss: 3.298 (3.17)  Time: 0.662s, 1546.97/s  (0.663s, 1545.36/s)  LR: 1.095e-04  Data: 0.012 (0.015)
Train: 236 [1250/1251 (100%)]  Loss: 3.225 (3.17)  Time: 0.649s, 1578.29/s  (0.663s, 1545.39/s)  LR: 1.094e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.821 (2.821)  Loss:  0.4097 (0.4097)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.162 (0.318)  Loss:  0.5161 (0.8398)  Acc@1: 87.0283 (80.2840)  Acc@5: 98.2311 (95.2820)
Train: 237 [   0/1251 (  0%)]  Loss: 2.927 (2.93)  Time: 3.957s,  258.75/s  (3.957s,  258.75/s)  LR: 1.094e-04  Data: 1.604 (1.604)
Train: 237 [  50/1251 (  4%)]  Loss: 2.795 (2.86)  Time: 0.641s, 1596.31/s  (0.685s, 1493.85/s)  LR: 1.093e-04  Data: 0.013 (0.045)
Train: 237 [ 100/1251 (  8%)]  Loss: 3.068 (2.93)  Time: 0.645s, 1586.97/s  (0.670s, 1527.64/s)  LR: 1.091e-04  Data: 0.013 (0.029)
Train: 237 [ 150/1251 ( 12%)]  Loss: 3.169 (2.99)  Time: 0.666s, 1538.39/s  (0.667s, 1534.18/s)  LR: 1.090e-04  Data: 0.013 (0.024)
Train: 237 [ 200/1251 ( 16%)]  Loss: 3.303 (3.05)  Time: 0.655s, 1563.80/s  (0.666s, 1536.97/s)  LR: 1.089e-04  Data: 0.014 (0.022)
Train: 237 [ 250/1251 ( 20%)]  Loss: 3.190 (3.08)  Time: 0.666s, 1538.28/s  (0.666s, 1538.31/s)  LR: 1.088e-04  Data: 0.013 (0.020)
Train: 237 [ 300/1251 ( 24%)]  Loss: 3.060 (3.07)  Time: 0.664s, 1542.96/s  (0.665s, 1538.90/s)  LR: 1.086e-04  Data: 0.016 (0.019)
Train: 237 [ 350/1251 ( 28%)]  Loss: 3.541 (3.13)  Time: 0.656s, 1561.76/s  (0.665s, 1539.34/s)  LR: 1.085e-04  Data: 0.014 (0.018)
Train: 237 [ 400/1251 ( 32%)]  Loss: 3.359 (3.16)  Time: 0.665s, 1539.76/s  (0.665s, 1540.28/s)  LR: 1.084e-04  Data: 0.011 (0.018)
Train: 237 [ 450/1251 ( 36%)]  Loss: 3.049 (3.15)  Time: 0.659s, 1553.01/s  (0.665s, 1540.93/s)  LR: 1.082e-04  Data: 0.013 (0.017)
Train: 237 [ 500/1251 ( 40%)]  Loss: 2.912 (3.12)  Time: 0.660s, 1551.06/s  (0.664s, 1541.57/s)  LR: 1.081e-04  Data: 0.013 (0.017)
Train: 237 [ 550/1251 ( 44%)]  Loss: 3.131 (3.13)  Time: 0.666s, 1537.08/s  (0.664s, 1541.80/s)  LR: 1.080e-04  Data: 0.017 (0.017)
Train: 237 [ 600/1251 ( 48%)]  Loss: 2.979 (3.11)  Time: 0.668s, 1531.92/s  (0.664s, 1542.05/s)  LR: 1.079e-04  Data: 0.013 (0.016)
Train: 237 [ 650/1251 ( 52%)]  Loss: 3.153 (3.12)  Time: 0.662s, 1547.52/s  (0.664s, 1542.76/s)  LR: 1.077e-04  Data: 0.013 (0.016)
Train: 237 [ 700/1251 ( 56%)]  Loss: 2.919 (3.10)  Time: 0.653s, 1567.12/s  (0.664s, 1543.17/s)  LR: 1.076e-04  Data: 0.013 (0.016)
Train: 237 [ 750/1251 ( 60%)]  Loss: 2.824 (3.09)  Time: 0.658s, 1557.21/s  (0.663s, 1543.51/s)  LR: 1.075e-04  Data: 0.015 (0.016)
Train: 237 [ 800/1251 ( 64%)]  Loss: 3.154 (3.09)  Time: 0.666s, 1536.79/s  (0.663s, 1543.66/s)  LR: 1.074e-04  Data: 0.013 (0.016)
Train: 237 [ 850/1251 ( 68%)]  Loss: 3.136 (3.09)  Time: 0.676s, 1514.46/s  (0.663s, 1543.88/s)  LR: 1.072e-04  Data: 0.014 (0.016)
Train: 237 [ 900/1251 ( 72%)]  Loss: 3.025 (3.09)  Time: 0.660s, 1551.30/s  (0.663s, 1544.35/s)  LR: 1.071e-04  Data: 0.014 (0.015)
Train: 237 [ 950/1251 ( 76%)]  Loss: 2.868 (3.08)  Time: 0.664s, 1541.30/s  (0.663s, 1544.61/s)  LR: 1.070e-04  Data: 0.013 (0.015)
Train: 237 [1000/1251 ( 80%)]  Loss: 3.197 (3.08)  Time: 0.655s, 1563.80/s  (0.663s, 1544.54/s)  LR: 1.069e-04  Data: 0.013 (0.015)
Train: 237 [1050/1251 ( 84%)]  Loss: 3.142 (3.09)  Time: 0.670s, 1527.66/s  (0.663s, 1544.60/s)  LR: 1.067e-04  Data: 0.014 (0.015)
Train: 237 [1100/1251 ( 88%)]  Loss: 3.237 (3.09)  Time: 0.662s, 1546.91/s  (0.663s, 1544.70/s)  LR: 1.066e-04  Data: 0.014 (0.015)
Train: 237 [1150/1251 ( 92%)]  Loss: 3.201 (3.10)  Time: 0.650s, 1574.73/s  (0.663s, 1544.75/s)  LR: 1.065e-04  Data: 0.013 (0.015)
Train: 237 [1200/1251 ( 96%)]  Loss: 3.385 (3.11)  Time: 0.667s, 1536.33/s  (0.663s, 1544.76/s)  LR: 1.064e-04  Data: 0.013 (0.015)
Train: 237 [1250/1251 (100%)]  Loss: 2.934 (3.10)  Time: 0.657s, 1558.31/s  (0.663s, 1544.76/s)  LR: 1.062e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.906 (2.906)  Loss:  0.3965 (0.3965)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.161 (0.322)  Loss:  0.5249 (0.8334)  Acc@1: 86.6745 (80.4960)  Acc@5: 97.7594 (95.3780)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-237.pth.tar', 80.49600002929688)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-235.pth.tar', 80.41599997314454)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-234.pth.tar', 80.33400015625)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-229.pth.tar', 80.31199997314454)

Train: 238 [   0/1251 (  0%)]  Loss: 2.893 (2.89)  Time: 2.896s,  353.54/s  (2.896s,  353.54/s)  LR: 1.062e-04  Data: 1.661 (1.661)
Train: 238 [  50/1251 (  4%)]  Loss: 3.019 (2.96)  Time: 0.637s, 1606.74/s  (0.673s, 1522.45/s)  LR: 1.061e-04  Data: 0.013 (0.046)
Train: 238 [ 100/1251 (  8%)]  Loss: 3.161 (3.02)  Time: 0.660s, 1552.32/s  (0.663s, 1543.68/s)  LR: 1.060e-04  Data: 0.013 (0.030)
Train: 238 [ 150/1251 ( 12%)]  Loss: 3.131 (3.05)  Time: 0.651s, 1572.29/s  (0.661s, 1549.75/s)  LR: 1.058e-04  Data: 0.013 (0.025)
Train: 238 [ 200/1251 ( 16%)]  Loss: 2.828 (3.01)  Time: 0.658s, 1555.71/s  (0.660s, 1552.10/s)  LR: 1.057e-04  Data: 0.014 (0.022)
Train: 238 [ 250/1251 ( 20%)]  Loss: 2.747 (2.96)  Time: 0.670s, 1527.22/s  (0.660s, 1551.63/s)  LR: 1.056e-04  Data: 0.017 (0.021)
Train: 238 [ 300/1251 ( 24%)]  Loss: 2.905 (2.95)  Time: 0.668s, 1533.40/s  (0.660s, 1551.81/s)  LR: 1.055e-04  Data: 0.013 (0.019)
Train: 238 [ 350/1251 ( 28%)]  Loss: 3.087 (2.97)  Time: 0.655s, 1563.16/s  (0.660s, 1551.78/s)  LR: 1.053e-04  Data: 0.015 (0.019)
Train: 238 [ 400/1251 ( 32%)]  Loss: 3.182 (2.99)  Time: 0.654s, 1565.41/s  (0.660s, 1552.16/s)  LR: 1.052e-04  Data: 0.012 (0.018)
Train: 238 [ 450/1251 ( 36%)]  Loss: 3.267 (3.02)  Time: 0.655s, 1563.12/s  (0.660s, 1552.02/s)  LR: 1.051e-04  Data: 0.013 (0.018)
Train: 238 [ 500/1251 ( 40%)]  Loss: 3.446 (3.06)  Time: 0.658s, 1556.41/s  (0.660s, 1552.59/s)  LR: 1.050e-04  Data: 0.016 (0.017)
Train: 238 [ 550/1251 ( 44%)]  Loss: 2.849 (3.04)  Time: 0.651s, 1574.08/s  (0.659s, 1552.84/s)  LR: 1.048e-04  Data: 0.013 (0.017)
Train: 238 [ 600/1251 ( 48%)]  Loss: 2.880 (3.03)  Time: 0.664s, 1542.74/s  (0.659s, 1552.93/s)  LR: 1.047e-04  Data: 0.016 (0.017)
Train: 238 [ 650/1251 ( 52%)]  Loss: 3.004 (3.03)  Time: 0.655s, 1563.80/s  (0.659s, 1552.74/s)  LR: 1.046e-04  Data: 0.014 (0.017)
Train: 238 [ 700/1251 ( 56%)]  Loss: 2.949 (3.02)  Time: 0.660s, 1551.83/s  (0.659s, 1552.84/s)  LR: 1.045e-04  Data: 0.013 (0.016)
Train: 238 [ 750/1251 ( 60%)]  Loss: 3.219 (3.04)  Time: 0.655s, 1564.13/s  (0.659s, 1552.94/s)  LR: 1.043e-04  Data: 0.015 (0.016)
Train: 238 [ 800/1251 ( 64%)]  Loss: 3.028 (3.03)  Time: 0.657s, 1558.09/s  (0.659s, 1552.94/s)  LR: 1.042e-04  Data: 0.013 (0.016)
Train: 238 [ 850/1251 ( 68%)]  Loss: 3.353 (3.05)  Time: 0.657s, 1558.24/s  (0.659s, 1553.09/s)  LR: 1.041e-04  Data: 0.018 (0.016)
Train: 238 [ 900/1251 ( 72%)]  Loss: 3.248 (3.06)  Time: 0.656s, 1560.47/s  (0.659s, 1553.30/s)  LR: 1.040e-04  Data: 0.013 (0.016)
Train: 238 [ 950/1251 ( 76%)]  Loss: 2.893 (3.05)  Time: 0.662s, 1547.11/s  (0.659s, 1553.29/s)  LR: 1.038e-04  Data: 0.013 (0.016)
Train: 238 [1000/1251 ( 80%)]  Loss: 3.323 (3.07)  Time: 0.664s, 1541.96/s  (0.659s, 1553.26/s)  LR: 1.037e-04  Data: 0.013 (0.016)
Train: 238 [1050/1251 ( 84%)]  Loss: 3.235 (3.07)  Time: 0.651s, 1573.11/s  (0.659s, 1553.09/s)  LR: 1.036e-04  Data: 0.014 (0.016)
Train: 238 [1100/1251 ( 88%)]  Loss: 3.282 (3.08)  Time: 0.657s, 1559.14/s  (0.659s, 1552.89/s)  LR: 1.035e-04  Data: 0.012 (0.015)
Train: 238 [1150/1251 ( 92%)]  Loss: 3.203 (3.09)  Time: 0.660s, 1551.81/s  (0.659s, 1552.84/s)  LR: 1.033e-04  Data: 0.017 (0.015)
Train: 238 [1200/1251 ( 96%)]  Loss: 3.329 (3.10)  Time: 0.659s, 1553.52/s  (0.660s, 1552.58/s)  LR: 1.032e-04  Data: 0.014 (0.015)
Train: 238 [1250/1251 (100%)]  Loss: 2.916 (3.09)  Time: 0.649s, 1577.25/s  (0.660s, 1552.60/s)  LR: 1.031e-04  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.878 (2.878)  Loss:  0.3984 (0.3984)  Acc@1: 92.0898 (92.0898)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.161 (0.319)  Loss:  0.5127 (0.8340)  Acc@1: 87.6179 (80.4840)  Acc@5: 98.2311 (95.3200)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-237.pth.tar', 80.49600002929688)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-238.pth.tar', 80.48400005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-235.pth.tar', 80.41599997314454)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-234.pth.tar', 80.33400015625)

Train: 239 [   0/1251 (  0%)]  Loss: 2.863 (2.86)  Time: 3.190s,  321.02/s  (3.190s,  321.02/s)  LR: 1.031e-04  Data: 2.174 (2.174)
Train: 239 [  50/1251 (  4%)]  Loss: 3.074 (2.97)  Time: 0.645s, 1587.49/s  (0.674s, 1518.63/s)  LR: 1.030e-04  Data: 0.012 (0.056)
Train: 239 [ 100/1251 (  8%)]  Loss: 3.146 (3.03)  Time: 0.660s, 1551.55/s  (0.665s, 1540.29/s)  LR: 1.028e-04  Data: 0.012 (0.035)
Train: 239 [ 150/1251 ( 12%)]  Loss: 3.046 (3.03)  Time: 0.668s, 1532.76/s  (0.662s, 1545.66/s)  LR: 1.027e-04  Data: 0.016 (0.028)
Train: 239 [ 200/1251 ( 16%)]  Loss: 3.157 (3.06)  Time: 0.657s, 1559.48/s  (0.663s, 1545.45/s)  LR: 1.026e-04  Data: 0.013 (0.024)
Train: 239 [ 250/1251 ( 20%)]  Loss: 2.989 (3.05)  Time: 0.658s, 1556.35/s  (0.662s, 1546.32/s)  LR: 1.025e-04  Data: 0.016 (0.022)
Train: 239 [ 300/1251 ( 24%)]  Loss: 3.233 (3.07)  Time: 0.665s, 1539.69/s  (0.662s, 1547.06/s)  LR: 1.024e-04  Data: 0.013 (0.021)
Train: 239 [ 350/1251 ( 28%)]  Loss: 2.852 (3.05)  Time: 0.654s, 1564.65/s  (0.661s, 1548.14/s)  LR: 1.022e-04  Data: 0.013 (0.020)
Train: 239 [ 400/1251 ( 32%)]  Loss: 3.061 (3.05)  Time: 0.653s, 1569.19/s  (0.661s, 1548.46/s)  LR: 1.021e-04  Data: 0.014 (0.019)
Train: 239 [ 450/1251 ( 36%)]  Loss: 3.084 (3.05)  Time: 0.662s, 1545.67/s  (0.661s, 1548.65/s)  LR: 1.020e-04  Data: 0.013 (0.019)
Train: 239 [ 500/1251 ( 40%)]  Loss: 3.324 (3.08)  Time: 0.657s, 1558.69/s  (0.661s, 1548.63/s)  LR: 1.019e-04  Data: 0.013 (0.018)
Train: 239 [ 550/1251 ( 44%)]  Loss: 3.250 (3.09)  Time: 0.662s, 1546.86/s  (0.661s, 1548.96/s)  LR: 1.017e-04  Data: 0.013 (0.018)
Train: 239 [ 600/1251 ( 48%)]  Loss: 3.181 (3.10)  Time: 0.664s, 1542.97/s  (0.661s, 1548.87/s)  LR: 1.016e-04  Data: 0.014 (0.018)
Train: 239 [ 650/1251 ( 52%)]  Loss: 3.154 (3.10)  Time: 0.656s, 1559.84/s  (0.661s, 1549.18/s)  LR: 1.015e-04  Data: 0.013 (0.017)
Train: 239 [ 700/1251 ( 56%)]  Loss: 2.982 (3.09)  Time: 0.662s, 1546.17/s  (0.661s, 1549.41/s)  LR: 1.014e-04  Data: 0.012 (0.017)
Train: 239 [ 750/1251 ( 60%)]  Loss: 3.292 (3.11)  Time: 0.667s, 1535.33/s  (0.661s, 1549.65/s)  LR: 1.012e-04  Data: 0.014 (0.017)
Train: 239 [ 800/1251 ( 64%)]  Loss: 3.146 (3.11)  Time: 0.646s, 1585.72/s  (0.661s, 1549.87/s)  LR: 1.011e-04  Data: 0.014 (0.017)
Train: 239 [ 850/1251 ( 68%)]  Loss: 2.963 (3.10)  Time: 0.654s, 1566.14/s  (0.661s, 1550.31/s)  LR: 1.010e-04  Data: 0.013 (0.017)
Train: 239 [ 900/1251 ( 72%)]  Loss: 2.974 (3.09)  Time: 0.673s, 1521.12/s  (0.660s, 1550.47/s)  LR: 1.009e-04  Data: 0.015 (0.016)
Train: 239 [ 950/1251 ( 76%)]  Loss: 3.073 (3.09)  Time: 0.664s, 1541.69/s  (0.660s, 1550.49/s)  LR: 1.007e-04  Data: 0.012 (0.016)
Train: 239 [1000/1251 ( 80%)]  Loss: 3.161 (3.10)  Time: 0.659s, 1554.00/s  (0.660s, 1550.50/s)  LR: 1.006e-04  Data: 0.014 (0.016)
Train: 239 [1050/1251 ( 84%)]  Loss: 3.025 (3.09)  Time: 0.664s, 1543.32/s  (0.661s, 1550.28/s)  LR: 1.005e-04  Data: 0.013 (0.016)
Train: 239 [1100/1251 ( 88%)]  Loss: 3.188 (3.10)  Time: 0.660s, 1550.68/s  (0.661s, 1550.29/s)  LR: 1.004e-04  Data: 0.013 (0.016)
Train: 239 [1150/1251 ( 92%)]  Loss: 3.111 (3.10)  Time: 0.666s, 1538.10/s  (0.661s, 1550.16/s)  LR: 1.003e-04  Data: 0.017 (0.016)
Train: 239 [1200/1251 ( 96%)]  Loss: 3.114 (3.10)  Time: 0.667s, 1535.13/s  (0.661s, 1549.96/s)  LR: 1.001e-04  Data: 0.013 (0.016)
Train: 239 [1250/1251 (100%)]  Loss: 3.245 (3.10)  Time: 0.642s, 1594.10/s  (0.661s, 1550.13/s)  LR: 1.000e-04  Data: 0.000 (0.016)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.777 (2.777)  Loss:  0.3879 (0.3879)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.8281 (98.8281)
Test: [  48/48]  Time: 0.161 (0.325)  Loss:  0.5122 (0.8400)  Acc@1: 87.5000 (80.4140)  Acc@5: 97.7594 (95.3260)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-237.pth.tar', 80.49600002929688)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-238.pth.tar', 80.48400005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-235.pth.tar', 80.41599997314454)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-239.pth.tar', 80.414)

Train: 240 [   0/1251 (  0%)]  Loss: 3.028 (3.03)  Time: 3.386s,  302.38/s  (3.386s,  302.38/s)  LR: 1.000e-04  Data: 1.664 (1.664)
Train: 240 [  50/1251 (  4%)]  Loss: 3.243 (3.14)  Time: 0.648s, 1581.12/s  (0.682s, 1500.58/s)  LR: 9.989e-05  Data: 0.014 (0.047)
Train: 240 [ 100/1251 (  8%)]  Loss: 2.836 (3.04)  Time: 0.656s, 1560.29/s  (0.668s, 1531.90/s)  LR: 9.977e-05  Data: 0.013 (0.030)
Train: 240 [ 150/1251 ( 12%)]  Loss: 3.128 (3.06)  Time: 0.670s, 1527.99/s  (0.665s, 1539.42/s)  LR: 9.964e-05  Data: 0.013 (0.025)
Train: 240 [ 200/1251 ( 16%)]  Loss: 3.295 (3.11)  Time: 0.665s, 1540.78/s  (0.665s, 1540.65/s)  LR: 9.952e-05  Data: 0.014 (0.022)
Train: 240 [ 250/1251 ( 20%)]  Loss: 3.053 (3.10)  Time: 0.660s, 1550.83/s  (0.664s, 1541.70/s)  LR: 9.940e-05  Data: 0.013 (0.020)
Train: 240 [ 300/1251 ( 24%)]  Loss: 2.905 (3.07)  Time: 0.661s, 1548.33/s  (0.664s, 1542.29/s)  LR: 9.928e-05  Data: 0.016 (0.019)
Train: 240 [ 350/1251 ( 28%)]  Loss: 3.028 (3.06)  Time: 0.666s, 1537.16/s  (0.664s, 1542.93/s)  LR: 9.916e-05  Data: 0.013 (0.019)
Train: 240 [ 400/1251 ( 32%)]  Loss: 2.980 (3.06)  Time: 0.664s, 1543.19/s  (0.664s, 1543.26/s)  LR: 9.903e-05  Data: 0.013 (0.018)
Train: 240 [ 450/1251 ( 36%)]  Loss: 2.886 (3.04)  Time: 0.664s, 1541.97/s  (0.663s, 1543.34/s)  LR: 9.891e-05  Data: 0.015 (0.017)
Train: 240 [ 500/1251 ( 40%)]  Loss: 3.185 (3.05)  Time: 0.663s, 1545.27/s  (0.663s, 1543.45/s)  LR: 9.879e-05  Data: 0.013 (0.017)
Train: 240 [ 550/1251 ( 44%)]  Loss: 3.225 (3.07)  Time: 0.661s, 1548.54/s  (0.663s, 1544.02/s)  LR: 9.867e-05  Data: 0.015 (0.017)
Train: 240 [ 600/1251 ( 48%)]  Loss: 3.044 (3.06)  Time: 0.660s, 1550.59/s  (0.663s, 1544.49/s)  LR: 9.855e-05  Data: 0.013 (0.017)
Train: 240 [ 650/1251 ( 52%)]  Loss: 2.993 (3.06)  Time: 0.661s, 1548.30/s  (0.663s, 1545.07/s)  LR: 9.843e-05  Data: 0.013 (0.016)
Train: 240 [ 700/1251 ( 56%)]  Loss: 3.076 (3.06)  Time: 0.655s, 1564.26/s  (0.663s, 1545.66/s)  LR: 9.831e-05  Data: 0.013 (0.016)
Train: 240 [ 750/1251 ( 60%)]  Loss: 2.975 (3.06)  Time: 0.654s, 1564.93/s  (0.662s, 1545.85/s)  LR: 9.818e-05  Data: 0.013 (0.016)
Train: 240 [ 800/1251 ( 64%)]  Loss: 3.207 (3.06)  Time: 0.657s, 1558.25/s  (0.662s, 1546.10/s)  LR: 9.806e-05  Data: 0.014 (0.016)
Train: 240 [ 850/1251 ( 68%)]  Loss: 3.279 (3.08)  Time: 0.670s, 1529.05/s  (0.662s, 1546.20/s)  LR: 9.794e-05  Data: 0.013 (0.016)
Train: 240 [ 900/1251 ( 72%)]  Loss: 3.183 (3.08)  Time: 0.654s, 1566.33/s  (0.662s, 1546.52/s)  LR: 9.782e-05  Data: 0.013 (0.016)
Train: 240 [ 950/1251 ( 76%)]  Loss: 3.414 (3.10)  Time: 0.667s, 1534.97/s  (0.662s, 1546.79/s)  LR: 9.770e-05  Data: 0.013 (0.015)
Train: 240 [1000/1251 ( 80%)]  Loss: 2.908 (3.09)  Time: 0.658s, 1556.95/s  (0.662s, 1547.04/s)  LR: 9.758e-05  Data: 0.013 (0.015)
Train: 240 [1050/1251 ( 84%)]  Loss: 2.921 (3.08)  Time: 0.657s, 1558.41/s  (0.662s, 1547.15/s)  LR: 9.746e-05  Data: 0.013 (0.015)
Train: 240 [1100/1251 ( 88%)]  Loss: 3.093 (3.08)  Time: 0.656s, 1560.76/s  (0.662s, 1547.35/s)  LR: 9.734e-05  Data: 0.014 (0.015)
Train: 240 [1150/1251 ( 92%)]  Loss: 3.237 (3.09)  Time: 0.659s, 1553.77/s  (0.662s, 1547.53/s)  LR: 9.722e-05  Data: 0.014 (0.015)
Train: 240 [1200/1251 ( 96%)]  Loss: 2.834 (3.08)  Time: 0.656s, 1562.07/s  (0.662s, 1547.61/s)  LR: 9.709e-05  Data: 0.012 (0.015)
Train: 240 [1250/1251 (100%)]  Loss: 3.084 (3.08)  Time: 0.652s, 1570.29/s  (0.662s, 1547.66/s)  LR: 9.697e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.920 (2.920)  Loss:  0.3921 (0.3921)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.161 (0.316)  Loss:  0.5112 (0.8246)  Acc@1: 87.5000 (80.4920)  Acc@5: 98.3491 (95.4540)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-237.pth.tar', 80.49600002929688)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-240.pth.tar', 80.492)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-238.pth.tar', 80.48400005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-235.pth.tar', 80.41599997314454)

Train: 241 [   0/1251 (  0%)]  Loss: 2.620 (2.62)  Time: 3.446s,  297.14/s  (3.446s,  297.14/s)  LR: 9.697e-05  Data: 1.872 (1.872)
Train: 241 [  50/1251 (  4%)]  Loss: 3.278 (2.95)  Time: 0.649s, 1578.01/s  (0.682s, 1501.19/s)  LR: 9.685e-05  Data: 0.013 (0.050)
Train: 241 [ 100/1251 (  8%)]  Loss: 2.988 (2.96)  Time: 0.655s, 1562.88/s  (0.669s, 1531.29/s)  LR: 9.673e-05  Data: 0.016 (0.032)
Train: 241 [ 150/1251 ( 12%)]  Loss: 3.483 (3.09)  Time: 0.653s, 1567.13/s  (0.666s, 1538.38/s)  LR: 9.661e-05  Data: 0.016 (0.026)
Train: 241 [ 200/1251 ( 16%)]  Loss: 3.052 (3.08)  Time: 0.652s, 1571.56/s  (0.664s, 1541.34/s)  LR: 9.649e-05  Data: 0.013 (0.023)
Train: 241 [ 250/1251 ( 20%)]  Loss: 3.184 (3.10)  Time: 0.671s, 1525.93/s  (0.664s, 1542.48/s)  LR: 9.637e-05  Data: 0.013 (0.021)
Train: 241 [ 300/1251 ( 24%)]  Loss: 2.880 (3.07)  Time: 0.665s, 1538.92/s  (0.663s, 1543.65/s)  LR: 9.625e-05  Data: 0.013 (0.020)
Train: 241 [ 350/1251 ( 28%)]  Loss: 3.043 (3.07)  Time: 0.662s, 1547.95/s  (0.663s, 1544.11/s)  LR: 9.613e-05  Data: 0.016 (0.019)
Train: 241 [ 400/1251 ( 32%)]  Loss: 2.999 (3.06)  Time: 0.666s, 1537.63/s  (0.663s, 1544.69/s)  LR: 9.601e-05  Data: 0.012 (0.018)
Train: 241 [ 450/1251 ( 36%)]  Loss: 3.032 (3.06)  Time: 0.662s, 1546.43/s  (0.663s, 1545.03/s)  LR: 9.589e-05  Data: 0.012 (0.018)
Train: 241 [ 500/1251 ( 40%)]  Loss: 3.247 (3.07)  Time: 0.656s, 1561.48/s  (0.663s, 1545.21/s)  LR: 9.577e-05  Data: 0.013 (0.017)
Train: 241 [ 550/1251 ( 44%)]  Loss: 2.988 (3.07)  Time: 0.665s, 1539.32/s  (0.663s, 1544.90/s)  LR: 9.565e-05  Data: 0.013 (0.017)
Train: 241 [ 600/1251 ( 48%)]  Loss: 2.996 (3.06)  Time: 0.664s, 1542.02/s  (0.663s, 1545.15/s)  LR: 9.553e-05  Data: 0.012 (0.017)
Train: 241 [ 650/1251 ( 52%)]  Loss: 2.882 (3.05)  Time: 0.652s, 1570.02/s  (0.663s, 1545.56/s)  LR: 9.541e-05  Data: 0.012 (0.016)
Train: 241 [ 700/1251 ( 56%)]  Loss: 3.060 (3.05)  Time: 0.656s, 1560.39/s  (0.663s, 1545.62/s)  LR: 9.529e-05  Data: 0.016 (0.016)
Train: 241 [ 750/1251 ( 60%)]  Loss: 3.106 (3.05)  Time: 0.666s, 1537.38/s  (0.663s, 1545.61/s)  LR: 9.517e-05  Data: 0.014 (0.016)
Train: 241 [ 800/1251 ( 64%)]  Loss: 2.803 (3.04)  Time: 0.663s, 1544.27/s  (0.662s, 1545.69/s)  LR: 9.505e-05  Data: 0.014 (0.016)
Train: 241 [ 850/1251 ( 68%)]  Loss: 3.020 (3.04)  Time: 0.664s, 1543.01/s  (0.662s, 1545.76/s)  LR: 9.493e-05  Data: 0.013 (0.016)
Train: 241 [ 900/1251 ( 72%)]  Loss: 3.182 (3.04)  Time: 0.663s, 1544.96/s  (0.662s, 1545.74/s)  LR: 9.481e-05  Data: 0.014 (0.016)
Train: 241 [ 950/1251 ( 76%)]  Loss: 3.040 (3.04)  Time: 0.666s, 1536.51/s  (0.662s, 1545.86/s)  LR: 9.469e-05  Data: 0.014 (0.016)
Train: 241 [1000/1251 ( 80%)]  Loss: 3.191 (3.05)  Time: 0.673s, 1522.36/s  (0.662s, 1545.71/s)  LR: 9.457e-05  Data: 0.014 (0.016)
Train: 241 [1050/1251 ( 84%)]  Loss: 2.941 (3.05)  Time: 0.661s, 1550.30/s  (0.663s, 1545.60/s)  LR: 9.445e-05  Data: 0.013 (0.015)
Train: 241 [1100/1251 ( 88%)]  Loss: 2.869 (3.04)  Time: 0.669s, 1530.78/s  (0.663s, 1545.40/s)  LR: 9.434e-05  Data: 0.013 (0.015)
Train: 241 [1150/1251 ( 92%)]  Loss: 3.096 (3.04)  Time: 0.664s, 1542.44/s  (0.663s, 1545.19/s)  LR: 9.422e-05  Data: 0.014 (0.015)
Train: 241 [1200/1251 ( 96%)]  Loss: 2.634 (3.02)  Time: 0.663s, 1545.32/s  (0.663s, 1544.97/s)  LR: 9.410e-05  Data: 0.016 (0.015)
Train: 241 [1250/1251 (100%)]  Loss: 3.005 (3.02)  Time: 0.649s, 1577.07/s  (0.663s, 1544.94/s)  LR: 9.398e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.829 (2.829)  Loss:  0.3928 (0.3928)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.161 (0.319)  Loss:  0.4993 (0.8279)  Acc@1: 87.5000 (80.6640)  Acc@5: 98.4670 (95.4480)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-241.pth.tar', 80.664)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-237.pth.tar', 80.49600002929688)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-240.pth.tar', 80.492)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-238.pth.tar', 80.48400005126953)

Train: 242 [   0/1251 (  0%)]  Loss: 3.056 (3.06)  Time: 3.295s,  310.81/s  (3.295s,  310.81/s)  LR: 9.398e-05  Data: 1.881 (1.881)
Train: 242 [  50/1251 (  4%)]  Loss: 3.179 (3.12)  Time: 0.652s, 1571.15/s  (0.684s, 1496.06/s)  LR: 9.386e-05  Data: 0.017 (0.050)
Train: 242 [ 100/1251 (  8%)]  Loss: 3.184 (3.14)  Time: 0.660s, 1551.78/s  (0.671s, 1527.15/s)  LR: 9.374e-05  Data: 0.013 (0.032)
Train: 242 [ 150/1251 ( 12%)]  Loss: 3.000 (3.10)  Time: 0.657s, 1559.04/s  (0.667s, 1535.63/s)  LR: 9.362e-05  Data: 0.014 (0.026)
Train: 242 [ 200/1251 ( 16%)]  Loss: 3.209 (3.13)  Time: 0.663s, 1543.34/s  (0.666s, 1538.26/s)  LR: 9.350e-05  Data: 0.013 (0.023)
Train: 242 [ 250/1251 ( 20%)]  Loss: 2.927 (3.09)  Time: 0.665s, 1540.62/s  (0.666s, 1538.45/s)  LR: 9.338e-05  Data: 0.013 (0.021)
Train: 242 [ 300/1251 ( 24%)]  Loss: 3.167 (3.10)  Time: 0.666s, 1537.93/s  (0.665s, 1538.86/s)  LR: 9.326e-05  Data: 0.012 (0.020)
Train: 242 [ 350/1251 ( 28%)]  Loss: 2.959 (3.09)  Time: 0.670s, 1529.25/s  (0.665s, 1539.08/s)  LR: 9.315e-05  Data: 0.013 (0.019)
Train: 242 [ 400/1251 ( 32%)]  Loss: 3.128 (3.09)  Time: 0.660s, 1550.67/s  (0.665s, 1539.50/s)  LR: 9.303e-05  Data: 0.013 (0.018)
Train: 242 [ 450/1251 ( 36%)]  Loss: 3.043 (3.09)  Time: 0.669s, 1531.08/s  (0.665s, 1538.84/s)  LR: 9.291e-05  Data: 0.013 (0.018)
Train: 242 [ 500/1251 ( 40%)]  Loss: 3.244 (3.10)  Time: 0.669s, 1531.57/s  (0.665s, 1538.94/s)  LR: 9.279e-05  Data: 0.014 (0.017)
Train: 242 [ 550/1251 ( 44%)]  Loss: 3.161 (3.10)  Time: 0.663s, 1543.34/s  (0.665s, 1539.15/s)  LR: 9.267e-05  Data: 0.013 (0.017)
Train: 242 [ 600/1251 ( 48%)]  Loss: 3.270 (3.12)  Time: 0.663s, 1543.83/s  (0.665s, 1539.54/s)  LR: 9.256e-05  Data: 0.013 (0.017)
Train: 242 [ 650/1251 ( 52%)]  Loss: 2.998 (3.11)  Time: 0.661s, 1548.12/s  (0.665s, 1540.09/s)  LR: 9.244e-05  Data: 0.016 (0.016)
Train: 242 [ 700/1251 ( 56%)]  Loss: 3.162 (3.11)  Time: 0.659s, 1553.09/s  (0.665s, 1540.48/s)  LR: 9.232e-05  Data: 0.016 (0.016)
Train: 242 [ 750/1251 ( 60%)]  Loss: 3.016 (3.11)  Time: 0.657s, 1558.52/s  (0.665s, 1540.79/s)  LR: 9.220e-05  Data: 0.013 (0.016)
Train: 242 [ 800/1251 ( 64%)]  Loss: 3.200 (3.11)  Time: 0.667s, 1534.65/s  (0.664s, 1541.33/s)  LR: 9.208e-05  Data: 0.013 (0.016)
Train: 242 [ 850/1251 ( 68%)]  Loss: 2.864 (3.10)  Time: 0.660s, 1551.62/s  (0.664s, 1541.78/s)  LR: 9.197e-05  Data: 0.013 (0.016)
Train: 242 [ 900/1251 ( 72%)]  Loss: 3.002 (3.09)  Time: 0.668s, 1533.67/s  (0.664s, 1542.25/s)  LR: 9.185e-05  Data: 0.013 (0.016)
Train: 242 [ 950/1251 ( 76%)]  Loss: 3.005 (3.09)  Time: 0.660s, 1551.89/s  (0.664s, 1542.56/s)  LR: 9.173e-05  Data: 0.016 (0.016)
Train: 242 [1000/1251 ( 80%)]  Loss: 3.103 (3.09)  Time: 0.669s, 1530.38/s  (0.664s, 1542.68/s)  LR: 9.161e-05  Data: 0.015 (0.015)
Train: 242 [1050/1251 ( 84%)]  Loss: 3.248 (3.10)  Time: 0.662s, 1546.40/s  (0.664s, 1542.48/s)  LR: 9.150e-05  Data: 0.015 (0.015)
Train: 242 [1100/1251 ( 88%)]  Loss: 3.041 (3.09)  Time: 0.666s, 1537.50/s  (0.664s, 1542.48/s)  LR: 9.138e-05  Data: 0.013 (0.015)
Train: 242 [1150/1251 ( 92%)]  Loss: 3.277 (3.10)  Time: 0.668s, 1532.18/s  (0.664s, 1542.48/s)  LR: 9.126e-05  Data: 0.014 (0.015)
Train: 242 [1200/1251 ( 96%)]  Loss: 3.244 (3.11)  Time: 0.666s, 1538.57/s  (0.664s, 1542.60/s)  LR: 9.114e-05  Data: 0.013 (0.015)
Train: 242 [1250/1251 (100%)]  Loss: 3.147 (3.11)  Time: 0.644s, 1588.97/s  (0.664s, 1542.66/s)  LR: 9.103e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.825 (2.825)  Loss:  0.3911 (0.3911)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.326)  Loss:  0.5024 (0.8314)  Acc@1: 87.6179 (80.6820)  Acc@5: 98.4670 (95.3740)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-242.pth.tar', 80.68200005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-241.pth.tar', 80.664)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-237.pth.tar', 80.49600002929688)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-240.pth.tar', 80.492)

Train: 243 [   0/1251 (  0%)]  Loss: 3.073 (3.07)  Time: 3.334s,  307.13/s  (3.334s,  307.13/s)  LR: 9.103e-05  Data: 1.718 (1.718)
Train: 243 [  50/1251 (  4%)]  Loss: 3.008 (3.04)  Time: 0.652s, 1571.72/s  (0.676s, 1515.03/s)  LR: 9.091e-05  Data: 0.017 (0.048)
Train: 243 [ 100/1251 (  8%)]  Loss: 3.066 (3.05)  Time: 0.657s, 1559.19/s  (0.666s, 1536.97/s)  LR: 9.079e-05  Data: 0.013 (0.031)
Train: 243 [ 150/1251 ( 12%)]  Loss: 2.971 (3.03)  Time: 0.664s, 1541.05/s  (0.664s, 1541.59/s)  LR: 9.067e-05  Data: 0.013 (0.025)
Train: 243 [ 200/1251 ( 16%)]  Loss: 3.001 (3.02)  Time: 0.665s, 1540.55/s  (0.663s, 1543.58/s)  LR: 9.056e-05  Data: 0.013 (0.022)
Train: 243 [ 250/1251 ( 20%)]  Loss: 3.088 (3.03)  Time: 0.659s, 1554.22/s  (0.663s, 1544.33/s)  LR: 9.044e-05  Data: 0.015 (0.021)
Train: 243 [ 300/1251 ( 24%)]  Loss: 2.839 (3.01)  Time: 0.655s, 1562.38/s  (0.663s, 1544.96/s)  LR: 9.032e-05  Data: 0.014 (0.019)
Train: 243 [ 350/1251 ( 28%)]  Loss: 2.914 (3.00)  Time: 0.645s, 1588.35/s  (0.663s, 1545.48/s)  LR: 9.021e-05  Data: 0.015 (0.019)
Train: 243 [ 400/1251 ( 32%)]  Loss: 2.964 (2.99)  Time: 0.661s, 1548.79/s  (0.662s, 1545.78/s)  LR: 9.009e-05  Data: 0.013 (0.018)
Train: 243 [ 450/1251 ( 36%)]  Loss: 3.144 (3.01)  Time: 0.659s, 1553.09/s  (0.662s, 1545.86/s)  LR: 8.997e-05  Data: 0.013 (0.018)
Train: 243 [ 500/1251 ( 40%)]  Loss: 2.845 (2.99)  Time: 0.668s, 1532.45/s  (0.663s, 1545.55/s)  LR: 8.986e-05  Data: 0.013 (0.017)
Train: 243 [ 550/1251 ( 44%)]  Loss: 3.040 (3.00)  Time: 0.658s, 1555.54/s  (0.662s, 1545.83/s)  LR: 8.974e-05  Data: 0.016 (0.017)
Train: 243 [ 600/1251 ( 48%)]  Loss: 3.061 (3.00)  Time: 0.668s, 1532.29/s  (0.662s, 1545.96/s)  LR: 8.963e-05  Data: 0.014 (0.017)
Train: 243 [ 650/1251 ( 52%)]  Loss: 3.172 (3.01)  Time: 0.664s, 1541.75/s  (0.662s, 1546.31/s)  LR: 8.951e-05  Data: 0.013 (0.016)
Train: 243 [ 700/1251 ( 56%)]  Loss: 3.100 (3.02)  Time: 0.660s, 1552.00/s  (0.662s, 1546.29/s)  LR: 8.939e-05  Data: 0.013 (0.016)
Train: 243 [ 750/1251 ( 60%)]  Loss: 2.882 (3.01)  Time: 0.655s, 1564.02/s  (0.662s, 1546.27/s)  LR: 8.928e-05  Data: 0.013 (0.016)
Train: 243 [ 800/1251 ( 64%)]  Loss: 2.859 (3.00)  Time: 0.667s, 1536.33/s  (0.662s, 1545.94/s)  LR: 8.916e-05  Data: 0.012 (0.016)
Train: 243 [ 850/1251 ( 68%)]  Loss: 2.981 (3.00)  Time: 0.671s, 1525.06/s  (0.663s, 1545.37/s)  LR: 8.905e-05  Data: 0.014 (0.016)
Train: 243 [ 900/1251 ( 72%)]  Loss: 2.964 (3.00)  Time: 0.662s, 1546.22/s  (0.663s, 1545.02/s)  LR: 8.893e-05  Data: 0.012 (0.016)
Train: 243 [ 950/1251 ( 76%)]  Loss: 3.100 (3.00)  Time: 0.671s, 1525.52/s  (0.663s, 1544.78/s)  LR: 8.881e-05  Data: 0.012 (0.015)
Train: 243 [1000/1251 ( 80%)]  Loss: 3.029 (3.00)  Time: 0.663s, 1544.05/s  (0.663s, 1544.67/s)  LR: 8.870e-05  Data: 0.012 (0.015)
Train: 243 [1050/1251 ( 84%)]  Loss: 2.882 (3.00)  Time: 0.658s, 1555.22/s  (0.663s, 1544.67/s)  LR: 8.858e-05  Data: 0.013 (0.015)
Train: 243 [1100/1251 ( 88%)]  Loss: 2.976 (3.00)  Time: 0.680s, 1506.01/s  (0.663s, 1544.23/s)  LR: 8.847e-05  Data: 0.013 (0.015)
Train: 243 [1150/1251 ( 92%)]  Loss: 3.237 (3.01)  Time: 0.675s, 1517.42/s  (0.663s, 1544.02/s)  LR: 8.835e-05  Data: 0.012 (0.015)
Train: 243 [1200/1251 ( 96%)]  Loss: 3.088 (3.01)  Time: 0.664s, 1542.09/s  (0.663s, 1543.89/s)  LR: 8.824e-05  Data: 0.012 (0.015)
Train: 243 [1250/1251 (100%)]  Loss: 2.835 (3.00)  Time: 0.654s, 1565.96/s  (0.663s, 1544.04/s)  LR: 8.812e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.854 (2.854)  Loss:  0.3762 (0.3762)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.8281 (98.8281)
Test: [  48/48]  Time: 0.161 (0.328)  Loss:  0.5171 (0.8218)  Acc@1: 86.4387 (80.4900)  Acc@5: 98.1132 (95.4020)
Train: 244 [   0/1251 (  0%)]  Loss: 3.051 (3.05)  Time: 3.596s,  284.75/s  (3.596s,  284.75/s)  LR: 8.812e-05  Data: 1.651 (1.651)
Train: 244 [  50/1251 (  4%)]  Loss: 2.865 (2.96)  Time: 0.643s, 1593.68/s  (0.687s, 1491.30/s)  LR: 8.800e-05  Data: 0.012 (0.046)
Train: 244 [ 100/1251 (  8%)]  Loss: 3.152 (3.02)  Time: 0.649s, 1577.85/s  (0.671s, 1526.39/s)  LR: 8.789e-05  Data: 0.014 (0.030)
Train: 244 [ 150/1251 ( 12%)]  Loss: 3.070 (3.03)  Time: 0.667s, 1535.20/s  (0.668s, 1533.96/s)  LR: 8.777e-05  Data: 0.013 (0.025)
Train: 244 [ 200/1251 ( 16%)]  Loss: 2.792 (2.99)  Time: 0.659s, 1554.00/s  (0.666s, 1537.94/s)  LR: 8.766e-05  Data: 0.013 (0.022)
Train: 244 [ 250/1251 ( 20%)]  Loss: 2.807 (2.96)  Time: 0.653s, 1567.01/s  (0.665s, 1540.72/s)  LR: 8.754e-05  Data: 0.012 (0.020)
Train: 244 [ 300/1251 ( 24%)]  Loss: 2.606 (2.91)  Time: 0.670s, 1529.15/s  (0.664s, 1542.20/s)  LR: 8.743e-05  Data: 0.014 (0.019)
Train: 244 [ 350/1251 ( 28%)]  Loss: 3.059 (2.93)  Time: 0.666s, 1536.93/s  (0.664s, 1542.21/s)  LR: 8.731e-05  Data: 0.013 (0.018)
Train: 244 [ 400/1251 ( 32%)]  Loss: 3.098 (2.94)  Time: 0.672s, 1524.22/s  (0.664s, 1542.97/s)  LR: 8.720e-05  Data: 0.013 (0.018)
Train: 244 [ 450/1251 ( 36%)]  Loss: 2.918 (2.94)  Time: 0.660s, 1552.68/s  (0.663s, 1543.97/s)  LR: 8.709e-05  Data: 0.013 (0.017)
Train: 244 [ 500/1251 ( 40%)]  Loss: 2.718 (2.92)  Time: 0.667s, 1535.55/s  (0.663s, 1544.54/s)  LR: 8.697e-05  Data: 0.016 (0.017)
Train: 244 [ 550/1251 ( 44%)]  Loss: 3.078 (2.93)  Time: 0.654s, 1566.57/s  (0.663s, 1545.27/s)  LR: 8.686e-05  Data: 0.013 (0.017)
Train: 244 [ 600/1251 ( 48%)]  Loss: 2.858 (2.93)  Time: 0.661s, 1548.57/s  (0.663s, 1545.53/s)  LR: 8.674e-05  Data: 0.013 (0.017)
Train: 244 [ 650/1251 ( 52%)]  Loss: 2.852 (2.92)  Time: 0.661s, 1548.29/s  (0.662s, 1545.67/s)  LR: 8.663e-05  Data: 0.013 (0.016)
Train: 244 [ 700/1251 ( 56%)]  Loss: 2.938 (2.92)  Time: 0.658s, 1556.31/s  (0.663s, 1545.45/s)  LR: 8.651e-05  Data: 0.012 (0.016)
Train: 244 [ 750/1251 ( 60%)]  Loss: 3.114 (2.94)  Time: 0.657s, 1557.54/s  (0.663s, 1545.39/s)  LR: 8.640e-05  Data: 0.014 (0.016)
Train: 244 [ 800/1251 ( 64%)]  Loss: 3.106 (2.95)  Time: 0.665s, 1539.08/s  (0.663s, 1545.52/s)  LR: 8.629e-05  Data: 0.013 (0.016)
Train: 244 [ 850/1251 ( 68%)]  Loss: 2.911 (2.94)  Time: 0.668s, 1533.12/s  (0.663s, 1545.59/s)  LR: 8.617e-05  Data: 0.014 (0.016)
Train: 244 [ 900/1251 ( 72%)]  Loss: 3.110 (2.95)  Time: 0.665s, 1540.08/s  (0.663s, 1545.30/s)  LR: 8.606e-05  Data: 0.014 (0.016)
Train: 244 [ 950/1251 ( 76%)]  Loss: 2.855 (2.95)  Time: 0.659s, 1553.30/s  (0.663s, 1544.96/s)  LR: 8.594e-05  Data: 0.014 (0.015)
Train: 244 [1000/1251 ( 80%)]  Loss: 3.188 (2.96)  Time: 0.664s, 1541.25/s  (0.663s, 1544.65/s)  LR: 8.583e-05  Data: 0.014 (0.015)
Train: 244 [1050/1251 ( 84%)]  Loss: 3.254 (2.97)  Time: 0.663s, 1544.80/s  (0.663s, 1544.78/s)  LR: 8.572e-05  Data: 0.014 (0.015)
Train: 244 [1100/1251 ( 88%)]  Loss: 2.896 (2.97)  Time: 0.657s, 1559.27/s  (0.663s, 1544.75/s)  LR: 8.560e-05  Data: 0.013 (0.015)
Train: 244 [1150/1251 ( 92%)]  Loss: 3.136 (2.98)  Time: 0.672s, 1524.49/s  (0.663s, 1544.64/s)  LR: 8.549e-05  Data: 0.014 (0.015)
Train: 244 [1200/1251 ( 96%)]  Loss: 3.141 (2.98)  Time: 0.665s, 1541.00/s  (0.663s, 1544.50/s)  LR: 8.537e-05  Data: 0.014 (0.015)
Train: 244 [1250/1251 (100%)]  Loss: 2.991 (2.98)  Time: 0.650s, 1575.48/s  (0.663s, 1544.58/s)  LR: 8.526e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.866 (2.866)  Loss:  0.3655 (0.3655)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.161 (0.321)  Loss:  0.5049 (0.8223)  Acc@1: 86.7925 (80.6820)  Acc@5: 98.1132 (95.4140)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-244.pth.tar', 80.68200020996093)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-242.pth.tar', 80.68200005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-241.pth.tar', 80.664)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-237.pth.tar', 80.49600002929688)

Train: 245 [   0/1251 (  0%)]  Loss: 2.923 (2.92)  Time: 2.997s,  341.64/s  (2.997s,  341.64/s)  LR: 8.526e-05  Data: 1.854 (1.854)
Train: 245 [  50/1251 (  4%)]  Loss: 3.080 (3.00)  Time: 0.645s, 1588.16/s  (0.676s, 1515.65/s)  LR: 8.515e-05  Data: 0.014 (0.050)
Train: 245 [ 100/1251 (  8%)]  Loss: 3.055 (3.02)  Time: 0.650s, 1574.46/s  (0.665s, 1538.70/s)  LR: 8.503e-05  Data: 0.016 (0.032)
Train: 245 [ 150/1251 ( 12%)]  Loss: 3.094 (3.04)  Time: 0.660s, 1550.54/s  (0.663s, 1543.97/s)  LR: 8.492e-05  Data: 0.014 (0.026)
Train: 245 [ 200/1251 ( 16%)]  Loss: 3.043 (3.04)  Time: 0.663s, 1544.04/s  (0.663s, 1544.97/s)  LR: 8.481e-05  Data: 0.013 (0.023)
Train: 245 [ 250/1251 ( 20%)]  Loss: 3.211 (3.07)  Time: 0.664s, 1541.66/s  (0.663s, 1545.50/s)  LR: 8.469e-05  Data: 0.016 (0.021)
Train: 245 [ 300/1251 ( 24%)]  Loss: 3.117 (3.07)  Time: 0.664s, 1543.09/s  (0.662s, 1546.21/s)  LR: 8.458e-05  Data: 0.013 (0.020)
Train: 245 [ 350/1251 ( 28%)]  Loss: 3.063 (3.07)  Time: 0.652s, 1570.27/s  (0.662s, 1546.25/s)  LR: 8.447e-05  Data: 0.013 (0.019)
Train: 245 [ 400/1251 ( 32%)]  Loss: 3.206 (3.09)  Time: 0.663s, 1545.17/s  (0.662s, 1546.50/s)  LR: 8.435e-05  Data: 0.014 (0.018)
Train: 245 [ 450/1251 ( 36%)]  Loss: 3.130 (3.09)  Time: 0.665s, 1540.08/s  (0.662s, 1547.13/s)  LR: 8.424e-05  Data: 0.014 (0.018)
Train: 245 [ 500/1251 ( 40%)]  Loss: 3.079 (3.09)  Time: 0.660s, 1552.46/s  (0.662s, 1547.39/s)  LR: 8.413e-05  Data: 0.013 (0.018)
Train: 245 [ 550/1251 ( 44%)]  Loss: 3.297 (3.11)  Time: 0.651s, 1574.06/s  (0.661s, 1548.17/s)  LR: 8.402e-05  Data: 0.014 (0.017)
Train: 245 [ 600/1251 ( 48%)]  Loss: 2.970 (3.10)  Time: 0.662s, 1547.79/s  (0.661s, 1548.84/s)  LR: 8.390e-05  Data: 0.014 (0.017)
Train: 245 [ 650/1251 ( 52%)]  Loss: 3.083 (3.10)  Time: 0.659s, 1554.67/s  (0.661s, 1549.18/s)  LR: 8.379e-05  Data: 0.014 (0.017)
Train: 245 [ 700/1251 ( 56%)]  Loss: 3.063 (3.09)  Time: 0.668s, 1533.24/s  (0.661s, 1549.24/s)  LR: 8.368e-05  Data: 0.014 (0.016)
Train: 245 [ 750/1251 ( 60%)]  Loss: 3.177 (3.10)  Time: 0.657s, 1557.55/s  (0.661s, 1549.05/s)  LR: 8.357e-05  Data: 0.013 (0.016)
Train: 245 [ 800/1251 ( 64%)]  Loss: 3.004 (3.09)  Time: 0.667s, 1534.47/s  (0.661s, 1548.64/s)  LR: 8.345e-05  Data: 0.018 (0.016)
Train: 245 [ 850/1251 ( 68%)]  Loss: 2.763 (3.08)  Time: 0.664s, 1541.01/s  (0.661s, 1548.37/s)  LR: 8.334e-05  Data: 0.013 (0.016)
Train: 245 [ 900/1251 ( 72%)]  Loss: 2.641 (3.05)  Time: 0.663s, 1544.57/s  (0.661s, 1548.39/s)  LR: 8.323e-05  Data: 0.014 (0.016)
Train: 245 [ 950/1251 ( 76%)]  Loss: 3.110 (3.06)  Time: 0.661s, 1548.02/s  (0.661s, 1548.46/s)  LR: 8.312e-05  Data: 0.013 (0.016)
Train: 245 [1000/1251 ( 80%)]  Loss: 2.932 (3.05)  Time: 0.666s, 1536.49/s  (0.661s, 1548.49/s)  LR: 8.301e-05  Data: 0.017 (0.016)
Train: 245 [1050/1251 ( 84%)]  Loss: 3.179 (3.06)  Time: 0.657s, 1558.90/s  (0.661s, 1548.33/s)  LR: 8.289e-05  Data: 0.014 (0.016)
Train: 245 [1100/1251 ( 88%)]  Loss: 3.159 (3.06)  Time: 0.656s, 1561.04/s  (0.661s, 1548.48/s)  LR: 8.278e-05  Data: 0.016 (0.015)
Train: 245 [1150/1251 ( 92%)]  Loss: 2.902 (3.05)  Time: 0.665s, 1539.97/s  (0.661s, 1548.63/s)  LR: 8.267e-05  Data: 0.014 (0.015)
Train: 245 [1200/1251 ( 96%)]  Loss: 3.235 (3.06)  Time: 0.665s, 1538.90/s  (0.661s, 1548.56/s)  LR: 8.256e-05  Data: 0.016 (0.015)
Train: 245 [1250/1251 (100%)]  Loss: 3.243 (3.07)  Time: 0.658s, 1555.82/s  (0.661s, 1548.51/s)  LR: 8.245e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.904 (2.904)  Loss:  0.3823 (0.3823)  Acc@1: 93.3594 (93.3594)  Acc@5: 98.9258 (98.9258)
Test: [  48/48]  Time: 0.163 (0.325)  Loss:  0.5186 (0.8290)  Acc@1: 86.9104 (80.7480)  Acc@5: 98.4670 (95.4480)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-245.pth.tar', 80.7480000024414)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-244.pth.tar', 80.68200020996093)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-242.pth.tar', 80.68200005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-241.pth.tar', 80.664)

Train: 246 [   0/1251 (  0%)]  Loss: 3.260 (3.26)  Time: 3.098s,  330.50/s  (3.098s,  330.50/s)  LR: 8.244e-05  Data: 1.763 (1.763)
Train: 246 [  50/1251 (  4%)]  Loss: 3.199 (3.23)  Time: 0.654s, 1566.07/s  (0.685s, 1495.83/s)  LR: 8.233e-05  Data: 0.014 (0.048)
Train: 246 [ 100/1251 (  8%)]  Loss: 3.323 (3.26)  Time: 0.665s, 1540.62/s  (0.671s, 1526.02/s)  LR: 8.222e-05  Data: 0.014 (0.031)
Train: 246 [ 150/1251 ( 12%)]  Loss: 2.976 (3.19)  Time: 0.661s, 1549.97/s  (0.668s, 1532.59/s)  LR: 8.211e-05  Data: 0.013 (0.025)
Train: 246 [ 200/1251 ( 16%)]  Loss: 3.261 (3.20)  Time: 0.655s, 1563.87/s  (0.666s, 1536.58/s)  LR: 8.200e-05  Data: 0.013 (0.023)
Train: 246 [ 250/1251 ( 20%)]  Loss: 3.194 (3.20)  Time: 0.673s, 1521.43/s  (0.666s, 1537.50/s)  LR: 8.189e-05  Data: 0.013 (0.021)
Train: 246 [ 300/1251 ( 24%)]  Loss: 3.000 (3.17)  Time: 0.641s, 1597.19/s  (0.666s, 1538.11/s)  LR: 8.178e-05  Data: 0.012 (0.020)
Train: 246 [ 350/1251 ( 28%)]  Loss: 2.807 (3.13)  Time: 0.665s, 1540.44/s  (0.666s, 1538.54/s)  LR: 8.167e-05  Data: 0.014 (0.019)
Train: 246 [ 400/1251 ( 32%)]  Loss: 2.920 (3.10)  Time: 0.661s, 1549.67/s  (0.666s, 1538.47/s)  LR: 8.155e-05  Data: 0.015 (0.018)
Train: 246 [ 450/1251 ( 36%)]  Loss: 3.045 (3.10)  Time: 0.658s, 1555.62/s  (0.665s, 1539.11/s)  LR: 8.144e-05  Data: 0.013 (0.018)
Train: 246 [ 500/1251 ( 40%)]  Loss: 2.825 (3.07)  Time: 0.673s, 1521.74/s  (0.665s, 1540.06/s)  LR: 8.133e-05  Data: 0.016 (0.017)
Train: 246 [ 550/1251 ( 44%)]  Loss: 3.162 (3.08)  Time: 0.670s, 1529.26/s  (0.665s, 1540.23/s)  LR: 8.122e-05  Data: 0.013 (0.017)
Train: 246 [ 600/1251 ( 48%)]  Loss: 2.991 (3.07)  Time: 0.656s, 1560.76/s  (0.665s, 1540.85/s)  LR: 8.111e-05  Data: 0.014 (0.017)
Train: 246 [ 650/1251 ( 52%)]  Loss: 3.274 (3.09)  Time: 0.665s, 1538.74/s  (0.664s, 1541.36/s)  LR: 8.100e-05  Data: 0.014 (0.016)
Train: 246 [ 700/1251 ( 56%)]  Loss: 3.365 (3.11)  Time: 0.657s, 1558.15/s  (0.664s, 1541.84/s)  LR: 8.089e-05  Data: 0.019 (0.016)
Train: 246 [ 750/1251 ( 60%)]  Loss: 3.146 (3.11)  Time: 0.661s, 1549.00/s  (0.664s, 1542.44/s)  LR: 8.078e-05  Data: 0.013 (0.016)
Train: 246 [ 800/1251 ( 64%)]  Loss: 3.001 (3.10)  Time: 0.656s, 1560.38/s  (0.664s, 1543.13/s)  LR: 8.067e-05  Data: 0.013 (0.016)
Train: 246 [ 850/1251 ( 68%)]  Loss: 3.028 (3.10)  Time: 0.662s, 1547.83/s  (0.663s, 1543.90/s)  LR: 8.056e-05  Data: 0.012 (0.016)
Train: 246 [ 900/1251 ( 72%)]  Loss: 2.934 (3.09)  Time: 0.659s, 1554.90/s  (0.663s, 1544.42/s)  LR: 8.045e-05  Data: 0.012 (0.016)
Train: 246 [ 950/1251 ( 76%)]  Loss: 3.012 (3.09)  Time: 0.662s, 1546.16/s  (0.663s, 1545.11/s)  LR: 8.034e-05  Data: 0.014 (0.016)
Train: 246 [1000/1251 ( 80%)]  Loss: 3.133 (3.09)  Time: 0.663s, 1545.12/s  (0.663s, 1545.31/s)  LR: 8.023e-05  Data: 0.013 (0.015)
Train: 246 [1050/1251 ( 84%)]  Loss: 3.039 (3.09)  Time: 0.662s, 1545.93/s  (0.663s, 1545.47/s)  LR: 8.012e-05  Data: 0.014 (0.015)
Train: 246 [1100/1251 ( 88%)]  Loss: 3.011 (3.08)  Time: 0.663s, 1543.95/s  (0.662s, 1545.78/s)  LR: 8.001e-05  Data: 0.014 (0.015)
Train: 246 [1150/1251 ( 92%)]  Loss: 3.045 (3.08)  Time: 0.658s, 1556.36/s  (0.662s, 1545.97/s)  LR: 7.990e-05  Data: 0.014 (0.015)
Train: 246 [1200/1251 ( 96%)]  Loss: 3.307 (3.09)  Time: 0.661s, 1549.15/s  (0.662s, 1546.12/s)  LR: 7.979e-05  Data: 0.014 (0.015)
Train: 246 [1250/1251 (100%)]  Loss: 3.228 (3.10)  Time: 0.651s, 1572.95/s  (0.662s, 1546.23/s)  LR: 7.968e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.834 (2.834)  Loss:  0.3789 (0.3789)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.161 (0.325)  Loss:  0.4893 (0.8285)  Acc@1: 88.7972 (80.7840)  Acc@5: 98.1132 (95.4680)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-246.pth.tar', 80.78400004638672)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-245.pth.tar', 80.7480000024414)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-244.pth.tar', 80.68200020996093)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-242.pth.tar', 80.68200005126953)

Train: 247 [   0/1251 (  0%)]  Loss: 3.033 (3.03)  Time: 3.383s,  302.67/s  (3.383s,  302.67/s)  LR: 7.968e-05  Data: 1.616 (1.616)
Train: 247 [  50/1251 (  4%)]  Loss: 3.102 (3.07)  Time: 0.639s, 1602.13/s  (0.676s, 1514.62/s)  LR: 7.957e-05  Data: 0.014 (0.045)
Train: 247 [ 100/1251 (  8%)]  Loss: 2.857 (3.00)  Time: 0.650s, 1575.30/s  (0.664s, 1543.14/s)  LR: 7.946e-05  Data: 0.016 (0.030)
Train: 247 [ 150/1251 ( 12%)]  Loss: 2.943 (2.98)  Time: 0.653s, 1569.32/s  (0.661s, 1549.69/s)  LR: 7.935e-05  Data: 0.013 (0.024)
Train: 247 [ 200/1251 ( 16%)]  Loss: 2.947 (2.98)  Time: 0.659s, 1553.74/s  (0.660s, 1550.72/s)  LR: 7.924e-05  Data: 0.014 (0.022)
Train: 247 [ 250/1251 ( 20%)]  Loss: 2.987 (2.98)  Time: 0.660s, 1552.16/s  (0.660s, 1551.30/s)  LR: 7.913e-05  Data: 0.013 (0.020)
Train: 247 [ 300/1251 ( 24%)]  Loss: 2.836 (2.96)  Time: 0.660s, 1552.06/s  (0.660s, 1551.52/s)  LR: 7.902e-05  Data: 0.013 (0.019)
Train: 247 [ 350/1251 ( 28%)]  Loss: 3.201 (2.99)  Time: 0.655s, 1563.62/s  (0.660s, 1551.76/s)  LR: 7.891e-05  Data: 0.014 (0.018)
Train: 247 [ 400/1251 ( 32%)]  Loss: 3.051 (3.00)  Time: 0.657s, 1558.48/s  (0.660s, 1551.90/s)  LR: 7.880e-05  Data: 0.013 (0.018)
Train: 247 [ 450/1251 ( 36%)]  Loss: 3.083 (3.00)  Time: 0.653s, 1567.44/s  (0.660s, 1551.59/s)  LR: 7.869e-05  Data: 0.014 (0.017)
Train: 247 [ 500/1251 ( 40%)]  Loss: 2.837 (2.99)  Time: 0.669s, 1530.61/s  (0.660s, 1551.23/s)  LR: 7.858e-05  Data: 0.013 (0.017)
Train: 247 [ 550/1251 ( 44%)]  Loss: 3.312 (3.02)  Time: 0.659s, 1554.83/s  (0.660s, 1551.15/s)  LR: 7.847e-05  Data: 0.014 (0.017)
Train: 247 [ 600/1251 ( 48%)]  Loss: 2.960 (3.01)  Time: 0.674s, 1518.42/s  (0.660s, 1550.56/s)  LR: 7.836e-05  Data: 0.014 (0.016)
Train: 247 [ 650/1251 ( 52%)]  Loss: 3.060 (3.01)  Time: 0.665s, 1539.88/s  (0.660s, 1550.34/s)  LR: 7.826e-05  Data: 0.014 (0.016)
Train: 247 [ 700/1251 ( 56%)]  Loss: 2.952 (3.01)  Time: 0.661s, 1549.07/s  (0.660s, 1550.73/s)  LR: 7.815e-05  Data: 0.013 (0.016)
Train: 247 [ 750/1251 ( 60%)]  Loss: 3.154 (3.02)  Time: 0.655s, 1564.35/s  (0.660s, 1551.14/s)  LR: 7.804e-05  Data: 0.013 (0.016)
Train: 247 [ 800/1251 ( 64%)]  Loss: 3.238 (3.03)  Time: 0.656s, 1561.78/s  (0.660s, 1551.80/s)  LR: 7.793e-05  Data: 0.013 (0.016)
Train: 247 [ 850/1251 ( 68%)]  Loss: 3.090 (3.04)  Time: 0.656s, 1560.83/s  (0.660s, 1552.20/s)  LR: 7.782e-05  Data: 0.012 (0.016)
Train: 247 [ 900/1251 ( 72%)]  Loss: 2.707 (3.02)  Time: 0.666s, 1536.57/s  (0.660s, 1552.53/s)  LR: 7.771e-05  Data: 0.013 (0.016)
Train: 247 [ 950/1251 ( 76%)]  Loss: 3.049 (3.02)  Time: 0.655s, 1562.29/s  (0.659s, 1552.73/s)  LR: 7.760e-05  Data: 0.013 (0.015)
Train: 247 [1000/1251 ( 80%)]  Loss: 3.092 (3.02)  Time: 0.654s, 1566.67/s  (0.659s, 1553.04/s)  LR: 7.750e-05  Data: 0.015 (0.015)
Train: 247 [1050/1251 ( 84%)]  Loss: 3.142 (3.03)  Time: 0.666s, 1537.05/s  (0.659s, 1553.30/s)  LR: 7.739e-05  Data: 0.013 (0.015)
Train: 247 [1100/1251 ( 88%)]  Loss: 3.111 (3.03)  Time: 0.661s, 1550.10/s  (0.659s, 1553.63/s)  LR: 7.728e-05  Data: 0.013 (0.015)
Train: 247 [1150/1251 ( 92%)]  Loss: 3.199 (3.04)  Time: 0.652s, 1569.57/s  (0.659s, 1553.90/s)  LR: 7.717e-05  Data: 0.014 (0.015)
Train: 247 [1200/1251 ( 96%)]  Loss: 2.966 (3.04)  Time: 0.655s, 1564.32/s  (0.659s, 1553.95/s)  LR: 7.706e-05  Data: 0.013 (0.015)
Train: 247 [1250/1251 (100%)]  Loss: 2.992 (3.03)  Time: 0.641s, 1596.48/s  (0.659s, 1554.39/s)  LR: 7.696e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.860 (2.860)  Loss:  0.3872 (0.3872)  Acc@1: 93.3594 (93.3594)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.161 (0.318)  Loss:  0.5005 (0.8311)  Acc@1: 87.8538 (80.7940)  Acc@5: 98.1132 (95.4960)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-247.pth.tar', 80.79399989501952)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-246.pth.tar', 80.78400004638672)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-245.pth.tar', 80.7480000024414)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-244.pth.tar', 80.68200020996093)

Train: 248 [   0/1251 (  0%)]  Loss: 3.127 (3.13)  Time: 3.082s,  332.24/s  (3.082s,  332.24/s)  LR: 7.695e-05  Data: 1.795 (1.795)
Train: 248 [  50/1251 (  4%)]  Loss: 2.989 (3.06)  Time: 0.645s, 1586.83/s  (0.668s, 1533.62/s)  LR: 7.685e-05  Data: 0.016 (0.049)
Train: 248 [ 100/1251 (  8%)]  Loss: 3.109 (3.08)  Time: 0.652s, 1571.28/s  (0.657s, 1559.03/s)  LR: 7.674e-05  Data: 0.013 (0.032)
Train: 248 [ 150/1251 ( 12%)]  Loss: 3.200 (3.11)  Time: 0.656s, 1561.89/s  (0.656s, 1561.95/s)  LR: 7.663e-05  Data: 0.013 (0.026)
Train: 248 [ 200/1251 ( 16%)]  Loss: 2.932 (3.07)  Time: 0.657s, 1558.65/s  (0.655s, 1562.21/s)  LR: 7.652e-05  Data: 0.012 (0.023)
Train: 248 [ 250/1251 ( 20%)]  Loss: 3.005 (3.06)  Time: 0.651s, 1571.96/s  (0.656s, 1561.85/s)  LR: 7.642e-05  Data: 0.013 (0.021)
Train: 248 [ 300/1251 ( 24%)]  Loss: 3.306 (3.10)  Time: 0.660s, 1552.26/s  (0.656s, 1561.48/s)  LR: 7.631e-05  Data: 0.014 (0.020)
Train: 248 [ 350/1251 ( 28%)]  Loss: 2.997 (3.08)  Time: 0.654s, 1565.89/s  (0.656s, 1560.82/s)  LR: 7.620e-05  Data: 0.014 (0.019)
Train: 248 [ 400/1251 ( 32%)]  Loss: 2.615 (3.03)  Time: 0.663s, 1543.85/s  (0.657s, 1559.70/s)  LR: 7.609e-05  Data: 0.013 (0.018)
Train: 248 [ 450/1251 ( 36%)]  Loss: 2.752 (3.00)  Time: 0.657s, 1558.09/s  (0.657s, 1559.15/s)  LR: 7.599e-05  Data: 0.013 (0.018)
Train: 248 [ 500/1251 ( 40%)]  Loss: 3.160 (3.02)  Time: 0.663s, 1544.83/s  (0.657s, 1558.20/s)  LR: 7.588e-05  Data: 0.014 (0.017)
Train: 248 [ 550/1251 ( 44%)]  Loss: 2.977 (3.01)  Time: 0.654s, 1566.18/s  (0.657s, 1557.76/s)  LR: 7.577e-05  Data: 0.013 (0.017)
Train: 248 [ 600/1251 ( 48%)]  Loss: 3.272 (3.03)  Time: 0.651s, 1573.49/s  (0.657s, 1557.46/s)  LR: 7.567e-05  Data: 0.013 (0.017)
Train: 248 [ 650/1251 ( 52%)]  Loss: 2.797 (3.02)  Time: 0.653s, 1567.57/s  (0.657s, 1557.46/s)  LR: 7.556e-05  Data: 0.013 (0.017)
Train: 248 [ 700/1251 ( 56%)]  Loss: 2.982 (3.01)  Time: 0.661s, 1549.53/s  (0.658s, 1557.10/s)  LR: 7.545e-05  Data: 0.013 (0.016)
Train: 248 [ 750/1251 ( 60%)]  Loss: 2.763 (3.00)  Time: 0.651s, 1573.17/s  (0.658s, 1556.61/s)  LR: 7.534e-05  Data: 0.013 (0.016)
Train: 248 [ 800/1251 ( 64%)]  Loss: 3.077 (3.00)  Time: 0.654s, 1564.91/s  (0.658s, 1556.41/s)  LR: 7.524e-05  Data: 0.013 (0.016)
Train: 248 [ 850/1251 ( 68%)]  Loss: 3.200 (3.01)  Time: 0.666s, 1538.32/s  (0.658s, 1556.01/s)  LR: 7.513e-05  Data: 0.016 (0.016)
Train: 248 [ 900/1251 ( 72%)]  Loss: 2.919 (3.01)  Time: 0.664s, 1542.65/s  (0.658s, 1555.60/s)  LR: 7.502e-05  Data: 0.013 (0.016)
Train: 248 [ 950/1251 ( 76%)]  Loss: 3.219 (3.02)  Time: 0.664s, 1541.68/s  (0.658s, 1555.32/s)  LR: 7.492e-05  Data: 0.013 (0.016)
Train: 248 [1000/1251 ( 80%)]  Loss: 3.229 (3.03)  Time: 0.657s, 1557.42/s  (0.658s, 1555.08/s)  LR: 7.481e-05  Data: 0.013 (0.016)
Train: 248 [1050/1251 ( 84%)]  Loss: 3.095 (3.03)  Time: 0.650s, 1576.35/s  (0.659s, 1554.77/s)  LR: 7.471e-05  Data: 0.012 (0.015)
Train: 248 [1100/1251 ( 88%)]  Loss: 2.937 (3.03)  Time: 0.664s, 1543.02/s  (0.659s, 1554.69/s)  LR: 7.460e-05  Data: 0.016 (0.015)
Train: 248 [1150/1251 ( 92%)]  Loss: 3.201 (3.04)  Time: 0.655s, 1563.02/s  (0.659s, 1554.63/s)  LR: 7.449e-05  Data: 0.014 (0.015)
Train: 248 [1200/1251 ( 96%)]  Loss: 3.101 (3.04)  Time: 0.672s, 1524.39/s  (0.659s, 1554.72/s)  LR: 7.439e-05  Data: 0.013 (0.015)
Train: 248 [1250/1251 (100%)]  Loss: 2.818 (3.03)  Time: 0.645s, 1587.43/s  (0.659s, 1554.86/s)  LR: 7.428e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.872 (2.872)  Loss:  0.3818 (0.3818)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.161 (0.317)  Loss:  0.5010 (0.8220)  Acc@1: 88.0896 (80.8440)  Acc@5: 98.1132 (95.3840)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-248.pth.tar', 80.8439999975586)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-247.pth.tar', 80.79399989501952)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-246.pth.tar', 80.78400004638672)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-245.pth.tar', 80.7480000024414)

Train: 249 [   0/1251 (  0%)]  Loss: 3.264 (3.26)  Time: 3.022s,  338.88/s  (3.022s,  338.88/s)  LR: 7.428e-05  Data: 1.638 (1.638)
Train: 249 [  50/1251 (  4%)]  Loss: 3.038 (3.15)  Time: 0.644s, 1589.70/s  (0.669s, 1529.66/s)  LR: 7.417e-05  Data: 0.013 (0.046)
Train: 249 [ 100/1251 (  8%)]  Loss: 2.686 (3.00)  Time: 0.649s, 1577.53/s  (0.658s, 1556.45/s)  LR: 7.407e-05  Data: 0.013 (0.030)
Train: 249 [ 150/1251 ( 12%)]  Loss: 3.147 (3.03)  Time: 0.648s, 1580.81/s  (0.655s, 1562.81/s)  LR: 7.396e-05  Data: 0.012 (0.025)
Train: 249 [ 200/1251 ( 16%)]  Loss: 3.155 (3.06)  Time: 0.660s, 1550.39/s  (0.655s, 1564.52/s)  LR: 7.386e-05  Data: 0.013 (0.022)
Train: 249 [ 250/1251 ( 20%)]  Loss: 2.964 (3.04)  Time: 0.647s, 1582.85/s  (0.654s, 1565.17/s)  LR: 7.375e-05  Data: 0.014 (0.020)
Train: 249 [ 300/1251 ( 24%)]  Loss: 2.876 (3.02)  Time: 0.659s, 1553.71/s  (0.654s, 1565.47/s)  LR: 7.364e-05  Data: 0.013 (0.019)
Train: 249 [ 350/1251 ( 28%)]  Loss: 3.100 (3.03)  Time: 0.651s, 1572.97/s  (0.654s, 1565.05/s)  LR: 7.354e-05  Data: 0.014 (0.018)
Train: 249 [ 400/1251 ( 32%)]  Loss: 2.891 (3.01)  Time: 0.664s, 1542.44/s  (0.655s, 1563.97/s)  LR: 7.343e-05  Data: 0.013 (0.018)
Train: 249 [ 450/1251 ( 36%)]  Loss: 2.777 (2.99)  Time: 0.661s, 1550.17/s  (0.655s, 1563.16/s)  LR: 7.333e-05  Data: 0.014 (0.017)
Train: 249 [ 500/1251 ( 40%)]  Loss: 3.016 (2.99)  Time: 0.662s, 1547.72/s  (0.655s, 1562.74/s)  LR: 7.322e-05  Data: 0.014 (0.017)
Train: 249 [ 550/1251 ( 44%)]  Loss: 2.854 (2.98)  Time: 0.654s, 1564.68/s  (0.655s, 1562.88/s)  LR: 7.312e-05  Data: 0.014 (0.017)
Train: 249 [ 600/1251 ( 48%)]  Loss: 2.963 (2.98)  Time: 0.640s, 1599.29/s  (0.655s, 1563.55/s)  LR: 7.301e-05  Data: 0.016 (0.016)
Train: 249 [ 650/1251 ( 52%)]  Loss: 3.036 (2.98)  Time: 0.646s, 1585.32/s  (0.655s, 1564.06/s)  LR: 7.291e-05  Data: 0.016 (0.016)
Train: 249 [ 700/1251 ( 56%)]  Loss: 3.176 (3.00)  Time: 0.650s, 1575.45/s  (0.655s, 1564.10/s)  LR: 7.280e-05  Data: 0.013 (0.016)
Train: 249 [ 750/1251 ( 60%)]  Loss: 2.634 (2.97)  Time: 0.645s, 1586.52/s  (0.655s, 1564.23/s)  LR: 7.270e-05  Data: 0.014 (0.016)
Train: 249 [ 800/1251 ( 64%)]  Loss: 3.151 (2.98)  Time: 0.659s, 1553.02/s  (0.655s, 1564.32/s)  LR: 7.259e-05  Data: 0.017 (0.016)
Train: 249 [ 850/1251 ( 68%)]  Loss: 3.172 (2.99)  Time: 0.662s, 1545.66/s  (0.655s, 1564.24/s)  LR: 7.249e-05  Data: 0.014 (0.016)
Train: 249 [ 900/1251 ( 72%)]  Loss: 3.105 (3.00)  Time: 0.651s, 1573.33/s  (0.655s, 1564.19/s)  LR: 7.238e-05  Data: 0.015 (0.016)
Train: 249 [ 950/1251 ( 76%)]  Loss: 2.998 (3.00)  Time: 0.653s, 1567.91/s  (0.655s, 1564.35/s)  LR: 7.228e-05  Data: 0.014 (0.015)
Train: 249 [1000/1251 ( 80%)]  Loss: 2.675 (2.98)  Time: 0.656s, 1560.59/s  (0.654s, 1564.88/s)  LR: 7.217e-05  Data: 0.014 (0.015)
Train: 249 [1050/1251 ( 84%)]  Loss: 2.926 (2.98)  Time: 0.644s, 1590.92/s  (0.654s, 1565.21/s)  LR: 7.207e-05  Data: 0.013 (0.015)
Train: 249 [1100/1251 ( 88%)]  Loss: 2.997 (2.98)  Time: 0.648s, 1580.30/s  (0.654s, 1565.30/s)  LR: 7.197e-05  Data: 0.013 (0.015)
Train: 249 [1150/1251 ( 92%)]  Loss: 3.033 (2.98)  Time: 0.653s, 1567.72/s  (0.654s, 1565.45/s)  LR: 7.186e-05  Data: 0.018 (0.015)
Train: 249 [1200/1251 ( 96%)]  Loss: 2.970 (2.98)  Time: 0.653s, 1567.27/s  (0.654s, 1565.24/s)  LR: 7.176e-05  Data: 0.013 (0.015)
Train: 249 [1250/1251 (100%)]  Loss: 2.943 (2.98)  Time: 0.643s, 1593.48/s  (0.654s, 1565.39/s)  LR: 7.165e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.894 (2.894)  Loss:  0.3696 (0.3696)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.161 (0.327)  Loss:  0.5161 (0.8148)  Acc@1: 87.0283 (80.8520)  Acc@5: 97.7594 (95.4240)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-249.pth.tar', 80.8519999243164)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-248.pth.tar', 80.8439999975586)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-247.pth.tar', 80.79399989501952)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-246.pth.tar', 80.78400004638672)

Train: 250 [   0/1251 (  0%)]  Loss: 3.181 (3.18)  Time: 3.122s,  328.00/s  (3.122s,  328.00/s)  LR: 7.165e-05  Data: 1.744 (1.744)
Train: 250 [  50/1251 (  4%)]  Loss: 2.923 (3.05)  Time: 0.638s, 1603.93/s  (0.675s, 1518.11/s)  LR: 7.155e-05  Data: 0.013 (0.048)
Train: 250 [ 100/1251 (  8%)]  Loss: 3.000 (3.03)  Time: 0.646s, 1585.26/s  (0.661s, 1550.08/s)  LR: 7.144e-05  Data: 0.013 (0.031)
Train: 250 [ 150/1251 ( 12%)]  Loss: 2.527 (2.91)  Time: 0.652s, 1570.46/s  (0.658s, 1557.01/s)  LR: 7.134e-05  Data: 0.013 (0.025)
Train: 250 [ 200/1251 ( 16%)]  Loss: 3.106 (2.95)  Time: 0.653s, 1567.03/s  (0.657s, 1558.76/s)  LR: 7.123e-05  Data: 0.014 (0.022)
Train: 250 [ 250/1251 ( 20%)]  Loss: 2.826 (2.93)  Time: 0.664s, 1542.44/s  (0.657s, 1558.73/s)  LR: 7.113e-05  Data: 0.016 (0.021)
Train: 250 [ 300/1251 ( 24%)]  Loss: 2.854 (2.92)  Time: 0.659s, 1552.72/s  (0.657s, 1559.40/s)  LR: 7.103e-05  Data: 0.014 (0.019)
Train: 250 [ 350/1251 ( 28%)]  Loss: 2.919 (2.92)  Time: 0.657s, 1559.78/s  (0.657s, 1559.76/s)  LR: 7.092e-05  Data: 0.015 (0.019)
Train: 250 [ 400/1251 ( 32%)]  Loss: 3.268 (2.96)  Time: 0.646s, 1584.84/s  (0.656s, 1559.99/s)  LR: 7.082e-05  Data: 0.016 (0.018)
Train: 250 [ 450/1251 ( 36%)]  Loss: 3.128 (2.97)  Time: 0.646s, 1585.88/s  (0.656s, 1560.31/s)  LR: 7.072e-05  Data: 0.013 (0.018)
Train: 250 [ 500/1251 ( 40%)]  Loss: 2.896 (2.97)  Time: 0.652s, 1569.99/s  (0.656s, 1560.41/s)  LR: 7.061e-05  Data: 0.013 (0.017)
Train: 250 [ 550/1251 ( 44%)]  Loss: 3.001 (2.97)  Time: 0.653s, 1568.62/s  (0.656s, 1560.73/s)  LR: 7.051e-05  Data: 0.014 (0.017)
Train: 250 [ 600/1251 ( 48%)]  Loss: 2.677 (2.95)  Time: 0.665s, 1539.31/s  (0.656s, 1560.55/s)  LR: 7.041e-05  Data: 0.014 (0.017)
Train: 250 [ 650/1251 ( 52%)]  Loss: 3.020 (2.95)  Time: 0.648s, 1579.30/s  (0.656s, 1560.49/s)  LR: 7.030e-05  Data: 0.012 (0.016)
Train: 250 [ 700/1251 ( 56%)]  Loss: 3.136 (2.96)  Time: 0.655s, 1564.35/s  (0.656s, 1560.76/s)  LR: 7.020e-05  Data: 0.013 (0.016)
Train: 250 [ 750/1251 ( 60%)]  Loss: 3.027 (2.97)  Time: 0.659s, 1553.08/s  (0.656s, 1560.83/s)  LR: 7.010e-05  Data: 0.013 (0.016)
Train: 250 [ 800/1251 ( 64%)]  Loss: 2.755 (2.96)  Time: 0.666s, 1537.64/s  (0.656s, 1560.72/s)  LR: 6.999e-05  Data: 0.013 (0.016)
Train: 250 [ 850/1251 ( 68%)]  Loss: 3.056 (2.96)  Time: 0.661s, 1550.07/s  (0.656s, 1560.48/s)  LR: 6.989e-05  Data: 0.015 (0.016)
Train: 250 [ 900/1251 ( 72%)]  Loss: 2.975 (2.96)  Time: 0.654s, 1565.61/s  (0.656s, 1560.43/s)  LR: 6.979e-05  Data: 0.017 (0.016)
Train: 250 [ 950/1251 ( 76%)]  Loss: 3.227 (2.98)  Time: 0.658s, 1557.30/s  (0.656s, 1560.27/s)  LR: 6.969e-05  Data: 0.014 (0.016)
Train: 250 [1000/1251 ( 80%)]  Loss: 3.031 (2.98)  Time: 0.661s, 1548.66/s  (0.656s, 1560.04/s)  LR: 6.958e-05  Data: 0.013 (0.015)
Train: 250 [1050/1251 ( 84%)]  Loss: 2.908 (2.97)  Time: 0.652s, 1569.86/s  (0.656s, 1559.91/s)  LR: 6.948e-05  Data: 0.012 (0.015)
Train: 250 [1100/1251 ( 88%)]  Loss: 2.800 (2.97)  Time: 0.668s, 1532.15/s  (0.657s, 1559.69/s)  LR: 6.938e-05  Data: 0.013 (0.015)
Train: 250 [1150/1251 ( 92%)]  Loss: 2.962 (2.97)  Time: 0.662s, 1545.97/s  (0.657s, 1559.49/s)  LR: 6.928e-05  Data: 0.014 (0.015)
Train: 250 [1200/1251 ( 96%)]  Loss: 2.931 (2.97)  Time: 0.654s, 1566.53/s  (0.657s, 1559.35/s)  LR: 6.917e-05  Data: 0.014 (0.015)
Train: 250 [1250/1251 (100%)]  Loss: 3.000 (2.97)  Time: 0.648s, 1579.85/s  (0.657s, 1559.32/s)  LR: 6.907e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.290 (3.290)  Loss:  0.3733 (0.3733)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.161 (0.323)  Loss:  0.5010 (0.8071)  Acc@1: 87.1462 (81.0900)  Acc@5: 97.9953 (95.4440)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-250.pth.tar', 81.08999997558594)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-249.pth.tar', 80.8519999243164)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-248.pth.tar', 80.8439999975586)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-247.pth.tar', 80.79399989501952)

Train: 251 [   0/1251 (  0%)]  Loss: 3.033 (3.03)  Time: 3.268s,  313.35/s  (3.268s,  313.35/s)  LR: 6.907e-05  Data: 1.714 (1.714)
Train: 251 [  50/1251 (  4%)]  Loss: 2.897 (2.97)  Time: 0.640s, 1600.37/s  (0.674s, 1519.20/s)  LR: 6.897e-05  Data: 0.014 (0.047)
Train: 251 [ 100/1251 (  8%)]  Loss: 3.171 (3.03)  Time: 0.657s, 1558.43/s  (0.662s, 1546.84/s)  LR: 6.886e-05  Data: 0.013 (0.031)
Train: 251 [ 150/1251 ( 12%)]  Loss: 2.886 (3.00)  Time: 0.655s, 1563.54/s  (0.659s, 1553.16/s)  LR: 6.876e-05  Data: 0.013 (0.025)
Train: 251 [ 200/1251 ( 16%)]  Loss: 3.190 (3.04)  Time: 0.652s, 1570.75/s  (0.659s, 1554.20/s)  LR: 6.866e-05  Data: 0.013 (0.022)
Train: 251 [ 250/1251 ( 20%)]  Loss: 2.941 (3.02)  Time: 0.651s, 1572.90/s  (0.659s, 1554.80/s)  LR: 6.856e-05  Data: 0.015 (0.020)
Train: 251 [ 300/1251 ( 24%)]  Loss: 3.162 (3.04)  Time: 0.657s, 1558.52/s  (0.658s, 1555.32/s)  LR: 6.846e-05  Data: 0.013 (0.019)
Train: 251 [ 350/1251 ( 28%)]  Loss: 3.124 (3.05)  Time: 0.650s, 1574.43/s  (0.658s, 1556.11/s)  LR: 6.836e-05  Data: 0.015 (0.018)
Train: 251 [ 400/1251 ( 32%)]  Loss: 2.999 (3.04)  Time: 0.647s, 1582.61/s  (0.658s, 1556.77/s)  LR: 6.825e-05  Data: 0.013 (0.018)
Train: 251 [ 450/1251 ( 36%)]  Loss: 3.039 (3.04)  Time: 0.655s, 1563.49/s  (0.658s, 1556.91/s)  LR: 6.815e-05  Data: 0.016 (0.017)
Train: 251 [ 500/1251 ( 40%)]  Loss: 2.996 (3.04)  Time: 0.648s, 1579.57/s  (0.658s, 1557.20/s)  LR: 6.805e-05  Data: 0.013 (0.017)
Train: 251 [ 550/1251 ( 44%)]  Loss: 2.742 (3.01)  Time: 0.651s, 1573.69/s  (0.657s, 1557.71/s)  LR: 6.795e-05  Data: 0.013 (0.017)
Train: 251 [ 600/1251 ( 48%)]  Loss: 3.176 (3.03)  Time: 0.651s, 1572.46/s  (0.657s, 1557.46/s)  LR: 6.785e-05  Data: 0.013 (0.016)
Train: 251 [ 650/1251 ( 52%)]  Loss: 3.203 (3.04)  Time: 0.654s, 1564.79/s  (0.657s, 1557.55/s)  LR: 6.775e-05  Data: 0.014 (0.016)
Train: 251 [ 700/1251 ( 56%)]  Loss: 2.990 (3.04)  Time: 0.659s, 1554.87/s  (0.657s, 1557.66/s)  LR: 6.765e-05  Data: 0.013 (0.016)
Train: 251 [ 750/1251 ( 60%)]  Loss: 3.020 (3.04)  Time: 0.655s, 1563.76/s  (0.657s, 1557.88/s)  LR: 6.754e-05  Data: 0.014 (0.016)
Train: 251 [ 800/1251 ( 64%)]  Loss: 3.023 (3.03)  Time: 0.667s, 1534.18/s  (0.657s, 1557.85/s)  LR: 6.744e-05  Data: 0.018 (0.016)
Train: 251 [ 850/1251 ( 68%)]  Loss: 3.074 (3.04)  Time: 0.663s, 1545.31/s  (0.658s, 1557.31/s)  LR: 6.734e-05  Data: 0.014 (0.016)
Train: 251 [ 900/1251 ( 72%)]  Loss: 3.003 (3.04)  Time: 0.652s, 1569.36/s  (0.657s, 1557.51/s)  LR: 6.724e-05  Data: 0.014 (0.016)
Train: 251 [ 950/1251 ( 76%)]  Loss: 3.106 (3.04)  Time: 0.650s, 1574.64/s  (0.657s, 1557.72/s)  LR: 6.714e-05  Data: 0.015 (0.015)
Train: 251 [1000/1251 ( 80%)]  Loss: 2.860 (3.03)  Time: 0.655s, 1563.74/s  (0.657s, 1557.77/s)  LR: 6.704e-05  Data: 0.014 (0.015)
Train: 251 [1050/1251 ( 84%)]  Loss: 2.918 (3.03)  Time: 0.657s, 1558.73/s  (0.657s, 1557.71/s)  LR: 6.694e-05  Data: 0.014 (0.015)
Train: 251 [1100/1251 ( 88%)]  Loss: 3.031 (3.03)  Time: 0.661s, 1549.52/s  (0.657s, 1557.57/s)  LR: 6.684e-05  Data: 0.013 (0.015)
Train: 251 [1150/1251 ( 92%)]  Loss: 3.118 (3.03)  Time: 0.662s, 1546.90/s  (0.657s, 1557.60/s)  LR: 6.674e-05  Data: 0.014 (0.015)
Train: 251 [1200/1251 ( 96%)]  Loss: 3.023 (3.03)  Time: 0.657s, 1558.24/s  (0.657s, 1557.77/s)  LR: 6.664e-05  Data: 0.013 (0.015)
Train: 251 [1250/1251 (100%)]  Loss: 3.032 (3.03)  Time: 0.635s, 1612.02/s  (0.657s, 1557.95/s)  LR: 6.654e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.882 (2.882)  Loss:  0.3770 (0.3770)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.321)  Loss:  0.5117 (0.8159)  Acc@1: 86.6745 (80.8960)  Acc@5: 97.8774 (95.5280)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-250.pth.tar', 81.08999997558594)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-251.pth.tar', 80.89599989990235)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-249.pth.tar', 80.8519999243164)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-248.pth.tar', 80.8439999975586)

Train: 252 [   0/1251 (  0%)]  Loss: 3.281 (3.28)  Time: 3.371s,  303.75/s  (3.371s,  303.75/s)  LR: 6.654e-05  Data: 1.869 (1.869)
Train: 252 [  50/1251 (  4%)]  Loss: 3.006 (3.14)  Time: 0.643s, 1592.22/s  (0.676s, 1515.29/s)  LR: 6.644e-05  Data: 0.013 (0.050)
Train: 252 [ 100/1251 (  8%)]  Loss: 3.005 (3.10)  Time: 0.651s, 1572.78/s  (0.663s, 1545.45/s)  LR: 6.633e-05  Data: 0.012 (0.032)
Train: 252 [ 150/1251 ( 12%)]  Loss: 2.862 (3.04)  Time: 0.653s, 1568.79/s  (0.660s, 1552.64/s)  LR: 6.623e-05  Data: 0.014 (0.026)
Train: 252 [ 200/1251 ( 16%)]  Loss: 3.231 (3.08)  Time: 0.651s, 1573.76/s  (0.659s, 1555.01/s)  LR: 6.613e-05  Data: 0.013 (0.023)
Train: 252 [ 250/1251 ( 20%)]  Loss: 2.975 (3.06)  Time: 0.657s, 1557.69/s  (0.658s, 1556.93/s)  LR: 6.603e-05  Data: 0.015 (0.021)
Train: 252 [ 300/1251 ( 24%)]  Loss: 3.017 (3.05)  Time: 0.655s, 1563.34/s  (0.658s, 1557.10/s)  LR: 6.593e-05  Data: 0.015 (0.020)
Train: 252 [ 350/1251 ( 28%)]  Loss: 2.918 (3.04)  Time: 0.662s, 1546.93/s  (0.658s, 1556.78/s)  LR: 6.584e-05  Data: 0.013 (0.019)
Train: 252 [ 400/1251 ( 32%)]  Loss: 3.097 (3.04)  Time: 0.652s, 1569.52/s  (0.658s, 1556.20/s)  LR: 6.574e-05  Data: 0.014 (0.018)
Train: 252 [ 450/1251 ( 36%)]  Loss: 2.924 (3.03)  Time: 0.662s, 1546.81/s  (0.658s, 1556.30/s)  LR: 6.564e-05  Data: 0.013 (0.018)
Train: 252 [ 500/1251 ( 40%)]  Loss: 2.995 (3.03)  Time: 0.652s, 1570.31/s  (0.658s, 1556.09/s)  LR: 6.554e-05  Data: 0.013 (0.017)
Train: 252 [ 550/1251 ( 44%)]  Loss: 3.145 (3.04)  Time: 0.655s, 1564.48/s  (0.658s, 1555.94/s)  LR: 6.544e-05  Data: 0.014 (0.017)
Train: 252 [ 600/1251 ( 48%)]  Loss: 3.116 (3.04)  Time: 0.657s, 1559.00/s  (0.658s, 1556.09/s)  LR: 6.534e-05  Data: 0.013 (0.017)
Train: 252 [ 650/1251 ( 52%)]  Loss: 3.277 (3.06)  Time: 0.654s, 1566.63/s  (0.658s, 1555.95/s)  LR: 6.524e-05  Data: 0.014 (0.017)
Train: 252 [ 700/1251 ( 56%)]  Loss: 3.240 (3.07)  Time: 0.656s, 1559.99/s  (0.658s, 1555.74/s)  LR: 6.514e-05  Data: 0.014 (0.016)
Train: 252 [ 750/1251 ( 60%)]  Loss: 3.079 (3.07)  Time: 0.654s, 1566.85/s  (0.658s, 1555.49/s)  LR: 6.504e-05  Data: 0.013 (0.016)
Train: 252 [ 800/1251 ( 64%)]  Loss: 2.788 (3.06)  Time: 0.661s, 1548.64/s  (0.658s, 1555.65/s)  LR: 6.494e-05  Data: 0.013 (0.016)
Train: 252 [ 850/1251 ( 68%)]  Loss: 3.078 (3.06)  Time: 0.655s, 1562.79/s  (0.658s, 1555.49/s)  LR: 6.484e-05  Data: 0.013 (0.016)
Train: 252 [ 900/1251 ( 72%)]  Loss: 3.005 (3.05)  Time: 0.663s, 1545.57/s  (0.658s, 1555.15/s)  LR: 6.474e-05  Data: 0.016 (0.016)
Train: 252 [ 950/1251 ( 76%)]  Loss: 3.064 (3.06)  Time: 0.656s, 1560.27/s  (0.659s, 1554.54/s)  LR: 6.464e-05  Data: 0.016 (0.016)
Train: 252 [1000/1251 ( 80%)]  Loss: 2.998 (3.05)  Time: 0.668s, 1533.45/s  (0.659s, 1554.11/s)  LR: 6.454e-05  Data: 0.013 (0.016)
Train: 252 [1050/1251 ( 84%)]  Loss: 3.223 (3.06)  Time: 0.668s, 1533.69/s  (0.659s, 1553.96/s)  LR: 6.445e-05  Data: 0.014 (0.016)
Train: 252 [1100/1251 ( 88%)]  Loss: 3.152 (3.06)  Time: 0.657s, 1558.11/s  (0.659s, 1553.93/s)  LR: 6.435e-05  Data: 0.014 (0.015)
Train: 252 [1150/1251 ( 92%)]  Loss: 2.965 (3.06)  Time: 0.654s, 1566.37/s  (0.659s, 1554.08/s)  LR: 6.425e-05  Data: 0.013 (0.015)
Train: 252 [1200/1251 ( 96%)]  Loss: 3.106 (3.06)  Time: 0.652s, 1570.65/s  (0.659s, 1554.24/s)  LR: 6.415e-05  Data: 0.014 (0.015)
Train: 252 [1250/1251 (100%)]  Loss: 3.038 (3.06)  Time: 0.636s, 1609.51/s  (0.659s, 1554.65/s)  LR: 6.405e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.869 (2.869)  Loss:  0.3696 (0.3696)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.162 (0.321)  Loss:  0.4749 (0.8056)  Acc@1: 87.7359 (81.0660)  Acc@5: 98.4670 (95.5420)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-250.pth.tar', 81.08999997558594)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-252.pth.tar', 81.06600010253906)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-251.pth.tar', 80.89599989990235)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-249.pth.tar', 80.8519999243164)

Train: 253 [   0/1251 (  0%)]  Loss: 2.613 (2.61)  Time: 3.655s,  280.13/s  (3.655s,  280.13/s)  LR: 6.405e-05  Data: 1.718 (1.718)
Train: 253 [  50/1251 (  4%)]  Loss: 3.163 (2.89)  Time: 0.648s, 1581.31/s  (0.678s, 1510.32/s)  LR: 6.395e-05  Data: 0.014 (0.048)
Train: 253 [ 100/1251 (  8%)]  Loss: 2.814 (2.86)  Time: 0.643s, 1592.62/s  (0.663s, 1545.25/s)  LR: 6.385e-05  Data: 0.013 (0.031)
Train: 253 [ 150/1251 ( 12%)]  Loss: 2.998 (2.90)  Time: 0.658s, 1557.26/s  (0.659s, 1552.89/s)  LR: 6.375e-05  Data: 0.015 (0.025)
Train: 253 [ 200/1251 ( 16%)]  Loss: 2.914 (2.90)  Time: 0.651s, 1573.00/s  (0.658s, 1555.45/s)  LR: 6.366e-05  Data: 0.013 (0.022)
Train: 253 [ 250/1251 ( 20%)]  Loss: 2.838 (2.89)  Time: 0.651s, 1573.03/s  (0.658s, 1555.43/s)  LR: 6.356e-05  Data: 0.013 (0.021)
Train: 253 [ 300/1251 ( 24%)]  Loss: 3.018 (2.91)  Time: 0.656s, 1560.98/s  (0.658s, 1556.41/s)  LR: 6.346e-05  Data: 0.012 (0.019)
Train: 253 [ 350/1251 ( 28%)]  Loss: 2.961 (2.91)  Time: 0.660s, 1552.18/s  (0.658s, 1556.22/s)  LR: 6.336e-05  Data: 0.013 (0.019)
Train: 253 [ 400/1251 ( 32%)]  Loss: 3.118 (2.94)  Time: 0.657s, 1559.29/s  (0.658s, 1555.94/s)  LR: 6.326e-05  Data: 0.014 (0.018)
Train: 253 [ 450/1251 ( 36%)]  Loss: 3.266 (2.97)  Time: 0.665s, 1540.45/s  (0.658s, 1556.21/s)  LR: 6.317e-05  Data: 0.015 (0.017)
Train: 253 [ 500/1251 ( 40%)]  Loss: 2.860 (2.96)  Time: 0.661s, 1549.14/s  (0.658s, 1556.19/s)  LR: 6.307e-05  Data: 0.013 (0.017)
Train: 253 [ 550/1251 ( 44%)]  Loss: 2.969 (2.96)  Time: 0.658s, 1556.07/s  (0.658s, 1556.23/s)  LR: 6.297e-05  Data: 0.013 (0.017)
Train: 253 [ 600/1251 ( 48%)]  Loss: 2.726 (2.94)  Time: 0.661s, 1549.38/s  (0.658s, 1556.02/s)  LR: 6.287e-05  Data: 0.014 (0.017)
Train: 253 [ 650/1251 ( 52%)]  Loss: 2.543 (2.91)  Time: 0.659s, 1554.23/s  (0.658s, 1555.63/s)  LR: 6.278e-05  Data: 0.013 (0.016)
Train: 253 [ 700/1251 ( 56%)]  Loss: 2.948 (2.92)  Time: 0.661s, 1549.70/s  (0.658s, 1555.22/s)  LR: 6.268e-05  Data: 0.015 (0.016)
Train: 253 [ 750/1251 ( 60%)]  Loss: 3.095 (2.93)  Time: 0.660s, 1552.12/s  (0.659s, 1554.89/s)  LR: 6.258e-05  Data: 0.013 (0.016)
Train: 253 [ 800/1251 ( 64%)]  Loss: 3.060 (2.94)  Time: 0.663s, 1543.60/s  (0.659s, 1554.60/s)  LR: 6.249e-05  Data: 0.014 (0.016)
Train: 253 [ 850/1251 ( 68%)]  Loss: 2.729 (2.92)  Time: 0.666s, 1537.31/s  (0.659s, 1554.29/s)  LR: 6.239e-05  Data: 0.013 (0.016)
Train: 253 [ 900/1251 ( 72%)]  Loss: 2.915 (2.92)  Time: 0.661s, 1550.17/s  (0.659s, 1553.97/s)  LR: 6.229e-05  Data: 0.012 (0.016)
Train: 253 [ 950/1251 ( 76%)]  Loss: 3.102 (2.93)  Time: 0.666s, 1537.43/s  (0.659s, 1553.81/s)  LR: 6.219e-05  Data: 0.014 (0.016)
Train: 253 [1000/1251 ( 80%)]  Loss: 2.952 (2.93)  Time: 0.655s, 1564.15/s  (0.659s, 1553.62/s)  LR: 6.210e-05  Data: 0.013 (0.015)
Train: 253 [1050/1251 ( 84%)]  Loss: 3.306 (2.95)  Time: 0.656s, 1562.13/s  (0.659s, 1553.44/s)  LR: 6.200e-05  Data: 0.012 (0.015)
Train: 253 [1100/1251 ( 88%)]  Loss: 2.802 (2.94)  Time: 0.667s, 1535.16/s  (0.659s, 1553.52/s)  LR: 6.190e-05  Data: 0.020 (0.015)
Train: 253 [1150/1251 ( 92%)]  Loss: 2.907 (2.94)  Time: 0.662s, 1547.40/s  (0.659s, 1553.52/s)  LR: 6.181e-05  Data: 0.013 (0.015)
Train: 253 [1200/1251 ( 96%)]  Loss: 3.233 (2.95)  Time: 0.661s, 1549.58/s  (0.659s, 1553.32/s)  LR: 6.171e-05  Data: 0.013 (0.015)
Train: 253 [1250/1251 (100%)]  Loss: 2.857 (2.95)  Time: 0.643s, 1592.62/s  (0.659s, 1553.69/s)  LR: 6.161e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.843 (2.843)  Loss:  0.3718 (0.3718)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.161 (0.324)  Loss:  0.4988 (0.8139)  Acc@1: 87.2642 (81.0180)  Acc@5: 98.2311 (95.5620)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-250.pth.tar', 81.08999997558594)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-252.pth.tar', 81.06600010253906)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-253.pth.tar', 81.01800002685547)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-251.pth.tar', 80.89599989990235)

Train: 254 [   0/1251 (  0%)]  Loss: 2.918 (2.92)  Time: 3.102s,  330.07/s  (3.102s,  330.07/s)  LR: 6.161e-05  Data: 1.838 (1.838)
Train: 254 [  50/1251 (  4%)]  Loss: 3.268 (3.09)  Time: 0.639s, 1601.55/s  (0.671s, 1527.09/s)  LR: 6.152e-05  Data: 0.016 (0.050)
Train: 254 [ 100/1251 (  8%)]  Loss: 2.954 (3.05)  Time: 0.652s, 1571.73/s  (0.658s, 1556.27/s)  LR: 6.142e-05  Data: 0.013 (0.032)
Train: 254 [ 150/1251 ( 12%)]  Loss: 3.137 (3.07)  Time: 0.651s, 1572.80/s  (0.656s, 1561.06/s)  LR: 6.132e-05  Data: 0.013 (0.026)
Train: 254 [ 200/1251 ( 16%)]  Loss: 3.344 (3.12)  Time: 0.658s, 1556.84/s  (0.656s, 1560.78/s)  LR: 6.123e-05  Data: 0.013 (0.023)
Train: 254 [ 250/1251 ( 20%)]  Loss: 3.054 (3.11)  Time: 0.649s, 1578.63/s  (0.656s, 1560.51/s)  LR: 6.113e-05  Data: 0.012 (0.021)
Train: 254 [ 300/1251 ( 24%)]  Loss: 3.078 (3.11)  Time: 0.665s, 1539.57/s  (0.657s, 1559.27/s)  LR: 6.103e-05  Data: 0.013 (0.020)
Train: 254 [ 350/1251 ( 28%)]  Loss: 2.743 (3.06)  Time: 0.660s, 1550.63/s  (0.657s, 1558.35/s)  LR: 6.094e-05  Data: 0.014 (0.019)
Train: 254 [ 400/1251 ( 32%)]  Loss: 2.949 (3.05)  Time: 0.665s, 1539.34/s  (0.657s, 1557.54/s)  LR: 6.084e-05  Data: 0.013 (0.018)
Train: 254 [ 450/1251 ( 36%)]  Loss: 3.056 (3.05)  Time: 0.661s, 1548.52/s  (0.658s, 1557.12/s)  LR: 6.075e-05  Data: 0.014 (0.018)
Train: 254 [ 500/1251 ( 40%)]  Loss: 2.950 (3.04)  Time: 0.667s, 1535.66/s  (0.658s, 1557.12/s)  LR: 6.065e-05  Data: 0.013 (0.017)
Train: 254 [ 550/1251 ( 44%)]  Loss: 3.080 (3.04)  Time: 0.661s, 1549.76/s  (0.658s, 1557.07/s)  LR: 6.056e-05  Data: 0.013 (0.017)
Train: 254 [ 600/1251 ( 48%)]  Loss: 2.938 (3.04)  Time: 0.658s, 1555.12/s  (0.658s, 1557.08/s)  LR: 6.046e-05  Data: 0.013 (0.017)
Train: 254 [ 650/1251 ( 52%)]  Loss: 3.012 (3.03)  Time: 0.660s, 1550.88/s  (0.658s, 1556.77/s)  LR: 6.036e-05  Data: 0.013 (0.016)
Train: 254 [ 700/1251 ( 56%)]  Loss: 2.770 (3.02)  Time: 0.667s, 1534.52/s  (0.658s, 1556.38/s)  LR: 6.027e-05  Data: 0.012 (0.016)
Train: 254 [ 750/1251 ( 60%)]  Loss: 2.845 (3.01)  Time: 0.662s, 1547.95/s  (0.658s, 1556.12/s)  LR: 6.017e-05  Data: 0.013 (0.016)
Train: 254 [ 800/1251 ( 64%)]  Loss: 3.101 (3.01)  Time: 0.653s, 1568.47/s  (0.658s, 1555.83/s)  LR: 6.008e-05  Data: 0.013 (0.016)
Train: 254 [ 850/1251 ( 68%)]  Loss: 2.887 (3.00)  Time: 0.673s, 1521.38/s  (0.659s, 1554.99/s)  LR: 5.998e-05  Data: 0.012 (0.016)
Train: 254 [ 900/1251 ( 72%)]  Loss: 2.981 (3.00)  Time: 0.664s, 1542.45/s  (0.659s, 1554.63/s)  LR: 5.989e-05  Data: 0.013 (0.016)
Train: 254 [ 950/1251 ( 76%)]  Loss: 3.061 (3.01)  Time: 0.666s, 1538.04/s  (0.659s, 1554.47/s)  LR: 5.979e-05  Data: 0.013 (0.016)
Train: 254 [1000/1251 ( 80%)]  Loss: 3.012 (3.01)  Time: 0.658s, 1557.11/s  (0.659s, 1554.40/s)  LR: 5.970e-05  Data: 0.013 (0.015)
Train: 254 [1050/1251 ( 84%)]  Loss: 2.931 (3.00)  Time: 0.665s, 1538.72/s  (0.659s, 1554.16/s)  LR: 5.960e-05  Data: 0.013 (0.015)
Train: 254 [1100/1251 ( 88%)]  Loss: 2.911 (3.00)  Time: 0.662s, 1547.00/s  (0.659s, 1553.83/s)  LR: 5.951e-05  Data: 0.013 (0.015)
Train: 254 [1150/1251 ( 92%)]  Loss: 2.814 (2.99)  Time: 0.663s, 1544.32/s  (0.659s, 1553.62/s)  LR: 5.941e-05  Data: 0.014 (0.015)
Train: 254 [1200/1251 ( 96%)]  Loss: 2.926 (2.99)  Time: 0.664s, 1542.51/s  (0.659s, 1553.34/s)  LR: 5.932e-05  Data: 0.016 (0.015)
Train: 254 [1250/1251 (100%)]  Loss: 3.058 (2.99)  Time: 0.651s, 1571.97/s  (0.659s, 1553.20/s)  LR: 5.922e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.819 (2.819)  Loss:  0.3909 (0.3909)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.322)  Loss:  0.5015 (0.8096)  Acc@1: 87.6179 (81.0940)  Acc@5: 98.2311 (95.5760)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-254.pth.tar', 81.093999921875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-250.pth.tar', 81.08999997558594)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-252.pth.tar', 81.06600010253906)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-253.pth.tar', 81.01800002685547)

Train: 255 [   0/1251 (  0%)]  Loss: 3.005 (3.01)  Time: 3.721s,  275.16/s  (3.721s,  275.16/s)  LR: 5.922e-05  Data: 1.797 (1.797)
Train: 255 [  50/1251 (  4%)]  Loss: 2.839 (2.92)  Time: 0.645s, 1587.91/s  (0.682s, 1500.57/s)  LR: 5.913e-05  Data: 0.014 (0.049)
Train: 255 [ 100/1251 (  8%)]  Loss: 2.941 (2.93)  Time: 0.658s, 1555.97/s  (0.668s, 1532.67/s)  LR: 5.903e-05  Data: 0.014 (0.032)
Train: 255 [ 150/1251 ( 12%)]  Loss: 2.864 (2.91)  Time: 0.655s, 1562.52/s  (0.664s, 1541.32/s)  LR: 5.894e-05  Data: 0.013 (0.026)
Train: 255 [ 200/1251 ( 16%)]  Loss: 2.906 (2.91)  Time: 0.650s, 1574.37/s  (0.663s, 1543.91/s)  LR: 5.884e-05  Data: 0.013 (0.023)
Train: 255 [ 250/1251 ( 20%)]  Loss: 3.102 (2.94)  Time: 0.655s, 1564.39/s  (0.662s, 1546.19/s)  LR: 5.875e-05  Data: 0.016 (0.021)
Train: 255 [ 300/1251 ( 24%)]  Loss: 3.283 (2.99)  Time: 0.658s, 1557.24/s  (0.662s, 1547.42/s)  LR: 5.866e-05  Data: 0.012 (0.020)
Train: 255 [ 350/1251 ( 28%)]  Loss: 3.040 (3.00)  Time: 0.652s, 1570.46/s  (0.661s, 1548.87/s)  LR: 5.856e-05  Data: 0.012 (0.019)
Train: 255 [ 400/1251 ( 32%)]  Loss: 2.879 (2.98)  Time: 0.652s, 1570.04/s  (0.660s, 1550.74/s)  LR: 5.847e-05  Data: 0.013 (0.018)
Train: 255 [ 450/1251 ( 36%)]  Loss: 2.807 (2.97)  Time: 0.653s, 1568.39/s  (0.660s, 1551.81/s)  LR: 5.837e-05  Data: 0.016 (0.018)
Train: 255 [ 500/1251 ( 40%)]  Loss: 3.141 (2.98)  Time: 0.657s, 1558.60/s  (0.660s, 1552.08/s)  LR: 5.828e-05  Data: 0.014 (0.017)
Train: 255 [ 550/1251 ( 44%)]  Loss: 3.035 (2.99)  Time: 0.656s, 1562.11/s  (0.660s, 1552.03/s)  LR: 5.819e-05  Data: 0.014 (0.017)
Train: 255 [ 600/1251 ( 48%)]  Loss: 3.204 (3.00)  Time: 0.663s, 1545.63/s  (0.660s, 1552.02/s)  LR: 5.809e-05  Data: 0.013 (0.017)
Train: 255 [ 650/1251 ( 52%)]  Loss: 2.883 (2.99)  Time: 0.659s, 1554.04/s  (0.660s, 1551.87/s)  LR: 5.800e-05  Data: 0.014 (0.017)
Train: 255 [ 700/1251 ( 56%)]  Loss: 3.223 (3.01)  Time: 0.661s, 1549.53/s  (0.660s, 1551.64/s)  LR: 5.791e-05  Data: 0.014 (0.016)
Train: 255 [ 750/1251 ( 60%)]  Loss: 2.953 (3.01)  Time: 0.657s, 1557.88/s  (0.660s, 1551.41/s)  LR: 5.781e-05  Data: 0.014 (0.016)
Train: 255 [ 800/1251 ( 64%)]  Loss: 2.856 (3.00)  Time: 0.664s, 1542.52/s  (0.660s, 1551.18/s)  LR: 5.772e-05  Data: 0.015 (0.016)
Train: 255 [ 850/1251 ( 68%)]  Loss: 3.091 (3.00)  Time: 0.660s, 1550.94/s  (0.660s, 1551.10/s)  LR: 5.763e-05  Data: 0.013 (0.016)
Train: 255 [ 900/1251 ( 72%)]  Loss: 2.806 (2.99)  Time: 0.662s, 1547.52/s  (0.660s, 1550.91/s)  LR: 5.753e-05  Data: 0.013 (0.016)
Train: 255 [ 950/1251 ( 76%)]  Loss: 3.133 (3.00)  Time: 0.652s, 1569.64/s  (0.660s, 1551.12/s)  LR: 5.744e-05  Data: 0.014 (0.016)
Train: 255 [1000/1251 ( 80%)]  Loss: 2.948 (3.00)  Time: 0.657s, 1557.56/s  (0.660s, 1551.42/s)  LR: 5.735e-05  Data: 0.014 (0.016)
Train: 255 [1050/1251 ( 84%)]  Loss: 3.052 (3.00)  Time: 0.653s, 1568.47/s  (0.660s, 1551.57/s)  LR: 5.725e-05  Data: 0.016 (0.016)
Train: 255 [1100/1251 ( 88%)]  Loss: 3.135 (3.01)  Time: 0.655s, 1562.66/s  (0.660s, 1551.96/s)  LR: 5.716e-05  Data: 0.015 (0.015)
Train: 255 [1150/1251 ( 92%)]  Loss: 2.915 (3.00)  Time: 0.662s, 1547.33/s  (0.660s, 1552.08/s)  LR: 5.707e-05  Data: 0.016 (0.015)
Train: 255 [1200/1251 ( 96%)]  Loss: 3.069 (3.00)  Time: 0.654s, 1565.43/s  (0.660s, 1552.11/s)  LR: 5.698e-05  Data: 0.013 (0.015)
Train: 255 [1250/1251 (100%)]  Loss: 2.854 (3.00)  Time: 0.649s, 1576.66/s  (0.660s, 1552.42/s)  LR: 5.688e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.877 (2.877)  Loss:  0.3789 (0.3789)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.8281 (98.8281)
Test: [  48/48]  Time: 0.161 (0.326)  Loss:  0.5063 (0.8095)  Acc@1: 88.0896 (81.0720)  Acc@5: 98.2311 (95.5800)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-254.pth.tar', 81.093999921875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-250.pth.tar', 81.08999997558594)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-255.pth.tar', 81.07199999755859)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-252.pth.tar', 81.06600010253906)

Train: 256 [   0/1251 (  0%)]  Loss: 2.985 (2.98)  Time: 3.196s,  320.37/s  (3.196s,  320.37/s)  LR: 5.688e-05  Data: 1.707 (1.707)
Train: 256 [  50/1251 (  4%)]  Loss: 2.914 (2.95)  Time: 0.644s, 1590.56/s  (0.674s, 1520.01/s)  LR: 5.679e-05  Data: 0.013 (0.047)
Train: 256 [ 100/1251 (  8%)]  Loss: 3.073 (2.99)  Time: 0.646s, 1585.46/s  (0.662s, 1546.81/s)  LR: 5.670e-05  Data: 0.013 (0.031)
Train: 256 [ 150/1251 ( 12%)]  Loss: 2.843 (2.95)  Time: 0.659s, 1553.27/s  (0.659s, 1552.74/s)  LR: 5.660e-05  Data: 0.013 (0.025)
Train: 256 [ 200/1251 ( 16%)]  Loss: 3.113 (2.99)  Time: 0.652s, 1569.56/s  (0.659s, 1553.64/s)  LR: 5.651e-05  Data: 0.013 (0.022)
Train: 256 [ 250/1251 ( 20%)]  Loss: 3.022 (2.99)  Time: 0.657s, 1558.42/s  (0.659s, 1554.79/s)  LR: 5.642e-05  Data: 0.013 (0.021)
Train: 256 [ 300/1251 ( 24%)]  Loss: 3.304 (3.04)  Time: 0.662s, 1547.69/s  (0.659s, 1554.63/s)  LR: 5.633e-05  Data: 0.013 (0.019)
Train: 256 [ 350/1251 ( 28%)]  Loss: 2.743 (3.00)  Time: 0.657s, 1558.05/s  (0.659s, 1554.61/s)  LR: 5.624e-05  Data: 0.013 (0.019)
Train: 256 [ 400/1251 ( 32%)]  Loss: 3.047 (3.00)  Time: 0.654s, 1566.09/s  (0.659s, 1554.62/s)  LR: 5.614e-05  Data: 0.013 (0.018)
Train: 256 [ 450/1251 ( 36%)]  Loss: 2.677 (2.97)  Time: 0.652s, 1570.83/s  (0.659s, 1554.19/s)  LR: 5.605e-05  Data: 0.013 (0.017)
Train: 256 [ 500/1251 ( 40%)]  Loss: 3.279 (3.00)  Time: 0.661s, 1548.31/s  (0.659s, 1554.10/s)  LR: 5.596e-05  Data: 0.016 (0.017)
Train: 256 [ 550/1251 ( 44%)]  Loss: 2.970 (3.00)  Time: 0.655s, 1564.00/s  (0.659s, 1554.07/s)  LR: 5.587e-05  Data: 0.013 (0.017)
Train: 256 [ 600/1251 ( 48%)]  Loss: 2.949 (2.99)  Time: 0.661s, 1549.22/s  (0.659s, 1553.91/s)  LR: 5.578e-05  Data: 0.013 (0.017)
Train: 256 [ 650/1251 ( 52%)]  Loss: 3.016 (3.00)  Time: 0.662s, 1546.71/s  (0.659s, 1553.96/s)  LR: 5.568e-05  Data: 0.013 (0.016)
Train: 256 [ 700/1251 ( 56%)]  Loss: 3.001 (3.00)  Time: 0.653s, 1567.75/s  (0.659s, 1554.09/s)  LR: 5.559e-05  Data: 0.013 (0.016)
Train: 256 [ 750/1251 ( 60%)]  Loss: 3.130 (3.00)  Time: 0.651s, 1573.02/s  (0.659s, 1554.06/s)  LR: 5.550e-05  Data: 0.014 (0.016)
Train: 256 [ 800/1251 ( 64%)]  Loss: 2.997 (3.00)  Time: 0.660s, 1550.70/s  (0.659s, 1554.08/s)  LR: 5.541e-05  Data: 0.013 (0.016)
Train: 256 [ 850/1251 ( 68%)]  Loss: 2.986 (3.00)  Time: 0.655s, 1563.40/s  (0.659s, 1554.00/s)  LR: 5.532e-05  Data: 0.013 (0.016)
Train: 256 [ 900/1251 ( 72%)]  Loss: 2.892 (3.00)  Time: 0.664s, 1542.68/s  (0.659s, 1554.21/s)  LR: 5.523e-05  Data: 0.015 (0.016)
Train: 256 [ 950/1251 ( 76%)]  Loss: 2.992 (3.00)  Time: 0.670s, 1528.64/s  (0.659s, 1554.34/s)  LR: 5.514e-05  Data: 0.013 (0.015)
Train: 256 [1000/1251 ( 80%)]  Loss: 2.880 (2.99)  Time: 0.654s, 1565.74/s  (0.659s, 1554.19/s)  LR: 5.505e-05  Data: 0.016 (0.015)
Train: 256 [1050/1251 ( 84%)]  Loss: 3.131 (3.00)  Time: 0.647s, 1583.08/s  (0.659s, 1554.05/s)  LR: 5.495e-05  Data: 0.013 (0.015)
Train: 256 [1100/1251 ( 88%)]  Loss: 2.854 (2.99)  Time: 0.660s, 1552.32/s  (0.659s, 1553.83/s)  LR: 5.486e-05  Data: 0.013 (0.015)
Train: 256 [1150/1251 ( 92%)]  Loss: 2.613 (2.98)  Time: 0.650s, 1575.85/s  (0.659s, 1554.02/s)  LR: 5.477e-05  Data: 0.014 (0.015)
Train: 256 [1200/1251 ( 96%)]  Loss: 3.083 (2.98)  Time: 0.644s, 1589.75/s  (0.659s, 1554.32/s)  LR: 5.468e-05  Data: 0.017 (0.015)
Train: 256 [1250/1251 (100%)]  Loss: 3.174 (2.99)  Time: 0.640s, 1599.66/s  (0.659s, 1554.50/s)  LR: 5.459e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.971 (2.971)  Loss:  0.3706 (0.3706)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.161 (0.319)  Loss:  0.4941 (0.8043)  Acc@1: 87.7358 (81.1160)  Acc@5: 97.9953 (95.5900)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-256.pth.tar', 81.11599997314453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-254.pth.tar', 81.093999921875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-250.pth.tar', 81.08999997558594)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-255.pth.tar', 81.07199999755859)

Train: 257 [   0/1251 (  0%)]  Loss: 2.955 (2.95)  Time: 3.445s,  297.22/s  (3.445s,  297.22/s)  LR: 5.459e-05  Data: 1.837 (1.837)
Train: 257 [  50/1251 (  4%)]  Loss: 3.040 (3.00)  Time: 0.642s, 1595.92/s  (0.677s, 1511.71/s)  LR: 5.450e-05  Data: 0.013 (0.049)
Train: 257 [ 100/1251 (  8%)]  Loss: 2.841 (2.95)  Time: 0.648s, 1580.93/s  (0.663s, 1544.77/s)  LR: 5.441e-05  Data: 0.013 (0.032)
Train: 257 [ 150/1251 ( 12%)]  Loss: 3.007 (2.96)  Time: 0.658s, 1556.58/s  (0.659s, 1553.40/s)  LR: 5.432e-05  Data: 0.014 (0.026)
Train: 257 [ 200/1251 ( 16%)]  Loss: 2.920 (2.95)  Time: 0.666s, 1538.66/s  (0.657s, 1557.92/s)  LR: 5.423e-05  Data: 0.015 (0.023)
Train: 257 [ 250/1251 ( 20%)]  Loss: 3.075 (2.97)  Time: 0.645s, 1588.62/s  (0.657s, 1559.76/s)  LR: 5.414e-05  Data: 0.013 (0.021)
Train: 257 [ 300/1251 ( 24%)]  Loss: 2.976 (2.97)  Time: 0.662s, 1547.83/s  (0.656s, 1560.11/s)  LR: 5.405e-05  Data: 0.013 (0.020)
Train: 257 [ 350/1251 ( 28%)]  Loss: 3.158 (3.00)  Time: 0.657s, 1557.58/s  (0.656s, 1560.51/s)  LR: 5.396e-05  Data: 0.012 (0.019)
Train: 257 [ 400/1251 ( 32%)]  Loss: 3.037 (3.00)  Time: 0.658s, 1556.58/s  (0.656s, 1560.56/s)  LR: 5.387e-05  Data: 0.014 (0.018)
Train: 257 [ 450/1251 ( 36%)]  Loss: 3.297 (3.03)  Time: 0.651s, 1574.01/s  (0.656s, 1560.95/s)  LR: 5.378e-05  Data: 0.014 (0.018)
Train: 257 [ 500/1251 ( 40%)]  Loss: 2.893 (3.02)  Time: 0.654s, 1566.80/s  (0.656s, 1561.01/s)  LR: 5.369e-05  Data: 0.013 (0.017)
Train: 257 [ 550/1251 ( 44%)]  Loss: 2.913 (3.01)  Time: 0.654s, 1566.06/s  (0.656s, 1561.19/s)  LR: 5.360e-05  Data: 0.013 (0.017)
Train: 257 [ 600/1251 ( 48%)]  Loss: 3.037 (3.01)  Time: 0.659s, 1554.33/s  (0.656s, 1560.97/s)  LR: 5.351e-05  Data: 0.014 (0.017)
Train: 257 [ 650/1251 ( 52%)]  Loss: 3.007 (3.01)  Time: 0.661s, 1550.00/s  (0.656s, 1560.47/s)  LR: 5.342e-05  Data: 0.016 (0.017)
Train: 257 [ 700/1251 ( 56%)]  Loss: 2.999 (3.01)  Time: 0.664s, 1542.14/s  (0.656s, 1560.12/s)  LR: 5.333e-05  Data: 0.016 (0.016)
Train: 257 [ 750/1251 ( 60%)]  Loss: 3.255 (3.03)  Time: 0.658s, 1555.78/s  (0.657s, 1559.73/s)  LR: 5.324e-05  Data: 0.017 (0.016)
Train: 257 [ 800/1251 ( 64%)]  Loss: 3.385 (3.05)  Time: 0.653s, 1568.59/s  (0.657s, 1559.66/s)  LR: 5.315e-05  Data: 0.013 (0.016)
Train: 257 [ 850/1251 ( 68%)]  Loss: 2.961 (3.04)  Time: 0.654s, 1565.95/s  (0.657s, 1559.37/s)  LR: 5.306e-05  Data: 0.014 (0.016)
Train: 257 [ 900/1251 ( 72%)]  Loss: 3.079 (3.04)  Time: 0.663s, 1543.90/s  (0.657s, 1558.66/s)  LR: 5.297e-05  Data: 0.013 (0.016)
Train: 257 [ 950/1251 ( 76%)]  Loss: 3.112 (3.05)  Time: 0.658s, 1555.38/s  (0.657s, 1558.28/s)  LR: 5.288e-05  Data: 0.013 (0.016)
Train: 257 [1000/1251 ( 80%)]  Loss: 2.989 (3.04)  Time: 0.657s, 1557.63/s  (0.657s, 1558.00/s)  LR: 5.279e-05  Data: 0.012 (0.016)
Train: 257 [1050/1251 ( 84%)]  Loss: 3.145 (3.05)  Time: 0.672s, 1524.90/s  (0.657s, 1557.62/s)  LR: 5.270e-05  Data: 0.013 (0.015)
Train: 257 [1100/1251 ( 88%)]  Loss: 2.928 (3.04)  Time: 0.668s, 1532.09/s  (0.658s, 1557.15/s)  LR: 5.261e-05  Data: 0.013 (0.015)
Train: 257 [1150/1251 ( 92%)]  Loss: 2.975 (3.04)  Time: 0.662s, 1546.54/s  (0.658s, 1556.93/s)  LR: 5.253e-05  Data: 0.014 (0.015)
Train: 257 [1200/1251 ( 96%)]  Loss: 2.752 (3.03)  Time: 0.658s, 1556.41/s  (0.658s, 1556.67/s)  LR: 5.244e-05  Data: 0.012 (0.015)
Train: 257 [1250/1251 (100%)]  Loss: 2.971 (3.03)  Time: 0.636s, 1609.95/s  (0.658s, 1556.91/s)  LR: 5.235e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.894 (2.894)  Loss:  0.3896 (0.3896)  Acc@1: 92.2852 (92.2852)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.162 (0.320)  Loss:  0.5005 (0.8116)  Acc@1: 88.3255 (81.1160)  Acc@5: 98.4670 (95.5720)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-256.pth.tar', 81.11599997314453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-257.pth.tar', 81.11599997070313)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-254.pth.tar', 81.093999921875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-250.pth.tar', 81.08999997558594)

Train: 258 [   0/1251 (  0%)]  Loss: 3.313 (3.31)  Time: 3.283s,  311.95/s  (3.283s,  311.95/s)  LR: 5.235e-05  Data: 1.468 (1.468)
Train: 258 [  50/1251 (  4%)]  Loss: 3.133 (3.22)  Time: 0.644s, 1589.67/s  (0.674s, 1520.41/s)  LR: 5.226e-05  Data: 0.016 (0.043)
Train: 258 [ 100/1251 (  8%)]  Loss: 2.684 (3.04)  Time: 0.654s, 1564.63/s  (0.662s, 1545.84/s)  LR: 5.217e-05  Data: 0.013 (0.029)
Train: 258 [ 150/1251 ( 12%)]  Loss: 2.855 (3.00)  Time: 0.656s, 1560.34/s  (0.660s, 1550.35/s)  LR: 5.208e-05  Data: 0.014 (0.024)
Train: 258 [ 200/1251 ( 16%)]  Loss: 3.131 (3.02)  Time: 0.660s, 1551.92/s  (0.659s, 1552.98/s)  LR: 5.199e-05  Data: 0.015 (0.021)
Train: 258 [ 250/1251 ( 20%)]  Loss: 3.023 (3.02)  Time: 0.648s, 1580.33/s  (0.659s, 1553.82/s)  LR: 5.190e-05  Data: 0.014 (0.020)
Train: 258 [ 300/1251 ( 24%)]  Loss: 3.140 (3.04)  Time: 0.657s, 1558.72/s  (0.659s, 1554.12/s)  LR: 5.182e-05  Data: 0.014 (0.019)
Train: 258 [ 350/1251 ( 28%)]  Loss: 3.100 (3.05)  Time: 0.664s, 1542.16/s  (0.659s, 1554.41/s)  LR: 5.173e-05  Data: 0.014 (0.018)
Train: 258 [ 400/1251 ( 32%)]  Loss: 2.982 (3.04)  Time: 0.660s, 1550.86/s  (0.659s, 1554.07/s)  LR: 5.164e-05  Data: 0.014 (0.018)
Train: 258 [ 450/1251 ( 36%)]  Loss: 3.075 (3.04)  Time: 0.670s, 1528.30/s  (0.659s, 1553.80/s)  LR: 5.155e-05  Data: 0.018 (0.017)
Train: 258 [ 500/1251 ( 40%)]  Loss: 2.947 (3.03)  Time: 0.659s, 1554.73/s  (0.659s, 1553.39/s)  LR: 5.146e-05  Data: 0.013 (0.017)
Train: 258 [ 550/1251 ( 44%)]  Loss: 2.871 (3.02)  Time: 0.663s, 1544.39/s  (0.659s, 1553.05/s)  LR: 5.138e-05  Data: 0.014 (0.017)
Train: 258 [ 600/1251 ( 48%)]  Loss: 2.878 (3.01)  Time: 0.668s, 1532.80/s  (0.659s, 1553.11/s)  LR: 5.129e-05  Data: 0.014 (0.017)
Train: 258 [ 650/1251 ( 52%)]  Loss: 2.813 (3.00)  Time: 0.658s, 1556.85/s  (0.659s, 1553.04/s)  LR: 5.120e-05  Data: 0.016 (0.016)
Train: 258 [ 700/1251 ( 56%)]  Loss: 3.091 (3.00)  Time: 0.661s, 1549.61/s  (0.659s, 1552.81/s)  LR: 5.111e-05  Data: 0.015 (0.016)
Train: 258 [ 750/1251 ( 60%)]  Loss: 2.951 (3.00)  Time: 0.659s, 1552.80/s  (0.659s, 1552.80/s)  LR: 5.103e-05  Data: 0.017 (0.016)
Train: 258 [ 800/1251 ( 64%)]  Loss: 3.242 (3.01)  Time: 0.672s, 1524.46/s  (0.660s, 1552.60/s)  LR: 5.094e-05  Data: 0.020 (0.016)
Train: 258 [ 850/1251 ( 68%)]  Loss: 3.090 (3.02)  Time: 0.663s, 1543.78/s  (0.660s, 1552.20/s)  LR: 5.085e-05  Data: 0.013 (0.016)
Train: 258 [ 900/1251 ( 72%)]  Loss: 3.040 (3.02)  Time: 0.649s, 1576.93/s  (0.660s, 1551.80/s)  LR: 5.076e-05  Data: 0.014 (0.016)
Train: 258 [ 950/1251 ( 76%)]  Loss: 3.168 (3.03)  Time: 0.658s, 1557.12/s  (0.660s, 1551.68/s)  LR: 5.068e-05  Data: 0.013 (0.016)
Train: 258 [1000/1251 ( 80%)]  Loss: 3.024 (3.03)  Time: 0.664s, 1543.31/s  (0.660s, 1551.81/s)  LR: 5.059e-05  Data: 0.013 (0.015)
Train: 258 [1050/1251 ( 84%)]  Loss: 2.829 (3.02)  Time: 0.661s, 1548.88/s  (0.660s, 1551.85/s)  LR: 5.050e-05  Data: 0.013 (0.015)
Train: 258 [1100/1251 ( 88%)]  Loss: 2.746 (3.01)  Time: 0.653s, 1568.46/s  (0.660s, 1551.96/s)  LR: 5.042e-05  Data: 0.013 (0.015)
Train: 258 [1150/1251 ( 92%)]  Loss: 2.919 (3.00)  Time: 0.659s, 1553.26/s  (0.660s, 1552.12/s)  LR: 5.033e-05  Data: 0.013 (0.015)
Train: 258 [1200/1251 ( 96%)]  Loss: 3.226 (3.01)  Time: 0.660s, 1552.36/s  (0.660s, 1552.41/s)  LR: 5.024e-05  Data: 0.013 (0.015)
Train: 258 [1250/1251 (100%)]  Loss: 2.990 (3.01)  Time: 0.644s, 1589.70/s  (0.660s, 1552.67/s)  LR: 5.016e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.879 (2.879)  Loss:  0.3843 (0.3843)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.161 (0.316)  Loss:  0.4968 (0.8136)  Acc@1: 87.5000 (81.0820)  Acc@5: 98.1132 (95.5980)
Train: 259 [   0/1251 (  0%)]  Loss: 3.083 (3.08)  Time: 3.511s,  291.67/s  (3.511s,  291.67/s)  LR: 5.015e-05  Data: 1.656 (1.656)
Train: 259 [  50/1251 (  4%)]  Loss: 2.861 (2.97)  Time: 0.648s, 1580.42/s  (0.678s, 1511.35/s)  LR: 5.007e-05  Data: 0.014 (0.046)
Train: 259 [ 100/1251 (  8%)]  Loss: 3.152 (3.03)  Time: 0.650s, 1574.98/s  (0.663s, 1545.47/s)  LR: 4.998e-05  Data: 0.013 (0.030)
Train: 259 [ 150/1251 ( 12%)]  Loss: 3.164 (3.06)  Time: 0.651s, 1572.48/s  (0.659s, 1553.90/s)  LR: 4.989e-05  Data: 0.013 (0.025)
Train: 259 [ 200/1251 ( 16%)]  Loss: 3.057 (3.06)  Time: 0.667s, 1534.42/s  (0.658s, 1555.47/s)  LR: 4.981e-05  Data: 0.012 (0.022)
Train: 259 [ 250/1251 ( 20%)]  Loss: 2.830 (3.02)  Time: 0.657s, 1558.28/s  (0.658s, 1556.38/s)  LR: 4.972e-05  Data: 0.015 (0.020)
Train: 259 [ 300/1251 ( 24%)]  Loss: 3.068 (3.03)  Time: 0.646s, 1584.75/s  (0.658s, 1557.17/s)  LR: 4.963e-05  Data: 0.013 (0.019)
Train: 259 [ 350/1251 ( 28%)]  Loss: 2.997 (3.03)  Time: 0.657s, 1558.02/s  (0.658s, 1557.40/s)  LR: 4.955e-05  Data: 0.015 (0.019)
Train: 259 [ 400/1251 ( 32%)]  Loss: 2.882 (3.01)  Time: 0.652s, 1569.42/s  (0.658s, 1557.30/s)  LR: 4.946e-05  Data: 0.014 (0.018)
Train: 259 [ 450/1251 ( 36%)]  Loss: 2.908 (3.00)  Time: 0.661s, 1548.50/s  (0.658s, 1557.41/s)  LR: 4.938e-05  Data: 0.013 (0.018)
Train: 259 [ 500/1251 ( 40%)]  Loss: 3.099 (3.01)  Time: 0.658s, 1555.68/s  (0.657s, 1557.83/s)  LR: 4.929e-05  Data: 0.016 (0.017)
Train: 259 [ 550/1251 ( 44%)]  Loss: 2.820 (2.99)  Time: 0.654s, 1565.14/s  (0.657s, 1557.79/s)  LR: 4.920e-05  Data: 0.013 (0.017)
Train: 259 [ 600/1251 ( 48%)]  Loss: 2.857 (2.98)  Time: 0.662s, 1546.47/s  (0.657s, 1557.49/s)  LR: 4.912e-05  Data: 0.013 (0.017)
Train: 259 [ 650/1251 ( 52%)]  Loss: 2.949 (2.98)  Time: 0.665s, 1540.98/s  (0.658s, 1556.85/s)  LR: 4.903e-05  Data: 0.013 (0.016)
Train: 259 [ 700/1251 ( 56%)]  Loss: 3.137 (2.99)  Time: 0.667s, 1534.15/s  (0.658s, 1556.15/s)  LR: 4.895e-05  Data: 0.015 (0.016)
Train: 259 [ 750/1251 ( 60%)]  Loss: 3.175 (3.00)  Time: 0.657s, 1557.66/s  (0.658s, 1555.52/s)  LR: 4.886e-05  Data: 0.013 (0.016)
Train: 259 [ 800/1251 ( 64%)]  Loss: 3.068 (3.01)  Time: 0.661s, 1549.54/s  (0.659s, 1554.86/s)  LR: 4.878e-05  Data: 0.013 (0.016)
Train: 259 [ 850/1251 ( 68%)]  Loss: 2.980 (3.00)  Time: 0.653s, 1567.67/s  (0.659s, 1554.57/s)  LR: 4.869e-05  Data: 0.013 (0.016)
Train: 259 [ 900/1251 ( 72%)]  Loss: 2.607 (2.98)  Time: 0.652s, 1571.48/s  (0.659s, 1554.31/s)  LR: 4.861e-05  Data: 0.014 (0.016)
Train: 259 [ 950/1251 ( 76%)]  Loss: 3.182 (2.99)  Time: 0.661s, 1550.00/s  (0.659s, 1554.38/s)  LR: 4.852e-05  Data: 0.012 (0.016)
Train: 259 [1000/1251 ( 80%)]  Loss: 2.982 (2.99)  Time: 0.661s, 1549.96/s  (0.659s, 1554.09/s)  LR: 4.844e-05  Data: 0.012 (0.016)
Train: 259 [1050/1251 ( 84%)]  Loss: 3.142 (3.00)  Time: 0.666s, 1538.12/s  (0.659s, 1553.84/s)  LR: 4.835e-05  Data: 0.013 (0.015)
Train: 259 [1100/1251 ( 88%)]  Loss: 3.021 (3.00)  Time: 0.660s, 1552.41/s  (0.659s, 1553.74/s)  LR: 4.827e-05  Data: 0.013 (0.015)
Train: 259 [1150/1251 ( 92%)]  Loss: 2.785 (2.99)  Time: 0.655s, 1564.17/s  (0.659s, 1553.49/s)  LR: 4.818e-05  Data: 0.013 (0.015)
Train: 259 [1200/1251 ( 96%)]  Loss: 3.051 (2.99)  Time: 0.656s, 1561.23/s  (0.659s, 1553.31/s)  LR: 4.810e-05  Data: 0.015 (0.015)
Train: 259 [1250/1251 (100%)]  Loss: 2.610 (2.98)  Time: 0.635s, 1611.35/s  (0.659s, 1553.46/s)  LR: 4.801e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.928 (2.928)  Loss:  0.3721 (0.3721)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.161 (0.324)  Loss:  0.5010 (0.8005)  Acc@1: 86.9104 (81.1840)  Acc@5: 98.1132 (95.6220)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-259.pth.tar', 81.18400000244141)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-256.pth.tar', 81.11599997314453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-257.pth.tar', 81.11599997070313)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-254.pth.tar', 81.093999921875)

Train: 260 [   0/1251 (  0%)]  Loss: 3.159 (3.16)  Time: 3.078s,  332.67/s  (3.078s,  332.67/s)  LR: 4.801e-05  Data: 1.618 (1.618)
Train: 260 [  50/1251 (  4%)]  Loss: 3.228 (3.19)  Time: 0.644s, 1590.03/s  (0.671s, 1527.10/s)  LR: 4.792e-05  Data: 0.016 (0.046)
Train: 260 [ 100/1251 (  8%)]  Loss: 3.045 (3.14)  Time: 0.653s, 1568.31/s  (0.659s, 1553.22/s)  LR: 4.784e-05  Data: 0.014 (0.030)
Train: 260 [ 150/1251 ( 12%)]  Loss: 2.611 (3.01)  Time: 0.659s, 1554.45/s  (0.657s, 1557.97/s)  LR: 4.776e-05  Data: 0.013 (0.024)
Train: 260 [ 200/1251 ( 16%)]  Loss: 3.064 (3.02)  Time: 0.657s, 1559.78/s  (0.657s, 1559.20/s)  LR: 4.767e-05  Data: 0.013 (0.022)
Train: 260 [ 250/1251 ( 20%)]  Loss: 2.810 (2.99)  Time: 0.648s, 1580.04/s  (0.657s, 1558.90/s)  LR: 4.759e-05  Data: 0.013 (0.020)
Train: 260 [ 300/1251 ( 24%)]  Loss: 2.887 (2.97)  Time: 0.654s, 1565.30/s  (0.657s, 1559.34/s)  LR: 4.750e-05  Data: 0.013 (0.019)
Train: 260 [ 350/1251 ( 28%)]  Loss: 2.993 (2.97)  Time: 0.661s, 1548.84/s  (0.657s, 1559.08/s)  LR: 4.742e-05  Data: 0.013 (0.018)
Train: 260 [ 400/1251 ( 32%)]  Loss: 2.971 (2.97)  Time: 0.656s, 1559.95/s  (0.657s, 1558.61/s)  LR: 4.733e-05  Data: 0.014 (0.018)
Train: 260 [ 450/1251 ( 36%)]  Loss: 2.892 (2.97)  Time: 0.657s, 1557.64/s  (0.657s, 1558.39/s)  LR: 4.725e-05  Data: 0.016 (0.017)
Train: 260 [ 500/1251 ( 40%)]  Loss: 2.603 (2.93)  Time: 0.649s, 1578.11/s  (0.657s, 1558.35/s)  LR: 4.717e-05  Data: 0.013 (0.017)
Train: 260 [ 550/1251 ( 44%)]  Loss: 3.034 (2.94)  Time: 0.663s, 1544.31/s  (0.657s, 1557.62/s)  LR: 4.708e-05  Data: 0.013 (0.017)
Train: 260 [ 600/1251 ( 48%)]  Loss: 2.709 (2.92)  Time: 0.664s, 1543.23/s  (0.658s, 1557.04/s)  LR: 4.700e-05  Data: 0.012 (0.016)
Train: 260 [ 650/1251 ( 52%)]  Loss: 2.688 (2.91)  Time: 0.660s, 1552.36/s  (0.658s, 1556.62/s)  LR: 4.692e-05  Data: 0.014 (0.016)
Train: 260 [ 700/1251 ( 56%)]  Loss: 3.021 (2.91)  Time: 0.667s, 1534.82/s  (0.658s, 1556.41/s)  LR: 4.683e-05  Data: 0.015 (0.016)
Train: 260 [ 750/1251 ( 60%)]  Loss: 2.856 (2.91)  Time: 0.661s, 1548.12/s  (0.658s, 1555.99/s)  LR: 4.675e-05  Data: 0.013 (0.016)
Train: 260 [ 800/1251 ( 64%)]  Loss: 3.168 (2.93)  Time: 0.659s, 1553.32/s  (0.658s, 1555.73/s)  LR: 4.666e-05  Data: 0.014 (0.016)
Train: 260 [ 850/1251 ( 68%)]  Loss: 2.665 (2.91)  Time: 0.662s, 1546.75/s  (0.658s, 1555.19/s)  LR: 4.658e-05  Data: 0.015 (0.016)
Train: 260 [ 900/1251 ( 72%)]  Loss: 2.891 (2.91)  Time: 0.661s, 1549.53/s  (0.659s, 1554.77/s)  LR: 4.650e-05  Data: 0.013 (0.015)
Train: 260 [ 950/1251 ( 76%)]  Loss: 3.074 (2.92)  Time: 0.663s, 1544.01/s  (0.659s, 1554.17/s)  LR: 4.641e-05  Data: 0.014 (0.015)
Train: 260 [1000/1251 ( 80%)]  Loss: 2.911 (2.92)  Time: 0.663s, 1545.49/s  (0.659s, 1553.69/s)  LR: 4.633e-05  Data: 0.013 (0.015)
Train: 260 [1050/1251 ( 84%)]  Loss: 2.963 (2.92)  Time: 0.669s, 1530.84/s  (0.659s, 1553.35/s)  LR: 4.625e-05  Data: 0.013 (0.015)
Train: 260 [1100/1251 ( 88%)]  Loss: 3.006 (2.92)  Time: 0.661s, 1549.88/s  (0.659s, 1552.85/s)  LR: 4.617e-05  Data: 0.013 (0.015)
Train: 260 [1150/1251 ( 92%)]  Loss: 2.734 (2.92)  Time: 0.664s, 1542.56/s  (0.660s, 1552.67/s)  LR: 4.608e-05  Data: 0.013 (0.015)
Train: 260 [1200/1251 ( 96%)]  Loss: 3.199 (2.93)  Time: 0.671s, 1526.85/s  (0.660s, 1552.39/s)  LR: 4.600e-05  Data: 0.013 (0.015)
Train: 260 [1250/1251 (100%)]  Loss: 3.030 (2.93)  Time: 0.644s, 1590.83/s  (0.660s, 1552.41/s)  LR: 4.592e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.012 (3.012)  Loss:  0.3667 (0.3667)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.163 (0.322)  Loss:  0.4854 (0.7974)  Acc@1: 87.3821 (81.1360)  Acc@5: 98.3491 (95.6680)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-259.pth.tar', 81.18400000244141)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-260.pth.tar', 81.136000078125)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-256.pth.tar', 81.11599997314453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-257.pth.tar', 81.11599997070313)

Train: 261 [   0/1251 (  0%)]  Loss: 2.929 (2.93)  Time: 3.958s,  258.74/s  (3.958s,  258.74/s)  LR: 4.592e-05  Data: 2.081 (2.081)
Train: 261 [  50/1251 (  4%)]  Loss: 2.854 (2.89)  Time: 0.649s, 1578.90/s  (0.686s, 1492.02/s)  LR: 4.583e-05  Data: 0.016 (0.055)
Train: 261 [ 100/1251 (  8%)]  Loss: 2.629 (2.80)  Time: 0.653s, 1567.75/s  (0.669s, 1530.13/s)  LR: 4.575e-05  Data: 0.014 (0.034)
Train: 261 [ 150/1251 ( 12%)]  Loss: 2.955 (2.84)  Time: 0.664s, 1541.37/s  (0.664s, 1541.19/s)  LR: 4.567e-05  Data: 0.012 (0.028)
Train: 261 [ 200/1251 ( 16%)]  Loss: 3.035 (2.88)  Time: 0.662s, 1547.19/s  (0.663s, 1545.49/s)  LR: 4.559e-05  Data: 0.016 (0.024)
Train: 261 [ 250/1251 ( 20%)]  Loss: 3.064 (2.91)  Time: 0.657s, 1557.49/s  (0.661s, 1548.23/s)  LR: 4.550e-05  Data: 0.013 (0.022)
Train: 261 [ 300/1251 ( 24%)]  Loss: 2.921 (2.91)  Time: 0.667s, 1536.28/s  (0.661s, 1549.84/s)  LR: 4.542e-05  Data: 0.013 (0.021)
Train: 261 [ 350/1251 ( 28%)]  Loss: 2.928 (2.91)  Time: 0.662s, 1547.72/s  (0.660s, 1551.16/s)  LR: 4.534e-05  Data: 0.013 (0.020)
Train: 261 [ 400/1251 ( 32%)]  Loss: 3.056 (2.93)  Time: 0.658s, 1556.34/s  (0.660s, 1551.59/s)  LR: 4.526e-05  Data: 0.013 (0.019)
Train: 261 [ 450/1251 ( 36%)]  Loss: 2.999 (2.94)  Time: 0.655s, 1562.78/s  (0.660s, 1552.35/s)  LR: 4.517e-05  Data: 0.013 (0.018)
Train: 261 [ 500/1251 ( 40%)]  Loss: 3.027 (2.95)  Time: 0.658s, 1557.16/s  (0.660s, 1552.65/s)  LR: 4.509e-05  Data: 0.013 (0.018)
Train: 261 [ 550/1251 ( 44%)]  Loss: 3.197 (2.97)  Time: 0.655s, 1563.40/s  (0.659s, 1553.13/s)  LR: 4.501e-05  Data: 0.016 (0.018)
Train: 261 [ 600/1251 ( 48%)]  Loss: 2.949 (2.96)  Time: 0.651s, 1573.98/s  (0.659s, 1553.80/s)  LR: 4.493e-05  Data: 0.013 (0.017)
Train: 261 [ 650/1251 ( 52%)]  Loss: 3.014 (2.97)  Time: 0.655s, 1563.27/s  (0.659s, 1554.20/s)  LR: 4.485e-05  Data: 0.013 (0.017)
Train: 261 [ 700/1251 ( 56%)]  Loss: 3.046 (2.97)  Time: 0.657s, 1557.64/s  (0.659s, 1554.65/s)  LR: 4.477e-05  Data: 0.014 (0.017)
Train: 261 [ 750/1251 ( 60%)]  Loss: 3.238 (2.99)  Time: 0.655s, 1564.12/s  (0.659s, 1554.65/s)  LR: 4.468e-05  Data: 0.017 (0.016)
Train: 261 [ 800/1251 ( 64%)]  Loss: 2.592 (2.97)  Time: 0.647s, 1581.87/s  (0.658s, 1555.07/s)  LR: 4.460e-05  Data: 0.012 (0.016)
Train: 261 [ 850/1251 ( 68%)]  Loss: 3.009 (2.97)  Time: 0.657s, 1557.98/s  (0.658s, 1555.29/s)  LR: 4.452e-05  Data: 0.013 (0.016)
Train: 261 [ 900/1251 ( 72%)]  Loss: 2.982 (2.97)  Time: 0.665s, 1540.58/s  (0.658s, 1555.49/s)  LR: 4.444e-05  Data: 0.013 (0.016)
Train: 261 [ 950/1251 ( 76%)]  Loss: 2.806 (2.96)  Time: 0.656s, 1561.45/s  (0.658s, 1555.60/s)  LR: 4.436e-05  Data: 0.014 (0.016)
Train: 261 [1000/1251 ( 80%)]  Loss: 2.837 (2.96)  Time: 0.667s, 1534.40/s  (0.658s, 1555.59/s)  LR: 4.428e-05  Data: 0.016 (0.016)
Train: 261 [1050/1251 ( 84%)]  Loss: 3.142 (2.96)  Time: 0.660s, 1551.34/s  (0.658s, 1555.40/s)  LR: 4.420e-05  Data: 0.014 (0.016)
Train: 261 [1100/1251 ( 88%)]  Loss: 3.023 (2.97)  Time: 0.659s, 1552.72/s  (0.658s, 1555.39/s)  LR: 4.412e-05  Data: 0.016 (0.016)
Train: 261 [1150/1251 ( 92%)]  Loss: 3.037 (2.97)  Time: 0.662s, 1547.02/s  (0.658s, 1555.33/s)  LR: 4.403e-05  Data: 0.013 (0.016)
Train: 261 [1200/1251 ( 96%)]  Loss: 2.799 (2.96)  Time: 0.654s, 1565.75/s  (0.658s, 1555.22/s)  LR: 4.395e-05  Data: 0.013 (0.015)
Train: 261 [1250/1251 (100%)]  Loss: 3.002 (2.96)  Time: 0.649s, 1578.93/s  (0.659s, 1554.84/s)  LR: 4.387e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.832 (2.832)  Loss:  0.3679 (0.3679)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.8281 (98.8281)
Test: [  48/48]  Time: 0.162 (0.317)  Loss:  0.4917 (0.7983)  Acc@1: 87.6179 (81.3220)  Acc@5: 98.2311 (95.6520)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-261.pth.tar', 81.32200005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-259.pth.tar', 81.18400000244141)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-260.pth.tar', 81.136000078125)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-256.pth.tar', 81.11599997314453)

Train: 262 [   0/1251 (  0%)]  Loss: 2.861 (2.86)  Time: 3.194s,  320.57/s  (3.194s,  320.57/s)  LR: 4.387e-05  Data: 1.750 (1.750)
Train: 262 [  50/1251 (  4%)]  Loss: 2.910 (2.89)  Time: 0.643s, 1593.61/s  (0.675s, 1516.46/s)  LR: 4.379e-05  Data: 0.014 (0.048)
Train: 262 [ 100/1251 (  8%)]  Loss: 3.008 (2.93)  Time: 0.656s, 1560.81/s  (0.664s, 1542.21/s)  LR: 4.371e-05  Data: 0.013 (0.031)
Train: 262 [ 150/1251 ( 12%)]  Loss: 2.506 (2.82)  Time: 0.646s, 1586.24/s  (0.661s, 1548.38/s)  LR: 4.363e-05  Data: 0.013 (0.025)
Train: 262 [ 200/1251 ( 16%)]  Loss: 2.888 (2.83)  Time: 0.654s, 1565.93/s  (0.660s, 1551.16/s)  LR: 4.355e-05  Data: 0.013 (0.022)
Train: 262 [ 250/1251 ( 20%)]  Loss: 2.985 (2.86)  Time: 0.660s, 1552.43/s  (0.659s, 1552.84/s)  LR: 4.347e-05  Data: 0.013 (0.021)
Train: 262 [ 300/1251 ( 24%)]  Loss: 3.153 (2.90)  Time: 0.656s, 1561.38/s  (0.659s, 1553.82/s)  LR: 4.339e-05  Data: 0.013 (0.019)
Train: 262 [ 350/1251 ( 28%)]  Loss: 2.990 (2.91)  Time: 0.659s, 1554.68/s  (0.659s, 1554.33/s)  LR: 4.331e-05  Data: 0.013 (0.019)
Train: 262 [ 400/1251 ( 32%)]  Loss: 2.845 (2.91)  Time: 0.662s, 1547.50/s  (0.659s, 1554.00/s)  LR: 4.323e-05  Data: 0.013 (0.018)
Train: 262 [ 450/1251 ( 36%)]  Loss: 2.952 (2.91)  Time: 0.669s, 1530.43/s  (0.659s, 1553.81/s)  LR: 4.315e-05  Data: 0.013 (0.017)
Train: 262 [ 500/1251 ( 40%)]  Loss: 2.943 (2.91)  Time: 0.653s, 1567.98/s  (0.659s, 1554.08/s)  LR: 4.307e-05  Data: 0.014 (0.017)
Train: 262 [ 550/1251 ( 44%)]  Loss: 2.948 (2.92)  Time: 0.658s, 1557.15/s  (0.659s, 1554.28/s)  LR: 4.299e-05  Data: 0.014 (0.017)
Train: 262 [ 600/1251 ( 48%)]  Loss: 3.169 (2.94)  Time: 0.660s, 1552.27/s  (0.659s, 1554.03/s)  LR: 4.291e-05  Data: 0.013 (0.017)
Train: 262 [ 650/1251 ( 52%)]  Loss: 3.023 (2.94)  Time: 0.667s, 1534.90/s  (0.659s, 1553.98/s)  LR: 4.283e-05  Data: 0.016 (0.016)
Train: 262 [ 700/1251 ( 56%)]  Loss: 2.918 (2.94)  Time: 0.664s, 1543.03/s  (0.659s, 1554.02/s)  LR: 4.275e-05  Data: 0.014 (0.016)
Train: 262 [ 750/1251 ( 60%)]  Loss: 2.951 (2.94)  Time: 0.660s, 1552.68/s  (0.659s, 1554.05/s)  LR: 4.267e-05  Data: 0.014 (0.016)
Train: 262 [ 800/1251 ( 64%)]  Loss: 2.923 (2.94)  Time: 0.663s, 1543.33/s  (0.659s, 1554.19/s)  LR: 4.259e-05  Data: 0.013 (0.016)
Train: 262 [ 850/1251 ( 68%)]  Loss: 2.891 (2.94)  Time: 0.659s, 1554.12/s  (0.659s, 1553.99/s)  LR: 4.251e-05  Data: 0.012 (0.016)
Train: 262 [ 900/1251 ( 72%)]  Loss: 3.055 (2.94)  Time: 0.668s, 1532.45/s  (0.659s, 1553.74/s)  LR: 4.243e-05  Data: 0.014 (0.016)
Train: 262 [ 950/1251 ( 76%)]  Loss: 2.954 (2.94)  Time: 0.654s, 1566.20/s  (0.659s, 1553.79/s)  LR: 4.235e-05  Data: 0.019 (0.015)
Train: 262 [1000/1251 ( 80%)]  Loss: 2.896 (2.94)  Time: 0.669s, 1530.57/s  (0.659s, 1553.54/s)  LR: 4.227e-05  Data: 0.013 (0.015)
Train: 262 [1050/1251 ( 84%)]  Loss: 3.053 (2.95)  Time: 0.668s, 1531.97/s  (0.659s, 1553.04/s)  LR: 4.219e-05  Data: 0.016 (0.015)
Train: 262 [1100/1251 ( 88%)]  Loss: 3.028 (2.95)  Time: 0.651s, 1572.53/s  (0.660s, 1552.57/s)  LR: 4.212e-05  Data: 0.013 (0.015)
Train: 262 [1150/1251 ( 92%)]  Loss: 2.546 (2.93)  Time: 0.669s, 1530.52/s  (0.660s, 1552.49/s)  LR: 4.204e-05  Data: 0.013 (0.015)
Train: 262 [1200/1251 ( 96%)]  Loss: 2.909 (2.93)  Time: 0.662s, 1545.70/s  (0.660s, 1552.10/s)  LR: 4.196e-05  Data: 0.014 (0.015)
Train: 262 [1250/1251 (100%)]  Loss: 2.834 (2.93)  Time: 0.653s, 1568.47/s  (0.660s, 1552.03/s)  LR: 4.188e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.830 (2.830)  Loss:  0.3730 (0.3730)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.161 (0.316)  Loss:  0.4878 (0.8040)  Acc@1: 87.8538 (81.2800)  Acc@5: 98.2311 (95.6660)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-261.pth.tar', 81.32200005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-262.pth.tar', 81.2800001538086)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-259.pth.tar', 81.18400000244141)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-260.pth.tar', 81.136000078125)

Train: 263 [   0/1251 (  0%)]  Loss: 2.893 (2.89)  Time: 3.378s,  303.17/s  (3.378s,  303.17/s)  LR: 4.188e-05  Data: 1.719 (1.719)
Train: 263 [  50/1251 (  4%)]  Loss: 2.911 (2.90)  Time: 0.649s, 1577.78/s  (0.683s, 1498.62/s)  LR: 4.180e-05  Data: 0.014 (0.047)
Train: 263 [ 100/1251 (  8%)]  Loss: 3.052 (2.95)  Time: 0.667s, 1535.71/s  (0.670s, 1528.90/s)  LR: 4.172e-05  Data: 0.014 (0.031)
Train: 263 [ 150/1251 ( 12%)]  Loss: 3.057 (2.98)  Time: 0.657s, 1559.36/s  (0.666s, 1536.80/s)  LR: 4.164e-05  Data: 0.014 (0.025)
Train: 263 [ 200/1251 ( 16%)]  Loss: 3.013 (2.99)  Time: 0.661s, 1548.80/s  (0.665s, 1538.98/s)  LR: 4.156e-05  Data: 0.012 (0.022)
Train: 263 [ 250/1251 ( 20%)]  Loss: 3.118 (3.01)  Time: 0.665s, 1539.21/s  (0.665s, 1540.09/s)  LR: 4.149e-05  Data: 0.013 (0.020)
Train: 263 [ 300/1251 ( 24%)]  Loss: 2.806 (2.98)  Time: 0.663s, 1545.06/s  (0.664s, 1541.40/s)  LR: 4.141e-05  Data: 0.013 (0.019)
Train: 263 [ 350/1251 ( 28%)]  Loss: 2.699 (2.94)  Time: 0.663s, 1545.42/s  (0.664s, 1542.33/s)  LR: 4.133e-05  Data: 0.014 (0.019)
Train: 263 [ 400/1251 ( 32%)]  Loss: 3.015 (2.95)  Time: 0.661s, 1549.30/s  (0.664s, 1542.53/s)  LR: 4.125e-05  Data: 0.013 (0.018)
Train: 263 [ 450/1251 ( 36%)]  Loss: 2.777 (2.93)  Time: 0.668s, 1531.82/s  (0.664s, 1543.16/s)  LR: 4.117e-05  Data: 0.013 (0.017)
Train: 263 [ 500/1251 ( 40%)]  Loss: 2.989 (2.94)  Time: 0.658s, 1555.34/s  (0.664s, 1542.98/s)  LR: 4.110e-05  Data: 0.013 (0.017)
Train: 263 [ 550/1251 ( 44%)]  Loss: 2.957 (2.94)  Time: 0.659s, 1554.59/s  (0.664s, 1542.80/s)  LR: 4.102e-05  Data: 0.012 (0.017)
Train: 263 [ 600/1251 ( 48%)]  Loss: 3.110 (2.95)  Time: 0.663s, 1545.14/s  (0.664s, 1543.08/s)  LR: 4.094e-05  Data: 0.013 (0.016)
Train: 263 [ 650/1251 ( 52%)]  Loss: 3.124 (2.97)  Time: 0.660s, 1550.54/s  (0.664s, 1543.22/s)  LR: 4.086e-05  Data: 0.013 (0.016)
Train: 263 [ 700/1251 ( 56%)]  Loss: 2.990 (2.97)  Time: 0.667s, 1534.38/s  (0.663s, 1543.98/s)  LR: 4.078e-05  Data: 0.014 (0.016)
Train: 263 [ 750/1251 ( 60%)]  Loss: 2.622 (2.95)  Time: 0.658s, 1557.20/s  (0.663s, 1544.53/s)  LR: 4.071e-05  Data: 0.013 (0.016)
Train: 263 [ 800/1251 ( 64%)]  Loss: 2.927 (2.94)  Time: 0.662s, 1546.26/s  (0.663s, 1545.00/s)  LR: 4.063e-05  Data: 0.012 (0.016)
Train: 263 [ 850/1251 ( 68%)]  Loss: 2.862 (2.94)  Time: 0.660s, 1551.35/s  (0.663s, 1545.41/s)  LR: 4.055e-05  Data: 0.013 (0.016)
Train: 263 [ 900/1251 ( 72%)]  Loss: 2.885 (2.94)  Time: 0.658s, 1556.88/s  (0.662s, 1545.75/s)  LR: 4.047e-05  Data: 0.013 (0.015)
Train: 263 [ 950/1251 ( 76%)]  Loss: 2.909 (2.94)  Time: 0.658s, 1556.66/s  (0.662s, 1546.02/s)  LR: 4.040e-05  Data: 0.013 (0.015)
Train: 263 [1000/1251 ( 80%)]  Loss: 3.227 (2.95)  Time: 0.671s, 1527.07/s  (0.662s, 1546.22/s)  LR: 4.032e-05  Data: 0.013 (0.015)
Train: 263 [1050/1251 ( 84%)]  Loss: 3.111 (2.96)  Time: 0.660s, 1550.95/s  (0.662s, 1546.43/s)  LR: 4.024e-05  Data: 0.013 (0.015)
Train: 263 [1100/1251 ( 88%)]  Loss: 2.868 (2.95)  Time: 0.662s, 1547.96/s  (0.662s, 1546.66/s)  LR: 4.017e-05  Data: 0.012 (0.015)
Train: 263 [1150/1251 ( 92%)]  Loss: 2.865 (2.95)  Time: 0.668s, 1533.85/s  (0.662s, 1546.84/s)  LR: 4.009e-05  Data: 0.015 (0.015)
Train: 263 [1200/1251 ( 96%)]  Loss: 3.013 (2.95)  Time: 0.659s, 1553.18/s  (0.662s, 1547.06/s)  LR: 4.001e-05  Data: 0.014 (0.015)
Train: 263 [1250/1251 (100%)]  Loss: 3.073 (2.96)  Time: 0.645s, 1587.35/s  (0.662s, 1547.31/s)  LR: 3.994e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.994 (2.994)  Loss:  0.3669 (0.3669)  Acc@1: 93.2617 (93.2617)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.161 (0.323)  Loss:  0.4858 (0.7972)  Acc@1: 87.3821 (81.3920)  Acc@5: 98.2311 (95.6000)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-263.pth.tar', 81.39200020751953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-261.pth.tar', 81.32200005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-262.pth.tar', 81.2800001538086)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-259.pth.tar', 81.18400000244141)

Train: 264 [   0/1251 (  0%)]  Loss: 2.871 (2.87)  Time: 2.998s,  341.52/s  (2.998s,  341.52/s)  LR: 3.993e-05  Data: 1.653 (1.653)
Train: 264 [  50/1251 (  4%)]  Loss: 3.076 (2.97)  Time: 0.651s, 1571.98/s  (0.675s, 1517.40/s)  LR: 3.986e-05  Data: 0.013 (0.046)
Train: 264 [ 100/1251 (  8%)]  Loss: 2.923 (2.96)  Time: 0.667s, 1535.78/s  (0.666s, 1537.66/s)  LR: 3.978e-05  Data: 0.013 (0.030)
Train: 264 [ 150/1251 ( 12%)]  Loss: 2.918 (2.95)  Time: 0.653s, 1568.30/s  (0.664s, 1541.90/s)  LR: 3.971e-05  Data: 0.013 (0.025)
Train: 264 [ 200/1251 ( 16%)]  Loss: 3.079 (2.97)  Time: 0.655s, 1563.47/s  (0.664s, 1543.25/s)  LR: 3.963e-05  Data: 0.016 (0.022)
Train: 264 [ 250/1251 ( 20%)]  Loss: 3.136 (3.00)  Time: 0.661s, 1548.24/s  (0.663s, 1543.76/s)  LR: 3.955e-05  Data: 0.013 (0.020)
Train: 264 [ 300/1251 ( 24%)]  Loss: 2.793 (2.97)  Time: 0.667s, 1536.34/s  (0.663s, 1544.56/s)  LR: 3.948e-05  Data: 0.014 (0.019)
Train: 264 [ 350/1251 ( 28%)]  Loss: 2.802 (2.95)  Time: 0.659s, 1553.76/s  (0.663s, 1545.36/s)  LR: 3.940e-05  Data: 0.013 (0.018)
Train: 264 [ 400/1251 ( 32%)]  Loss: 2.852 (2.94)  Time: 0.653s, 1569.05/s  (0.662s, 1545.70/s)  LR: 3.932e-05  Data: 0.012 (0.018)
Train: 264 [ 450/1251 ( 36%)]  Loss: 2.809 (2.93)  Time: 0.666s, 1538.68/s  (0.663s, 1545.45/s)  LR: 3.925e-05  Data: 0.014 (0.017)
Train: 264 [ 500/1251 ( 40%)]  Loss: 2.443 (2.88)  Time: 0.660s, 1550.73/s  (0.662s, 1546.50/s)  LR: 3.917e-05  Data: 0.013 (0.017)
Train: 264 [ 550/1251 ( 44%)]  Loss: 2.758 (2.87)  Time: 0.662s, 1546.91/s  (0.662s, 1547.09/s)  LR: 3.910e-05  Data: 0.014 (0.017)
Train: 264 [ 600/1251 ( 48%)]  Loss: 3.202 (2.90)  Time: 0.662s, 1546.57/s  (0.662s, 1547.38/s)  LR: 3.902e-05  Data: 0.014 (0.016)
Train: 264 [ 650/1251 ( 52%)]  Loss: 2.935 (2.90)  Time: 0.648s, 1580.97/s  (0.662s, 1547.62/s)  LR: 3.895e-05  Data: 0.013 (0.016)
Train: 264 [ 700/1251 ( 56%)]  Loss: 3.108 (2.91)  Time: 0.660s, 1552.12/s  (0.662s, 1547.91/s)  LR: 3.887e-05  Data: 0.018 (0.016)
Train: 264 [ 750/1251 ( 60%)]  Loss: 2.964 (2.92)  Time: 0.661s, 1549.85/s  (0.661s, 1548.23/s)  LR: 3.879e-05  Data: 0.013 (0.016)
Train: 264 [ 800/1251 ( 64%)]  Loss: 3.037 (2.92)  Time: 0.662s, 1546.33/s  (0.661s, 1548.68/s)  LR: 3.872e-05  Data: 0.013 (0.016)
Train: 264 [ 850/1251 ( 68%)]  Loss: 3.058 (2.93)  Time: 0.658s, 1555.85/s  (0.661s, 1548.85/s)  LR: 3.864e-05  Data: 0.013 (0.016)
Train: 264 [ 900/1251 ( 72%)]  Loss: 3.039 (2.94)  Time: 0.668s, 1532.68/s  (0.661s, 1548.84/s)  LR: 3.857e-05  Data: 0.013 (0.015)
Train: 264 [ 950/1251 ( 76%)]  Loss: 2.900 (2.94)  Time: 0.659s, 1553.33/s  (0.661s, 1549.02/s)  LR: 3.849e-05  Data: 0.014 (0.015)
Train: 264 [1000/1251 ( 80%)]  Loss: 2.955 (2.94)  Time: 0.667s, 1534.50/s  (0.661s, 1549.24/s)  LR: 3.842e-05  Data: 0.014 (0.015)
Train: 264 [1050/1251 ( 84%)]  Loss: 3.038 (2.94)  Time: 0.662s, 1546.14/s  (0.661s, 1549.54/s)  LR: 3.834e-05  Data: 0.017 (0.015)
Train: 264 [1100/1251 ( 88%)]  Loss: 2.826 (2.94)  Time: 0.663s, 1544.71/s  (0.661s, 1549.45/s)  LR: 3.827e-05  Data: 0.014 (0.015)
Train: 264 [1150/1251 ( 92%)]  Loss: 3.092 (2.94)  Time: 0.659s, 1555.02/s  (0.661s, 1549.41/s)  LR: 3.819e-05  Data: 0.012 (0.015)
Train: 264 [1200/1251 ( 96%)]  Loss: 2.734 (2.93)  Time: 0.670s, 1528.34/s  (0.661s, 1549.16/s)  LR: 3.812e-05  Data: 0.014 (0.015)
Train: 264 [1250/1251 (100%)]  Loss: 3.149 (2.94)  Time: 0.646s, 1584.36/s  (0.661s, 1549.29/s)  LR: 3.804e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.825 (2.825)  Loss:  0.3809 (0.3809)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.166 (0.316)  Loss:  0.4924 (0.8060)  Acc@1: 87.6179 (81.2720)  Acc@5: 98.8207 (95.6080)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-263.pth.tar', 81.39200020751953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-261.pth.tar', 81.32200005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-262.pth.tar', 81.2800001538086)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-264.pth.tar', 81.27200005126953)

Train: 265 [   0/1251 (  0%)]  Loss: 3.201 (3.20)  Time: 3.431s,  298.43/s  (3.431s,  298.43/s)  LR: 3.804e-05  Data: 1.688 (1.688)
Train: 265 [  50/1251 (  4%)]  Loss: 2.711 (2.96)  Time: 0.647s, 1582.48/s  (0.683s, 1499.70/s)  LR: 3.797e-05  Data: 0.013 (0.047)
Train: 265 [ 100/1251 (  8%)]  Loss: 2.909 (2.94)  Time: 0.659s, 1552.95/s  (0.667s, 1535.28/s)  LR: 3.789e-05  Data: 0.013 (0.031)
Train: 265 [ 150/1251 ( 12%)]  Loss: 3.002 (2.96)  Time: 0.668s, 1533.65/s  (0.664s, 1542.27/s)  LR: 3.782e-05  Data: 0.013 (0.025)
Train: 265 [ 200/1251 ( 16%)]  Loss: 2.781 (2.92)  Time: 0.651s, 1571.90/s  (0.663s, 1544.93/s)  LR: 3.774e-05  Data: 0.014 (0.022)
Train: 265 [ 250/1251 ( 20%)]  Loss: 2.968 (2.93)  Time: 0.665s, 1539.35/s  (0.662s, 1546.39/s)  LR: 3.767e-05  Data: 0.015 (0.020)
Train: 265 [ 300/1251 ( 24%)]  Loss: 2.944 (2.93)  Time: 0.661s, 1549.10/s  (0.662s, 1547.41/s)  LR: 3.760e-05  Data: 0.013 (0.019)
Train: 265 [ 350/1251 ( 28%)]  Loss: 2.956 (2.93)  Time: 0.659s, 1553.44/s  (0.662s, 1547.76/s)  LR: 3.752e-05  Data: 0.013 (0.018)
Train: 265 [ 400/1251 ( 32%)]  Loss: 2.821 (2.92)  Time: 0.665s, 1539.55/s  (0.661s, 1548.16/s)  LR: 3.745e-05  Data: 0.014 (0.018)
Train: 265 [ 450/1251 ( 36%)]  Loss: 3.007 (2.93)  Time: 0.661s, 1549.92/s  (0.661s, 1548.74/s)  LR: 3.737e-05  Data: 0.014 (0.017)
Train: 265 [ 500/1251 ( 40%)]  Loss: 2.906 (2.93)  Time: 0.663s, 1544.61/s  (0.661s, 1548.94/s)  LR: 3.730e-05  Data: 0.013 (0.017)
Train: 265 [ 550/1251 ( 44%)]  Loss: 2.912 (2.93)  Time: 0.660s, 1552.21/s  (0.661s, 1549.81/s)  LR: 3.723e-05  Data: 0.013 (0.017)
Train: 265 [ 600/1251 ( 48%)]  Loss: 2.924 (2.93)  Time: 0.657s, 1559.64/s  (0.660s, 1550.59/s)  LR: 3.715e-05  Data: 0.013 (0.016)
Train: 265 [ 650/1251 ( 52%)]  Loss: 3.020 (2.93)  Time: 0.662s, 1547.27/s  (0.660s, 1551.41/s)  LR: 3.708e-05  Data: 0.013 (0.016)
Train: 265 [ 700/1251 ( 56%)]  Loss: 2.943 (2.93)  Time: 0.652s, 1570.85/s  (0.660s, 1551.97/s)  LR: 3.701e-05  Data: 0.013 (0.016)
Train: 265 [ 750/1251 ( 60%)]  Loss: 2.522 (2.91)  Time: 0.658s, 1555.86/s  (0.660s, 1552.52/s)  LR: 3.693e-05  Data: 0.013 (0.016)
Train: 265 [ 800/1251 ( 64%)]  Loss: 2.943 (2.91)  Time: 0.655s, 1564.42/s  (0.659s, 1553.05/s)  LR: 3.686e-05  Data: 0.014 (0.016)
Train: 265 [ 850/1251 ( 68%)]  Loss: 3.150 (2.92)  Time: 0.656s, 1559.90/s  (0.659s, 1553.38/s)  LR: 3.679e-05  Data: 0.014 (0.016)
Train: 265 [ 900/1251 ( 72%)]  Loss: 2.889 (2.92)  Time: 0.655s, 1562.76/s  (0.659s, 1553.86/s)  LR: 3.671e-05  Data: 0.013 (0.016)
Train: 265 [ 950/1251 ( 76%)]  Loss: 2.994 (2.93)  Time: 0.668s, 1532.19/s  (0.659s, 1553.73/s)  LR: 3.664e-05  Data: 0.014 (0.016)
Train: 265 [1000/1251 ( 80%)]  Loss: 3.194 (2.94)  Time: 0.660s, 1551.41/s  (0.659s, 1553.84/s)  LR: 3.657e-05  Data: 0.017 (0.015)
Train: 265 [1050/1251 ( 84%)]  Loss: 2.867 (2.93)  Time: 0.655s, 1562.17/s  (0.659s, 1553.93/s)  LR: 3.649e-05  Data: 0.013 (0.015)
Train: 265 [1100/1251 ( 88%)]  Loss: 2.878 (2.93)  Time: 0.663s, 1543.98/s  (0.659s, 1553.96/s)  LR: 3.642e-05  Data: 0.017 (0.015)
Train: 265 [1150/1251 ( 92%)]  Loss: 3.114 (2.94)  Time: 0.662s, 1546.18/s  (0.659s, 1554.04/s)  LR: 3.635e-05  Data: 0.012 (0.015)
Train: 265 [1200/1251 ( 96%)]  Loss: 2.857 (2.94)  Time: 0.663s, 1544.48/s  (0.659s, 1554.08/s)  LR: 3.627e-05  Data: 0.014 (0.015)
Train: 265 [1250/1251 (100%)]  Loss: 2.947 (2.94)  Time: 0.658s, 1556.49/s  (0.659s, 1554.10/s)  LR: 3.620e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.986 (2.986)  Loss:  0.3723 (0.3723)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.7305 (98.7305)
Test: [  48/48]  Time: 0.162 (0.324)  Loss:  0.4993 (0.8013)  Acc@1: 87.6179 (81.3840)  Acc@5: 98.3491 (95.5940)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-263.pth.tar', 81.39200020751953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-265.pth.tar', 81.38400005126954)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-261.pth.tar', 81.32200005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-262.pth.tar', 81.2800001538086)

Train: 266 [   0/1251 (  0%)]  Loss: 2.749 (2.75)  Time: 2.961s,  345.82/s  (2.961s,  345.82/s)  LR: 3.620e-05  Data: 1.592 (1.592)
Train: 266 [  50/1251 (  4%)]  Loss: 3.083 (2.92)  Time: 0.640s, 1599.84/s  (0.673s, 1521.00/s)  LR: 3.613e-05  Data: 0.017 (0.045)
Train: 266 [ 100/1251 (  8%)]  Loss: 3.248 (3.03)  Time: 0.647s, 1582.12/s  (0.661s, 1548.92/s)  LR: 3.606e-05  Data: 0.014 (0.030)
Train: 266 [ 150/1251 ( 12%)]  Loss: 3.155 (3.06)  Time: 0.654s, 1565.51/s  (0.659s, 1553.06/s)  LR: 3.598e-05  Data: 0.014 (0.024)
Train: 266 [ 200/1251 ( 16%)]  Loss: 2.801 (3.01)  Time: 0.663s, 1543.83/s  (0.659s, 1553.30/s)  LR: 3.591e-05  Data: 0.013 (0.022)
Train: 266 [ 250/1251 ( 20%)]  Loss: 2.839 (2.98)  Time: 0.657s, 1558.13/s  (0.659s, 1553.37/s)  LR: 3.584e-05  Data: 0.013 (0.020)
Train: 266 [ 300/1251 ( 24%)]  Loss: 3.175 (3.01)  Time: 0.649s, 1578.59/s  (0.659s, 1554.42/s)  LR: 3.577e-05  Data: 0.016 (0.019)
Train: 266 [ 350/1251 ( 28%)]  Loss: 3.009 (3.01)  Time: 0.669s, 1531.21/s  (0.659s, 1554.86/s)  LR: 3.569e-05  Data: 0.014 (0.018)
Train: 266 [ 400/1251 ( 32%)]  Loss: 3.048 (3.01)  Time: 0.657s, 1559.08/s  (0.659s, 1554.71/s)  LR: 3.562e-05  Data: 0.014 (0.018)
Train: 266 [ 450/1251 ( 36%)]  Loss: 2.866 (3.00)  Time: 0.671s, 1526.51/s  (0.659s, 1554.99/s)  LR: 3.555e-05  Data: 0.013 (0.017)
Train: 266 [ 500/1251 ( 40%)]  Loss: 2.736 (2.97)  Time: 0.660s, 1551.61/s  (0.658s, 1555.24/s)  LR: 3.548e-05  Data: 0.015 (0.017)
Train: 266 [ 550/1251 ( 44%)]  Loss: 3.179 (2.99)  Time: 0.651s, 1572.50/s  (0.658s, 1555.66/s)  LR: 3.541e-05  Data: 0.013 (0.017)
Train: 266 [ 600/1251 ( 48%)]  Loss: 2.925 (2.99)  Time: 0.649s, 1577.32/s  (0.658s, 1556.05/s)  LR: 3.534e-05  Data: 0.013 (0.016)
Train: 266 [ 650/1251 ( 52%)]  Loss: 3.003 (2.99)  Time: 0.653s, 1567.71/s  (0.658s, 1556.41/s)  LR: 3.526e-05  Data: 0.014 (0.016)
Train: 266 [ 700/1251 ( 56%)]  Loss: 2.946 (2.98)  Time: 0.667s, 1535.46/s  (0.658s, 1556.07/s)  LR: 3.519e-05  Data: 0.014 (0.016)
Train: 266 [ 750/1251 ( 60%)]  Loss: 2.674 (2.96)  Time: 0.652s, 1570.23/s  (0.658s, 1555.87/s)  LR: 3.512e-05  Data: 0.012 (0.016)
Train: 266 [ 800/1251 ( 64%)]  Loss: 2.850 (2.96)  Time: 0.651s, 1572.31/s  (0.658s, 1555.71/s)  LR: 3.505e-05  Data: 0.013 (0.016)
Train: 266 [ 850/1251 ( 68%)]  Loss: 3.038 (2.96)  Time: 0.663s, 1543.58/s  (0.658s, 1555.46/s)  LR: 3.498e-05  Data: 0.013 (0.016)
Train: 266 [ 900/1251 ( 72%)]  Loss: 3.021 (2.97)  Time: 0.658s, 1556.61/s  (0.658s, 1555.28/s)  LR: 3.491e-05  Data: 0.014 (0.015)
Train: 266 [ 950/1251 ( 76%)]  Loss: 2.907 (2.96)  Time: 0.646s, 1584.96/s  (0.658s, 1555.62/s)  LR: 3.484e-05  Data: 0.016 (0.015)
Train: 266 [1000/1251 ( 80%)]  Loss: 3.241 (2.98)  Time: 0.652s, 1570.83/s  (0.658s, 1555.97/s)  LR: 3.477e-05  Data: 0.014 (0.015)
Train: 266 [1050/1251 ( 84%)]  Loss: 2.945 (2.97)  Time: 0.656s, 1561.60/s  (0.658s, 1556.11/s)  LR: 3.469e-05  Data: 0.013 (0.015)
Train: 266 [1100/1251 ( 88%)]  Loss: 2.888 (2.97)  Time: 0.657s, 1558.97/s  (0.658s, 1556.12/s)  LR: 3.462e-05  Data: 0.013 (0.015)
Train: 266 [1150/1251 ( 92%)]  Loss: 2.945 (2.97)  Time: 0.657s, 1559.60/s  (0.658s, 1556.14/s)  LR: 3.455e-05  Data: 0.013 (0.015)
Train: 266 [1200/1251 ( 96%)]  Loss: 2.814 (2.96)  Time: 0.659s, 1553.21/s  (0.658s, 1556.26/s)  LR: 3.448e-05  Data: 0.012 (0.015)
Train: 266 [1250/1251 (100%)]  Loss: 3.018 (2.97)  Time: 0.640s, 1599.63/s  (0.658s, 1556.52/s)  LR: 3.441e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.890 (2.890)  Loss:  0.3770 (0.3770)  Acc@1: 92.6758 (92.6758)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.320)  Loss:  0.4998 (0.8038)  Acc@1: 87.3821 (81.2700)  Acc@5: 98.4670 (95.6340)
Train: 267 [   0/1251 (  0%)]  Loss: 2.787 (2.79)  Time: 3.391s,  301.98/s  (3.391s,  301.98/s)  LR: 3.441e-05  Data: 1.610 (1.610)
Train: 267 [  50/1251 (  4%)]  Loss: 2.964 (2.88)  Time: 0.640s, 1600.81/s  (0.680s, 1505.44/s)  LR: 3.434e-05  Data: 0.013 (0.045)
Train: 267 [ 100/1251 (  8%)]  Loss: 3.151 (2.97)  Time: 0.657s, 1558.45/s  (0.665s, 1539.61/s)  LR: 3.427e-05  Data: 0.014 (0.029)
Train: 267 [ 150/1251 ( 12%)]  Loss: 2.869 (2.94)  Time: 0.656s, 1560.97/s  (0.662s, 1547.63/s)  LR: 3.420e-05  Data: 0.016 (0.024)
Train: 267 [ 200/1251 ( 16%)]  Loss: 2.999 (2.95)  Time: 0.659s, 1554.38/s  (0.661s, 1549.12/s)  LR: 3.413e-05  Data: 0.013 (0.022)
Train: 267 [ 250/1251 ( 20%)]  Loss: 3.012 (2.96)  Time: 0.664s, 1542.89/s  (0.661s, 1549.32/s)  LR: 3.406e-05  Data: 0.017 (0.020)
Train: 267 [ 300/1251 ( 24%)]  Loss: 3.148 (2.99)  Time: 0.657s, 1559.08/s  (0.660s, 1550.53/s)  LR: 3.399e-05  Data: 0.016 (0.019)
Train: 267 [ 350/1251 ( 28%)]  Loss: 3.035 (3.00)  Time: 0.659s, 1554.86/s  (0.660s, 1550.98/s)  LR: 3.392e-05  Data: 0.014 (0.018)
Train: 267 [ 400/1251 ( 32%)]  Loss: 2.837 (2.98)  Time: 0.665s, 1540.00/s  (0.660s, 1550.81/s)  LR: 3.385e-05  Data: 0.013 (0.018)
Train: 267 [ 450/1251 ( 36%)]  Loss: 2.886 (2.97)  Time: 0.653s, 1567.48/s  (0.660s, 1551.06/s)  LR: 3.378e-05  Data: 0.016 (0.017)
Train: 267 [ 500/1251 ( 40%)]  Loss: 3.067 (2.98)  Time: 0.653s, 1567.52/s  (0.660s, 1551.10/s)  LR: 3.371e-05  Data: 0.013 (0.017)
Train: 267 [ 550/1251 ( 44%)]  Loss: 2.768 (2.96)  Time: 0.660s, 1551.09/s  (0.660s, 1551.20/s)  LR: 3.364e-05  Data: 0.013 (0.016)
Train: 267 [ 600/1251 ( 48%)]  Loss: 2.950 (2.96)  Time: 0.657s, 1559.41/s  (0.660s, 1551.08/s)  LR: 3.357e-05  Data: 0.013 (0.016)
Train: 267 [ 650/1251 ( 52%)]  Loss: 3.106 (2.97)  Time: 0.656s, 1561.80/s  (0.660s, 1551.09/s)  LR: 3.350e-05  Data: 0.014 (0.016)
Train: 267 [ 700/1251 ( 56%)]  Loss: 2.843 (2.96)  Time: 0.655s, 1563.04/s  (0.660s, 1551.01/s)  LR: 3.343e-05  Data: 0.016 (0.016)
Train: 267 [ 750/1251 ( 60%)]  Loss: 3.229 (2.98)  Time: 0.661s, 1548.47/s  (0.660s, 1551.04/s)  LR: 3.336e-05  Data: 0.014 (0.016)
Train: 267 [ 800/1251 ( 64%)]  Loss: 2.814 (2.97)  Time: 0.663s, 1544.57/s  (0.660s, 1550.89/s)  LR: 3.329e-05  Data: 0.013 (0.016)
Train: 267 [ 850/1251 ( 68%)]  Loss: 3.120 (2.98)  Time: 0.657s, 1559.28/s  (0.660s, 1550.85/s)  LR: 3.322e-05  Data: 0.013 (0.015)
Train: 267 [ 900/1251 ( 72%)]  Loss: 2.952 (2.98)  Time: 0.660s, 1550.70/s  (0.660s, 1550.90/s)  LR: 3.315e-05  Data: 0.013 (0.015)
Train: 267 [ 950/1251 ( 76%)]  Loss: 3.138 (2.98)  Time: 0.658s, 1557.08/s  (0.660s, 1550.90/s)  LR: 3.309e-05  Data: 0.013 (0.015)
Train: 267 [1000/1251 ( 80%)]  Loss: 3.148 (2.99)  Time: 0.662s, 1545.75/s  (0.660s, 1550.87/s)  LR: 3.302e-05  Data: 0.013 (0.015)
Train: 267 [1050/1251 ( 84%)]  Loss: 2.990 (2.99)  Time: 0.659s, 1554.54/s  (0.660s, 1550.85/s)  LR: 3.295e-05  Data: 0.013 (0.015)
Train: 267 [1100/1251 ( 88%)]  Loss: 3.020 (2.99)  Time: 0.662s, 1547.84/s  (0.660s, 1550.78/s)  LR: 3.288e-05  Data: 0.014 (0.015)
Train: 267 [1150/1251 ( 92%)]  Loss: 2.995 (2.99)  Time: 0.648s, 1581.36/s  (0.660s, 1550.82/s)  LR: 3.281e-05  Data: 0.013 (0.015)
Train: 267 [1200/1251 ( 96%)]  Loss: 2.561 (2.98)  Time: 0.659s, 1552.79/s  (0.660s, 1550.86/s)  LR: 3.274e-05  Data: 0.016 (0.015)
Train: 267 [1250/1251 (100%)]  Loss: 2.915 (2.97)  Time: 0.639s, 1601.87/s  (0.660s, 1551.09/s)  LR: 3.267e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.985 (2.985)  Loss:  0.3809 (0.3809)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.320)  Loss:  0.5112 (0.8013)  Acc@1: 87.1462 (81.4100)  Acc@5: 98.7028 (95.6560)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-267.pth.tar', 81.40999997558593)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-263.pth.tar', 81.39200020751953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-265.pth.tar', 81.38400005126954)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-261.pth.tar', 81.32200005126953)

Train: 268 [   0/1251 (  0%)]  Loss: 3.030 (3.03)  Time: 3.271s,  313.05/s  (3.271s,  313.05/s)  LR: 3.267e-05  Data: 1.803 (1.803)
Train: 268 [  50/1251 (  4%)]  Loss: 2.820 (2.93)  Time: 0.642s, 1595.46/s  (0.674s, 1519.27/s)  LR: 3.260e-05  Data: 0.014 (0.049)
Train: 268 [ 100/1251 (  8%)]  Loss: 2.941 (2.93)  Time: 0.648s, 1579.03/s  (0.662s, 1547.29/s)  LR: 3.253e-05  Data: 0.014 (0.031)
Train: 268 [ 150/1251 ( 12%)]  Loss: 2.714 (2.88)  Time: 0.655s, 1562.43/s  (0.659s, 1553.37/s)  LR: 3.247e-05  Data: 0.012 (0.025)
Train: 268 [ 200/1251 ( 16%)]  Loss: 2.952 (2.89)  Time: 0.659s, 1553.39/s  (0.658s, 1555.42/s)  LR: 3.240e-05  Data: 0.013 (0.023)
Train: 268 [ 250/1251 ( 20%)]  Loss: 2.980 (2.91)  Time: 0.651s, 1572.63/s  (0.658s, 1555.18/s)  LR: 3.233e-05  Data: 0.013 (0.021)
Train: 268 [ 300/1251 ( 24%)]  Loss: 2.933 (2.91)  Time: 0.661s, 1549.07/s  (0.658s, 1555.46/s)  LR: 3.226e-05  Data: 0.013 (0.020)
Train: 268 [ 350/1251 ( 28%)]  Loss: 3.091 (2.93)  Time: 0.650s, 1575.78/s  (0.658s, 1556.25/s)  LR: 3.219e-05  Data: 0.013 (0.019)
Train: 268 [ 400/1251 ( 32%)]  Loss: 2.905 (2.93)  Time: 0.652s, 1571.30/s  (0.658s, 1556.74/s)  LR: 3.213e-05  Data: 0.013 (0.018)
Train: 268 [ 450/1251 ( 36%)]  Loss: 2.987 (2.94)  Time: 0.665s, 1539.99/s  (0.658s, 1556.24/s)  LR: 3.206e-05  Data: 0.013 (0.018)
Train: 268 [ 500/1251 ( 40%)]  Loss: 2.622 (2.91)  Time: 0.651s, 1573.17/s  (0.658s, 1556.23/s)  LR: 3.199e-05  Data: 0.013 (0.017)
Train: 268 [ 550/1251 ( 44%)]  Loss: 3.237 (2.93)  Time: 0.656s, 1560.15/s  (0.658s, 1556.51/s)  LR: 3.192e-05  Data: 0.014 (0.017)
Train: 268 [ 600/1251 ( 48%)]  Loss: 3.141 (2.95)  Time: 0.659s, 1552.91/s  (0.658s, 1556.25/s)  LR: 3.186e-05  Data: 0.013 (0.017)
Train: 268 [ 650/1251 ( 52%)]  Loss: 3.157 (2.96)  Time: 0.653s, 1568.96/s  (0.658s, 1556.12/s)  LR: 3.179e-05  Data: 0.014 (0.016)
Train: 268 [ 700/1251 ( 56%)]  Loss: 2.863 (2.96)  Time: 0.659s, 1553.93/s  (0.658s, 1555.92/s)  LR: 3.172e-05  Data: 0.014 (0.016)
Train: 268 [ 750/1251 ( 60%)]  Loss: 2.687 (2.94)  Time: 0.654s, 1565.19/s  (0.658s, 1555.44/s)  LR: 3.165e-05  Data: 0.013 (0.016)
Train: 268 [ 800/1251 ( 64%)]  Loss: 2.856 (2.94)  Time: 0.658s, 1555.51/s  (0.659s, 1554.92/s)  LR: 3.159e-05  Data: 0.014 (0.016)
Train: 268 [ 850/1251 ( 68%)]  Loss: 3.007 (2.94)  Time: 0.661s, 1548.40/s  (0.659s, 1554.98/s)  LR: 3.152e-05  Data: 0.016 (0.016)
Train: 268 [ 900/1251 ( 72%)]  Loss: 2.698 (2.93)  Time: 0.665s, 1540.84/s  (0.659s, 1554.91/s)  LR: 3.145e-05  Data: 0.015 (0.016)
Train: 268 [ 950/1251 ( 76%)]  Loss: 3.021 (2.93)  Time: 0.662s, 1547.15/s  (0.659s, 1554.82/s)  LR: 3.139e-05  Data: 0.014 (0.016)
Train: 268 [1000/1251 ( 80%)]  Loss: 3.302 (2.95)  Time: 0.651s, 1573.04/s  (0.659s, 1554.89/s)  LR: 3.132e-05  Data: 0.014 (0.015)
Train: 268 [1050/1251 ( 84%)]  Loss: 3.321 (2.97)  Time: 0.655s, 1564.49/s  (0.658s, 1555.07/s)  LR: 3.125e-05  Data: 0.014 (0.015)
Train: 268 [1100/1251 ( 88%)]  Loss: 3.249 (2.98)  Time: 0.654s, 1565.30/s  (0.658s, 1555.20/s)  LR: 3.118e-05  Data: 0.015 (0.015)
Train: 268 [1150/1251 ( 92%)]  Loss: 3.069 (2.98)  Time: 0.662s, 1545.82/s  (0.658s, 1555.14/s)  LR: 3.112e-05  Data: 0.016 (0.015)
Train: 268 [1200/1251 ( 96%)]  Loss: 3.054 (2.99)  Time: 0.657s, 1558.54/s  (0.659s, 1555.00/s)  LR: 3.105e-05  Data: 0.017 (0.015)
Train: 268 [1250/1251 (100%)]  Loss: 3.086 (2.99)  Time: 0.648s, 1580.66/s  (0.659s, 1554.91/s)  LR: 3.099e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.882 (2.882)  Loss:  0.3867 (0.3867)  Acc@1: 92.3828 (92.3828)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.161 (0.323)  Loss:  0.5063 (0.7989)  Acc@1: 87.1462 (81.4100)  Acc@5: 98.5849 (95.7060)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-268.pth.tar', 81.41000010498047)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-267.pth.tar', 81.40999997558593)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-263.pth.tar', 81.39200020751953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-265.pth.tar', 81.38400005126954)

Train: 269 [   0/1251 (  0%)]  Loss: 3.031 (3.03)  Time: 3.158s,  324.26/s  (3.158s,  324.26/s)  LR: 3.098e-05  Data: 1.701 (1.701)
Train: 269 [  50/1251 (  4%)]  Loss: 3.211 (3.12)  Time: 0.645s, 1587.62/s  (0.677s, 1512.71/s)  LR: 3.092e-05  Data: 0.014 (0.046)
Train: 269 [ 100/1251 (  8%)]  Loss: 3.093 (3.11)  Time: 0.648s, 1580.34/s  (0.663s, 1543.86/s)  LR: 3.085e-05  Data: 0.013 (0.030)
Train: 269 [ 150/1251 ( 12%)]  Loss: 2.775 (3.03)  Time: 0.656s, 1560.62/s  (0.660s, 1551.09/s)  LR: 3.079e-05  Data: 0.014 (0.025)
Train: 269 [ 200/1251 ( 16%)]  Loss: 3.038 (3.03)  Time: 0.652s, 1570.51/s  (0.659s, 1554.29/s)  LR: 3.072e-05  Data: 0.014 (0.022)
Train: 269 [ 250/1251 ( 20%)]  Loss: 3.214 (3.06)  Time: 0.651s, 1573.68/s  (0.658s, 1557.21/s)  LR: 3.065e-05  Data: 0.014 (0.020)
Train: 269 [ 300/1251 ( 24%)]  Loss: 2.987 (3.05)  Time: 0.663s, 1545.08/s  (0.657s, 1558.48/s)  LR: 3.059e-05  Data: 0.013 (0.019)
Train: 269 [ 350/1251 ( 28%)]  Loss: 3.024 (3.05)  Time: 0.657s, 1557.76/s  (0.657s, 1559.35/s)  LR: 3.052e-05  Data: 0.014 (0.018)
Train: 269 [ 400/1251 ( 32%)]  Loss: 2.664 (3.00)  Time: 0.655s, 1562.32/s  (0.657s, 1559.73/s)  LR: 3.046e-05  Data: 0.013 (0.018)
Train: 269 [ 450/1251 ( 36%)]  Loss: 3.089 (3.01)  Time: 0.659s, 1554.72/s  (0.657s, 1559.69/s)  LR: 3.039e-05  Data: 0.013 (0.017)
Train: 269 [ 500/1251 ( 40%)]  Loss: 2.802 (2.99)  Time: 0.655s, 1564.33/s  (0.657s, 1559.31/s)  LR: 3.032e-05  Data: 0.013 (0.017)
Train: 269 [ 550/1251 ( 44%)]  Loss: 2.822 (2.98)  Time: 0.659s, 1554.07/s  (0.657s, 1558.97/s)  LR: 3.026e-05  Data: 0.013 (0.017)
Train: 269 [ 600/1251 ( 48%)]  Loss: 2.824 (2.97)  Time: 0.661s, 1549.89/s  (0.657s, 1558.64/s)  LR: 3.019e-05  Data: 0.014 (0.016)
Train: 269 [ 650/1251 ( 52%)]  Loss: 3.137 (2.98)  Time: 0.660s, 1551.20/s  (0.657s, 1558.25/s)  LR: 3.013e-05  Data: 0.013 (0.016)
Train: 269 [ 700/1251 ( 56%)]  Loss: 2.839 (2.97)  Time: 0.652s, 1570.20/s  (0.657s, 1557.74/s)  LR: 3.006e-05  Data: 0.013 (0.016)
Train: 269 [ 750/1251 ( 60%)]  Loss: 3.127 (2.98)  Time: 0.663s, 1545.20/s  (0.658s, 1557.27/s)  LR: 3.000e-05  Data: 0.013 (0.016)
Train: 269 [ 800/1251 ( 64%)]  Loss: 2.954 (2.98)  Time: 0.662s, 1547.62/s  (0.658s, 1556.63/s)  LR: 2.993e-05  Data: 0.013 (0.016)
Train: 269 [ 850/1251 ( 68%)]  Loss: 2.947 (2.98)  Time: 0.668s, 1531.93/s  (0.658s, 1556.26/s)  LR: 2.987e-05  Data: 0.017 (0.015)
Train: 269 [ 900/1251 ( 72%)]  Loss: 2.884 (2.97)  Time: 0.661s, 1549.95/s  (0.658s, 1555.86/s)  LR: 2.980e-05  Data: 0.013 (0.015)
Train: 269 [ 950/1251 ( 76%)]  Loss: 2.932 (2.97)  Time: 0.662s, 1547.12/s  (0.658s, 1555.70/s)  LR: 2.974e-05  Data: 0.013 (0.015)
Train: 269 [1000/1251 ( 80%)]  Loss: 2.950 (2.97)  Time: 0.663s, 1544.13/s  (0.658s, 1555.48/s)  LR: 2.967e-05  Data: 0.013 (0.015)
Train: 269 [1050/1251 ( 84%)]  Loss: 2.897 (2.97)  Time: 0.664s, 1543.25/s  (0.659s, 1555.00/s)  LR: 2.961e-05  Data: 0.013 (0.015)
Train: 269 [1100/1251 ( 88%)]  Loss: 3.243 (2.98)  Time: 0.662s, 1547.11/s  (0.659s, 1554.75/s)  LR: 2.954e-05  Data: 0.013 (0.015)
Train: 269 [1150/1251 ( 92%)]  Loss: 2.915 (2.97)  Time: 0.657s, 1558.51/s  (0.659s, 1554.64/s)  LR: 2.948e-05  Data: 0.015 (0.015)
Train: 269 [1200/1251 ( 96%)]  Loss: 3.061 (2.98)  Time: 0.668s, 1533.15/s  (0.659s, 1554.75/s)  LR: 2.941e-05  Data: 0.014 (0.015)
Train: 269 [1250/1251 (100%)]  Loss: 2.854 (2.97)  Time: 0.647s, 1583.47/s  (0.659s, 1554.92/s)  LR: 2.935e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.956 (2.956)  Loss:  0.3716 (0.3716)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.162 (0.318)  Loss:  0.4895 (0.7982)  Acc@1: 87.3821 (81.3840)  Acc@5: 98.9387 (95.6580)
Train: 270 [   0/1251 (  0%)]  Loss: 2.983 (2.98)  Time: 3.309s,  309.43/s  (3.309s,  309.43/s)  LR: 2.935e-05  Data: 1.952 (1.952)
Train: 270 [  50/1251 (  4%)]  Loss: 3.073 (3.03)  Time: 0.652s, 1571.53/s  (0.672s, 1524.66/s)  LR: 2.928e-05  Data: 0.015 (0.052)
Train: 270 [ 100/1251 (  8%)]  Loss: 3.025 (3.03)  Time: 0.653s, 1568.22/s  (0.662s, 1547.75/s)  LR: 2.922e-05  Data: 0.013 (0.033)
Train: 270 [ 150/1251 ( 12%)]  Loss: 3.129 (3.05)  Time: 0.655s, 1563.89/s  (0.660s, 1552.49/s)  LR: 2.916e-05  Data: 0.013 (0.027)
Train: 270 [ 200/1251 ( 16%)]  Loss: 3.056 (3.05)  Time: 0.653s, 1567.10/s  (0.659s, 1553.74/s)  LR: 2.909e-05  Data: 0.013 (0.023)
Train: 270 [ 250/1251 ( 20%)]  Loss: 2.787 (3.01)  Time: 0.662s, 1546.77/s  (0.659s, 1553.65/s)  LR: 2.903e-05  Data: 0.013 (0.021)
Train: 270 [ 300/1251 ( 24%)]  Loss: 2.813 (2.98)  Time: 0.657s, 1557.52/s  (0.659s, 1554.63/s)  LR: 2.896e-05  Data: 0.013 (0.020)
Train: 270 [ 350/1251 ( 28%)]  Loss: 3.065 (2.99)  Time: 0.658s, 1556.57/s  (0.659s, 1554.53/s)  LR: 2.890e-05  Data: 0.013 (0.019)
Train: 270 [ 400/1251 ( 32%)]  Loss: 3.057 (3.00)  Time: 0.653s, 1568.98/s  (0.659s, 1554.26/s)  LR: 2.884e-05  Data: 0.013 (0.019)
Train: 270 [ 450/1251 ( 36%)]  Loss: 2.921 (2.99)  Time: 0.665s, 1539.62/s  (0.659s, 1553.96/s)  LR: 2.877e-05  Data: 0.013 (0.018)
Train: 270 [ 500/1251 ( 40%)]  Loss: 2.667 (2.96)  Time: 0.662s, 1546.62/s  (0.659s, 1553.76/s)  LR: 2.871e-05  Data: 0.015 (0.018)
Train: 270 [ 550/1251 ( 44%)]  Loss: 2.950 (2.96)  Time: 0.655s, 1562.35/s  (0.659s, 1553.51/s)  LR: 2.865e-05  Data: 0.013 (0.017)
Train: 270 [ 600/1251 ( 48%)]  Loss: 3.029 (2.97)  Time: 0.660s, 1550.92/s  (0.659s, 1552.93/s)  LR: 2.858e-05  Data: 0.014 (0.017)
Train: 270 [ 650/1251 ( 52%)]  Loss: 2.845 (2.96)  Time: 0.659s, 1552.98/s  (0.660s, 1552.68/s)  LR: 2.852e-05  Data: 0.014 (0.017)
Train: 270 [ 700/1251 ( 56%)]  Loss: 2.636 (2.94)  Time: 0.665s, 1539.19/s  (0.660s, 1552.28/s)  LR: 2.846e-05  Data: 0.014 (0.016)
Train: 270 [ 750/1251 ( 60%)]  Loss: 2.834 (2.93)  Time: 0.655s, 1562.35/s  (0.660s, 1552.22/s)  LR: 2.839e-05  Data: 0.013 (0.016)
Train: 270 [ 800/1251 ( 64%)]  Loss: 2.949 (2.93)  Time: 0.664s, 1541.13/s  (0.660s, 1552.20/s)  LR: 2.833e-05  Data: 0.013 (0.016)
Train: 270 [ 850/1251 ( 68%)]  Loss: 2.977 (2.93)  Time: 0.655s, 1564.28/s  (0.660s, 1552.50/s)  LR: 2.827e-05  Data: 0.014 (0.016)
Train: 270 [ 900/1251 ( 72%)]  Loss: 2.878 (2.93)  Time: 0.650s, 1574.95/s  (0.660s, 1552.66/s)  LR: 2.820e-05  Data: 0.016 (0.016)
Train: 270 [ 950/1251 ( 76%)]  Loss: 2.688 (2.92)  Time: 0.652s, 1570.82/s  (0.659s, 1552.88/s)  LR: 2.814e-05  Data: 0.013 (0.016)
Train: 270 [1000/1251 ( 80%)]  Loss: 2.974 (2.92)  Time: 0.655s, 1564.17/s  (0.659s, 1553.00/s)  LR: 2.808e-05  Data: 0.013 (0.016)
Train: 270 [1050/1251 ( 84%)]  Loss: 2.854 (2.92)  Time: 0.652s, 1571.40/s  (0.659s, 1553.28/s)  LR: 2.802e-05  Data: 0.013 (0.015)
Train: 270 [1100/1251 ( 88%)]  Loss: 3.047 (2.92)  Time: 0.650s, 1574.35/s  (0.659s, 1553.41/s)  LR: 2.795e-05  Data: 0.014 (0.015)
Train: 270 [1150/1251 ( 92%)]  Loss: 2.957 (2.92)  Time: 0.659s, 1553.47/s  (0.659s, 1553.51/s)  LR: 2.789e-05  Data: 0.013 (0.015)
Train: 270 [1200/1251 ( 96%)]  Loss: 2.590 (2.91)  Time: 0.665s, 1539.88/s  (0.659s, 1553.78/s)  LR: 2.783e-05  Data: 0.015 (0.015)
Train: 270 [1250/1251 (100%)]  Loss: 2.987 (2.91)  Time: 0.643s, 1591.57/s  (0.659s, 1554.00/s)  LR: 2.777e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.795 (2.795)  Loss:  0.3716 (0.3716)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.161 (0.322)  Loss:  0.4868 (0.7952)  Acc@1: 87.7358 (81.3640)  Acc@5: 98.8207 (95.7340)
Train: 271 [   0/1251 (  0%)]  Loss: 3.028 (3.03)  Time: 3.239s,  316.12/s  (3.239s,  316.12/s)  LR: 2.776e-05  Data: 1.677 (1.677)
Train: 271 [  50/1251 (  4%)]  Loss: 3.293 (3.16)  Time: 0.641s, 1598.01/s  (0.670s, 1527.39/s)  LR: 2.770e-05  Data: 0.014 (0.048)
Train: 271 [ 100/1251 (  8%)]  Loss: 2.680 (3.00)  Time: 0.654s, 1565.39/s  (0.660s, 1552.01/s)  LR: 2.764e-05  Data: 0.013 (0.031)
Train: 271 [ 150/1251 ( 12%)]  Loss: 2.904 (2.98)  Time: 0.666s, 1537.80/s  (0.659s, 1554.97/s)  LR: 2.758e-05  Data: 0.012 (0.025)
Train: 271 [ 200/1251 ( 16%)]  Loss: 3.024 (2.99)  Time: 0.654s, 1566.33/s  (0.658s, 1556.36/s)  LR: 2.752e-05  Data: 0.015 (0.023)
Train: 271 [ 250/1251 ( 20%)]  Loss: 2.924 (2.98)  Time: 0.659s, 1553.06/s  (0.658s, 1556.87/s)  LR: 2.745e-05  Data: 0.013 (0.021)
Train: 271 [ 300/1251 ( 24%)]  Loss: 3.039 (2.98)  Time: 0.658s, 1555.79/s  (0.658s, 1557.37/s)  LR: 2.739e-05  Data: 0.013 (0.019)
Train: 271 [ 350/1251 ( 28%)]  Loss: 2.801 (2.96)  Time: 0.648s, 1580.17/s  (0.657s, 1557.54/s)  LR: 2.733e-05  Data: 0.014 (0.019)
Train: 271 [ 400/1251 ( 32%)]  Loss: 3.173 (2.99)  Time: 0.654s, 1564.99/s  (0.657s, 1557.43/s)  LR: 2.727e-05  Data: 0.012 (0.018)
Train: 271 [ 450/1251 ( 36%)]  Loss: 3.105 (3.00)  Time: 0.651s, 1572.80/s  (0.657s, 1557.75/s)  LR: 2.721e-05  Data: 0.013 (0.017)
Train: 271 [ 500/1251 ( 40%)]  Loss: 3.375 (3.03)  Time: 0.663s, 1543.37/s  (0.658s, 1557.27/s)  LR: 2.715e-05  Data: 0.013 (0.017)
Train: 271 [ 550/1251 ( 44%)]  Loss: 3.025 (3.03)  Time: 0.660s, 1551.59/s  (0.658s, 1556.70/s)  LR: 2.708e-05  Data: 0.013 (0.017)
Train: 271 [ 600/1251 ( 48%)]  Loss: 2.886 (3.02)  Time: 0.656s, 1561.08/s  (0.658s, 1555.99/s)  LR: 2.702e-05  Data: 0.012 (0.016)
Train: 271 [ 650/1251 ( 52%)]  Loss: 2.833 (3.01)  Time: 0.655s, 1562.80/s  (0.658s, 1555.92/s)  LR: 2.696e-05  Data: 0.013 (0.016)
Train: 271 [ 700/1251 ( 56%)]  Loss: 2.996 (3.01)  Time: 0.669s, 1531.11/s  (0.658s, 1555.92/s)  LR: 2.690e-05  Data: 0.013 (0.016)
Train: 271 [ 750/1251 ( 60%)]  Loss: 3.052 (3.01)  Time: 0.660s, 1551.78/s  (0.658s, 1555.63/s)  LR: 2.684e-05  Data: 0.013 (0.016)
Train: 271 [ 800/1251 ( 64%)]  Loss: 2.850 (3.00)  Time: 0.652s, 1570.70/s  (0.658s, 1555.45/s)  LR: 2.678e-05  Data: 0.013 (0.016)
Train: 271 [ 850/1251 ( 68%)]  Loss: 3.157 (3.01)  Time: 0.657s, 1558.67/s  (0.658s, 1555.42/s)  LR: 2.672e-05  Data: 0.013 (0.016)
Train: 271 [ 900/1251 ( 72%)]  Loss: 2.809 (3.00)  Time: 0.662s, 1547.50/s  (0.658s, 1555.49/s)  LR: 2.666e-05  Data: 0.013 (0.016)
Train: 271 [ 950/1251 ( 76%)]  Loss: 2.923 (2.99)  Time: 0.664s, 1542.21/s  (0.658s, 1555.52/s)  LR: 2.660e-05  Data: 0.013 (0.015)
Train: 271 [1000/1251 ( 80%)]  Loss: 3.172 (3.00)  Time: 0.658s, 1555.40/s  (0.658s, 1555.75/s)  LR: 2.654e-05  Data: 0.014 (0.015)
Train: 271 [1050/1251 ( 84%)]  Loss: 3.255 (3.01)  Time: 0.656s, 1562.16/s  (0.658s, 1556.02/s)  LR: 2.647e-05  Data: 0.013 (0.015)
Train: 271 [1100/1251 ( 88%)]  Loss: 3.105 (3.02)  Time: 0.660s, 1552.31/s  (0.658s, 1556.15/s)  LR: 2.641e-05  Data: 0.013 (0.015)
Train: 271 [1150/1251 ( 92%)]  Loss: 3.292 (3.03)  Time: 0.664s, 1542.74/s  (0.658s, 1555.97/s)  LR: 2.635e-05  Data: 0.014 (0.015)
Train: 271 [1200/1251 ( 96%)]  Loss: 3.155 (3.03)  Time: 0.660s, 1551.11/s  (0.658s, 1555.95/s)  LR: 2.629e-05  Data: 0.013 (0.015)
Train: 271 [1250/1251 (100%)]  Loss: 3.145 (3.04)  Time: 0.642s, 1594.40/s  (0.658s, 1556.10/s)  LR: 2.623e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.933 (2.933)  Loss:  0.3628 (0.3628)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.161 (0.320)  Loss:  0.4766 (0.7948)  Acc@1: 87.9717 (81.4220)  Acc@5: 98.5849 (95.6820)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-271.pth.tar', 81.4220000756836)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-268.pth.tar', 81.41000010498047)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-267.pth.tar', 81.40999997558593)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-263.pth.tar', 81.39200020751953)

Train: 272 [   0/1251 (  0%)]  Loss: 2.820 (2.82)  Time: 3.256s,  314.49/s  (3.256s,  314.49/s)  LR: 2.623e-05  Data: 1.690 (1.690)
Train: 272 [  50/1251 (  4%)]  Loss: 3.115 (2.97)  Time: 0.642s, 1594.27/s  (0.682s, 1500.47/s)  LR: 2.617e-05  Data: 0.016 (0.047)
Train: 272 [ 100/1251 (  8%)]  Loss: 3.177 (3.04)  Time: 0.655s, 1563.77/s  (0.667s, 1535.72/s)  LR: 2.611e-05  Data: 0.013 (0.031)
Train: 272 [ 150/1251 ( 12%)]  Loss: 2.733 (2.96)  Time: 0.653s, 1567.38/s  (0.662s, 1546.51/s)  LR: 2.605e-05  Data: 0.013 (0.025)
Train: 272 [ 200/1251 ( 16%)]  Loss: 3.187 (3.01)  Time: 0.651s, 1571.76/s  (0.660s, 1551.02/s)  LR: 2.599e-05  Data: 0.014 (0.022)
Train: 272 [ 250/1251 ( 20%)]  Loss: 2.883 (2.99)  Time: 0.656s, 1562.11/s  (0.659s, 1554.23/s)  LR: 2.593e-05  Data: 0.013 (0.021)
Train: 272 [ 300/1251 ( 24%)]  Loss: 3.067 (3.00)  Time: 0.651s, 1572.93/s  (0.658s, 1555.74/s)  LR: 2.587e-05  Data: 0.013 (0.019)
Train: 272 [ 350/1251 ( 28%)]  Loss: 2.968 (2.99)  Time: 0.670s, 1527.68/s  (0.658s, 1556.35/s)  LR: 2.581e-05  Data: 0.012 (0.019)
Train: 272 [ 400/1251 ( 32%)]  Loss: 2.805 (2.97)  Time: 0.658s, 1555.98/s  (0.658s, 1556.77/s)  LR: 2.575e-05  Data: 0.013 (0.018)
Train: 272 [ 450/1251 ( 36%)]  Loss: 2.993 (2.97)  Time: 0.651s, 1572.48/s  (0.658s, 1557.21/s)  LR: 2.569e-05  Data: 0.014 (0.018)
Train: 272 [ 500/1251 ( 40%)]  Loss: 2.852 (2.96)  Time: 0.656s, 1561.50/s  (0.658s, 1557.27/s)  LR: 2.563e-05  Data: 0.016 (0.017)
Train: 272 [ 550/1251 ( 44%)]  Loss: 3.011 (2.97)  Time: 0.657s, 1559.25/s  (0.658s, 1556.77/s)  LR: 2.558e-05  Data: 0.015 (0.017)
Train: 272 [ 600/1251 ( 48%)]  Loss: 2.846 (2.96)  Time: 0.652s, 1571.27/s  (0.658s, 1556.66/s)  LR: 2.552e-05  Data: 0.013 (0.017)
Train: 272 [ 650/1251 ( 52%)]  Loss: 2.707 (2.94)  Time: 0.659s, 1553.63/s  (0.658s, 1556.69/s)  LR: 2.546e-05  Data: 0.013 (0.017)
Train: 272 [ 700/1251 ( 56%)]  Loss: 3.193 (2.96)  Time: 0.662s, 1547.56/s  (0.658s, 1556.48/s)  LR: 2.540e-05  Data: 0.012 (0.016)
Train: 272 [ 750/1251 ( 60%)]  Loss: 2.736 (2.94)  Time: 0.656s, 1561.27/s  (0.658s, 1556.32/s)  LR: 2.534e-05  Data: 0.016 (0.016)
Train: 272 [ 800/1251 ( 64%)]  Loss: 3.057 (2.95)  Time: 0.652s, 1571.07/s  (0.658s, 1556.42/s)  LR: 2.528e-05  Data: 0.013 (0.016)
Train: 272 [ 850/1251 ( 68%)]  Loss: 2.929 (2.95)  Time: 0.654s, 1565.90/s  (0.658s, 1556.59/s)  LR: 2.522e-05  Data: 0.013 (0.016)
Train: 272 [ 900/1251 ( 72%)]  Loss: 3.001 (2.95)  Time: 0.662s, 1546.83/s  (0.658s, 1556.56/s)  LR: 2.516e-05  Data: 0.013 (0.016)
Train: 272 [ 950/1251 ( 76%)]  Loss: 2.925 (2.95)  Time: 0.660s, 1551.38/s  (0.658s, 1556.76/s)  LR: 2.510e-05  Data: 0.013 (0.016)
Train: 272 [1000/1251 ( 80%)]  Loss: 2.693 (2.94)  Time: 0.660s, 1551.60/s  (0.658s, 1556.67/s)  LR: 2.505e-05  Data: 0.014 (0.016)
Train: 272 [1050/1251 ( 84%)]  Loss: 2.917 (2.94)  Time: 0.659s, 1552.78/s  (0.658s, 1556.62/s)  LR: 2.499e-05  Data: 0.014 (0.016)
Train: 272 [1100/1251 ( 88%)]  Loss: 2.871 (2.93)  Time: 0.661s, 1548.01/s  (0.658s, 1556.38/s)  LR: 2.493e-05  Data: 0.016 (0.016)
Train: 272 [1150/1251 ( 92%)]  Loss: 2.779 (2.93)  Time: 0.664s, 1541.88/s  (0.658s, 1556.19/s)  LR: 2.487e-05  Data: 0.012 (0.016)
Train: 272 [1200/1251 ( 96%)]  Loss: 3.001 (2.93)  Time: 0.657s, 1558.45/s  (0.658s, 1556.00/s)  LR: 2.481e-05  Data: 0.016 (0.015)
Train: 272 [1250/1251 (100%)]  Loss: 2.843 (2.93)  Time: 0.647s, 1581.49/s  (0.658s, 1556.02/s)  LR: 2.475e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.795 (2.795)  Loss:  0.3772 (0.3772)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.161 (0.323)  Loss:  0.4917 (0.7975)  Acc@1: 87.3821 (81.3640)  Acc@5: 98.3491 (95.6760)
Train: 273 [   0/1251 (  0%)]  Loss: 2.580 (2.58)  Time: 3.330s,  307.49/s  (3.330s,  307.49/s)  LR: 2.475e-05  Data: 1.766 (1.766)
Train: 273 [  50/1251 (  4%)]  Loss: 2.792 (2.69)  Time: 0.651s, 1573.55/s  (0.675s, 1515.96/s)  LR: 2.469e-05  Data: 0.013 (0.048)
Train: 273 [ 100/1251 (  8%)]  Loss: 2.974 (2.78)  Time: 0.646s, 1584.95/s  (0.662s, 1547.14/s)  LR: 2.464e-05  Data: 0.017 (0.031)
Train: 273 [ 150/1251 ( 12%)]  Loss: 3.067 (2.85)  Time: 0.660s, 1552.13/s  (0.659s, 1553.25/s)  LR: 2.458e-05  Data: 0.012 (0.025)
Train: 273 [ 200/1251 ( 16%)]  Loss: 3.114 (2.91)  Time: 0.652s, 1569.89/s  (0.658s, 1555.48/s)  LR: 2.452e-05  Data: 0.015 (0.022)
Train: 273 [ 250/1251 ( 20%)]  Loss: 3.044 (2.93)  Time: 0.654s, 1565.89/s  (0.658s, 1556.34/s)  LR: 2.446e-05  Data: 0.013 (0.021)
Train: 273 [ 300/1251 ( 24%)]  Loss: 2.925 (2.93)  Time: 0.667s, 1536.00/s  (0.658s, 1556.71/s)  LR: 2.441e-05  Data: 0.012 (0.019)
Train: 273 [ 350/1251 ( 28%)]  Loss: 3.059 (2.94)  Time: 0.657s, 1557.99/s  (0.658s, 1556.85/s)  LR: 2.435e-05  Data: 0.016 (0.019)
Train: 273 [ 400/1251 ( 32%)]  Loss: 2.968 (2.95)  Time: 0.656s, 1562.01/s  (0.658s, 1556.83/s)  LR: 2.429e-05  Data: 0.012 (0.018)
Train: 273 [ 450/1251 ( 36%)]  Loss: 2.862 (2.94)  Time: 0.654s, 1564.64/s  (0.658s, 1556.67/s)  LR: 2.423e-05  Data: 0.014 (0.017)
Train: 273 [ 500/1251 ( 40%)]  Loss: 2.957 (2.94)  Time: 0.652s, 1571.15/s  (0.658s, 1557.17/s)  LR: 2.418e-05  Data: 0.013 (0.017)
Train: 273 [ 550/1251 ( 44%)]  Loss: 3.047 (2.95)  Time: 0.655s, 1562.39/s  (0.658s, 1556.98/s)  LR: 2.412e-05  Data: 0.013 (0.017)
Train: 273 [ 600/1251 ( 48%)]  Loss: 3.032 (2.96)  Time: 0.662s, 1547.51/s  (0.658s, 1557.25/s)  LR: 2.406e-05  Data: 0.016 (0.016)
Train: 273 [ 650/1251 ( 52%)]  Loss: 2.985 (2.96)  Time: 0.658s, 1555.28/s  (0.658s, 1557.33/s)  LR: 2.400e-05  Data: 0.013 (0.016)
Train: 273 [ 700/1251 ( 56%)]  Loss: 3.141 (2.97)  Time: 0.654s, 1566.86/s  (0.658s, 1557.21/s)  LR: 2.395e-05  Data: 0.018 (0.016)
Train: 273 [ 750/1251 ( 60%)]  Loss: 3.097 (2.98)  Time: 0.655s, 1562.51/s  (0.658s, 1557.18/s)  LR: 2.389e-05  Data: 0.015 (0.016)
Train: 273 [ 800/1251 ( 64%)]  Loss: 3.068 (2.98)  Time: 0.666s, 1538.19/s  (0.658s, 1557.36/s)  LR: 2.383e-05  Data: 0.013 (0.016)
Train: 273 [ 850/1251 ( 68%)]  Loss: 3.171 (2.99)  Time: 0.652s, 1570.72/s  (0.658s, 1557.15/s)  LR: 2.378e-05  Data: 0.011 (0.016)
Train: 273 [ 900/1251 ( 72%)]  Loss: 2.806 (2.98)  Time: 0.664s, 1542.66/s  (0.658s, 1557.00/s)  LR: 2.372e-05  Data: 0.013 (0.015)
Train: 273 [ 950/1251 ( 76%)]  Loss: 2.829 (2.98)  Time: 0.651s, 1573.75/s  (0.658s, 1556.79/s)  LR: 2.366e-05  Data: 0.014 (0.015)
Train: 273 [1000/1251 ( 80%)]  Loss: 2.967 (2.98)  Time: 0.660s, 1550.36/s  (0.658s, 1556.39/s)  LR: 2.361e-05  Data: 0.014 (0.015)
Train: 273 [1050/1251 ( 84%)]  Loss: 2.799 (2.97)  Time: 0.656s, 1561.18/s  (0.658s, 1556.14/s)  LR: 2.355e-05  Data: 0.014 (0.015)
Train: 273 [1100/1251 ( 88%)]  Loss: 3.048 (2.97)  Time: 0.653s, 1568.65/s  (0.658s, 1555.99/s)  LR: 2.349e-05  Data: 0.015 (0.015)
Train: 273 [1150/1251 ( 92%)]  Loss: 2.871 (2.97)  Time: 0.659s, 1553.86/s  (0.658s, 1555.81/s)  LR: 2.344e-05  Data: 0.014 (0.015)
Train: 273 [1200/1251 ( 96%)]  Loss: 3.009 (2.97)  Time: 0.652s, 1570.50/s  (0.658s, 1555.57/s)  LR: 2.338e-05  Data: 0.016 (0.015)
Train: 273 [1250/1251 (100%)]  Loss: 2.886 (2.97)  Time: 0.643s, 1592.59/s  (0.658s, 1555.52/s)  LR: 2.333e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.864 (2.864)  Loss:  0.3660 (0.3660)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.318)  Loss:  0.4812 (0.7917)  Acc@1: 87.5000 (81.4440)  Acc@5: 98.5849 (95.6880)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-273.pth.tar', 81.444)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-271.pth.tar', 81.4220000756836)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-268.pth.tar', 81.41000010498047)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-267.pth.tar', 81.40999997558593)

Train: 274 [   0/1251 (  0%)]  Loss: 3.034 (3.03)  Time: 3.031s,  337.82/s  (3.031s,  337.82/s)  LR: 2.333e-05  Data: 1.635 (1.635)
Train: 274 [  50/1251 (  4%)]  Loss: 3.004 (3.02)  Time: 0.645s, 1588.56/s  (0.668s, 1532.11/s)  LR: 2.327e-05  Data: 0.014 (0.045)
Train: 274 [ 100/1251 (  8%)]  Loss: 2.953 (3.00)  Time: 0.651s, 1572.88/s  (0.660s, 1552.39/s)  LR: 2.321e-05  Data: 0.014 (0.030)
Train: 274 [ 150/1251 ( 12%)]  Loss: 2.809 (2.95)  Time: 0.653s, 1567.24/s  (0.658s, 1556.33/s)  LR: 2.316e-05  Data: 0.014 (0.024)
Train: 274 [ 200/1251 ( 16%)]  Loss: 2.679 (2.90)  Time: 0.659s, 1553.11/s  (0.658s, 1557.02/s)  LR: 2.310e-05  Data: 0.013 (0.022)
Train: 274 [ 250/1251 ( 20%)]  Loss: 2.647 (2.85)  Time: 0.667s, 1535.18/s  (0.658s, 1557.11/s)  LR: 2.305e-05  Data: 0.013 (0.020)
Train: 274 [ 300/1251 ( 24%)]  Loss: 2.830 (2.85)  Time: 0.659s, 1553.86/s  (0.658s, 1557.27/s)  LR: 2.299e-05  Data: 0.013 (0.019)
Train: 274 [ 350/1251 ( 28%)]  Loss: 2.490 (2.81)  Time: 0.652s, 1570.14/s  (0.657s, 1557.76/s)  LR: 2.294e-05  Data: 0.014 (0.018)
Train: 274 [ 400/1251 ( 32%)]  Loss: 2.811 (2.81)  Time: 0.659s, 1554.72/s  (0.657s, 1558.37/s)  LR: 2.288e-05  Data: 0.013 (0.018)
Train: 274 [ 450/1251 ( 36%)]  Loss: 3.114 (2.84)  Time: 0.670s, 1528.15/s  (0.657s, 1558.10/s)  LR: 2.282e-05  Data: 0.013 (0.017)
Train: 274 [ 500/1251 ( 40%)]  Loss: 2.531 (2.81)  Time: 0.656s, 1560.18/s  (0.657s, 1558.08/s)  LR: 2.277e-05  Data: 0.014 (0.017)
Train: 274 [ 550/1251 ( 44%)]  Loss: 2.823 (2.81)  Time: 0.658s, 1556.90/s  (0.657s, 1558.18/s)  LR: 2.271e-05  Data: 0.015 (0.017)
Train: 274 [ 600/1251 ( 48%)]  Loss: 2.918 (2.82)  Time: 0.663s, 1545.28/s  (0.657s, 1558.29/s)  LR: 2.266e-05  Data: 0.013 (0.017)
Train: 274 [ 650/1251 ( 52%)]  Loss: 2.925 (2.83)  Time: 0.651s, 1572.75/s  (0.657s, 1558.17/s)  LR: 2.260e-05  Data: 0.013 (0.016)
Train: 274 [ 700/1251 ( 56%)]  Loss: 2.926 (2.83)  Time: 0.648s, 1580.53/s  (0.657s, 1558.12/s)  LR: 2.255e-05  Data: 0.013 (0.016)
Train: 274 [ 750/1251 ( 60%)]  Loss: 2.911 (2.84)  Time: 0.660s, 1550.75/s  (0.657s, 1558.20/s)  LR: 2.250e-05  Data: 0.014 (0.016)
Train: 274 [ 800/1251 ( 64%)]  Loss: 2.963 (2.85)  Time: 0.659s, 1553.05/s  (0.657s, 1558.25/s)  LR: 2.244e-05  Data: 0.013 (0.016)
Train: 274 [ 850/1251 ( 68%)]  Loss: 3.240 (2.87)  Time: 0.662s, 1545.67/s  (0.657s, 1558.25/s)  LR: 2.239e-05  Data: 0.013 (0.016)
Train: 274 [ 900/1251 ( 72%)]  Loss: 3.106 (2.88)  Time: 0.654s, 1565.21/s  (0.657s, 1558.36/s)  LR: 2.233e-05  Data: 0.014 (0.016)
Train: 274 [ 950/1251 ( 76%)]  Loss: 2.939 (2.88)  Time: 0.664s, 1541.80/s  (0.657s, 1558.32/s)  LR: 2.228e-05  Data: 0.014 (0.016)
Train: 274 [1000/1251 ( 80%)]  Loss: 3.121 (2.89)  Time: 0.650s, 1574.94/s  (0.657s, 1558.28/s)  LR: 2.222e-05  Data: 0.013 (0.015)
Train: 274 [1050/1251 ( 84%)]  Loss: 2.657 (2.88)  Time: 0.663s, 1545.50/s  (0.657s, 1558.36/s)  LR: 2.217e-05  Data: 0.012 (0.015)
Train: 274 [1100/1251 ( 88%)]  Loss: 3.053 (2.89)  Time: 0.655s, 1562.21/s  (0.657s, 1558.23/s)  LR: 2.211e-05  Data: 0.013 (0.015)
Train: 274 [1150/1251 ( 92%)]  Loss: 3.206 (2.90)  Time: 0.667s, 1534.95/s  (0.657s, 1558.08/s)  LR: 2.206e-05  Data: 0.012 (0.015)
Train: 274 [1200/1251 ( 96%)]  Loss: 3.071 (2.91)  Time: 0.656s, 1561.43/s  (0.657s, 1557.88/s)  LR: 2.201e-05  Data: 0.014 (0.015)
Train: 274 [1250/1251 (100%)]  Loss: 2.719 (2.90)  Time: 0.643s, 1591.40/s  (0.657s, 1557.98/s)  LR: 2.195e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.872 (2.872)  Loss:  0.3677 (0.3677)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.161 (0.316)  Loss:  0.4868 (0.7920)  Acc@1: 87.9717 (81.4740)  Acc@5: 98.4670 (95.6540)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-274.pth.tar', 81.47400007568359)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-273.pth.tar', 81.444)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-271.pth.tar', 81.4220000756836)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-268.pth.tar', 81.41000010498047)

Train: 275 [   0/1251 (  0%)]  Loss: 2.846 (2.85)  Time: 3.355s,  305.20/s  (3.355s,  305.20/s)  LR: 2.195e-05  Data: 1.578 (1.578)
Train: 275 [  50/1251 (  4%)]  Loss: 3.010 (2.93)  Time: 0.646s, 1584.79/s  (0.681s, 1503.73/s)  LR: 2.190e-05  Data: 0.013 (0.045)
Train: 275 [ 100/1251 (  8%)]  Loss: 3.087 (2.98)  Time: 0.660s, 1552.17/s  (0.666s, 1536.49/s)  LR: 2.184e-05  Data: 0.013 (0.029)
Train: 275 [ 150/1251 ( 12%)]  Loss: 2.702 (2.91)  Time: 0.663s, 1545.29/s  (0.663s, 1544.06/s)  LR: 2.179e-05  Data: 0.013 (0.024)
Train: 275 [ 200/1251 ( 16%)]  Loss: 2.833 (2.90)  Time: 0.661s, 1548.20/s  (0.662s, 1546.05/s)  LR: 2.174e-05  Data: 0.015 (0.022)
Train: 275 [ 250/1251 ( 20%)]  Loss: 2.892 (2.89)  Time: 0.660s, 1550.66/s  (0.662s, 1546.78/s)  LR: 2.168e-05  Data: 0.013 (0.020)
Train: 275 [ 300/1251 ( 24%)]  Loss: 2.824 (2.88)  Time: 0.665s, 1538.92/s  (0.661s, 1548.35/s)  LR: 2.163e-05  Data: 0.013 (0.019)
Train: 275 [ 350/1251 ( 28%)]  Loss: 2.920 (2.89)  Time: 0.656s, 1559.89/s  (0.661s, 1549.35/s)  LR: 2.158e-05  Data: 0.014 (0.018)
Train: 275 [ 400/1251 ( 32%)]  Loss: 2.988 (2.90)  Time: 0.643s, 1592.09/s  (0.660s, 1550.43/s)  LR: 2.152e-05  Data: 0.014 (0.018)
Train: 275 [ 450/1251 ( 36%)]  Loss: 2.982 (2.91)  Time: 0.660s, 1551.40/s  (0.660s, 1551.18/s)  LR: 2.147e-05  Data: 0.013 (0.017)
Train: 275 [ 500/1251 ( 40%)]  Loss: 2.978 (2.91)  Time: 0.658s, 1556.15/s  (0.660s, 1552.12/s)  LR: 2.142e-05  Data: 0.014 (0.017)
Train: 275 [ 550/1251 ( 44%)]  Loss: 2.755 (2.90)  Time: 0.658s, 1555.30/s  (0.660s, 1552.59/s)  LR: 2.136e-05  Data: 0.013 (0.017)
Train: 275 [ 600/1251 ( 48%)]  Loss: 3.011 (2.91)  Time: 0.653s, 1568.35/s  (0.659s, 1553.55/s)  LR: 2.131e-05  Data: 0.013 (0.016)
Train: 275 [ 650/1251 ( 52%)]  Loss: 3.313 (2.94)  Time: 0.650s, 1574.23/s  (0.659s, 1554.68/s)  LR: 2.126e-05  Data: 0.013 (0.016)
Train: 275 [ 700/1251 ( 56%)]  Loss: 2.691 (2.92)  Time: 0.661s, 1548.66/s  (0.658s, 1555.78/s)  LR: 2.120e-05  Data: 0.014 (0.016)
Train: 275 [ 750/1251 ( 60%)]  Loss: 2.998 (2.93)  Time: 0.654s, 1564.67/s  (0.658s, 1556.34/s)  LR: 2.115e-05  Data: 0.014 (0.016)
Train: 275 [ 800/1251 ( 64%)]  Loss: 2.846 (2.92)  Time: 0.658s, 1557.22/s  (0.658s, 1556.90/s)  LR: 2.110e-05  Data: 0.013 (0.016)
Train: 275 [ 850/1251 ( 68%)]  Loss: 2.996 (2.93)  Time: 0.662s, 1546.80/s  (0.658s, 1556.95/s)  LR: 2.105e-05  Data: 0.014 (0.016)
Train: 275 [ 900/1251 ( 72%)]  Loss: 3.060 (2.93)  Time: 0.660s, 1550.97/s  (0.658s, 1556.96/s)  LR: 2.099e-05  Data: 0.015 (0.016)
Train: 275 [ 950/1251 ( 76%)]  Loss: 2.776 (2.93)  Time: 0.654s, 1565.87/s  (0.658s, 1556.85/s)  LR: 2.094e-05  Data: 0.013 (0.015)
Train: 275 [1000/1251 ( 80%)]  Loss: 3.080 (2.93)  Time: 0.657s, 1557.55/s  (0.658s, 1556.70/s)  LR: 2.089e-05  Data: 0.014 (0.015)
Train: 275 [1050/1251 ( 84%)]  Loss: 3.128 (2.94)  Time: 0.669s, 1531.67/s  (0.658s, 1556.52/s)  LR: 2.084e-05  Data: 0.016 (0.015)
Train: 275 [1100/1251 ( 88%)]  Loss: 3.121 (2.95)  Time: 0.671s, 1525.30/s  (0.658s, 1556.21/s)  LR: 2.079e-05  Data: 0.013 (0.015)
Train: 275 [1150/1251 ( 92%)]  Loss: 3.224 (2.96)  Time: 0.663s, 1544.28/s  (0.658s, 1555.98/s)  LR: 2.073e-05  Data: 0.013 (0.015)
Train: 275 [1200/1251 ( 96%)]  Loss: 3.171 (2.97)  Time: 0.664s, 1542.59/s  (0.658s, 1555.80/s)  LR: 2.068e-05  Data: 0.013 (0.015)
Train: 275 [1250/1251 (100%)]  Loss: 2.670 (2.96)  Time: 0.650s, 1576.36/s  (0.658s, 1555.73/s)  LR: 2.063e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.026 (3.026)  Loss:  0.3760 (0.3760)  Acc@1: 92.4805 (92.4805)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.161 (0.322)  Loss:  0.5005 (0.8023)  Acc@1: 87.0283 (81.4120)  Acc@5: 98.5849 (95.7140)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-274.pth.tar', 81.47400007568359)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-273.pth.tar', 81.444)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-271.pth.tar', 81.4220000756836)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-275.pth.tar', 81.41200005371094)

Train: 276 [   0/1251 (  0%)]  Loss: 2.988 (2.99)  Time: 3.207s,  319.32/s  (3.207s,  319.32/s)  LR: 2.063e-05  Data: 1.660 (1.660)
Train: 276 [  50/1251 (  4%)]  Loss: 2.789 (2.89)  Time: 0.646s, 1585.92/s  (0.677s, 1512.30/s)  LR: 2.058e-05  Data: 0.017 (0.046)
Train: 276 [ 100/1251 (  8%)]  Loss: 2.901 (2.89)  Time: 0.668s, 1532.77/s  (0.666s, 1537.66/s)  LR: 2.053e-05  Data: 0.013 (0.030)
Train: 276 [ 150/1251 ( 12%)]  Loss: 3.105 (2.95)  Time: 0.660s, 1551.00/s  (0.664s, 1542.56/s)  LR: 2.047e-05  Data: 0.013 (0.025)
Train: 276 [ 200/1251 ( 16%)]  Loss: 2.810 (2.92)  Time: 0.664s, 1542.66/s  (0.663s, 1545.63/s)  LR: 2.042e-05  Data: 0.013 (0.022)
Train: 276 [ 250/1251 ( 20%)]  Loss: 3.122 (2.95)  Time: 0.655s, 1563.55/s  (0.662s, 1546.87/s)  LR: 2.037e-05  Data: 0.013 (0.020)
Train: 276 [ 300/1251 ( 24%)]  Loss: 2.820 (2.93)  Time: 0.662s, 1546.76/s  (0.662s, 1547.51/s)  LR: 2.032e-05  Data: 0.014 (0.019)
Train: 276 [ 350/1251 ( 28%)]  Loss: 2.626 (2.90)  Time: 0.663s, 1543.49/s  (0.662s, 1547.73/s)  LR: 2.027e-05  Data: 0.014 (0.019)
Train: 276 [ 400/1251 ( 32%)]  Loss: 2.864 (2.89)  Time: 0.654s, 1564.68/s  (0.661s, 1549.18/s)  LR: 2.022e-05  Data: 0.014 (0.018)
Train: 276 [ 450/1251 ( 36%)]  Loss: 3.007 (2.90)  Time: 0.657s, 1557.46/s  (0.661s, 1549.80/s)  LR: 2.017e-05  Data: 0.013 (0.018)
Train: 276 [ 500/1251 ( 40%)]  Loss: 2.946 (2.91)  Time: 0.659s, 1552.96/s  (0.661s, 1549.73/s)  LR: 2.012e-05  Data: 0.013 (0.017)
Train: 276 [ 550/1251 ( 44%)]  Loss: 3.136 (2.93)  Time: 0.663s, 1544.83/s  (0.661s, 1549.79/s)  LR: 2.006e-05  Data: 0.014 (0.017)
Train: 276 [ 600/1251 ( 48%)]  Loss: 2.706 (2.91)  Time: 0.654s, 1565.80/s  (0.660s, 1550.42/s)  LR: 2.001e-05  Data: 0.013 (0.017)
Train: 276 [ 650/1251 ( 52%)]  Loss: 3.101 (2.92)  Time: 0.654s, 1564.58/s  (0.660s, 1551.12/s)  LR: 1.996e-05  Data: 0.013 (0.016)
Train: 276 [ 700/1251 ( 56%)]  Loss: 2.798 (2.91)  Time: 0.656s, 1561.07/s  (0.660s, 1551.66/s)  LR: 1.991e-05  Data: 0.013 (0.016)
Train: 276 [ 750/1251 ( 60%)]  Loss: 2.780 (2.91)  Time: 0.653s, 1568.64/s  (0.660s, 1551.99/s)  LR: 1.986e-05  Data: 0.014 (0.016)
Train: 276 [ 800/1251 ( 64%)]  Loss: 2.940 (2.91)  Time: 0.656s, 1561.19/s  (0.660s, 1552.36/s)  LR: 1.981e-05  Data: 0.015 (0.016)
Train: 276 [ 850/1251 ( 68%)]  Loss: 2.985 (2.91)  Time: 0.665s, 1538.83/s  (0.660s, 1552.31/s)  LR: 1.976e-05  Data: 0.012 (0.016)
Train: 276 [ 900/1251 ( 72%)]  Loss: 2.923 (2.91)  Time: 0.653s, 1567.92/s  (0.660s, 1552.39/s)  LR: 1.971e-05  Data: 0.013 (0.016)
Train: 276 [ 950/1251 ( 76%)]  Loss: 2.917 (2.91)  Time: 0.660s, 1550.40/s  (0.660s, 1552.39/s)  LR: 1.966e-05  Data: 0.013 (0.016)
Train: 276 [1000/1251 ( 80%)]  Loss: 2.615 (2.90)  Time: 0.656s, 1560.31/s  (0.660s, 1552.46/s)  LR: 1.961e-05  Data: 0.014 (0.016)
Train: 276 [1050/1251 ( 84%)]  Loss: 3.087 (2.91)  Time: 0.668s, 1532.08/s  (0.660s, 1552.47/s)  LR: 1.956e-05  Data: 0.013 (0.015)
Train: 276 [1100/1251 ( 88%)]  Loss: 2.859 (2.91)  Time: 0.662s, 1547.44/s  (0.660s, 1552.44/s)  LR: 1.951e-05  Data: 0.012 (0.015)
Train: 276 [1150/1251 ( 92%)]  Loss: 2.874 (2.90)  Time: 0.661s, 1549.75/s  (0.660s, 1552.47/s)  LR: 1.946e-05  Data: 0.012 (0.015)
Train: 276 [1200/1251 ( 96%)]  Loss: 2.905 (2.90)  Time: 0.647s, 1582.24/s  (0.660s, 1552.09/s)  LR: 1.941e-05  Data: 0.013 (0.015)
Train: 276 [1250/1251 (100%)]  Loss: 2.999 (2.91)  Time: 0.649s, 1577.25/s  (0.660s, 1551.94/s)  LR: 1.936e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.765 (2.765)  Loss:  0.3760 (0.3760)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.162 (0.316)  Loss:  0.4883 (0.7942)  Acc@1: 87.2641 (81.5160)  Acc@5: 98.7028 (95.6880)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-276.pth.tar', 81.51599989746094)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-274.pth.tar', 81.47400007568359)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-273.pth.tar', 81.444)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-271.pth.tar', 81.4220000756836)

Train: 277 [   0/1251 (  0%)]  Loss: 3.087 (3.09)  Time: 3.161s,  323.94/s  (3.161s,  323.94/s)  LR: 1.936e-05  Data: 1.664 (1.664)
Train: 277 [  50/1251 (  4%)]  Loss: 2.765 (2.93)  Time: 0.652s, 1571.52/s  (0.673s, 1522.29/s)  LR: 1.931e-05  Data: 0.013 (0.046)
Train: 277 [ 100/1251 (  8%)]  Loss: 3.237 (3.03)  Time: 0.649s, 1577.01/s  (0.662s, 1545.91/s)  LR: 1.926e-05  Data: 0.013 (0.030)
Train: 277 [ 150/1251 ( 12%)]  Loss: 2.984 (3.02)  Time: 0.660s, 1550.72/s  (0.661s, 1549.75/s)  LR: 1.921e-05  Data: 0.013 (0.025)
Train: 277 [ 200/1251 ( 16%)]  Loss: 2.871 (2.99)  Time: 0.664s, 1541.28/s  (0.661s, 1550.11/s)  LR: 1.916e-05  Data: 0.013 (0.022)
Train: 277 [ 250/1251 ( 20%)]  Loss: 2.843 (2.96)  Time: 0.656s, 1561.19/s  (0.660s, 1550.50/s)  LR: 1.911e-05  Data: 0.015 (0.020)
Train: 277 [ 300/1251 ( 24%)]  Loss: 2.997 (2.97)  Time: 0.660s, 1551.62/s  (0.660s, 1551.01/s)  LR: 1.906e-05  Data: 0.013 (0.019)
Train: 277 [ 350/1251 ( 28%)]  Loss: 3.149 (2.99)  Time: 0.665s, 1539.29/s  (0.660s, 1551.97/s)  LR: 1.901e-05  Data: 0.013 (0.019)
Train: 277 [ 400/1251 ( 32%)]  Loss: 2.658 (2.95)  Time: 0.659s, 1552.90/s  (0.660s, 1552.60/s)  LR: 1.897e-05  Data: 0.013 (0.018)
Train: 277 [ 450/1251 ( 36%)]  Loss: 2.610 (2.92)  Time: 0.651s, 1571.83/s  (0.659s, 1553.14/s)  LR: 1.892e-05  Data: 0.014 (0.018)
Train: 277 [ 500/1251 ( 40%)]  Loss: 2.948 (2.92)  Time: 0.653s, 1567.80/s  (0.659s, 1553.39/s)  LR: 1.887e-05  Data: 0.013 (0.017)
Train: 277 [ 550/1251 ( 44%)]  Loss: 2.818 (2.91)  Time: 0.660s, 1552.68/s  (0.659s, 1553.45/s)  LR: 1.882e-05  Data: 0.013 (0.017)
Train: 277 [ 600/1251 ( 48%)]  Loss: 2.681 (2.90)  Time: 0.658s, 1556.44/s  (0.659s, 1553.59/s)  LR: 1.877e-05  Data: 0.014 (0.017)
Train: 277 [ 650/1251 ( 52%)]  Loss: 2.911 (2.90)  Time: 0.660s, 1551.81/s  (0.659s, 1553.68/s)  LR: 1.872e-05  Data: 0.017 (0.016)
Train: 277 [ 700/1251 ( 56%)]  Loss: 2.998 (2.90)  Time: 0.658s, 1556.22/s  (0.659s, 1553.63/s)  LR: 1.867e-05  Data: 0.013 (0.016)
Train: 277 [ 750/1251 ( 60%)]  Loss: 3.032 (2.91)  Time: 0.657s, 1557.62/s  (0.659s, 1553.78/s)  LR: 1.862e-05  Data: 0.013 (0.016)
Train: 277 [ 800/1251 ( 64%)]  Loss: 3.107 (2.92)  Time: 0.661s, 1549.72/s  (0.659s, 1553.82/s)  LR: 1.858e-05  Data: 0.014 (0.016)
Train: 277 [ 850/1251 ( 68%)]  Loss: 2.770 (2.91)  Time: 0.658s, 1556.02/s  (0.659s, 1553.76/s)  LR: 1.853e-05  Data: 0.013 (0.016)
Train: 277 [ 900/1251 ( 72%)]  Loss: 3.127 (2.93)  Time: 0.650s, 1574.86/s  (0.659s, 1553.80/s)  LR: 1.848e-05  Data: 0.013 (0.016)
Train: 277 [ 950/1251 ( 76%)]  Loss: 2.720 (2.92)  Time: 0.657s, 1559.12/s  (0.659s, 1553.65/s)  LR: 1.843e-05  Data: 0.017 (0.016)
Train: 277 [1000/1251 ( 80%)]  Loss: 3.337 (2.94)  Time: 0.652s, 1569.51/s  (0.659s, 1553.49/s)  LR: 1.838e-05  Data: 0.013 (0.015)
Train: 277 [1050/1251 ( 84%)]  Loss: 3.061 (2.94)  Time: 0.660s, 1551.91/s  (0.659s, 1553.51/s)  LR: 1.834e-05  Data: 0.013 (0.015)
Train: 277 [1100/1251 ( 88%)]  Loss: 3.196 (2.95)  Time: 0.660s, 1550.71/s  (0.659s, 1553.43/s)  LR: 1.829e-05  Data: 0.013 (0.015)
Train: 277 [1150/1251 ( 92%)]  Loss: 2.696 (2.94)  Time: 0.669s, 1531.74/s  (0.659s, 1553.36/s)  LR: 1.824e-05  Data: 0.014 (0.015)
Train: 277 [1200/1251 ( 96%)]  Loss: 2.923 (2.94)  Time: 0.657s, 1557.66/s  (0.659s, 1553.45/s)  LR: 1.819e-05  Data: 0.013 (0.015)
Train: 277 [1250/1251 (100%)]  Loss: 3.179 (2.95)  Time: 0.648s, 1580.68/s  (0.659s, 1553.62/s)  LR: 1.814e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.750 (2.750)  Loss:  0.3760 (0.3760)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.162 (0.315)  Loss:  0.4949 (0.7968)  Acc@1: 87.6179 (81.4440)  Acc@5: 98.7028 (95.6920)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-276.pth.tar', 81.51599989746094)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-274.pth.tar', 81.47400007568359)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-277.pth.tar', 81.44400005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-273.pth.tar', 81.444)

Train: 278 [   0/1251 (  0%)]  Loss: 2.833 (2.83)  Time: 3.112s,  329.07/s  (3.112s,  329.07/s)  LR: 1.814e-05  Data: 1.753 (1.753)
Train: 278 [  50/1251 (  4%)]  Loss: 2.890 (2.86)  Time: 0.647s, 1581.48/s  (0.672s, 1523.10/s)  LR: 1.810e-05  Data: 0.017 (0.047)
Train: 278 [ 100/1251 (  8%)]  Loss: 2.934 (2.89)  Time: 0.663s, 1545.22/s  (0.662s, 1547.77/s)  LR: 1.805e-05  Data: 0.016 (0.031)
Train: 278 [ 150/1251 ( 12%)]  Loss: 2.916 (2.89)  Time: 0.654s, 1565.50/s  (0.660s, 1551.96/s)  LR: 1.800e-05  Data: 0.014 (0.025)
Train: 278 [ 200/1251 ( 16%)]  Loss: 2.942 (2.90)  Time: 0.667s, 1535.99/s  (0.660s, 1552.04/s)  LR: 1.795e-05  Data: 0.013 (0.022)
Train: 278 [ 250/1251 ( 20%)]  Loss: 2.890 (2.90)  Time: 0.661s, 1549.62/s  (0.660s, 1552.25/s)  LR: 1.791e-05  Data: 0.013 (0.021)
Train: 278 [ 300/1251 ( 24%)]  Loss: 3.050 (2.92)  Time: 0.650s, 1576.59/s  (0.660s, 1552.15/s)  LR: 1.786e-05  Data: 0.014 (0.019)
Train: 278 [ 350/1251 ( 28%)]  Loss: 3.104 (2.94)  Time: 0.664s, 1542.02/s  (0.660s, 1552.01/s)  LR: 1.781e-05  Data: 0.014 (0.019)
Train: 278 [ 400/1251 ( 32%)]  Loss: 3.218 (2.98)  Time: 0.652s, 1569.44/s  (0.660s, 1552.14/s)  LR: 1.777e-05  Data: 0.014 (0.018)
Train: 278 [ 450/1251 ( 36%)]  Loss: 2.956 (2.97)  Time: 0.657s, 1559.69/s  (0.660s, 1552.18/s)  LR: 1.772e-05  Data: 0.013 (0.018)
Train: 278 [ 500/1251 ( 40%)]  Loss: 3.114 (2.99)  Time: 0.668s, 1533.78/s  (0.660s, 1552.24/s)  LR: 1.767e-05  Data: 0.013 (0.017)
Train: 278 [ 550/1251 ( 44%)]  Loss: 2.907 (2.98)  Time: 0.657s, 1558.59/s  (0.660s, 1552.19/s)  LR: 1.763e-05  Data: 0.013 (0.017)
Train: 278 [ 600/1251 ( 48%)]  Loss: 3.161 (2.99)  Time: 0.656s, 1559.81/s  (0.660s, 1551.99/s)  LR: 1.758e-05  Data: 0.016 (0.017)
Train: 278 [ 650/1251 ( 52%)]  Loss: 2.730 (2.97)  Time: 0.662s, 1546.79/s  (0.660s, 1552.33/s)  LR: 1.753e-05  Data: 0.016 (0.016)
Train: 278 [ 700/1251 ( 56%)]  Loss: 2.752 (2.96)  Time: 0.658s, 1555.63/s  (0.659s, 1552.77/s)  LR: 1.749e-05  Data: 0.013 (0.016)
Train: 278 [ 750/1251 ( 60%)]  Loss: 3.114 (2.97)  Time: 0.652s, 1570.94/s  (0.659s, 1553.06/s)  LR: 1.744e-05  Data: 0.014 (0.016)
Train: 278 [ 800/1251 ( 64%)]  Loss: 2.652 (2.95)  Time: 0.654s, 1565.63/s  (0.659s, 1553.16/s)  LR: 1.739e-05  Data: 0.013 (0.016)
Train: 278 [ 850/1251 ( 68%)]  Loss: 2.454 (2.92)  Time: 0.663s, 1545.27/s  (0.659s, 1553.35/s)  LR: 1.735e-05  Data: 0.013 (0.016)
Train: 278 [ 900/1251 ( 72%)]  Loss: 3.007 (2.93)  Time: 0.657s, 1559.53/s  (0.659s, 1553.56/s)  LR: 1.730e-05  Data: 0.012 (0.016)
Train: 278 [ 950/1251 ( 76%)]  Loss: 2.854 (2.92)  Time: 0.655s, 1562.82/s  (0.659s, 1553.63/s)  LR: 1.726e-05  Data: 0.013 (0.015)
Train: 278 [1000/1251 ( 80%)]  Loss: 2.723 (2.91)  Time: 0.657s, 1558.49/s  (0.659s, 1553.55/s)  LR: 1.721e-05  Data: 0.014 (0.015)
Train: 278 [1050/1251 ( 84%)]  Loss: 3.125 (2.92)  Time: 0.665s, 1538.84/s  (0.659s, 1553.56/s)  LR: 1.716e-05  Data: 0.016 (0.015)
Train: 278 [1100/1251 ( 88%)]  Loss: 2.864 (2.92)  Time: 0.658s, 1555.60/s  (0.659s, 1553.68/s)  LR: 1.712e-05  Data: 0.013 (0.015)
Train: 278 [1150/1251 ( 92%)]  Loss: 3.020 (2.93)  Time: 0.662s, 1545.72/s  (0.659s, 1553.70/s)  LR: 1.707e-05  Data: 0.013 (0.015)
Train: 278 [1200/1251 ( 96%)]  Loss: 2.598 (2.91)  Time: 0.660s, 1551.61/s  (0.659s, 1553.55/s)  LR: 1.703e-05  Data: 0.013 (0.015)
Train: 278 [1250/1251 (100%)]  Loss: 2.636 (2.90)  Time: 0.646s, 1584.43/s  (0.659s, 1553.59/s)  LR: 1.698e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.873 (2.873)  Loss:  0.3718 (0.3718)  Acc@1: 93.3594 (93.3594)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.321)  Loss:  0.4868 (0.7984)  Acc@1: 87.8538 (81.4520)  Acc@5: 98.5849 (95.6600)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-276.pth.tar', 81.51599989746094)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-274.pth.tar', 81.47400007568359)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-278.pth.tar', 81.45200002441406)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-277.pth.tar', 81.44400005126953)

Train: 279 [   0/1251 (  0%)]  Loss: 2.656 (2.66)  Time: 3.361s,  304.66/s  (3.361s,  304.66/s)  LR: 1.698e-05  Data: 1.789 (1.789)
Train: 279 [  50/1251 (  4%)]  Loss: 2.830 (2.74)  Time: 0.653s, 1569.34/s  (0.680s, 1505.26/s)  LR: 1.694e-05  Data: 0.013 (0.049)
Train: 279 [ 100/1251 (  8%)]  Loss: 2.970 (2.82)  Time: 0.662s, 1547.19/s  (0.668s, 1533.62/s)  LR: 1.689e-05  Data: 0.013 (0.031)
Train: 279 [ 150/1251 ( 12%)]  Loss: 2.831 (2.82)  Time: 0.653s, 1567.86/s  (0.665s, 1540.24/s)  LR: 1.684e-05  Data: 0.012 (0.026)
Train: 279 [ 200/1251 ( 16%)]  Loss: 3.046 (2.87)  Time: 0.662s, 1547.63/s  (0.664s, 1541.97/s)  LR: 1.680e-05  Data: 0.013 (0.022)
Train: 279 [ 250/1251 ( 20%)]  Loss: 2.478 (2.80)  Time: 0.663s, 1545.49/s  (0.664s, 1542.88/s)  LR: 1.675e-05  Data: 0.015 (0.021)
Train: 279 [ 300/1251 ( 24%)]  Loss: 2.768 (2.80)  Time: 0.667s, 1534.60/s  (0.663s, 1544.14/s)  LR: 1.671e-05  Data: 0.013 (0.019)
Train: 279 [ 350/1251 ( 28%)]  Loss: 2.735 (2.79)  Time: 0.665s, 1540.92/s  (0.663s, 1544.67/s)  LR: 1.666e-05  Data: 0.013 (0.019)
Train: 279 [ 400/1251 ( 32%)]  Loss: 2.884 (2.80)  Time: 0.662s, 1546.33/s  (0.663s, 1545.55/s)  LR: 1.662e-05  Data: 0.012 (0.018)
Train: 279 [ 450/1251 ( 36%)]  Loss: 2.532 (2.77)  Time: 0.663s, 1545.56/s  (0.662s, 1545.99/s)  LR: 1.658e-05  Data: 0.013 (0.017)
Train: 279 [ 500/1251 ( 40%)]  Loss: 2.958 (2.79)  Time: 0.662s, 1547.71/s  (0.662s, 1546.06/s)  LR: 1.653e-05  Data: 0.013 (0.017)
Train: 279 [ 550/1251 ( 44%)]  Loss: 3.172 (2.82)  Time: 0.667s, 1535.14/s  (0.662s, 1545.82/s)  LR: 1.649e-05  Data: 0.013 (0.017)
Train: 279 [ 600/1251 ( 48%)]  Loss: 3.027 (2.84)  Time: 0.669s, 1531.76/s  (0.662s, 1545.92/s)  LR: 1.644e-05  Data: 0.013 (0.016)
Train: 279 [ 650/1251 ( 52%)]  Loss: 2.797 (2.83)  Time: 0.659s, 1553.75/s  (0.662s, 1545.73/s)  LR: 1.640e-05  Data: 0.014 (0.016)
Train: 279 [ 700/1251 ( 56%)]  Loss: 2.806 (2.83)  Time: 0.656s, 1560.29/s  (0.662s, 1545.81/s)  LR: 1.635e-05  Data: 0.013 (0.016)
Train: 279 [ 750/1251 ( 60%)]  Loss: 3.197 (2.86)  Time: 0.657s, 1559.20/s  (0.663s, 1545.61/s)  LR: 1.631e-05  Data: 0.013 (0.016)
Train: 279 [ 800/1251 ( 64%)]  Loss: 3.018 (2.87)  Time: 0.664s, 1543.05/s  (0.663s, 1545.11/s)  LR: 1.626e-05  Data: 0.015 (0.016)
Train: 279 [ 850/1251 ( 68%)]  Loss: 3.089 (2.88)  Time: 0.658s, 1556.08/s  (0.663s, 1545.18/s)  LR: 1.622e-05  Data: 0.014 (0.016)
Train: 279 [ 900/1251 ( 72%)]  Loss: 3.118 (2.89)  Time: 0.665s, 1540.10/s  (0.663s, 1545.06/s)  LR: 1.618e-05  Data: 0.015 (0.015)
Train: 279 [ 950/1251 ( 76%)]  Loss: 2.947 (2.89)  Time: 0.655s, 1563.10/s  (0.663s, 1545.10/s)  LR: 1.613e-05  Data: 0.013 (0.015)
Train: 279 [1000/1251 ( 80%)]  Loss: 3.181 (2.91)  Time: 0.661s, 1548.93/s  (0.663s, 1545.26/s)  LR: 1.609e-05  Data: 0.013 (0.015)
Train: 279 [1050/1251 ( 84%)]  Loss: 2.551 (2.89)  Time: 0.664s, 1542.62/s  (0.663s, 1545.50/s)  LR: 1.605e-05  Data: 0.013 (0.015)
Train: 279 [1100/1251 ( 88%)]  Loss: 2.908 (2.89)  Time: 0.660s, 1550.72/s  (0.662s, 1545.76/s)  LR: 1.600e-05  Data: 0.013 (0.015)
Train: 279 [1150/1251 ( 92%)]  Loss: 3.212 (2.90)  Time: 0.666s, 1537.69/s  (0.662s, 1545.84/s)  LR: 1.596e-05  Data: 0.012 (0.015)
Train: 279 [1200/1251 ( 96%)]  Loss: 2.751 (2.90)  Time: 0.663s, 1545.33/s  (0.662s, 1546.05/s)  LR: 1.591e-05  Data: 0.014 (0.015)
Train: 279 [1250/1251 (100%)]  Loss: 2.970 (2.90)  Time: 0.647s, 1581.96/s  (0.662s, 1546.41/s)  LR: 1.587e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.892 (2.892)  Loss:  0.3752 (0.3752)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.317)  Loss:  0.4861 (0.7977)  Acc@1: 87.8538 (81.4520)  Acc@5: 98.9387 (95.7240)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-276.pth.tar', 81.51599989746094)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-274.pth.tar', 81.47400007568359)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-278.pth.tar', 81.45200002441406)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-279.pth.tar', 81.45200002441406)

Train: 280 [   0/1251 (  0%)]  Loss: 2.871 (2.87)  Time: 3.094s,  331.02/s  (3.094s,  331.02/s)  LR: 1.587e-05  Data: 1.639 (1.639)
Train: 280 [  50/1251 (  4%)]  Loss: 2.849 (2.86)  Time: 0.642s, 1596.18/s  (0.670s, 1528.50/s)  LR: 1.583e-05  Data: 0.012 (0.046)
Train: 280 [ 100/1251 (  8%)]  Loss: 2.937 (2.89)  Time: 0.658s, 1557.07/s  (0.663s, 1545.35/s)  LR: 1.578e-05  Data: 0.013 (0.030)
Train: 280 [ 150/1251 ( 12%)]  Loss: 3.002 (2.91)  Time: 0.657s, 1557.64/s  (0.662s, 1547.66/s)  LR: 1.574e-05  Data: 0.014 (0.024)
Train: 280 [ 200/1251 ( 16%)]  Loss: 3.036 (2.94)  Time: 0.664s, 1542.38/s  (0.662s, 1547.16/s)  LR: 1.570e-05  Data: 0.013 (0.022)
Train: 280 [ 250/1251 ( 20%)]  Loss: 3.012 (2.95)  Time: 0.664s, 1542.84/s  (0.662s, 1546.02/s)  LR: 1.566e-05  Data: 0.013 (0.020)
Train: 280 [ 300/1251 ( 24%)]  Loss: 2.968 (2.95)  Time: 0.664s, 1542.58/s  (0.663s, 1545.14/s)  LR: 1.561e-05  Data: 0.014 (0.019)
Train: 280 [ 350/1251 ( 28%)]  Loss: 2.650 (2.92)  Time: 0.671s, 1527.06/s  (0.663s, 1543.96/s)  LR: 1.557e-05  Data: 0.016 (0.018)
Train: 280 [ 400/1251 ( 32%)]  Loss: 2.798 (2.90)  Time: 0.668s, 1533.53/s  (0.664s, 1543.24/s)  LR: 1.553e-05  Data: 0.013 (0.018)
Train: 280 [ 450/1251 ( 36%)]  Loss: 3.011 (2.91)  Time: 0.666s, 1536.77/s  (0.664s, 1542.79/s)  LR: 1.548e-05  Data: 0.014 (0.017)
Train: 280 [ 500/1251 ( 40%)]  Loss: 2.872 (2.91)  Time: 0.667s, 1535.86/s  (0.664s, 1542.60/s)  LR: 1.544e-05  Data: 0.013 (0.017)
Train: 280 [ 550/1251 ( 44%)]  Loss: 2.723 (2.89)  Time: 0.673s, 1522.65/s  (0.664s, 1542.86/s)  LR: 1.540e-05  Data: 0.014 (0.017)
Train: 280 [ 600/1251 ( 48%)]  Loss: 2.740 (2.88)  Time: 0.672s, 1522.88/s  (0.664s, 1543.05/s)  LR: 1.536e-05  Data: 0.018 (0.016)
Train: 280 [ 650/1251 ( 52%)]  Loss: 2.503 (2.86)  Time: 0.659s, 1553.20/s  (0.664s, 1542.77/s)  LR: 1.532e-05  Data: 0.013 (0.016)
Train: 280 [ 700/1251 ( 56%)]  Loss: 3.190 (2.88)  Time: 0.665s, 1540.07/s  (0.664s, 1542.50/s)  LR: 1.527e-05  Data: 0.014 (0.016)
Train: 280 [ 750/1251 ( 60%)]  Loss: 3.112 (2.89)  Time: 0.669s, 1530.31/s  (0.664s, 1542.55/s)  LR: 1.523e-05  Data: 0.013 (0.016)
Train: 280 [ 800/1251 ( 64%)]  Loss: 3.076 (2.90)  Time: 0.658s, 1555.72/s  (0.664s, 1542.51/s)  LR: 1.519e-05  Data: 0.014 (0.016)
Train: 280 [ 850/1251 ( 68%)]  Loss: 2.867 (2.90)  Time: 0.670s, 1528.29/s  (0.664s, 1542.61/s)  LR: 1.515e-05  Data: 0.013 (0.016)
Train: 280 [ 900/1251 ( 72%)]  Loss: 2.931 (2.90)  Time: 0.655s, 1564.17/s  (0.664s, 1542.57/s)  LR: 1.511e-05  Data: 0.015 (0.015)
Train: 280 [ 950/1251 ( 76%)]  Loss: 2.888 (2.90)  Time: 0.670s, 1528.37/s  (0.664s, 1542.40/s)  LR: 1.506e-05  Data: 0.013 (0.015)
Train: 280 [1000/1251 ( 80%)]  Loss: 2.445 (2.88)  Time: 0.661s, 1549.40/s  (0.664s, 1542.16/s)  LR: 1.502e-05  Data: 0.013 (0.015)
Train: 280 [1050/1251 ( 84%)]  Loss: 2.988 (2.88)  Time: 0.665s, 1540.14/s  (0.664s, 1542.08/s)  LR: 1.498e-05  Data: 0.013 (0.015)
Train: 280 [1100/1251 ( 88%)]  Loss: 2.940 (2.89)  Time: 0.669s, 1531.23/s  (0.664s, 1541.82/s)  LR: 1.494e-05  Data: 0.013 (0.015)
Train: 280 [1150/1251 ( 92%)]  Loss: 2.999 (2.89)  Time: 0.670s, 1527.95/s  (0.664s, 1541.71/s)  LR: 1.490e-05  Data: 0.014 (0.015)
Train: 280 [1200/1251 ( 96%)]  Loss: 3.144 (2.90)  Time: 0.665s, 1539.04/s  (0.664s, 1541.65/s)  LR: 1.486e-05  Data: 0.012 (0.015)
Train: 280 [1250/1251 (100%)]  Loss: 2.952 (2.90)  Time: 0.650s, 1574.68/s  (0.664s, 1541.54/s)  LR: 1.482e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.887 (2.887)  Loss:  0.3745 (0.3745)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.164 (0.317)  Loss:  0.4883 (0.7969)  Acc@1: 87.5000 (81.5620)  Acc@5: 98.5849 (95.7200)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-280.pth.tar', 81.56200012939453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-276.pth.tar', 81.51599989746094)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-274.pth.tar', 81.47400007568359)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-278.pth.tar', 81.45200002441406)

Train: 281 [   0/1251 (  0%)]  Loss: 3.228 (3.23)  Time: 3.731s,  274.45/s  (3.731s,  274.45/s)  LR: 1.481e-05  Data: 1.758 (1.758)
Train: 281 [  50/1251 (  4%)]  Loss: 3.068 (3.15)  Time: 0.647s, 1583.54/s  (0.689s, 1486.36/s)  LR: 1.477e-05  Data: 0.013 (0.048)
Train: 281 [ 100/1251 (  8%)]  Loss: 2.942 (3.08)  Time: 0.656s, 1559.83/s  (0.673s, 1521.27/s)  LR: 1.473e-05  Data: 0.013 (0.031)
Train: 281 [ 150/1251 ( 12%)]  Loss: 2.968 (3.05)  Time: 0.663s, 1545.11/s  (0.669s, 1530.09/s)  LR: 1.469e-05  Data: 0.014 (0.025)
Train: 281 [ 200/1251 ( 16%)]  Loss: 2.981 (3.04)  Time: 0.670s, 1529.03/s  (0.668s, 1533.29/s)  LR: 1.465e-05  Data: 0.013 (0.022)
Train: 281 [ 250/1251 ( 20%)]  Loss: 2.941 (3.02)  Time: 0.665s, 1540.51/s  (0.667s, 1535.11/s)  LR: 1.461e-05  Data: 0.013 (0.021)
Train: 281 [ 300/1251 ( 24%)]  Loss: 2.563 (2.96)  Time: 0.661s, 1549.44/s  (0.667s, 1535.83/s)  LR: 1.457e-05  Data: 0.013 (0.019)
Train: 281 [ 350/1251 ( 28%)]  Loss: 2.996 (2.96)  Time: 0.660s, 1551.42/s  (0.666s, 1536.85/s)  LR: 1.453e-05  Data: 0.016 (0.019)
Train: 281 [ 400/1251 ( 32%)]  Loss: 2.994 (2.96)  Time: 0.662s, 1547.40/s  (0.666s, 1537.27/s)  LR: 1.449e-05  Data: 0.014 (0.018)
Train: 281 [ 450/1251 ( 36%)]  Loss: 2.957 (2.96)  Time: 0.671s, 1525.56/s  (0.666s, 1537.60/s)  LR: 1.445e-05  Data: 0.013 (0.018)
Train: 281 [ 500/1251 ( 40%)]  Loss: 2.729 (2.94)  Time: 0.669s, 1530.90/s  (0.666s, 1538.03/s)  LR: 1.441e-05  Data: 0.013 (0.017)
Train: 281 [ 550/1251 ( 44%)]  Loss: 3.184 (2.96)  Time: 0.670s, 1528.10/s  (0.666s, 1538.62/s)  LR: 1.437e-05  Data: 0.013 (0.017)
Train: 281 [ 600/1251 ( 48%)]  Loss: 2.761 (2.95)  Time: 0.664s, 1541.19/s  (0.665s, 1538.98/s)  LR: 1.433e-05  Data: 0.013 (0.017)
Train: 281 [ 650/1251 ( 52%)]  Loss: 2.922 (2.95)  Time: 0.667s, 1535.19/s  (0.665s, 1539.32/s)  LR: 1.429e-05  Data: 0.013 (0.016)
Train: 281 [ 700/1251 ( 56%)]  Loss: 2.726 (2.93)  Time: 0.667s, 1536.11/s  (0.665s, 1539.50/s)  LR: 1.425e-05  Data: 0.013 (0.016)
Train: 281 [ 750/1251 ( 60%)]  Loss: 2.698 (2.92)  Time: 0.669s, 1531.24/s  (0.665s, 1539.72/s)  LR: 1.421e-05  Data: 0.013 (0.016)
Train: 281 [ 800/1251 ( 64%)]  Loss: 2.777 (2.91)  Time: 0.666s, 1538.55/s  (0.665s, 1539.93/s)  LR: 1.417e-05  Data: 0.015 (0.016)
Train: 281 [ 850/1251 ( 68%)]  Loss: 2.945 (2.91)  Time: 0.666s, 1536.71/s  (0.665s, 1539.57/s)  LR: 1.413e-05  Data: 0.014 (0.016)
Train: 281 [ 900/1251 ( 72%)]  Loss: 2.573 (2.89)  Time: 0.669s, 1530.71/s  (0.665s, 1539.55/s)  LR: 1.409e-05  Data: 0.013 (0.016)
Train: 281 [ 950/1251 ( 76%)]  Loss: 2.969 (2.90)  Time: 0.672s, 1523.44/s  (0.665s, 1539.52/s)  LR: 1.405e-05  Data: 0.013 (0.015)
Train: 281 [1000/1251 ( 80%)]  Loss: 2.782 (2.89)  Time: 0.663s, 1543.35/s  (0.665s, 1539.58/s)  LR: 1.401e-05  Data: 0.012 (0.015)
Train: 281 [1050/1251 ( 84%)]  Loss: 3.035 (2.90)  Time: 0.671s, 1526.56/s  (0.665s, 1539.36/s)  LR: 1.397e-05  Data: 0.013 (0.015)
Train: 281 [1100/1251 ( 88%)]  Loss: 2.903 (2.90)  Time: 0.663s, 1544.26/s  (0.665s, 1539.16/s)  LR: 1.393e-05  Data: 0.013 (0.015)
Train: 281 [1150/1251 ( 92%)]  Loss: 2.865 (2.90)  Time: 0.664s, 1541.09/s  (0.665s, 1538.95/s)  LR: 1.389e-05  Data: 0.014 (0.015)
Train: 281 [1200/1251 ( 96%)]  Loss: 2.792 (2.89)  Time: 0.670s, 1528.41/s  (0.665s, 1538.79/s)  LR: 1.385e-05  Data: 0.013 (0.015)
Train: 281 [1250/1251 (100%)]  Loss: 2.958 (2.89)  Time: 0.650s, 1575.99/s  (0.665s, 1538.81/s)  LR: 1.381e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.898 (2.898)  Loss:  0.3701 (0.3701)  Acc@1: 93.3594 (93.3594)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.169 (0.321)  Loss:  0.4873 (0.7922)  Acc@1: 87.9717 (81.5660)  Acc@5: 98.8207 (95.7340)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-281.pth.tar', 81.56600007568359)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-280.pth.tar', 81.56200012939453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-276.pth.tar', 81.51599989746094)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-274.pth.tar', 81.47400007568359)

Train: 282 [   0/1251 (  0%)]  Loss: 2.806 (2.81)  Time: 3.277s,  312.49/s  (3.277s,  312.49/s)  LR: 1.381e-05  Data: 1.912 (1.912)
Train: 282 [  50/1251 (  4%)]  Loss: 2.805 (2.81)  Time: 0.651s, 1572.19/s  (0.685s, 1495.39/s)  LR: 1.377e-05  Data: 0.016 (0.051)
Train: 282 [ 100/1251 (  8%)]  Loss: 2.802 (2.80)  Time: 0.665s, 1539.60/s  (0.671s, 1526.01/s)  LR: 1.373e-05  Data: 0.015 (0.033)
Train: 282 [ 150/1251 ( 12%)]  Loss: 3.084 (2.87)  Time: 0.662s, 1547.95/s  (0.668s, 1532.37/s)  LR: 1.369e-05  Data: 0.018 (0.027)
Train: 282 [ 200/1251 ( 16%)]  Loss: 3.062 (2.91)  Time: 0.666s, 1536.40/s  (0.667s, 1534.43/s)  LR: 1.366e-05  Data: 0.014 (0.024)
Train: 282 [ 250/1251 ( 20%)]  Loss: 3.128 (2.95)  Time: 0.658s, 1556.42/s  (0.667s, 1535.98/s)  LR: 1.362e-05  Data: 0.014 (0.022)
Train: 282 [ 300/1251 ( 24%)]  Loss: 2.776 (2.92)  Time: 0.669s, 1530.84/s  (0.667s, 1536.30/s)  LR: 1.358e-05  Data: 0.013 (0.021)
Train: 282 [ 350/1251 ( 28%)]  Loss: 2.989 (2.93)  Time: 0.664s, 1542.55/s  (0.666s, 1536.51/s)  LR: 1.354e-05  Data: 0.014 (0.020)
Train: 282 [ 400/1251 ( 32%)]  Loss: 3.178 (2.96)  Time: 0.661s, 1548.32/s  (0.667s, 1536.38/s)  LR: 1.350e-05  Data: 0.014 (0.019)
Train: 282 [ 450/1251 ( 36%)]  Loss: 2.979 (2.96)  Time: 0.672s, 1524.50/s  (0.666s, 1536.42/s)  LR: 1.346e-05  Data: 0.014 (0.018)
Train: 282 [ 500/1251 ( 40%)]  Loss: 2.639 (2.93)  Time: 0.666s, 1536.87/s  (0.666s, 1536.64/s)  LR: 1.343e-05  Data: 0.012 (0.018)
Train: 282 [ 550/1251 ( 44%)]  Loss: 2.700 (2.91)  Time: 0.659s, 1553.45/s  (0.666s, 1536.62/s)  LR: 1.339e-05  Data: 0.014 (0.017)
Train: 282 [ 600/1251 ( 48%)]  Loss: 3.034 (2.92)  Time: 0.673s, 1522.55/s  (0.666s, 1536.87/s)  LR: 1.335e-05  Data: 0.013 (0.017)
Train: 282 [ 650/1251 ( 52%)]  Loss: 3.094 (2.93)  Time: 0.657s, 1559.07/s  (0.666s, 1537.57/s)  LR: 1.331e-05  Data: 0.013 (0.017)
Train: 282 [ 700/1251 ( 56%)]  Loss: 2.775 (2.92)  Time: 0.665s, 1539.85/s  (0.666s, 1537.96/s)  LR: 1.327e-05  Data: 0.015 (0.017)
Train: 282 [ 750/1251 ( 60%)]  Loss: 3.002 (2.93)  Time: 0.665s, 1539.68/s  (0.666s, 1538.40/s)  LR: 1.324e-05  Data: 0.013 (0.016)
Train: 282 [ 800/1251 ( 64%)]  Loss: 2.951 (2.93)  Time: 0.671s, 1525.81/s  (0.665s, 1538.81/s)  LR: 1.320e-05  Data: 0.013 (0.016)
Train: 282 [ 850/1251 ( 68%)]  Loss: 3.254 (2.95)  Time: 0.660s, 1551.03/s  (0.665s, 1538.81/s)  LR: 1.316e-05  Data: 0.013 (0.016)
Train: 282 [ 900/1251 ( 72%)]  Loss: 2.904 (2.95)  Time: 0.667s, 1535.80/s  (0.665s, 1538.87/s)  LR: 1.312e-05  Data: 0.013 (0.016)
Train: 282 [ 950/1251 ( 76%)]  Loss: 2.908 (2.94)  Time: 0.659s, 1554.18/s  (0.665s, 1539.02/s)  LR: 1.309e-05  Data: 0.013 (0.016)
Train: 282 [1000/1251 ( 80%)]  Loss: 2.742 (2.93)  Time: 0.672s, 1524.92/s  (0.665s, 1538.98/s)  LR: 1.305e-05  Data: 0.013 (0.016)
Train: 282 [1050/1251 ( 84%)]  Loss: 3.022 (2.94)  Time: 0.661s, 1550.33/s  (0.665s, 1539.15/s)  LR: 1.301e-05  Data: 0.013 (0.016)
Train: 282 [1100/1251 ( 88%)]  Loss: 3.160 (2.95)  Time: 0.670s, 1527.34/s  (0.665s, 1539.26/s)  LR: 1.297e-05  Data: 0.013 (0.016)
Train: 282 [1150/1251 ( 92%)]  Loss: 3.139 (2.96)  Time: 0.675s, 1516.61/s  (0.665s, 1539.33/s)  LR: 1.294e-05  Data: 0.012 (0.015)
Train: 282 [1200/1251 ( 96%)]  Loss: 2.865 (2.95)  Time: 0.663s, 1544.09/s  (0.665s, 1539.53/s)  LR: 1.290e-05  Data: 0.012 (0.015)
Train: 282 [1250/1251 (100%)]  Loss: 3.024 (2.95)  Time: 0.660s, 1550.71/s  (0.665s, 1539.92/s)  LR: 1.286e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.749 (2.749)  Loss:  0.3735 (0.3735)  Acc@1: 93.2617 (93.2617)  Acc@5: 98.3398 (98.3398)
Test: [  48/48]  Time: 0.161 (0.320)  Loss:  0.4883 (0.7944)  Acc@1: 87.7358 (81.5740)  Acc@5: 98.5849 (95.6760)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-282.pth.tar', 81.57399997314454)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-281.pth.tar', 81.56600007568359)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-280.pth.tar', 81.56200012939453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-276.pth.tar', 81.51599989746094)

Train: 283 [   0/1251 (  0%)]  Loss: 2.976 (2.98)  Time: 3.481s,  294.19/s  (3.481s,  294.19/s)  LR: 1.286e-05  Data: 1.681 (1.681)
Train: 283 [  50/1251 (  4%)]  Loss: 3.142 (3.06)  Time: 0.649s, 1578.26/s  (0.682s, 1501.30/s)  LR: 1.283e-05  Data: 0.016 (0.047)
Train: 283 [ 100/1251 (  8%)]  Loss: 3.109 (3.08)  Time: 0.661s, 1549.95/s  (0.667s, 1534.78/s)  LR: 1.279e-05  Data: 0.017 (0.030)
Train: 283 [ 150/1251 ( 12%)]  Loss: 3.124 (3.09)  Time: 0.657s, 1559.20/s  (0.663s, 1544.62/s)  LR: 1.275e-05  Data: 0.014 (0.025)
Train: 283 [ 200/1251 ( 16%)]  Loss: 2.754 (3.02)  Time: 0.667s, 1535.31/s  (0.662s, 1545.97/s)  LR: 1.272e-05  Data: 0.014 (0.022)
Train: 283 [ 250/1251 ( 20%)]  Loss: 2.884 (3.00)  Time: 0.660s, 1551.57/s  (0.662s, 1546.18/s)  LR: 1.268e-05  Data: 0.012 (0.020)
Train: 283 [ 300/1251 ( 24%)]  Loss: 3.035 (3.00)  Time: 0.668s, 1533.21/s  (0.663s, 1545.62/s)  LR: 1.264e-05  Data: 0.013 (0.019)
Train: 283 [ 350/1251 ( 28%)]  Loss: 2.497 (2.94)  Time: 0.667s, 1535.47/s  (0.663s, 1545.24/s)  LR: 1.261e-05  Data: 0.012 (0.019)
Train: 283 [ 400/1251 ( 32%)]  Loss: 2.730 (2.92)  Time: 0.666s, 1538.68/s  (0.663s, 1544.80/s)  LR: 1.257e-05  Data: 0.014 (0.018)
Train: 283 [ 450/1251 ( 36%)]  Loss: 3.009 (2.93)  Time: 0.665s, 1540.04/s  (0.663s, 1544.75/s)  LR: 1.253e-05  Data: 0.017 (0.018)
Train: 283 [ 500/1251 ( 40%)]  Loss: 3.079 (2.94)  Time: 0.669s, 1530.98/s  (0.663s, 1544.29/s)  LR: 1.250e-05  Data: 0.014 (0.017)
Train: 283 [ 550/1251 ( 44%)]  Loss: 3.083 (2.95)  Time: 0.664s, 1542.37/s  (0.663s, 1543.80/s)  LR: 1.246e-05  Data: 0.017 (0.017)
Train: 283 [ 600/1251 ( 48%)]  Loss: 2.810 (2.94)  Time: 0.669s, 1529.75/s  (0.664s, 1543.16/s)  LR: 1.243e-05  Data: 0.014 (0.017)
Train: 283 [ 650/1251 ( 52%)]  Loss: 3.193 (2.96)  Time: 0.669s, 1529.90/s  (0.664s, 1542.80/s)  LR: 1.239e-05  Data: 0.016 (0.016)
Train: 283 [ 700/1251 ( 56%)]  Loss: 2.793 (2.95)  Time: 0.668s, 1531.87/s  (0.664s, 1542.51/s)  LR: 1.235e-05  Data: 0.013 (0.016)
Train: 283 [ 750/1251 ( 60%)]  Loss: 2.686 (2.93)  Time: 0.668s, 1533.96/s  (0.664s, 1542.39/s)  LR: 1.232e-05  Data: 0.013 (0.016)
Train: 283 [ 800/1251 ( 64%)]  Loss: 2.730 (2.92)  Time: 0.665s, 1539.71/s  (0.664s, 1542.13/s)  LR: 1.228e-05  Data: 0.013 (0.016)
Train: 283 [ 850/1251 ( 68%)]  Loss: 2.712 (2.91)  Time: 0.664s, 1543.22/s  (0.664s, 1541.97/s)  LR: 1.225e-05  Data: 0.013 (0.016)
Train: 283 [ 900/1251 ( 72%)]  Loss: 2.932 (2.91)  Time: 0.671s, 1526.79/s  (0.664s, 1541.96/s)  LR: 1.221e-05  Data: 0.017 (0.016)
Train: 283 [ 950/1251 ( 76%)]  Loss: 2.885 (2.91)  Time: 0.674s, 1519.19/s  (0.664s, 1541.67/s)  LR: 1.218e-05  Data: 0.013 (0.016)
Train: 283 [1000/1251 ( 80%)]  Loss: 3.297 (2.93)  Time: 0.668s, 1532.95/s  (0.664s, 1541.54/s)  LR: 1.214e-05  Data: 0.014 (0.015)
Train: 283 [1050/1251 ( 84%)]  Loss: 2.980 (2.93)  Time: 0.672s, 1524.45/s  (0.664s, 1541.47/s)  LR: 1.211e-05  Data: 0.013 (0.015)
Train: 283 [1100/1251 ( 88%)]  Loss: 3.029 (2.93)  Time: 0.654s, 1566.00/s  (0.664s, 1541.43/s)  LR: 1.207e-05  Data: 0.013 (0.015)
Train: 283 [1150/1251 ( 92%)]  Loss: 2.880 (2.93)  Time: 0.669s, 1531.15/s  (0.664s, 1541.72/s)  LR: 1.204e-05  Data: 0.013 (0.015)
Train: 283 [1200/1251 ( 96%)]  Loss: 2.783 (2.93)  Time: 0.666s, 1536.96/s  (0.664s, 1541.71/s)  LR: 1.200e-05  Data: 0.016 (0.015)
Train: 283 [1250/1251 (100%)]  Loss: 2.888 (2.92)  Time: 0.647s, 1582.15/s  (0.664s, 1541.93/s)  LR: 1.197e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.922 (2.922)  Loss:  0.3738 (0.3738)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.161 (0.322)  Loss:  0.4932 (0.7946)  Acc@1: 87.7359 (81.5180)  Acc@5: 98.5849 (95.7160)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-282.pth.tar', 81.57399997314454)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-281.pth.tar', 81.56600007568359)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-280.pth.tar', 81.56200012939453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-283.pth.tar', 81.51800010253906)

Train: 284 [   0/1251 (  0%)]  Loss: 2.641 (2.64)  Time: 3.792s,  270.06/s  (3.792s,  270.06/s)  LR: 1.197e-05  Data: 1.887 (1.887)
Train: 284 [  50/1251 (  4%)]  Loss: 3.141 (2.89)  Time: 0.651s, 1571.79/s  (0.681s, 1503.13/s)  LR: 1.193e-05  Data: 0.014 (0.051)
Train: 284 [ 100/1251 (  8%)]  Loss: 3.008 (2.93)  Time: 0.658s, 1555.80/s  (0.667s, 1535.47/s)  LR: 1.190e-05  Data: 0.013 (0.032)
Train: 284 [ 150/1251 ( 12%)]  Loss: 2.797 (2.90)  Time: 0.658s, 1555.95/s  (0.664s, 1542.98/s)  LR: 1.186e-05  Data: 0.014 (0.026)
Train: 284 [ 200/1251 ( 16%)]  Loss: 3.069 (2.93)  Time: 0.663s, 1544.80/s  (0.662s, 1545.91/s)  LR: 1.183e-05  Data: 0.013 (0.023)
Train: 284 [ 250/1251 ( 20%)]  Loss: 2.989 (2.94)  Time: 0.663s, 1545.64/s  (0.662s, 1547.82/s)  LR: 1.179e-05  Data: 0.014 (0.021)
Train: 284 [ 300/1251 ( 24%)]  Loss: 2.794 (2.92)  Time: 0.658s, 1557.39/s  (0.661s, 1548.87/s)  LR: 1.176e-05  Data: 0.014 (0.020)
Train: 284 [ 350/1251 ( 28%)]  Loss: 2.775 (2.90)  Time: 0.664s, 1541.76/s  (0.661s, 1549.74/s)  LR: 1.173e-05  Data: 0.012 (0.019)
Train: 284 [ 400/1251 ( 32%)]  Loss: 2.912 (2.90)  Time: 0.661s, 1549.78/s  (0.660s, 1550.35/s)  LR: 1.169e-05  Data: 0.014 (0.019)
Train: 284 [ 450/1251 ( 36%)]  Loss: 2.988 (2.91)  Time: 0.659s, 1553.34/s  (0.660s, 1550.75/s)  LR: 1.166e-05  Data: 0.018 (0.018)
Train: 284 [ 500/1251 ( 40%)]  Loss: 2.998 (2.92)  Time: 0.660s, 1550.86/s  (0.660s, 1551.01/s)  LR: 1.162e-05  Data: 0.016 (0.018)
Train: 284 [ 550/1251 ( 44%)]  Loss: 2.967 (2.92)  Time: 0.658s, 1555.07/s  (0.660s, 1551.92/s)  LR: 1.159e-05  Data: 0.013 (0.017)
Train: 284 [ 600/1251 ( 48%)]  Loss: 2.807 (2.91)  Time: 0.655s, 1564.20/s  (0.660s, 1552.34/s)  LR: 1.156e-05  Data: 0.013 (0.017)
Train: 284 [ 650/1251 ( 52%)]  Loss: 3.001 (2.92)  Time: 0.648s, 1580.62/s  (0.660s, 1552.59/s)  LR: 1.152e-05  Data: 0.014 (0.017)
Train: 284 [ 700/1251 ( 56%)]  Loss: 3.043 (2.93)  Time: 0.661s, 1550.27/s  (0.659s, 1552.78/s)  LR: 1.149e-05  Data: 0.016 (0.017)
Train: 284 [ 750/1251 ( 60%)]  Loss: 2.710 (2.92)  Time: 0.659s, 1554.82/s  (0.659s, 1552.80/s)  LR: 1.146e-05  Data: 0.013 (0.016)
Train: 284 [ 800/1251 ( 64%)]  Loss: 3.123 (2.93)  Time: 0.655s, 1563.86/s  (0.659s, 1553.02/s)  LR: 1.142e-05  Data: 0.013 (0.016)
Train: 284 [ 850/1251 ( 68%)]  Loss: 2.723 (2.92)  Time: 0.663s, 1544.71/s  (0.659s, 1552.98/s)  LR: 1.139e-05  Data: 0.014 (0.016)
Train: 284 [ 900/1251 ( 72%)]  Loss: 2.919 (2.92)  Time: 0.650s, 1576.13/s  (0.659s, 1553.09/s)  LR: 1.136e-05  Data: 0.013 (0.016)
Train: 284 [ 950/1251 ( 76%)]  Loss: 3.114 (2.93)  Time: 0.673s, 1522.42/s  (0.659s, 1553.35/s)  LR: 1.132e-05  Data: 0.013 (0.016)
Train: 284 [1000/1251 ( 80%)]  Loss: 3.054 (2.93)  Time: 0.658s, 1557.07/s  (0.659s, 1553.47/s)  LR: 1.129e-05  Data: 0.013 (0.016)
Train: 284 [1050/1251 ( 84%)]  Loss: 2.947 (2.93)  Time: 0.654s, 1564.69/s  (0.659s, 1553.62/s)  LR: 1.126e-05  Data: 0.014 (0.016)
Train: 284 [1100/1251 ( 88%)]  Loss: 2.745 (2.92)  Time: 0.655s, 1562.44/s  (0.659s, 1553.76/s)  LR: 1.122e-05  Data: 0.016 (0.016)
Train: 284 [1150/1251 ( 92%)]  Loss: 2.779 (2.92)  Time: 0.653s, 1567.43/s  (0.659s, 1553.92/s)  LR: 1.119e-05  Data: 0.012 (0.015)
Train: 284 [1200/1251 ( 96%)]  Loss: 2.884 (2.92)  Time: 0.663s, 1545.66/s  (0.659s, 1554.14/s)  LR: 1.116e-05  Data: 0.013 (0.015)
Train: 284 [1250/1251 (100%)]  Loss: 3.023 (2.92)  Time: 0.647s, 1581.51/s  (0.659s, 1554.29/s)  LR: 1.113e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.834 (2.834)  Loss:  0.3694 (0.3694)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.161 (0.323)  Loss:  0.4861 (0.7876)  Acc@1: 87.8538 (81.6180)  Acc@5: 98.5849 (95.6680)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-284.pth.tar', 81.61800002441406)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-282.pth.tar', 81.57399997314454)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-281.pth.tar', 81.56600007568359)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-280.pth.tar', 81.56200012939453)

Train: 285 [   0/1251 (  0%)]  Loss: 2.872 (2.87)  Time: 3.429s,  298.64/s  (3.429s,  298.64/s)  LR: 1.112e-05  Data: 1.582 (1.582)
Train: 285 [  50/1251 (  4%)]  Loss: 3.105 (2.99)  Time: 0.644s, 1590.26/s  (0.678s, 1511.24/s)  LR: 1.109e-05  Data: 0.013 (0.045)
Train: 285 [ 100/1251 (  8%)]  Loss: 3.270 (3.08)  Time: 0.654s, 1564.65/s  (0.663s, 1545.05/s)  LR: 1.106e-05  Data: 0.014 (0.030)
Train: 285 [ 150/1251 ( 12%)]  Loss: 2.938 (3.05)  Time: 0.654s, 1565.56/s  (0.659s, 1554.25/s)  LR: 1.103e-05  Data: 0.016 (0.025)
Train: 285 [ 200/1251 ( 16%)]  Loss: 2.975 (3.03)  Time: 0.654s, 1566.22/s  (0.658s, 1557.08/s)  LR: 1.099e-05  Data: 0.013 (0.022)
Train: 285 [ 250/1251 ( 20%)]  Loss: 2.990 (3.03)  Time: 0.651s, 1573.23/s  (0.657s, 1557.59/s)  LR: 1.096e-05  Data: 0.013 (0.020)
Train: 285 [ 300/1251 ( 24%)]  Loss: 2.780 (2.99)  Time: 0.652s, 1571.54/s  (0.657s, 1557.47/s)  LR: 1.093e-05  Data: 0.015 (0.019)
Train: 285 [ 350/1251 ( 28%)]  Loss: 2.869 (2.97)  Time: 0.663s, 1545.41/s  (0.658s, 1556.95/s)  LR: 1.090e-05  Data: 0.014 (0.018)
Train: 285 [ 400/1251 ( 32%)]  Loss: 3.071 (2.99)  Time: 0.663s, 1544.95/s  (0.658s, 1557.41/s)  LR: 1.087e-05  Data: 0.012 (0.018)
Train: 285 [ 450/1251 ( 36%)]  Loss: 3.132 (3.00)  Time: 0.651s, 1572.06/s  (0.657s, 1558.13/s)  LR: 1.083e-05  Data: 0.013 (0.017)
Train: 285 [ 500/1251 ( 40%)]  Loss: 2.567 (2.96)  Time: 0.648s, 1579.53/s  (0.657s, 1559.14/s)  LR: 1.080e-05  Data: 0.013 (0.017)
Train: 285 [ 550/1251 ( 44%)]  Loss: 2.567 (2.93)  Time: 0.658s, 1556.07/s  (0.657s, 1559.67/s)  LR: 1.077e-05  Data: 0.014 (0.017)
Train: 285 [ 600/1251 ( 48%)]  Loss: 3.129 (2.94)  Time: 0.654s, 1564.70/s  (0.656s, 1560.05/s)  LR: 1.074e-05  Data: 0.013 (0.017)
Train: 285 [ 650/1251 ( 52%)]  Loss: 2.912 (2.94)  Time: 0.656s, 1560.68/s  (0.656s, 1560.44/s)  LR: 1.071e-05  Data: 0.016 (0.016)
Train: 285 [ 700/1251 ( 56%)]  Loss: 3.030 (2.95)  Time: 0.649s, 1578.26/s  (0.656s, 1560.89/s)  LR: 1.068e-05  Data: 0.016 (0.016)
Train: 285 [ 750/1251 ( 60%)]  Loss: 2.995 (2.95)  Time: 0.651s, 1573.23/s  (0.656s, 1561.13/s)  LR: 1.065e-05  Data: 0.017 (0.016)
Train: 285 [ 800/1251 ( 64%)]  Loss: 3.015 (2.95)  Time: 0.662s, 1545.72/s  (0.656s, 1561.25/s)  LR: 1.061e-05  Data: 0.013 (0.016)
Train: 285 [ 850/1251 ( 68%)]  Loss: 2.901 (2.95)  Time: 0.648s, 1579.17/s  (0.656s, 1561.53/s)  LR: 1.058e-05  Data: 0.015 (0.016)
Train: 285 [ 900/1251 ( 72%)]  Loss: 2.996 (2.95)  Time: 0.647s, 1582.21/s  (0.656s, 1561.69/s)  LR: 1.055e-05  Data: 0.013 (0.016)
Train: 285 [ 950/1251 ( 76%)]  Loss: 2.647 (2.94)  Time: 0.651s, 1572.02/s  (0.656s, 1561.75/s)  LR: 1.052e-05  Data: 0.013 (0.016)
Train: 285 [1000/1251 ( 80%)]  Loss: 2.941 (2.94)  Time: 0.659s, 1553.97/s  (0.656s, 1561.68/s)  LR: 1.049e-05  Data: 0.013 (0.015)
Train: 285 [1050/1251 ( 84%)]  Loss: 3.027 (2.94)  Time: 0.646s, 1585.13/s  (0.656s, 1561.76/s)  LR: 1.046e-05  Data: 0.013 (0.015)
Train: 285 [1100/1251 ( 88%)]  Loss: 2.942 (2.94)  Time: 0.651s, 1573.83/s  (0.656s, 1561.69/s)  LR: 1.043e-05  Data: 0.013 (0.015)
Train: 285 [1150/1251 ( 92%)]  Loss: 2.972 (2.94)  Time: 0.652s, 1570.09/s  (0.656s, 1561.54/s)  LR: 1.040e-05  Data: 0.013 (0.015)
Train: 285 [1200/1251 ( 96%)]  Loss: 2.692 (2.93)  Time: 0.660s, 1551.89/s  (0.656s, 1561.09/s)  LR: 1.037e-05  Data: 0.014 (0.015)
Train: 285 [1250/1251 (100%)]  Loss: 3.084 (2.94)  Time: 0.647s, 1583.52/s  (0.656s, 1560.88/s)  LR: 1.034e-05  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.804 (2.804)  Loss:  0.3730 (0.3730)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.324)  Loss:  0.4885 (0.7880)  Acc@1: 87.7358 (81.6100)  Acc@5: 98.4670 (95.7800)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-284.pth.tar', 81.61800002441406)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-285.pth.tar', 81.60999997314453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-282.pth.tar', 81.57399997314454)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-281.pth.tar', 81.56600007568359)

Train: 286 [   0/1251 (  0%)]  Loss: 2.862 (2.86)  Time: 3.778s,  271.02/s  (3.778s,  271.02/s)  LR: 1.034e-05  Data: 1.591 (1.591)
Train: 286 [  50/1251 (  4%)]  Loss: 2.855 (2.86)  Time: 0.639s, 1603.46/s  (0.684s, 1496.38/s)  LR: 1.031e-05  Data: 0.013 (0.045)
Train: 286 [ 100/1251 (  8%)]  Loss: 2.875 (2.86)  Time: 0.661s, 1549.67/s  (0.667s, 1535.07/s)  LR: 1.028e-05  Data: 0.015 (0.030)
Train: 286 [ 150/1251 ( 12%)]  Loss: 2.690 (2.82)  Time: 0.656s, 1560.99/s  (0.663s, 1544.62/s)  LR: 1.025e-05  Data: 0.013 (0.025)
Train: 286 [ 200/1251 ( 16%)]  Loss: 3.195 (2.90)  Time: 0.660s, 1551.79/s  (0.662s, 1546.69/s)  LR: 1.022e-05  Data: 0.013 (0.022)
Train: 286 [ 250/1251 ( 20%)]  Loss: 2.966 (2.91)  Time: 0.656s, 1560.19/s  (0.661s, 1548.93/s)  LR: 1.019e-05  Data: 0.012 (0.020)
Train: 286 [ 300/1251 ( 24%)]  Loss: 2.904 (2.91)  Time: 0.656s, 1562.04/s  (0.660s, 1550.58/s)  LR: 1.016e-05  Data: 0.012 (0.019)
Train: 286 [ 350/1251 ( 28%)]  Loss: 3.101 (2.93)  Time: 0.656s, 1559.90/s  (0.660s, 1551.57/s)  LR: 1.013e-05  Data: 0.013 (0.018)
Train: 286 [ 400/1251 ( 32%)]  Loss: 2.992 (2.94)  Time: 0.659s, 1552.98/s  (0.660s, 1552.46/s)  LR: 1.010e-05  Data: 0.014 (0.018)
Train: 286 [ 450/1251 ( 36%)]  Loss: 3.032 (2.95)  Time: 0.660s, 1552.32/s  (0.659s, 1552.79/s)  LR: 1.007e-05  Data: 0.014 (0.017)
Train: 286 [ 500/1251 ( 40%)]  Loss: 2.824 (2.94)  Time: 0.665s, 1540.53/s  (0.659s, 1553.22/s)  LR: 1.004e-05  Data: 0.015 (0.017)
Train: 286 [ 550/1251 ( 44%)]  Loss: 2.690 (2.92)  Time: 0.662s, 1546.79/s  (0.659s, 1553.16/s)  LR: 1.001e-05  Data: 0.012 (0.017)
Train: 286 [ 600/1251 ( 48%)]  Loss: 2.733 (2.90)  Time: 0.661s, 1550.03/s  (0.659s, 1553.61/s)  LR: 9.978e-06  Data: 0.013 (0.016)
Train: 286 [ 650/1251 ( 52%)]  Loss: 3.015 (2.91)  Time: 0.654s, 1565.49/s  (0.659s, 1554.14/s)  LR: 9.948e-06  Data: 0.013 (0.016)
Train: 286 [ 700/1251 ( 56%)]  Loss: 3.168 (2.93)  Time: 0.654s, 1565.53/s  (0.659s, 1554.67/s)  LR: 9.919e-06  Data: 0.013 (0.016)
Train: 286 [ 750/1251 ( 60%)]  Loss: 3.100 (2.94)  Time: 0.661s, 1549.21/s  (0.659s, 1554.84/s)  LR: 9.890e-06  Data: 0.013 (0.016)
Train: 286 [ 800/1251 ( 64%)]  Loss: 3.012 (2.94)  Time: 0.652s, 1569.91/s  (0.659s, 1554.98/s)  LR: 9.861e-06  Data: 0.013 (0.016)
Train: 286 [ 850/1251 ( 68%)]  Loss: 3.033 (2.95)  Time: 0.657s, 1558.25/s  (0.659s, 1554.83/s)  LR: 9.832e-06  Data: 0.013 (0.016)
Train: 286 [ 900/1251 ( 72%)]  Loss: 3.243 (2.96)  Time: 0.673s, 1521.56/s  (0.659s, 1554.62/s)  LR: 9.803e-06  Data: 0.014 (0.016)
Train: 286 [ 950/1251 ( 76%)]  Loss: 3.013 (2.97)  Time: 0.659s, 1554.03/s  (0.659s, 1554.52/s)  LR: 9.774e-06  Data: 0.013 (0.015)
Train: 286 [1000/1251 ( 80%)]  Loss: 3.118 (2.97)  Time: 0.665s, 1539.71/s  (0.659s, 1554.30/s)  LR: 9.745e-06  Data: 0.014 (0.015)
Train: 286 [1050/1251 ( 84%)]  Loss: 2.818 (2.97)  Time: 0.655s, 1563.52/s  (0.659s, 1554.02/s)  LR: 9.717e-06  Data: 0.014 (0.015)
Train: 286 [1100/1251 ( 88%)]  Loss: 2.895 (2.96)  Time: 0.656s, 1561.70/s  (0.659s, 1554.02/s)  LR: 9.688e-06  Data: 0.013 (0.015)
Train: 286 [1150/1251 ( 92%)]  Loss: 2.912 (2.96)  Time: 0.655s, 1563.27/s  (0.659s, 1553.85/s)  LR: 9.660e-06  Data: 0.013 (0.015)
Train: 286 [1200/1251 ( 96%)]  Loss: 2.823 (2.95)  Time: 0.658s, 1556.88/s  (0.659s, 1553.81/s)  LR: 9.631e-06  Data: 0.017 (0.015)
Train: 286 [1250/1251 (100%)]  Loss: 2.733 (2.95)  Time: 0.650s, 1574.94/s  (0.659s, 1554.28/s)  LR: 9.603e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.836 (2.836)  Loss:  0.3723 (0.3723)  Acc@1: 92.5781 (92.5781)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.161 (0.323)  Loss:  0.4907 (0.7943)  Acc@1: 87.6179 (81.6100)  Acc@5: 98.5849 (95.7180)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-284.pth.tar', 81.61800002441406)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-286.pth.tar', 81.61000005126954)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-285.pth.tar', 81.60999997314453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-282.pth.tar', 81.57399997314454)

Train: 287 [   0/1251 (  0%)]  Loss: 2.775 (2.78)  Time: 3.210s,  319.05/s  (3.210s,  319.05/s)  LR: 9.602e-06  Data: 1.915 (1.915)
Train: 287 [  50/1251 (  4%)]  Loss: 3.057 (2.92)  Time: 0.645s, 1588.49/s  (0.676s, 1515.38/s)  LR: 9.574e-06  Data: 0.014 (0.051)
Train: 287 [ 100/1251 (  8%)]  Loss: 2.697 (2.84)  Time: 0.650s, 1574.82/s  (0.662s, 1546.26/s)  LR: 9.546e-06  Data: 0.014 (0.033)
Train: 287 [ 150/1251 ( 12%)]  Loss: 2.797 (2.83)  Time: 0.653s, 1568.78/s  (0.660s, 1551.73/s)  LR: 9.518e-06  Data: 0.013 (0.027)
Train: 287 [ 200/1251 ( 16%)]  Loss: 2.836 (2.83)  Time: 0.656s, 1560.02/s  (0.659s, 1553.64/s)  LR: 9.490e-06  Data: 0.013 (0.024)
Train: 287 [ 250/1251 ( 20%)]  Loss: 3.070 (2.87)  Time: 0.669s, 1530.54/s  (0.659s, 1554.45/s)  LR: 9.462e-06  Data: 0.012 (0.022)
Train: 287 [ 300/1251 ( 24%)]  Loss: 3.055 (2.90)  Time: 0.661s, 1549.68/s  (0.658s, 1555.37/s)  LR: 9.434e-06  Data: 0.013 (0.020)
Train: 287 [ 350/1251 ( 28%)]  Loss: 2.981 (2.91)  Time: 0.653s, 1568.66/s  (0.658s, 1556.30/s)  LR: 9.407e-06  Data: 0.013 (0.019)
Train: 287 [ 400/1251 ( 32%)]  Loss: 2.650 (2.88)  Time: 0.668s, 1533.18/s  (0.658s, 1556.32/s)  LR: 9.379e-06  Data: 0.013 (0.019)
Train: 287 [ 450/1251 ( 36%)]  Loss: 2.794 (2.87)  Time: 0.651s, 1572.70/s  (0.658s, 1556.51/s)  LR: 9.352e-06  Data: 0.014 (0.018)
Train: 287 [ 500/1251 ( 40%)]  Loss: 2.705 (2.86)  Time: 0.661s, 1550.13/s  (0.658s, 1556.63/s)  LR: 9.324e-06  Data: 0.014 (0.018)
Train: 287 [ 550/1251 ( 44%)]  Loss: 2.921 (2.86)  Time: 0.658s, 1555.82/s  (0.658s, 1556.53/s)  LR: 9.297e-06  Data: 0.013 (0.017)
Train: 287 [ 600/1251 ( 48%)]  Loss: 3.194 (2.89)  Time: 0.664s, 1541.77/s  (0.658s, 1556.22/s)  LR: 9.270e-06  Data: 0.014 (0.017)
Train: 287 [ 650/1251 ( 52%)]  Loss: 2.985 (2.89)  Time: 0.652s, 1570.38/s  (0.658s, 1556.16/s)  LR: 9.242e-06  Data: 0.016 (0.017)
Train: 287 [ 700/1251 ( 56%)]  Loss: 2.714 (2.88)  Time: 0.658s, 1555.64/s  (0.658s, 1555.96/s)  LR: 9.215e-06  Data: 0.013 (0.017)
Train: 287 [ 750/1251 ( 60%)]  Loss: 2.871 (2.88)  Time: 0.661s, 1549.26/s  (0.658s, 1555.59/s)  LR: 9.188e-06  Data: 0.013 (0.016)
Train: 287 [ 800/1251 ( 64%)]  Loss: 2.914 (2.88)  Time: 0.658s, 1555.73/s  (0.658s, 1555.35/s)  LR: 9.161e-06  Data: 0.016 (0.016)
Train: 287 [ 850/1251 ( 68%)]  Loss: 3.107 (2.90)  Time: 0.661s, 1550.05/s  (0.658s, 1555.17/s)  LR: 9.134e-06  Data: 0.017 (0.016)
Train: 287 [ 900/1251 ( 72%)]  Loss: 2.664 (2.88)  Time: 0.664s, 1543.05/s  (0.659s, 1554.73/s)  LR: 9.108e-06  Data: 0.013 (0.016)
Train: 287 [ 950/1251 ( 76%)]  Loss: 3.054 (2.89)  Time: 0.649s, 1577.85/s  (0.659s, 1554.84/s)  LR: 9.081e-06  Data: 0.013 (0.016)
Train: 287 [1000/1251 ( 80%)]  Loss: 3.080 (2.90)  Time: 0.650s, 1575.52/s  (0.659s, 1554.88/s)  LR: 9.055e-06  Data: 0.017 (0.016)
Train: 287 [1050/1251 ( 84%)]  Loss: 3.004 (2.91)  Time: 0.653s, 1567.22/s  (0.659s, 1554.77/s)  LR: 9.028e-06  Data: 0.013 (0.016)
Train: 287 [1100/1251 ( 88%)]  Loss: 3.118 (2.91)  Time: 0.666s, 1537.28/s  (0.659s, 1554.59/s)  LR: 9.002e-06  Data: 0.013 (0.016)
Train: 287 [1150/1251 ( 92%)]  Loss: 3.116 (2.92)  Time: 0.673s, 1522.28/s  (0.659s, 1554.25/s)  LR: 8.975e-06  Data: 0.013 (0.016)
Train: 287 [1200/1251 ( 96%)]  Loss: 3.091 (2.93)  Time: 0.662s, 1547.02/s  (0.659s, 1554.24/s)  LR: 8.949e-06  Data: 0.013 (0.015)
Train: 287 [1250/1251 (100%)]  Loss: 2.947 (2.93)  Time: 0.643s, 1591.48/s  (0.659s, 1554.21/s)  LR: 8.923e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.061 (3.061)  Loss:  0.3672 (0.3672)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.161 (0.321)  Loss:  0.4851 (0.7899)  Acc@1: 87.2642 (81.6260)  Acc@5: 98.7028 (95.7260)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-287.pth.tar', 81.62600002685546)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-284.pth.tar', 81.61800002441406)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-286.pth.tar', 81.61000005126954)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-285.pth.tar', 81.60999997314453)

Train: 288 [   0/1251 (  0%)]  Loss: 2.731 (2.73)  Time: 3.700s,  276.79/s  (3.700s,  276.79/s)  LR: 8.922e-06  Data: 1.831 (1.831)
Train: 288 [  50/1251 (  4%)]  Loss: 2.840 (2.79)  Time: 0.645s, 1588.68/s  (0.684s, 1497.59/s)  LR: 8.896e-06  Data: 0.013 (0.050)
Train: 288 [ 100/1251 (  8%)]  Loss: 2.657 (2.74)  Time: 0.665s, 1540.52/s  (0.668s, 1533.63/s)  LR: 8.870e-06  Data: 0.015 (0.032)
Train: 288 [ 150/1251 ( 12%)]  Loss: 3.019 (2.81)  Time: 0.654s, 1566.60/s  (0.664s, 1541.54/s)  LR: 8.845e-06  Data: 0.014 (0.026)
Train: 288 [ 200/1251 ( 16%)]  Loss: 3.059 (2.86)  Time: 0.653s, 1567.66/s  (0.662s, 1546.56/s)  LR: 8.819e-06  Data: 0.013 (0.023)
Train: 288 [ 250/1251 ( 20%)]  Loss: 3.094 (2.90)  Time: 0.649s, 1577.79/s  (0.662s, 1547.84/s)  LR: 8.793e-06  Data: 0.015 (0.021)
Train: 288 [ 300/1251 ( 24%)]  Loss: 2.797 (2.89)  Time: 0.661s, 1549.19/s  (0.662s, 1547.22/s)  LR: 8.767e-06  Data: 0.012 (0.020)
Train: 288 [ 350/1251 ( 28%)]  Loss: 2.937 (2.89)  Time: 0.662s, 1546.41/s  (0.662s, 1547.35/s)  LR: 8.742e-06  Data: 0.013 (0.019)
Train: 288 [ 400/1251 ( 32%)]  Loss: 3.138 (2.92)  Time: 0.664s, 1542.35/s  (0.662s, 1547.98/s)  LR: 8.716e-06  Data: 0.013 (0.019)
Train: 288 [ 450/1251 ( 36%)]  Loss: 2.961 (2.92)  Time: 0.650s, 1574.64/s  (0.661s, 1548.54/s)  LR: 8.691e-06  Data: 0.016 (0.018)
Train: 288 [ 500/1251 ( 40%)]  Loss: 2.847 (2.92)  Time: 0.660s, 1552.31/s  (0.661s, 1548.91/s)  LR: 8.666e-06  Data: 0.013 (0.018)
Train: 288 [ 550/1251 ( 44%)]  Loss: 2.939 (2.92)  Time: 0.668s, 1532.20/s  (0.661s, 1549.04/s)  LR: 8.641e-06  Data: 0.013 (0.017)
Train: 288 [ 600/1251 ( 48%)]  Loss: 2.771 (2.91)  Time: 0.652s, 1569.57/s  (0.661s, 1548.86/s)  LR: 8.615e-06  Data: 0.013 (0.017)
Train: 288 [ 650/1251 ( 52%)]  Loss: 3.135 (2.92)  Time: 0.670s, 1528.24/s  (0.661s, 1548.83/s)  LR: 8.590e-06  Data: 0.014 (0.017)
Train: 288 [ 700/1251 ( 56%)]  Loss: 2.869 (2.92)  Time: 0.663s, 1545.37/s  (0.661s, 1549.00/s)  LR: 8.566e-06  Data: 0.013 (0.017)
Train: 288 [ 750/1251 ( 60%)]  Loss: 2.756 (2.91)  Time: 0.668s, 1533.62/s  (0.661s, 1549.01/s)  LR: 8.541e-06  Data: 0.018 (0.016)
Train: 288 [ 800/1251 ( 64%)]  Loss: 3.093 (2.92)  Time: 0.667s, 1534.71/s  (0.661s, 1548.72/s)  LR: 8.516e-06  Data: 0.013 (0.016)
Train: 288 [ 850/1251 ( 68%)]  Loss: 3.010 (2.93)  Time: 0.664s, 1541.59/s  (0.661s, 1548.46/s)  LR: 8.491e-06  Data: 0.013 (0.016)
Train: 288 [ 900/1251 ( 72%)]  Loss: 2.841 (2.92)  Time: 0.661s, 1549.83/s  (0.661s, 1548.40/s)  LR: 8.467e-06  Data: 0.013 (0.016)
Train: 288 [ 950/1251 ( 76%)]  Loss: 2.859 (2.92)  Time: 0.654s, 1565.48/s  (0.661s, 1548.72/s)  LR: 8.442e-06  Data: 0.013 (0.016)
Train: 288 [1000/1251 ( 80%)]  Loss: 2.965 (2.92)  Time: 0.661s, 1548.14/s  (0.661s, 1548.60/s)  LR: 8.418e-06  Data: 0.014 (0.016)
Train: 288 [1050/1251 ( 84%)]  Loss: 2.575 (2.90)  Time: 0.655s, 1562.87/s  (0.661s, 1548.73/s)  LR: 8.393e-06  Data: 0.014 (0.016)
Train: 288 [1100/1251 ( 88%)]  Loss: 2.975 (2.91)  Time: 0.654s, 1566.24/s  (0.661s, 1548.91/s)  LR: 8.369e-06  Data: 0.013 (0.016)
Train: 288 [1150/1251 ( 92%)]  Loss: 3.004 (2.91)  Time: 0.664s, 1542.89/s  (0.661s, 1548.59/s)  LR: 8.345e-06  Data: 0.015 (0.015)
Train: 288 [1200/1251 ( 96%)]  Loss: 3.067 (2.92)  Time: 0.653s, 1567.68/s  (0.661s, 1548.63/s)  LR: 8.321e-06  Data: 0.013 (0.015)
Train: 288 [1250/1251 (100%)]  Loss: 3.143 (2.93)  Time: 0.651s, 1573.32/s  (0.661s, 1548.75/s)  LR: 8.297e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.776 (2.776)  Loss:  0.3713 (0.3713)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.317)  Loss:  0.4873 (0.7913)  Acc@1: 87.3821 (81.5900)  Acc@5: 98.8207 (95.7380)
Train: 289 [   0/1251 (  0%)]  Loss: 3.208 (3.21)  Time: 3.452s,  296.60/s  (3.452s,  296.60/s)  LR: 8.297e-06  Data: 1.627 (1.627)
Train: 289 [  50/1251 (  4%)]  Loss: 3.024 (3.12)  Time: 0.643s, 1593.51/s  (0.682s, 1501.55/s)  LR: 8.273e-06  Data: 0.016 (0.045)
Train: 289 [ 100/1251 (  8%)]  Loss: 3.065 (3.10)  Time: 0.651s, 1573.37/s  (0.666s, 1537.93/s)  LR: 8.249e-06  Data: 0.013 (0.030)
Train: 289 [ 150/1251 ( 12%)]  Loss: 2.867 (3.04)  Time: 0.659s, 1552.71/s  (0.663s, 1544.33/s)  LR: 8.225e-06  Data: 0.014 (0.024)
Train: 289 [ 200/1251 ( 16%)]  Loss: 2.980 (3.03)  Time: 0.650s, 1576.18/s  (0.662s, 1546.02/s)  LR: 8.202e-06  Data: 0.013 (0.022)
Train: 289 [ 250/1251 ( 20%)]  Loss: 2.959 (3.02)  Time: 0.651s, 1573.93/s  (0.661s, 1548.04/s)  LR: 8.178e-06  Data: 0.016 (0.020)
Train: 289 [ 300/1251 ( 24%)]  Loss: 3.029 (3.02)  Time: 0.659s, 1553.61/s  (0.661s, 1549.15/s)  LR: 8.155e-06  Data: 0.013 (0.019)
Train: 289 [ 350/1251 ( 28%)]  Loss: 3.006 (3.02)  Time: 0.662s, 1547.13/s  (0.661s, 1549.38/s)  LR: 8.131e-06  Data: 0.013 (0.018)
Train: 289 [ 400/1251 ( 32%)]  Loss: 2.977 (3.01)  Time: 0.661s, 1549.30/s  (0.661s, 1550.16/s)  LR: 8.108e-06  Data: 0.013 (0.018)
Train: 289 [ 450/1251 ( 36%)]  Loss: 2.983 (3.01)  Time: 0.654s, 1564.64/s  (0.660s, 1550.43/s)  LR: 8.085e-06  Data: 0.013 (0.017)
Train: 289 [ 500/1251 ( 40%)]  Loss: 3.028 (3.01)  Time: 0.662s, 1546.19/s  (0.660s, 1550.90/s)  LR: 8.062e-06  Data: 0.020 (0.017)
Train: 289 [ 550/1251 ( 44%)]  Loss: 3.032 (3.01)  Time: 0.652s, 1571.42/s  (0.660s, 1551.06/s)  LR: 8.039e-06  Data: 0.013 (0.017)
Train: 289 [ 600/1251 ( 48%)]  Loss: 2.607 (2.98)  Time: 0.659s, 1552.81/s  (0.660s, 1551.31/s)  LR: 8.016e-06  Data: 0.013 (0.016)
Train: 289 [ 650/1251 ( 52%)]  Loss: 2.886 (2.98)  Time: 0.662s, 1547.07/s  (0.660s, 1551.14/s)  LR: 7.993e-06  Data: 0.013 (0.016)
Train: 289 [ 700/1251 ( 56%)]  Loss: 2.873 (2.97)  Time: 0.650s, 1575.80/s  (0.660s, 1550.90/s)  LR: 7.970e-06  Data: 0.011 (0.016)
Train: 289 [ 750/1251 ( 60%)]  Loss: 2.819 (2.96)  Time: 0.666s, 1537.03/s  (0.660s, 1550.76/s)  LR: 7.947e-06  Data: 0.015 (0.016)
Train: 289 [ 800/1251 ( 64%)]  Loss: 2.816 (2.95)  Time: 0.654s, 1565.77/s  (0.660s, 1550.48/s)  LR: 7.925e-06  Data: 0.016 (0.016)
Train: 289 [ 850/1251 ( 68%)]  Loss: 2.789 (2.94)  Time: 0.661s, 1550.09/s  (0.661s, 1550.17/s)  LR: 7.902e-06  Data: 0.015 (0.016)
Train: 289 [ 900/1251 ( 72%)]  Loss: 2.670 (2.93)  Time: 0.667s, 1535.88/s  (0.661s, 1549.85/s)  LR: 7.880e-06  Data: 0.013 (0.015)
Train: 289 [ 950/1251 ( 76%)]  Loss: 2.681 (2.91)  Time: 0.658s, 1555.49/s  (0.661s, 1549.59/s)  LR: 7.858e-06  Data: 0.013 (0.015)
Train: 289 [1000/1251 ( 80%)]  Loss: 3.028 (2.92)  Time: 0.659s, 1554.67/s  (0.661s, 1549.40/s)  LR: 7.835e-06  Data: 0.014 (0.015)
Train: 289 [1050/1251 ( 84%)]  Loss: 2.976 (2.92)  Time: 0.660s, 1551.14/s  (0.661s, 1549.30/s)  LR: 7.813e-06  Data: 0.013 (0.015)
Train: 289 [1100/1251 ( 88%)]  Loss: 2.947 (2.92)  Time: 0.663s, 1545.64/s  (0.661s, 1549.21/s)  LR: 7.791e-06  Data: 0.013 (0.015)
Train: 289 [1150/1251 ( 92%)]  Loss: 2.681 (2.91)  Time: 0.660s, 1552.53/s  (0.661s, 1549.33/s)  LR: 7.769e-06  Data: 0.013 (0.015)
Train: 289 [1200/1251 ( 96%)]  Loss: 2.693 (2.91)  Time: 0.661s, 1548.11/s  (0.661s, 1549.29/s)  LR: 7.747e-06  Data: 0.014 (0.015)
Train: 289 [1250/1251 (100%)]  Loss: 2.794 (2.90)  Time: 0.652s, 1571.63/s  (0.661s, 1549.55/s)  LR: 7.725e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.740 (2.740)  Loss:  0.3721 (0.3721)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.161 (0.321)  Loss:  0.4873 (0.7941)  Acc@1: 87.5000 (81.5700)  Acc@5: 98.8207 (95.7320)
Train: 290 [   0/1251 (  0%)]  Loss: 2.924 (2.92)  Time: 3.345s,  306.11/s  (3.345s,  306.11/s)  LR: 7.725e-06  Data: 1.776 (1.776)
Train: 290 [  50/1251 (  4%)]  Loss: 2.909 (2.92)  Time: 0.638s, 1604.96/s  (0.673s, 1520.59/s)  LR: 7.703e-06  Data: 0.012 (0.048)
Train: 290 [ 100/1251 (  8%)]  Loss: 2.984 (2.94)  Time: 0.662s, 1547.18/s  (0.662s, 1547.51/s)  LR: 7.682e-06  Data: 0.013 (0.031)
Train: 290 [ 150/1251 ( 12%)]  Loss: 3.120 (2.98)  Time: 0.658s, 1555.69/s  (0.660s, 1551.81/s)  LR: 7.660e-06  Data: 0.013 (0.026)
Train: 290 [ 200/1251 ( 16%)]  Loss: 2.610 (2.91)  Time: 0.647s, 1583.40/s  (0.659s, 1554.63/s)  LR: 7.639e-06  Data: 0.013 (0.023)
Train: 290 [ 250/1251 ( 20%)]  Loss: 2.997 (2.92)  Time: 0.655s, 1563.90/s  (0.658s, 1555.55/s)  LR: 7.617e-06  Data: 0.013 (0.021)
Train: 290 [ 300/1251 ( 24%)]  Loss: 2.832 (2.91)  Time: 0.657s, 1559.12/s  (0.658s, 1556.10/s)  LR: 7.596e-06  Data: 0.014 (0.020)
Train: 290 [ 350/1251 ( 28%)]  Loss: 3.123 (2.94)  Time: 0.659s, 1554.09/s  (0.658s, 1556.43/s)  LR: 7.575e-06  Data: 0.013 (0.019)
Train: 290 [ 400/1251 ( 32%)]  Loss: 2.798 (2.92)  Time: 0.663s, 1544.32/s  (0.658s, 1556.69/s)  LR: 7.554e-06  Data: 0.013 (0.018)
Train: 290 [ 450/1251 ( 36%)]  Loss: 2.955 (2.93)  Time: 0.662s, 1547.84/s  (0.658s, 1556.64/s)  LR: 7.533e-06  Data: 0.013 (0.018)
Train: 290 [ 500/1251 ( 40%)]  Loss: 2.672 (2.90)  Time: 0.663s, 1545.38/s  (0.658s, 1555.48/s)  LR: 7.512e-06  Data: 0.014 (0.017)
Train: 290 [ 550/1251 ( 44%)]  Loss: 3.026 (2.91)  Time: 0.654s, 1565.22/s  (0.658s, 1555.22/s)  LR: 7.491e-06  Data: 0.014 (0.017)
Train: 290 [ 600/1251 ( 48%)]  Loss: 2.898 (2.91)  Time: 0.662s, 1547.42/s  (0.659s, 1554.56/s)  LR: 7.470e-06  Data: 0.013 (0.017)
Train: 290 [ 650/1251 ( 52%)]  Loss: 3.096 (2.92)  Time: 0.665s, 1539.53/s  (0.659s, 1553.44/s)  LR: 7.449e-06  Data: 0.013 (0.017)
Train: 290 [ 700/1251 ( 56%)]  Loss: 2.818 (2.92)  Time: 0.659s, 1552.82/s  (0.659s, 1553.08/s)  LR: 7.429e-06  Data: 0.012 (0.016)
Train: 290 [ 750/1251 ( 60%)]  Loss: 3.156 (2.93)  Time: 0.663s, 1544.43/s  (0.660s, 1552.33/s)  LR: 7.408e-06  Data: 0.014 (0.016)
Train: 290 [ 800/1251 ( 64%)]  Loss: 3.163 (2.95)  Time: 0.654s, 1565.33/s  (0.660s, 1551.89/s)  LR: 7.388e-06  Data: 0.014 (0.016)
Train: 290 [ 850/1251 ( 68%)]  Loss: 2.983 (2.95)  Time: 0.667s, 1534.31/s  (0.660s, 1551.78/s)  LR: 7.367e-06  Data: 0.013 (0.016)
Train: 290 [ 900/1251 ( 72%)]  Loss: 3.121 (2.96)  Time: 0.663s, 1543.81/s  (0.660s, 1551.37/s)  LR: 7.347e-06  Data: 0.012 (0.016)
Train: 290 [ 950/1251 ( 76%)]  Loss: 3.019 (2.96)  Time: 0.655s, 1562.78/s  (0.660s, 1551.43/s)  LR: 7.327e-06  Data: 0.013 (0.016)
Train: 290 [1000/1251 ( 80%)]  Loss: 2.824 (2.95)  Time: 0.661s, 1549.03/s  (0.660s, 1551.09/s)  LR: 7.307e-06  Data: 0.014 (0.016)
Train: 290 [1050/1251 ( 84%)]  Loss: 2.627 (2.94)  Time: 0.662s, 1546.78/s  (0.660s, 1550.85/s)  LR: 7.287e-06  Data: 0.013 (0.015)
Train: 290 [1100/1251 ( 88%)]  Loss: 2.825 (2.93)  Time: 0.656s, 1560.84/s  (0.660s, 1550.80/s)  LR: 7.267e-06  Data: 0.014 (0.015)
Train: 290 [1150/1251 ( 92%)]  Loss: 2.921 (2.93)  Time: 0.665s, 1538.95/s  (0.660s, 1550.64/s)  LR: 7.247e-06  Data: 0.014 (0.015)
Train: 290 [1200/1251 ( 96%)]  Loss: 2.852 (2.93)  Time: 0.659s, 1554.94/s  (0.660s, 1550.73/s)  LR: 7.228e-06  Data: 0.013 (0.015)
Train: 290 [1250/1251 (100%)]  Loss: 2.737 (2.92)  Time: 0.652s, 1570.03/s  (0.660s, 1550.85/s)  LR: 7.208e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.970 (2.970)  Loss:  0.3679 (0.3679)  Acc@1: 93.2617 (93.2617)  Acc@5: 98.6328 (98.6328)
Test: [  48/48]  Time: 0.162 (0.323)  Loss:  0.4917 (0.7932)  Acc@1: 87.6179 (81.6020)  Acc@5: 98.8208 (95.7320)
Train: 291 [   0/1251 (  0%)]  Loss: 3.091 (3.09)  Time: 4.423s,  231.51/s  (4.423s,  231.51/s)  LR: 7.208e-06  Data: 1.667 (1.667)
Train: 291 [  50/1251 (  4%)]  Loss: 2.934 (3.01)  Time: 0.643s, 1592.86/s  (0.689s, 1486.75/s)  LR: 7.188e-06  Data: 0.014 (0.046)
Train: 291 [ 100/1251 (  8%)]  Loss: 2.730 (2.92)  Time: 0.659s, 1553.63/s  (0.671s, 1526.03/s)  LR: 7.169e-06  Data: 0.013 (0.030)
Train: 291 [ 150/1251 ( 12%)]  Loss: 2.915 (2.92)  Time: 0.649s, 1578.12/s  (0.666s, 1537.20/s)  LR: 7.149e-06  Data: 0.013 (0.025)
Train: 291 [ 200/1251 ( 16%)]  Loss: 3.243 (2.98)  Time: 0.656s, 1561.53/s  (0.664s, 1541.60/s)  LR: 7.130e-06  Data: 0.015 (0.022)
Train: 291 [ 250/1251 ( 20%)]  Loss: 2.765 (2.95)  Time: 0.663s, 1545.62/s  (0.663s, 1544.28/s)  LR: 7.111e-06  Data: 0.015 (0.020)
Train: 291 [ 300/1251 ( 24%)]  Loss: 2.776 (2.92)  Time: 0.666s, 1538.65/s  (0.663s, 1545.42/s)  LR: 7.092e-06  Data: 0.013 (0.019)
Train: 291 [ 350/1251 ( 28%)]  Loss: 2.982 (2.93)  Time: 0.669s, 1530.80/s  (0.662s, 1546.24/s)  LR: 7.072e-06  Data: 0.013 (0.018)
Train: 291 [ 400/1251 ( 32%)]  Loss: 2.837 (2.92)  Time: 0.666s, 1538.04/s  (0.662s, 1546.31/s)  LR: 7.054e-06  Data: 0.013 (0.018)
Train: 291 [ 450/1251 ( 36%)]  Loss: 2.894 (2.92)  Time: 0.673s, 1522.45/s  (0.662s, 1546.04/s)  LR: 7.035e-06  Data: 0.015 (0.017)
Train: 291 [ 500/1251 ( 40%)]  Loss: 2.898 (2.92)  Time: 0.650s, 1574.38/s  (0.662s, 1546.07/s)  LR: 7.016e-06  Data: 0.013 (0.017)
Train: 291 [ 550/1251 ( 44%)]  Loss: 2.612 (2.89)  Time: 0.667s, 1534.47/s  (0.662s, 1545.69/s)  LR: 6.997e-06  Data: 0.013 (0.017)
Train: 291 [ 600/1251 ( 48%)]  Loss: 3.118 (2.91)  Time: 0.664s, 1542.73/s  (0.662s, 1545.74/s)  LR: 6.979e-06  Data: 0.012 (0.016)
Train: 291 [ 650/1251 ( 52%)]  Loss: 3.062 (2.92)  Time: 0.675s, 1517.70/s  (0.662s, 1545.66/s)  LR: 6.960e-06  Data: 0.016 (0.016)
Train: 291 [ 700/1251 ( 56%)]  Loss: 2.668 (2.90)  Time: 0.666s, 1537.69/s  (0.662s, 1545.66/s)  LR: 6.942e-06  Data: 0.014 (0.016)
Train: 291 [ 750/1251 ( 60%)]  Loss: 2.736 (2.89)  Time: 0.659s, 1553.63/s  (0.663s, 1545.60/s)  LR: 6.923e-06  Data: 0.016 (0.016)
Train: 291 [ 800/1251 ( 64%)]  Loss: 2.969 (2.90)  Time: 0.666s, 1537.07/s  (0.663s, 1545.33/s)  LR: 6.905e-06  Data: 0.014 (0.016)
Train: 291 [ 850/1251 ( 68%)]  Loss: 2.669 (2.88)  Time: 0.651s, 1572.80/s  (0.662s, 1545.79/s)  LR: 6.887e-06  Data: 0.015 (0.016)
Train: 291 [ 900/1251 ( 72%)]  Loss: 2.933 (2.89)  Time: 0.658s, 1555.92/s  (0.662s, 1546.09/s)  LR: 6.869e-06  Data: 0.014 (0.016)
Train: 291 [ 950/1251 ( 76%)]  Loss: 2.794 (2.88)  Time: 0.661s, 1548.20/s  (0.662s, 1546.49/s)  LR: 6.851e-06  Data: 0.013 (0.015)
Train: 291 [1000/1251 ( 80%)]  Loss: 2.812 (2.88)  Time: 0.655s, 1563.11/s  (0.662s, 1546.96/s)  LR: 6.833e-06  Data: 0.013 (0.015)
Train: 291 [1050/1251 ( 84%)]  Loss: 3.009 (2.88)  Time: 0.674s, 1519.38/s  (0.662s, 1547.12/s)  LR: 6.815e-06  Data: 0.016 (0.015)
Train: 291 [1100/1251 ( 88%)]  Loss: 2.864 (2.88)  Time: 0.651s, 1573.38/s  (0.662s, 1547.40/s)  LR: 6.797e-06  Data: 0.013 (0.015)
Train: 291 [1150/1251 ( 92%)]  Loss: 3.098 (2.89)  Time: 0.663s, 1543.52/s  (0.662s, 1547.74/s)  LR: 6.780e-06  Data: 0.013 (0.015)
Train: 291 [1200/1251 ( 96%)]  Loss: 3.049 (2.90)  Time: 0.660s, 1550.94/s  (0.661s, 1548.05/s)  LR: 6.762e-06  Data: 0.014 (0.015)
Train: 291 [1250/1251 (100%)]  Loss: 3.149 (2.91)  Time: 0.654s, 1566.28/s  (0.661s, 1548.16/s)  LR: 6.745e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.811 (2.811)  Loss:  0.3650 (0.3650)  Acc@1: 93.2617 (93.2617)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.322)  Loss:  0.4900 (0.7917)  Acc@1: 87.6179 (81.6420)  Acc@5: 98.5849 (95.7460)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-291.pth.tar', 81.64200005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-287.pth.tar', 81.62600002685546)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-284.pth.tar', 81.61800002441406)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-286.pth.tar', 81.61000005126954)

Train: 292 [   0/1251 (  0%)]  Loss: 3.017 (3.02)  Time: 3.769s,  271.72/s  (3.769s,  271.72/s)  LR: 6.744e-06  Data: 1.696 (1.696)
Train: 292 [  50/1251 (  4%)]  Loss: 3.028 (3.02)  Time: 0.643s, 1592.41/s  (0.681s, 1503.66/s)  LR: 6.727e-06  Data: 0.013 (0.047)
Train: 292 [ 100/1251 (  8%)]  Loss: 2.792 (2.95)  Time: 0.656s, 1560.44/s  (0.666s, 1538.60/s)  LR: 6.710e-06  Data: 0.014 (0.030)
Train: 292 [ 150/1251 ( 12%)]  Loss: 2.701 (2.88)  Time: 0.650s, 1574.56/s  (0.662s, 1546.53/s)  LR: 6.693e-06  Data: 0.013 (0.025)
Train: 292 [ 200/1251 ( 16%)]  Loss: 2.739 (2.86)  Time: 0.654s, 1566.34/s  (0.661s, 1548.55/s)  LR: 6.675e-06  Data: 0.013 (0.022)
Train: 292 [ 250/1251 ( 20%)]  Loss: 2.621 (2.82)  Time: 0.662s, 1545.77/s  (0.661s, 1549.43/s)  LR: 6.658e-06  Data: 0.013 (0.020)
Train: 292 [ 300/1251 ( 24%)]  Loss: 2.702 (2.80)  Time: 0.667s, 1535.54/s  (0.661s, 1549.18/s)  LR: 6.641e-06  Data: 0.013 (0.019)
Train: 292 [ 350/1251 ( 28%)]  Loss: 2.858 (2.81)  Time: 0.651s, 1572.92/s  (0.661s, 1549.45/s)  LR: 6.625e-06  Data: 0.013 (0.018)
Train: 292 [ 400/1251 ( 32%)]  Loss: 3.015 (2.83)  Time: 0.659s, 1553.27/s  (0.661s, 1549.92/s)  LR: 6.608e-06  Data: 0.013 (0.018)
Train: 292 [ 450/1251 ( 36%)]  Loss: 3.112 (2.86)  Time: 0.670s, 1528.22/s  (0.661s, 1549.87/s)  LR: 6.591e-06  Data: 0.013 (0.017)
Train: 292 [ 500/1251 ( 40%)]  Loss: 2.963 (2.87)  Time: 0.657s, 1557.60/s  (0.661s, 1549.97/s)  LR: 6.575e-06  Data: 0.013 (0.017)
Train: 292 [ 550/1251 ( 44%)]  Loss: 2.754 (2.86)  Time: 0.665s, 1539.91/s  (0.661s, 1549.42/s)  LR: 6.558e-06  Data: 0.014 (0.017)
Train: 292 [ 600/1251 ( 48%)]  Loss: 2.824 (2.86)  Time: 0.663s, 1545.03/s  (0.661s, 1549.14/s)  LR: 6.542e-06  Data: 0.016 (0.016)
Train: 292 [ 650/1251 ( 52%)]  Loss: 3.190 (2.88)  Time: 0.659s, 1553.29/s  (0.661s, 1548.99/s)  LR: 6.525e-06  Data: 0.013 (0.016)
Train: 292 [ 700/1251 ( 56%)]  Loss: 2.652 (2.86)  Time: 0.658s, 1556.04/s  (0.661s, 1549.04/s)  LR: 6.509e-06  Data: 0.016 (0.016)
Train: 292 [ 750/1251 ( 60%)]  Loss: 2.790 (2.86)  Time: 0.676s, 1514.07/s  (0.661s, 1548.81/s)  LR: 6.493e-06  Data: 0.013 (0.016)
Train: 292 [ 800/1251 ( 64%)]  Loss: 2.809 (2.86)  Time: 0.662s, 1547.78/s  (0.661s, 1548.59/s)  LR: 6.477e-06  Data: 0.013 (0.016)
Train: 292 [ 850/1251 ( 68%)]  Loss: 3.061 (2.87)  Time: 0.668s, 1533.42/s  (0.661s, 1548.33/s)  LR: 6.461e-06  Data: 0.013 (0.016)
Train: 292 [ 900/1251 ( 72%)]  Loss: 2.846 (2.87)  Time: 0.661s, 1548.50/s  (0.661s, 1548.35/s)  LR: 6.445e-06  Data: 0.014 (0.016)
Train: 292 [ 950/1251 ( 76%)]  Loss: 2.725 (2.86)  Time: 0.664s, 1541.77/s  (0.661s, 1548.70/s)  LR: 6.429e-06  Data: 0.014 (0.015)
Train: 292 [1000/1251 ( 80%)]  Loss: 3.073 (2.87)  Time: 0.659s, 1552.96/s  (0.661s, 1548.69/s)  LR: 6.413e-06  Data: 0.013 (0.015)
Train: 292 [1050/1251 ( 84%)]  Loss: 3.059 (2.88)  Time: 0.659s, 1554.26/s  (0.661s, 1548.95/s)  LR: 6.398e-06  Data: 0.015 (0.015)
Train: 292 [1100/1251 ( 88%)]  Loss: 2.790 (2.87)  Time: 0.659s, 1554.23/s  (0.661s, 1549.19/s)  LR: 6.382e-06  Data: 0.012 (0.015)
Train: 292 [1150/1251 ( 92%)]  Loss: 3.158 (2.89)  Time: 0.662s, 1546.59/s  (0.661s, 1549.47/s)  LR: 6.367e-06  Data: 0.014 (0.015)
Train: 292 [1200/1251 ( 96%)]  Loss: 3.017 (2.89)  Time: 0.654s, 1566.53/s  (0.661s, 1549.59/s)  LR: 6.351e-06  Data: 0.014 (0.015)
Train: 292 [1250/1251 (100%)]  Loss: 2.934 (2.89)  Time: 0.648s, 1581.09/s  (0.661s, 1549.81/s)  LR: 6.336e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.826 (2.826)  Loss:  0.3699 (0.3699)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.322)  Loss:  0.4888 (0.7925)  Acc@1: 87.7358 (81.6400)  Acc@5: 98.8208 (95.7960)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-291.pth.tar', 81.64200005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-292.pth.tar', 81.63999997314453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-287.pth.tar', 81.62600002685546)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-284.pth.tar', 81.61800002441406)

Train: 293 [   0/1251 (  0%)]  Loss: 2.962 (2.96)  Time: 4.074s,  251.35/s  (4.074s,  251.35/s)  LR: 6.336e-06  Data: 1.722 (1.722)
Train: 293 [  50/1251 (  4%)]  Loss: 2.805 (2.88)  Time: 0.649s, 1578.80/s  (0.689s, 1486.79/s)  LR: 6.321e-06  Data: 0.014 (0.047)
Train: 293 [ 100/1251 (  8%)]  Loss: 2.632 (2.80)  Time: 0.660s, 1552.47/s  (0.670s, 1528.17/s)  LR: 6.305e-06  Data: 0.013 (0.031)
Train: 293 [ 150/1251 ( 12%)]  Loss: 3.050 (2.86)  Time: 0.654s, 1564.82/s  (0.665s, 1539.07/s)  LR: 6.290e-06  Data: 0.014 (0.025)
Train: 293 [ 200/1251 ( 16%)]  Loss: 2.620 (2.81)  Time: 0.657s, 1559.38/s  (0.664s, 1542.73/s)  LR: 6.275e-06  Data: 0.013 (0.022)
Train: 293 [ 250/1251 ( 20%)]  Loss: 3.028 (2.85)  Time: 0.658s, 1555.33/s  (0.663s, 1543.75/s)  LR: 6.261e-06  Data: 0.013 (0.021)
Train: 293 [ 300/1251 ( 24%)]  Loss: 2.586 (2.81)  Time: 0.660s, 1552.32/s  (0.663s, 1545.35/s)  LR: 6.246e-06  Data: 0.013 (0.019)
Train: 293 [ 350/1251 ( 28%)]  Loss: 2.732 (2.80)  Time: 0.655s, 1562.21/s  (0.662s, 1545.77/s)  LR: 6.231e-06  Data: 0.014 (0.019)
Train: 293 [ 400/1251 ( 32%)]  Loss: 3.018 (2.83)  Time: 0.660s, 1551.35/s  (0.662s, 1546.24/s)  LR: 6.217e-06  Data: 0.016 (0.018)
Train: 293 [ 450/1251 ( 36%)]  Loss: 2.757 (2.82)  Time: 0.654s, 1565.96/s  (0.662s, 1546.68/s)  LR: 6.202e-06  Data: 0.014 (0.018)
Train: 293 [ 500/1251 ( 40%)]  Loss: 3.055 (2.84)  Time: 0.663s, 1543.64/s  (0.662s, 1546.53/s)  LR: 6.188e-06  Data: 0.013 (0.017)
Train: 293 [ 550/1251 ( 44%)]  Loss: 2.615 (2.82)  Time: 0.661s, 1549.34/s  (0.662s, 1546.23/s)  LR: 6.173e-06  Data: 0.013 (0.017)
Train: 293 [ 600/1251 ( 48%)]  Loss: 2.844 (2.82)  Time: 0.658s, 1555.27/s  (0.662s, 1546.33/s)  LR: 6.159e-06  Data: 0.017 (0.017)
Train: 293 [ 650/1251 ( 52%)]  Loss: 3.136 (2.85)  Time: 0.665s, 1540.84/s  (0.662s, 1546.44/s)  LR: 6.145e-06  Data: 0.012 (0.016)
Train: 293 [ 700/1251 ( 56%)]  Loss: 2.931 (2.85)  Time: 0.652s, 1570.42/s  (0.662s, 1546.79/s)  LR: 6.131e-06  Data: 0.013 (0.016)
Train: 293 [ 750/1251 ( 60%)]  Loss: 2.811 (2.85)  Time: 0.668s, 1533.57/s  (0.662s, 1547.02/s)  LR: 6.117e-06  Data: 0.014 (0.016)
Train: 293 [ 800/1251 ( 64%)]  Loss: 2.880 (2.85)  Time: 0.658s, 1556.72/s  (0.662s, 1547.31/s)  LR: 6.103e-06  Data: 0.013 (0.016)
Train: 293 [ 850/1251 ( 68%)]  Loss: 2.942 (2.86)  Time: 0.665s, 1540.95/s  (0.662s, 1547.56/s)  LR: 6.089e-06  Data: 0.013 (0.016)
Train: 293 [ 900/1251 ( 72%)]  Loss: 3.060 (2.87)  Time: 0.665s, 1540.25/s  (0.662s, 1547.58/s)  LR: 6.075e-06  Data: 0.014 (0.016)
Train: 293 [ 950/1251 ( 76%)]  Loss: 2.750 (2.86)  Time: 0.665s, 1540.91/s  (0.662s, 1547.58/s)  LR: 6.062e-06  Data: 0.013 (0.016)
Train: 293 [1000/1251 ( 80%)]  Loss: 2.905 (2.86)  Time: 0.661s, 1549.30/s  (0.662s, 1547.67/s)  LR: 6.048e-06  Data: 0.013 (0.015)
Train: 293 [1050/1251 ( 84%)]  Loss: 2.949 (2.87)  Time: 0.653s, 1567.51/s  (0.662s, 1547.80/s)  LR: 6.035e-06  Data: 0.016 (0.015)
Train: 293 [1100/1251 ( 88%)]  Loss: 2.864 (2.87)  Time: 0.666s, 1538.56/s  (0.662s, 1547.76/s)  LR: 6.021e-06  Data: 0.013 (0.015)
Train: 293 [1150/1251 ( 92%)]  Loss: 2.767 (2.86)  Time: 0.661s, 1550.32/s  (0.662s, 1547.71/s)  LR: 6.008e-06  Data: 0.014 (0.015)
Train: 293 [1200/1251 ( 96%)]  Loss: 2.981 (2.87)  Time: 0.670s, 1527.61/s  (0.662s, 1547.83/s)  LR: 5.995e-06  Data: 0.014 (0.015)
Train: 293 [1250/1251 (100%)]  Loss: 2.812 (2.87)  Time: 0.648s, 1580.37/s  (0.661s, 1548.12/s)  LR: 5.982e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.855 (2.855)  Loss:  0.3696 (0.3696)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.162 (0.320)  Loss:  0.4878 (0.7911)  Acc@1: 87.7358 (81.7120)  Acc@5: 98.7028 (95.7300)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-293.pth.tar', 81.71199997314453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-291.pth.tar', 81.64200005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-292.pth.tar', 81.63999997314453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-287.pth.tar', 81.62600002685546)

Train: 294 [   0/1251 (  0%)]  Loss: 2.781 (2.78)  Time: 3.621s,  282.83/s  (3.621s,  282.83/s)  LR: 5.981e-06  Data: 1.719 (1.719)
Train: 294 [  50/1251 (  4%)]  Loss: 3.097 (2.94)  Time: 0.645s, 1588.63/s  (0.678s, 1511.20/s)  LR: 5.968e-06  Data: 0.013 (0.047)
Train: 294 [ 100/1251 (  8%)]  Loss: 2.935 (2.94)  Time: 0.657s, 1558.26/s  (0.664s, 1542.57/s)  LR: 5.955e-06  Data: 0.013 (0.031)
Train: 294 [ 150/1251 ( 12%)]  Loss: 2.747 (2.89)  Time: 0.659s, 1554.69/s  (0.662s, 1547.43/s)  LR: 5.943e-06  Data: 0.013 (0.025)
Train: 294 [ 200/1251 ( 16%)]  Loss: 3.079 (2.93)  Time: 0.657s, 1558.09/s  (0.661s, 1550.17/s)  LR: 5.930e-06  Data: 0.017 (0.022)
Train: 294 [ 250/1251 ( 20%)]  Loss: 2.462 (2.85)  Time: 0.656s, 1560.97/s  (0.661s, 1549.90/s)  LR: 5.917e-06  Data: 0.013 (0.021)
Train: 294 [ 300/1251 ( 24%)]  Loss: 3.033 (2.88)  Time: 0.655s, 1562.55/s  (0.661s, 1550.29/s)  LR: 5.905e-06  Data: 0.014 (0.020)
Train: 294 [ 350/1251 ( 28%)]  Loss: 3.132 (2.91)  Time: 0.659s, 1554.08/s  (0.661s, 1549.86/s)  LR: 5.892e-06  Data: 0.013 (0.019)
Train: 294 [ 400/1251 ( 32%)]  Loss: 3.131 (2.93)  Time: 0.659s, 1553.60/s  (0.661s, 1549.51/s)  LR: 5.880e-06  Data: 0.013 (0.018)
Train: 294 [ 450/1251 ( 36%)]  Loss: 3.059 (2.95)  Time: 0.662s, 1545.82/s  (0.661s, 1549.76/s)  LR: 5.867e-06  Data: 0.013 (0.018)
Train: 294 [ 500/1251 ( 40%)]  Loss: 2.876 (2.94)  Time: 0.650s, 1575.25/s  (0.661s, 1549.79/s)  LR: 5.855e-06  Data: 0.013 (0.017)
Train: 294 [ 550/1251 ( 44%)]  Loss: 2.856 (2.93)  Time: 0.661s, 1550.22/s  (0.661s, 1549.91/s)  LR: 5.843e-06  Data: 0.013 (0.017)
Train: 294 [ 600/1251 ( 48%)]  Loss: 3.005 (2.94)  Time: 0.662s, 1547.74/s  (0.661s, 1550.21/s)  LR: 5.831e-06  Data: 0.012 (0.017)
Train: 294 [ 650/1251 ( 52%)]  Loss: 2.699 (2.92)  Time: 0.663s, 1545.07/s  (0.661s, 1550.27/s)  LR: 5.819e-06  Data: 0.014 (0.016)
Train: 294 [ 700/1251 ( 56%)]  Loss: 3.146 (2.94)  Time: 0.649s, 1577.32/s  (0.661s, 1550.24/s)  LR: 5.807e-06  Data: 0.013 (0.016)
Train: 294 [ 750/1251 ( 60%)]  Loss: 3.024 (2.94)  Time: 0.657s, 1558.50/s  (0.660s, 1550.65/s)  LR: 5.795e-06  Data: 0.013 (0.016)
Train: 294 [ 800/1251 ( 64%)]  Loss: 2.654 (2.92)  Time: 0.659s, 1553.90/s  (0.660s, 1550.89/s)  LR: 5.783e-06  Data: 0.013 (0.016)
Train: 294 [ 850/1251 ( 68%)]  Loss: 2.948 (2.93)  Time: 0.656s, 1560.75/s  (0.660s, 1550.91/s)  LR: 5.772e-06  Data: 0.013 (0.016)
Train: 294 [ 900/1251 ( 72%)]  Loss: 2.683 (2.91)  Time: 0.656s, 1560.46/s  (0.660s, 1550.97/s)  LR: 5.760e-06  Data: 0.013 (0.016)
Train: 294 [ 950/1251 ( 76%)]  Loss: 2.837 (2.91)  Time: 0.664s, 1542.76/s  (0.660s, 1550.85/s)  LR: 5.749e-06  Data: 0.013 (0.016)
Train: 294 [1000/1251 ( 80%)]  Loss: 2.926 (2.91)  Time: 0.656s, 1561.88/s  (0.660s, 1550.95/s)  LR: 5.737e-06  Data: 0.014 (0.015)
Train: 294 [1050/1251 ( 84%)]  Loss: 2.897 (2.91)  Time: 0.663s, 1544.02/s  (0.660s, 1551.00/s)  LR: 5.726e-06  Data: 0.013 (0.015)
Train: 294 [1100/1251 ( 88%)]  Loss: 2.857 (2.91)  Time: 0.653s, 1567.80/s  (0.660s, 1551.09/s)  LR: 5.715e-06  Data: 0.013 (0.015)
Train: 294 [1150/1251 ( 92%)]  Loss: 3.127 (2.92)  Time: 0.667s, 1534.92/s  (0.660s, 1551.13/s)  LR: 5.704e-06  Data: 0.013 (0.015)
Train: 294 [1200/1251 ( 96%)]  Loss: 3.183 (2.93)  Time: 0.662s, 1546.20/s  (0.660s, 1551.05/s)  LR: 5.693e-06  Data: 0.013 (0.015)
Train: 294 [1250/1251 (100%)]  Loss: 3.065 (2.93)  Time: 0.644s, 1590.48/s  (0.660s, 1551.15/s)  LR: 5.682e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.833 (2.833)  Loss:  0.3701 (0.3701)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.322)  Loss:  0.4900 (0.7920)  Acc@1: 87.6179 (81.6600)  Acc@5: 98.7028 (95.7280)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-293.pth.tar', 81.71199997314453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-294.pth.tar', 81.659999921875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-291.pth.tar', 81.64200005126953)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-292.pth.tar', 81.63999997314453)

Train: 295 [   0/1251 (  0%)]  Loss: 3.091 (3.09)  Time: 3.389s,  302.16/s  (3.389s,  302.16/s)  LR: 5.682e-06  Data: 1.827 (1.827)
Train: 295 [  50/1251 (  4%)]  Loss: 2.993 (3.04)  Time: 0.645s, 1588.39/s  (0.678s, 1510.69/s)  LR: 5.671e-06  Data: 0.014 (0.049)
Train: 295 [ 100/1251 (  8%)]  Loss: 3.038 (3.04)  Time: 0.654s, 1565.92/s  (0.666s, 1537.65/s)  LR: 5.660e-06  Data: 0.014 (0.031)
Train: 295 [ 150/1251 ( 12%)]  Loss: 2.997 (3.03)  Time: 0.655s, 1563.73/s  (0.664s, 1541.98/s)  LR: 5.649e-06  Data: 0.013 (0.025)
Train: 295 [ 200/1251 ( 16%)]  Loss: 3.080 (3.04)  Time: 0.668s, 1532.65/s  (0.663s, 1543.90/s)  LR: 5.639e-06  Data: 0.014 (0.022)
Train: 295 [ 250/1251 ( 20%)]  Loss: 3.318 (3.09)  Time: 0.666s, 1537.01/s  (0.663s, 1544.29/s)  LR: 5.628e-06  Data: 0.013 (0.021)
Train: 295 [ 300/1251 ( 24%)]  Loss: 2.904 (3.06)  Time: 0.670s, 1527.74/s  (0.663s, 1545.37/s)  LR: 5.618e-06  Data: 0.013 (0.019)
Train: 295 [ 350/1251 ( 28%)]  Loss: 2.992 (3.05)  Time: 0.659s, 1554.39/s  (0.662s, 1546.32/s)  LR: 5.607e-06  Data: 0.014 (0.019)
Train: 295 [ 400/1251 ( 32%)]  Loss: 3.080 (3.05)  Time: 0.661s, 1548.92/s  (0.662s, 1546.65/s)  LR: 5.597e-06  Data: 0.016 (0.018)
Train: 295 [ 450/1251 ( 36%)]  Loss: 3.115 (3.06)  Time: 0.662s, 1547.36/s  (0.662s, 1546.65/s)  LR: 5.587e-06  Data: 0.014 (0.018)
Train: 295 [ 500/1251 ( 40%)]  Loss: 2.951 (3.05)  Time: 0.655s, 1563.44/s  (0.662s, 1546.86/s)  LR: 5.577e-06  Data: 0.013 (0.017)
Train: 295 [ 550/1251 ( 44%)]  Loss: 3.149 (3.06)  Time: 0.664s, 1541.81/s  (0.662s, 1547.16/s)  LR: 5.567e-06  Data: 0.012 (0.017)
Train: 295 [ 600/1251 ( 48%)]  Loss: 3.006 (3.05)  Time: 0.656s, 1561.36/s  (0.662s, 1547.40/s)  LR: 5.557e-06  Data: 0.014 (0.017)
Train: 295 [ 650/1251 ( 52%)]  Loss: 2.720 (3.03)  Time: 0.655s, 1563.92/s  (0.662s, 1547.54/s)  LR: 5.547e-06  Data: 0.013 (0.016)
Train: 295 [ 700/1251 ( 56%)]  Loss: 2.706 (3.01)  Time: 0.655s, 1564.38/s  (0.662s, 1547.53/s)  LR: 5.538e-06  Data: 0.013 (0.016)
Train: 295 [ 750/1251 ( 60%)]  Loss: 2.976 (3.01)  Time: 0.663s, 1544.02/s  (0.662s, 1547.52/s)  LR: 5.528e-06  Data: 0.015 (0.016)
Train: 295 [ 800/1251 ( 64%)]  Loss: 3.071 (3.01)  Time: 0.660s, 1552.08/s  (0.662s, 1547.56/s)  LR: 5.518e-06  Data: 0.014 (0.016)
Train: 295 [ 850/1251 ( 68%)]  Loss: 3.028 (3.01)  Time: 0.658s, 1555.39/s  (0.662s, 1547.87/s)  LR: 5.509e-06  Data: 0.016 (0.016)
Train: 295 [ 900/1251 ( 72%)]  Loss: 2.844 (3.00)  Time: 0.659s, 1554.99/s  (0.662s, 1547.92/s)  LR: 5.500e-06  Data: 0.013 (0.016)
Train: 295 [ 950/1251 ( 76%)]  Loss: 2.737 (2.99)  Time: 0.662s, 1546.65/s  (0.661s, 1548.14/s)  LR: 5.490e-06  Data: 0.013 (0.015)
Train: 295 [1000/1251 ( 80%)]  Loss: 3.247 (3.00)  Time: 0.659s, 1554.95/s  (0.661s, 1548.46/s)  LR: 5.481e-06  Data: 0.013 (0.015)
Train: 295 [1050/1251 ( 84%)]  Loss: 2.764 (2.99)  Time: 0.659s, 1554.98/s  (0.661s, 1548.56/s)  LR: 5.472e-06  Data: 0.013 (0.015)
Train: 295 [1100/1251 ( 88%)]  Loss: 2.827 (2.98)  Time: 0.664s, 1542.37/s  (0.661s, 1548.71/s)  LR: 5.463e-06  Data: 0.013 (0.015)
Train: 295 [1150/1251 ( 92%)]  Loss: 3.181 (2.99)  Time: 0.644s, 1589.18/s  (0.661s, 1548.74/s)  LR: 5.454e-06  Data: 0.014 (0.015)
Train: 295 [1200/1251 ( 96%)]  Loss: 2.855 (2.99)  Time: 0.667s, 1534.92/s  (0.661s, 1548.64/s)  LR: 5.445e-06  Data: 0.014 (0.015)
Train: 295 [1250/1251 (100%)]  Loss: 3.098 (2.99)  Time: 0.645s, 1587.88/s  (0.661s, 1548.68/s)  LR: 5.436e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.005 (3.005)  Loss:  0.3711 (0.3711)  Acc@1: 93.2617 (93.2617)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.162 (0.315)  Loss:  0.4917 (0.7933)  Acc@1: 87.5000 (81.6180)  Acc@5: 98.7028 (95.7300)
Train: 296 [   0/1251 (  0%)]  Loss: 3.146 (3.15)  Time: 3.262s,  313.91/s  (3.262s,  313.91/s)  LR: 5.436e-06  Data: 1.833 (1.833)
Train: 296 [  50/1251 (  4%)]  Loss: 3.075 (3.11)  Time: 0.648s, 1579.64/s  (0.679s, 1507.60/s)  LR: 5.428e-06  Data: 0.016 (0.049)
Train: 296 [ 100/1251 (  8%)]  Loss: 2.952 (3.06)  Time: 0.646s, 1584.57/s  (0.666s, 1537.83/s)  LR: 5.419e-06  Data: 0.014 (0.032)
Train: 296 [ 150/1251 ( 12%)]  Loss: 2.549 (2.93)  Time: 0.660s, 1551.73/s  (0.663s, 1543.53/s)  LR: 5.410e-06  Data: 0.013 (0.026)
Train: 296 [ 200/1251 ( 16%)]  Loss: 3.019 (2.95)  Time: 0.651s, 1573.25/s  (0.662s, 1545.89/s)  LR: 5.402e-06  Data: 0.014 (0.023)
Train: 296 [ 250/1251 ( 20%)]  Loss: 3.050 (2.97)  Time: 0.659s, 1555.04/s  (0.662s, 1547.12/s)  LR: 5.394e-06  Data: 0.013 (0.021)
Train: 296 [ 300/1251 ( 24%)]  Loss: 2.743 (2.93)  Time: 0.671s, 1525.01/s  (0.662s, 1547.88/s)  LR: 5.385e-06  Data: 0.016 (0.020)
Train: 296 [ 350/1251 ( 28%)]  Loss: 3.140 (2.96)  Time: 0.657s, 1558.73/s  (0.661s, 1549.16/s)  LR: 5.377e-06  Data: 0.013 (0.019)
Train: 296 [ 400/1251 ( 32%)]  Loss: 3.100 (2.97)  Time: 0.657s, 1559.73/s  (0.661s, 1550.16/s)  LR: 5.369e-06  Data: 0.014 (0.018)
Train: 296 [ 450/1251 ( 36%)]  Loss: 2.885 (2.97)  Time: 0.653s, 1568.84/s  (0.660s, 1551.03/s)  LR: 5.361e-06  Data: 0.014 (0.018)
Train: 296 [ 500/1251 ( 40%)]  Loss: 2.734 (2.94)  Time: 0.658s, 1556.49/s  (0.660s, 1551.85/s)  LR: 5.353e-06  Data: 0.013 (0.017)
Train: 296 [ 550/1251 ( 44%)]  Loss: 3.052 (2.95)  Time: 0.662s, 1546.60/s  (0.660s, 1552.47/s)  LR: 5.346e-06  Data: 0.015 (0.017)
Train: 296 [ 600/1251 ( 48%)]  Loss: 2.919 (2.95)  Time: 0.661s, 1548.35/s  (0.660s, 1552.24/s)  LR: 5.338e-06  Data: 0.012 (0.017)
Train: 296 [ 650/1251 ( 52%)]  Loss: 3.178 (2.97)  Time: 0.661s, 1548.31/s  (0.660s, 1552.23/s)  LR: 5.330e-06  Data: 0.012 (0.016)
Train: 296 [ 700/1251 ( 56%)]  Loss: 2.920 (2.96)  Time: 0.655s, 1564.16/s  (0.660s, 1552.28/s)  LR: 5.323e-06  Data: 0.014 (0.016)
Train: 296 [ 750/1251 ( 60%)]  Loss: 2.952 (2.96)  Time: 0.665s, 1539.85/s  (0.660s, 1552.29/s)  LR: 5.315e-06  Data: 0.012 (0.016)
Train: 296 [ 800/1251 ( 64%)]  Loss: 2.878 (2.96)  Time: 0.661s, 1549.40/s  (0.660s, 1552.27/s)  LR: 5.308e-06  Data: 0.012 (0.016)
Train: 296 [ 850/1251 ( 68%)]  Loss: 2.827 (2.95)  Time: 0.663s, 1545.40/s  (0.660s, 1551.97/s)  LR: 5.301e-06  Data: 0.013 (0.016)
Train: 296 [ 900/1251 ( 72%)]  Loss: 2.980 (2.95)  Time: 0.654s, 1565.83/s  (0.660s, 1551.74/s)  LR: 5.293e-06  Data: 0.016 (0.016)
Train: 296 [ 950/1251 ( 76%)]  Loss: 2.884 (2.95)  Time: 0.659s, 1554.10/s  (0.660s, 1551.62/s)  LR: 5.286e-06  Data: 0.016 (0.015)
Train: 296 [1000/1251 ( 80%)]  Loss: 3.058 (2.95)  Time: 0.668s, 1532.11/s  (0.660s, 1551.46/s)  LR: 5.279e-06  Data: 0.014 (0.015)
Train: 296 [1050/1251 ( 84%)]  Loss: 2.761 (2.95)  Time: 0.666s, 1537.60/s  (0.660s, 1551.24/s)  LR: 5.272e-06  Data: 0.016 (0.015)
Train: 296 [1100/1251 ( 88%)]  Loss: 2.865 (2.94)  Time: 0.661s, 1548.63/s  (0.660s, 1551.02/s)  LR: 5.265e-06  Data: 0.014 (0.015)
Train: 296 [1150/1251 ( 92%)]  Loss: 3.061 (2.95)  Time: 0.655s, 1563.07/s  (0.660s, 1550.87/s)  LR: 5.259e-06  Data: 0.013 (0.015)
Train: 296 [1200/1251 ( 96%)]  Loss: 2.900 (2.95)  Time: 0.665s, 1540.30/s  (0.660s, 1550.65/s)  LR: 5.252e-06  Data: 0.016 (0.015)
Train: 296 [1250/1251 (100%)]  Loss: 2.761 (2.94)  Time: 0.658s, 1555.78/s  (0.660s, 1550.56/s)  LR: 5.245e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.776 (2.776)  Loss:  0.3682 (0.3682)  Acc@1: 93.2617 (93.2617)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.162 (0.323)  Loss:  0.4871 (0.7890)  Acc@1: 87.5000 (81.6280)  Acc@5: 98.7028 (95.7400)
Train: 297 [   0/1251 (  0%)]  Loss: 2.798 (2.80)  Time: 3.367s,  304.16/s  (3.367s,  304.16/s)  LR: 5.245e-06  Data: 1.659 (1.659)
Train: 297 [  50/1251 (  4%)]  Loss: 2.893 (2.85)  Time: 0.647s, 1582.95/s  (0.681s, 1502.74/s)  LR: 5.239e-06  Data: 0.014 (0.046)
Train: 297 [ 100/1251 (  8%)]  Loss: 3.062 (2.92)  Time: 0.657s, 1557.81/s  (0.667s, 1535.11/s)  LR: 5.232e-06  Data: 0.013 (0.030)
Train: 297 [ 150/1251 ( 12%)]  Loss: 3.038 (2.95)  Time: 0.658s, 1556.62/s  (0.665s, 1540.66/s)  LR: 5.226e-06  Data: 0.013 (0.025)
Train: 297 [ 200/1251 ( 16%)]  Loss: 2.865 (2.93)  Time: 0.660s, 1550.71/s  (0.664s, 1542.69/s)  LR: 5.220e-06  Data: 0.013 (0.022)
Train: 297 [ 250/1251 ( 20%)]  Loss: 3.138 (2.97)  Time: 0.669s, 1530.41/s  (0.663s, 1544.02/s)  LR: 5.214e-06  Data: 0.013 (0.020)
Train: 297 [ 300/1251 ( 24%)]  Loss: 2.864 (2.95)  Time: 0.666s, 1537.76/s  (0.663s, 1543.44/s)  LR: 5.208e-06  Data: 0.014 (0.019)
Train: 297 [ 350/1251 ( 28%)]  Loss: 2.766 (2.93)  Time: 0.667s, 1534.29/s  (0.663s, 1543.96/s)  LR: 5.202e-06  Data: 0.013 (0.018)
Train: 297 [ 400/1251 ( 32%)]  Loss: 2.721 (2.90)  Time: 0.656s, 1560.51/s  (0.663s, 1543.80/s)  LR: 5.196e-06  Data: 0.013 (0.018)
Train: 297 [ 450/1251 ( 36%)]  Loss: 3.049 (2.92)  Time: 0.662s, 1545.86/s  (0.663s, 1543.89/s)  LR: 5.190e-06  Data: 0.014 (0.017)
Train: 297 [ 500/1251 ( 40%)]  Loss: 3.172 (2.94)  Time: 0.669s, 1531.59/s  (0.663s, 1543.83/s)  LR: 5.184e-06  Data: 0.014 (0.017)
Train: 297 [ 550/1251 ( 44%)]  Loss: 2.805 (2.93)  Time: 0.666s, 1536.95/s  (0.663s, 1543.78/s)  LR: 5.179e-06  Data: 0.013 (0.017)
Train: 297 [ 600/1251 ( 48%)]  Loss: 2.937 (2.93)  Time: 0.666s, 1536.94/s  (0.663s, 1543.93/s)  LR: 5.173e-06  Data: 0.016 (0.017)
Train: 297 [ 650/1251 ( 52%)]  Loss: 3.006 (2.94)  Time: 0.667s, 1534.57/s  (0.663s, 1543.92/s)  LR: 5.168e-06  Data: 0.014 (0.016)
Train: 297 [ 700/1251 ( 56%)]  Loss: 3.111 (2.95)  Time: 0.660s, 1552.36/s  (0.663s, 1544.09/s)  LR: 5.162e-06  Data: 0.013 (0.016)
Train: 297 [ 750/1251 ( 60%)]  Loss: 2.839 (2.94)  Time: 0.662s, 1545.94/s  (0.663s, 1544.79/s)  LR: 5.157e-06  Data: 0.014 (0.016)
Train: 297 [ 800/1251 ( 64%)]  Loss: 2.777 (2.93)  Time: 0.666s, 1537.81/s  (0.663s, 1545.25/s)  LR: 5.152e-06  Data: 0.013 (0.016)
Train: 297 [ 850/1251 ( 68%)]  Loss: 3.081 (2.94)  Time: 0.650s, 1575.48/s  (0.663s, 1545.58/s)  LR: 5.147e-06  Data: 0.012 (0.016)
Train: 297 [ 900/1251 ( 72%)]  Loss: 2.927 (2.94)  Time: 0.658s, 1556.40/s  (0.662s, 1545.93/s)  LR: 5.142e-06  Data: 0.013 (0.015)
Train: 297 [ 950/1251 ( 76%)]  Loss: 2.584 (2.92)  Time: 0.663s, 1545.17/s  (0.662s, 1545.95/s)  LR: 5.137e-06  Data: 0.013 (0.015)
Train: 297 [1000/1251 ( 80%)]  Loss: 3.221 (2.94)  Time: 0.660s, 1552.01/s  (0.662s, 1545.87/s)  LR: 5.132e-06  Data: 0.014 (0.015)
Train: 297 [1050/1251 ( 84%)]  Loss: 2.956 (2.94)  Time: 0.657s, 1559.69/s  (0.662s, 1546.14/s)  LR: 5.127e-06  Data: 0.016 (0.015)
Train: 297 [1100/1251 ( 88%)]  Loss: 2.742 (2.93)  Time: 0.663s, 1544.02/s  (0.662s, 1546.23/s)  LR: 5.123e-06  Data: 0.013 (0.015)
Train: 297 [1150/1251 ( 92%)]  Loss: 3.023 (2.93)  Time: 0.664s, 1542.18/s  (0.662s, 1546.02/s)  LR: 5.118e-06  Data: 0.015 (0.015)
Train: 297 [1200/1251 ( 96%)]  Loss: 3.133 (2.94)  Time: 0.665s, 1539.85/s  (0.662s, 1546.02/s)  LR: 5.114e-06  Data: 0.013 (0.015)
Train: 297 [1250/1251 (100%)]  Loss: 2.937 (2.94)  Time: 0.654s, 1565.18/s  (0.662s, 1545.97/s)  LR: 5.109e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.788 (2.788)  Loss:  0.3662 (0.3662)  Acc@1: 93.2617 (93.2617)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.165 (0.321)  Loss:  0.4932 (0.7911)  Acc@1: 87.6179 (81.6540)  Acc@5: 98.7028 (95.7560)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-293.pth.tar', 81.71199997314453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-294.pth.tar', 81.659999921875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-297.pth.tar', 81.653999921875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-291.pth.tar', 81.64200005126953)

Train: 298 [   0/1251 (  0%)]  Loss: 2.731 (2.73)  Time: 3.413s,  300.04/s  (3.413s,  300.04/s)  LR: 5.109e-06  Data: 1.710 (1.710)
Train: 298 [  50/1251 (  4%)]  Loss: 2.823 (2.78)  Time: 0.648s, 1579.61/s  (0.680s, 1506.70/s)  LR: 5.105e-06  Data: 0.014 (0.047)
Train: 298 [ 100/1251 (  8%)]  Loss: 2.953 (2.84)  Time: 0.658s, 1555.37/s  (0.667s, 1536.23/s)  LR: 5.100e-06  Data: 0.014 (0.031)
Train: 298 [ 150/1251 ( 12%)]  Loss: 2.681 (2.80)  Time: 0.658s, 1556.40/s  (0.664s, 1542.77/s)  LR: 5.096e-06  Data: 0.013 (0.025)
Train: 298 [ 200/1251 ( 16%)]  Loss: 2.912 (2.82)  Time: 0.666s, 1538.03/s  (0.663s, 1545.45/s)  LR: 5.092e-06  Data: 0.014 (0.022)
Train: 298 [ 250/1251 ( 20%)]  Loss: 3.153 (2.88)  Time: 0.664s, 1542.71/s  (0.662s, 1546.18/s)  LR: 5.088e-06  Data: 0.013 (0.021)
Train: 298 [ 300/1251 ( 24%)]  Loss: 2.767 (2.86)  Time: 0.658s, 1556.54/s  (0.662s, 1547.09/s)  LR: 5.084e-06  Data: 0.013 (0.020)
Train: 298 [ 350/1251 ( 28%)]  Loss: 2.968 (2.87)  Time: 0.672s, 1523.35/s  (0.662s, 1546.76/s)  LR: 5.081e-06  Data: 0.013 (0.019)
Train: 298 [ 400/1251 ( 32%)]  Loss: 3.211 (2.91)  Time: 0.666s, 1538.66/s  (0.662s, 1545.69/s)  LR: 5.077e-06  Data: 0.014 (0.018)
Train: 298 [ 450/1251 ( 36%)]  Loss: 2.724 (2.89)  Time: 0.662s, 1546.30/s  (0.663s, 1544.80/s)  LR: 5.073e-06  Data: 0.014 (0.018)
Train: 298 [ 500/1251 ( 40%)]  Loss: 2.943 (2.90)  Time: 0.669s, 1531.31/s  (0.663s, 1543.93/s)  LR: 5.070e-06  Data: 0.017 (0.017)
Train: 298 [ 550/1251 ( 44%)]  Loss: 2.846 (2.89)  Time: 0.661s, 1549.67/s  (0.664s, 1543.01/s)  LR: 5.066e-06  Data: 0.012 (0.017)
Train: 298 [ 600/1251 ( 48%)]  Loss: 2.798 (2.89)  Time: 0.661s, 1548.39/s  (0.664s, 1542.40/s)  LR: 5.063e-06  Data: 0.014 (0.017)
Train: 298 [ 650/1251 ( 52%)]  Loss: 3.054 (2.90)  Time: 0.654s, 1565.18/s  (0.664s, 1542.42/s)  LR: 5.060e-06  Data: 0.013 (0.016)
Train: 298 [ 700/1251 ( 56%)]  Loss: 2.813 (2.89)  Time: 0.663s, 1544.24/s  (0.664s, 1542.63/s)  LR: 5.057e-06  Data: 0.013 (0.016)
Train: 298 [ 750/1251 ( 60%)]  Loss: 2.858 (2.89)  Time: 0.658s, 1556.16/s  (0.664s, 1542.89/s)  LR: 5.053e-06  Data: 0.013 (0.016)
Train: 298 [ 800/1251 ( 64%)]  Loss: 2.715 (2.88)  Time: 0.666s, 1537.84/s  (0.664s, 1543.23/s)  LR: 5.050e-06  Data: 0.012 (0.016)
Train: 298 [ 850/1251 ( 68%)]  Loss: 3.124 (2.89)  Time: 0.662s, 1545.73/s  (0.663s, 1543.47/s)  LR: 5.048e-06  Data: 0.014 (0.016)
Train: 298 [ 900/1251 ( 72%)]  Loss: 2.953 (2.90)  Time: 0.666s, 1537.69/s  (0.663s, 1543.52/s)  LR: 5.045e-06  Data: 0.014 (0.016)
Train: 298 [ 950/1251 ( 76%)]  Loss: 2.976 (2.90)  Time: 0.661s, 1550.29/s  (0.663s, 1543.92/s)  LR: 5.042e-06  Data: 0.013 (0.015)
Train: 298 [1000/1251 ( 80%)]  Loss: 2.998 (2.90)  Time: 0.662s, 1546.64/s  (0.663s, 1544.07/s)  LR: 5.039e-06  Data: 0.013 (0.015)
Train: 298 [1050/1251 ( 84%)]  Loss: 2.853 (2.90)  Time: 0.654s, 1566.46/s  (0.663s, 1543.89/s)  LR: 5.037e-06  Data: 0.018 (0.015)
Train: 298 [1100/1251 ( 88%)]  Loss: 2.897 (2.90)  Time: 0.667s, 1534.53/s  (0.663s, 1543.88/s)  LR: 5.034e-06  Data: 0.015 (0.015)
Train: 298 [1150/1251 ( 92%)]  Loss: 2.820 (2.90)  Time: 0.652s, 1569.66/s  (0.663s, 1543.83/s)  LR: 5.032e-06  Data: 0.012 (0.015)
Train: 298 [1200/1251 ( 96%)]  Loss: 2.796 (2.89)  Time: 0.662s, 1546.64/s  (0.663s, 1543.67/s)  LR: 5.030e-06  Data: 0.014 (0.015)
Train: 298 [1250/1251 (100%)]  Loss: 2.917 (2.90)  Time: 0.642s, 1593.81/s  (0.663s, 1543.79/s)  LR: 5.027e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.913 (2.913)  Loss:  0.3618 (0.3618)  Acc@1: 93.3594 (93.3594)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.162 (0.327)  Loss:  0.4802 (0.7870)  Acc@1: 87.8538 (81.6760)  Acc@5: 98.5849 (95.7120)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-293.pth.tar', 81.71199997314453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-298.pth.tar', 81.67600002441407)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-294.pth.tar', 81.659999921875)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-297.pth.tar', 81.653999921875)

Train: 299 [   0/1251 (  0%)]  Loss: 2.652 (2.65)  Time: 3.085s,  331.90/s  (3.085s,  331.90/s)  LR: 5.027e-06  Data: 1.571 (1.571)
Train: 299 [  50/1251 (  4%)]  Loss: 2.679 (2.67)  Time: 0.653s, 1567.11/s  (0.674s, 1518.49/s)  LR: 5.025e-06  Data: 0.016 (0.044)
Train: 299 [ 100/1251 (  8%)]  Loss: 2.781 (2.70)  Time: 0.656s, 1560.94/s  (0.666s, 1538.03/s)  LR: 5.023e-06  Data: 0.014 (0.029)
Train: 299 [ 150/1251 ( 12%)]  Loss: 2.926 (2.76)  Time: 0.671s, 1526.17/s  (0.665s, 1539.53/s)  LR: 5.021e-06  Data: 0.013 (0.024)
Train: 299 [ 200/1251 ( 16%)]  Loss: 3.132 (2.83)  Time: 0.669s, 1529.72/s  (0.665s, 1540.45/s)  LR: 5.019e-06  Data: 0.013 (0.021)
Train: 299 [ 250/1251 ( 20%)]  Loss: 2.743 (2.82)  Time: 0.672s, 1524.27/s  (0.665s, 1540.62/s)  LR: 5.017e-06  Data: 0.013 (0.020)
Train: 299 [ 300/1251 ( 24%)]  Loss: 3.194 (2.87)  Time: 0.672s, 1524.17/s  (0.665s, 1539.99/s)  LR: 5.016e-06  Data: 0.013 (0.019)
Train: 299 [ 350/1251 ( 28%)]  Loss: 2.751 (2.86)  Time: 0.669s, 1530.53/s  (0.665s, 1539.93/s)  LR: 5.014e-06  Data: 0.013 (0.018)
Train: 299 [ 400/1251 ( 32%)]  Loss: 2.855 (2.86)  Time: 0.660s, 1551.35/s  (0.665s, 1540.03/s)  LR: 5.013e-06  Data: 0.015 (0.018)
Train: 299 [ 450/1251 ( 36%)]  Loss: 3.036 (2.87)  Time: 0.659s, 1552.78/s  (0.665s, 1540.75/s)  LR: 5.011e-06  Data: 0.014 (0.017)
Train: 299 [ 500/1251 ( 40%)]  Loss: 2.419 (2.83)  Time: 0.656s, 1561.97/s  (0.664s, 1541.33/s)  LR: 5.010e-06  Data: 0.015 (0.017)
Train: 299 [ 550/1251 ( 44%)]  Loss: 2.853 (2.84)  Time: 0.665s, 1540.93/s  (0.664s, 1541.48/s)  LR: 5.009e-06  Data: 0.014 (0.017)
Train: 299 [ 600/1251 ( 48%)]  Loss: 2.797 (2.83)  Time: 0.658s, 1555.65/s  (0.664s, 1541.62/s)  LR: 5.007e-06  Data: 0.016 (0.016)
Train: 299 [ 650/1251 ( 52%)]  Loss: 2.778 (2.83)  Time: 0.668s, 1533.24/s  (0.664s, 1541.89/s)  LR: 5.006e-06  Data: 0.013 (0.016)
Train: 299 [ 700/1251 ( 56%)]  Loss: 2.949 (2.84)  Time: 0.658s, 1556.24/s  (0.664s, 1541.97/s)  LR: 5.005e-06  Data: 0.013 (0.016)
Train: 299 [ 750/1251 ( 60%)]  Loss: 3.021 (2.85)  Time: 0.667s, 1535.61/s  (0.664s, 1541.82/s)  LR: 5.004e-06  Data: 0.013 (0.016)
Train: 299 [ 800/1251 ( 64%)]  Loss: 2.790 (2.84)  Time: 0.670s, 1528.29/s  (0.664s, 1541.63/s)  LR: 5.004e-06  Data: 0.013 (0.016)
Train: 299 [ 850/1251 ( 68%)]  Loss: 3.097 (2.86)  Time: 0.666s, 1538.28/s  (0.664s, 1541.45/s)  LR: 5.003e-06  Data: 0.014 (0.016)
Train: 299 [ 900/1251 ( 72%)]  Loss: 2.918 (2.86)  Time: 0.658s, 1555.87/s  (0.664s, 1541.58/s)  LR: 5.002e-06  Data: 0.013 (0.015)
Train: 299 [ 950/1251 ( 76%)]  Loss: 2.799 (2.86)  Time: 0.667s, 1535.65/s  (0.664s, 1541.56/s)  LR: 5.002e-06  Data: 0.014 (0.015)
Train: 299 [1000/1251 ( 80%)]  Loss: 2.984 (2.86)  Time: 0.673s, 1521.34/s  (0.664s, 1541.42/s)  LR: 5.001e-06  Data: 0.013 (0.015)
Train: 299 [1050/1251 ( 84%)]  Loss: 2.922 (2.87)  Time: 0.667s, 1535.37/s  (0.664s, 1541.20/s)  LR: 5.001e-06  Data: 0.013 (0.015)
Train: 299 [1100/1251 ( 88%)]  Loss: 2.634 (2.86)  Time: 0.664s, 1542.70/s  (0.664s, 1541.04/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 299 [1150/1251 ( 92%)]  Loss: 2.534 (2.84)  Time: 0.671s, 1525.25/s  (0.665s, 1540.79/s)  LR: 5.000e-06  Data: 0.017 (0.015)
Train: 299 [1200/1251 ( 96%)]  Loss: 3.142 (2.86)  Time: 0.661s, 1550.17/s  (0.665s, 1540.52/s)  LR: 5.000e-06  Data: 0.014 (0.015)
Train: 299 [1250/1251 (100%)]  Loss: 2.981 (2.86)  Time: 0.657s, 1559.37/s  (0.665s, 1540.43/s)  LR: 5.000e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.023 (3.023)  Loss:  0.3652 (0.3652)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.165 (0.330)  Loss:  0.4885 (0.7886)  Acc@1: 87.8538 (81.7280)  Acc@5: 98.7028 (95.7800)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-299.pth.tar', 81.72800002441406)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-293.pth.tar', 81.71199997314453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-298.pth.tar', 81.67600002441407)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-294.pth.tar', 81.659999921875)

Train: 300 [   0/1251 (  0%)]  Loss: 2.775 (2.78)  Time: 3.214s,  318.57/s  (3.214s,  318.57/s)  LR: 5.000e-06  Data: 1.627 (1.627)
Train: 300 [  50/1251 (  4%)]  Loss: 3.118 (2.95)  Time: 0.655s, 1563.01/s  (0.680s, 1506.42/s)  LR: 5.000e-06  Data: 0.013 (0.045)
Train: 300 [ 100/1251 (  8%)]  Loss: 3.039 (2.98)  Time: 0.659s, 1552.90/s  (0.667s, 1535.06/s)  LR: 5.000e-06  Data: 0.013 (0.030)
Train: 300 [ 150/1251 ( 12%)]  Loss: 3.211 (3.04)  Time: 0.648s, 1580.92/s  (0.664s, 1541.69/s)  LR: 5.000e-06  Data: 0.013 (0.025)
Train: 300 [ 200/1251 ( 16%)]  Loss: 2.925 (3.01)  Time: 0.662s, 1547.28/s  (0.664s, 1543.15/s)  LR: 5.000e-06  Data: 0.014 (0.022)
Train: 300 [ 250/1251 ( 20%)]  Loss: 3.016 (3.01)  Time: 0.668s, 1531.82/s  (0.664s, 1542.78/s)  LR: 5.000e-06  Data: 0.013 (0.020)
Train: 300 [ 300/1251 ( 24%)]  Loss: 2.871 (2.99)  Time: 0.661s, 1550.21/s  (0.664s, 1542.31/s)  LR: 5.000e-06  Data: 0.013 (0.019)
Train: 300 [ 350/1251 ( 28%)]  Loss: 2.946 (2.99)  Time: 0.678s, 1509.91/s  (0.664s, 1541.88/s)  LR: 5.000e-06  Data: 0.013 (0.018)
Train: 300 [ 400/1251 ( 32%)]  Loss: 2.830 (2.97)  Time: 0.669s, 1529.91/s  (0.664s, 1541.96/s)  LR: 5.000e-06  Data: 0.012 (0.018)
Train: 300 [ 450/1251 ( 36%)]  Loss: 3.034 (2.98)  Time: 0.663s, 1544.21/s  (0.664s, 1542.42/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 300 [ 500/1251 ( 40%)]  Loss: 2.938 (2.97)  Time: 0.663s, 1545.37/s  (0.664s, 1542.48/s)  LR: 5.000e-06  Data: 0.014 (0.017)
Train: 300 [ 550/1251 ( 44%)]  Loss: 2.606 (2.94)  Time: 0.659s, 1552.99/s  (0.664s, 1542.95/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 300 [ 600/1251 ( 48%)]  Loss: 2.741 (2.93)  Time: 0.655s, 1562.52/s  (0.664s, 1543.28/s)  LR: 5.000e-06  Data: 0.017 (0.016)
Train: 300 [ 650/1251 ( 52%)]  Loss: 2.763 (2.92)  Time: 0.661s, 1549.27/s  (0.663s, 1543.52/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 300 [ 700/1251 ( 56%)]  Loss: 2.858 (2.91)  Time: 0.656s, 1560.36/s  (0.663s, 1543.86/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 300 [ 750/1251 ( 60%)]  Loss: 2.798 (2.90)  Time: 0.664s, 1541.74/s  (0.663s, 1544.22/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 300 [ 800/1251 ( 64%)]  Loss: 2.894 (2.90)  Time: 0.665s, 1540.56/s  (0.663s, 1544.16/s)  LR: 5.000e-06  Data: 0.012 (0.016)
Train: 300 [ 850/1251 ( 68%)]  Loss: 3.049 (2.91)  Time: 0.660s, 1550.51/s  (0.663s, 1544.06/s)  LR: 5.000e-06  Data: 0.014 (0.016)
Train: 300 [ 900/1251 ( 72%)]  Loss: 2.858 (2.91)  Time: 0.662s, 1545.75/s  (0.663s, 1544.15/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 300 [ 950/1251 ( 76%)]  Loss: 2.957 (2.91)  Time: 0.652s, 1570.59/s  (0.663s, 1544.34/s)  LR: 5.000e-06  Data: 0.015 (0.015)
Train: 300 [1000/1251 ( 80%)]  Loss: 2.804 (2.91)  Time: 0.669s, 1530.12/s  (0.663s, 1544.56/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 300 [1050/1251 ( 84%)]  Loss: 2.950 (2.91)  Time: 0.670s, 1527.53/s  (0.663s, 1544.49/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 300 [1100/1251 ( 88%)]  Loss: 2.850 (2.91)  Time: 0.668s, 1532.69/s  (0.663s, 1544.43/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 300 [1150/1251 ( 92%)]  Loss: 3.104 (2.91)  Time: 0.666s, 1537.20/s  (0.663s, 1544.23/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 300 [1200/1251 ( 96%)]  Loss: 2.830 (2.91)  Time: 0.658s, 1555.20/s  (0.663s, 1544.16/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 300 [1250/1251 (100%)]  Loss: 2.585 (2.90)  Time: 0.655s, 1562.45/s  (0.663s, 1544.25/s)  LR: 5.000e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.808 (2.808)  Loss:  0.3691 (0.3691)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.162 (0.321)  Loss:  0.4895 (0.7905)  Acc@1: 87.6179 (81.6220)  Acc@5: 98.5849 (95.7460)
Train: 301 [   0/1251 (  0%)]  Loss: 2.893 (2.89)  Time: 3.245s,  315.61/s  (3.245s,  315.61/s)  LR: 5.000e-06  Data: 1.698 (1.698)
Train: 301 [  50/1251 (  4%)]  Loss: 2.816 (2.85)  Time: 0.651s, 1573.28/s  (0.679s, 1508.82/s)  LR: 5.000e-06  Data: 0.013 (0.047)
Train: 301 [ 100/1251 (  8%)]  Loss: 2.853 (2.85)  Time: 0.652s, 1571.52/s  (0.665s, 1539.42/s)  LR: 5.000e-06  Data: 0.013 (0.030)
Train: 301 [ 150/1251 ( 12%)]  Loss: 2.990 (2.89)  Time: 0.654s, 1564.82/s  (0.662s, 1546.36/s)  LR: 5.000e-06  Data: 0.014 (0.025)
Train: 301 [ 200/1251 ( 16%)]  Loss: 3.045 (2.92)  Time: 0.662s, 1546.70/s  (0.661s, 1549.02/s)  LR: 5.000e-06  Data: 0.014 (0.022)
Train: 301 [ 250/1251 ( 20%)]  Loss: 2.699 (2.88)  Time: 0.655s, 1563.58/s  (0.661s, 1550.02/s)  LR: 5.000e-06  Data: 0.014 (0.020)
Train: 301 [ 300/1251 ( 24%)]  Loss: 3.113 (2.92)  Time: 0.661s, 1550.04/s  (0.660s, 1550.59/s)  LR: 5.000e-06  Data: 0.012 (0.019)
Train: 301 [ 350/1251 ( 28%)]  Loss: 2.857 (2.91)  Time: 0.663s, 1544.22/s  (0.660s, 1550.62/s)  LR: 5.000e-06  Data: 0.013 (0.019)
Train: 301 [ 400/1251 ( 32%)]  Loss: 2.716 (2.89)  Time: 0.658s, 1556.64/s  (0.660s, 1550.42/s)  LR: 5.000e-06  Data: 0.013 (0.018)
Train: 301 [ 450/1251 ( 36%)]  Loss: 2.813 (2.88)  Time: 0.654s, 1566.54/s  (0.661s, 1550.13/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 301 [ 500/1251 ( 40%)]  Loss: 2.806 (2.87)  Time: 0.661s, 1549.52/s  (0.661s, 1550.06/s)  LR: 5.000e-06  Data: 0.017 (0.017)
Train: 301 [ 550/1251 ( 44%)]  Loss: 3.086 (2.89)  Time: 0.651s, 1572.36/s  (0.661s, 1550.08/s)  LR: 5.000e-06  Data: 0.012 (0.017)
Train: 301 [ 600/1251 ( 48%)]  Loss: 2.756 (2.88)  Time: 0.664s, 1543.22/s  (0.661s, 1550.11/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 301 [ 650/1251 ( 52%)]  Loss: 2.732 (2.87)  Time: 0.650s, 1575.71/s  (0.660s, 1550.43/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 301 [ 700/1251 ( 56%)]  Loss: 3.107 (2.89)  Time: 0.653s, 1568.12/s  (0.660s, 1550.76/s)  LR: 5.000e-06  Data: 0.014 (0.016)
Train: 301 [ 750/1251 ( 60%)]  Loss: 3.066 (2.90)  Time: 0.666s, 1537.84/s  (0.660s, 1550.90/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 301 [ 800/1251 ( 64%)]  Loss: 3.042 (2.91)  Time: 0.661s, 1549.67/s  (0.660s, 1550.99/s)  LR: 5.000e-06  Data: 0.014 (0.016)
Train: 301 [ 850/1251 ( 68%)]  Loss: 2.674 (2.89)  Time: 0.658s, 1556.20/s  (0.660s, 1550.91/s)  LR: 5.000e-06  Data: 0.014 (0.016)
Train: 301 [ 900/1251 ( 72%)]  Loss: 2.772 (2.89)  Time: 0.657s, 1559.46/s  (0.660s, 1551.05/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 301 [ 950/1251 ( 76%)]  Loss: 2.983 (2.89)  Time: 0.661s, 1549.27/s  (0.660s, 1551.02/s)  LR: 5.000e-06  Data: 0.014 (0.015)
Train: 301 [1000/1251 ( 80%)]  Loss: 3.186 (2.91)  Time: 0.649s, 1578.76/s  (0.660s, 1551.19/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 301 [1050/1251 ( 84%)]  Loss: 3.016 (2.91)  Time: 0.660s, 1551.16/s  (0.660s, 1551.29/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 301 [1100/1251 ( 88%)]  Loss: 3.008 (2.91)  Time: 0.662s, 1545.94/s  (0.660s, 1551.46/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 301 [1150/1251 ( 92%)]  Loss: 2.885 (2.91)  Time: 0.655s, 1563.37/s  (0.660s, 1551.86/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 301 [1200/1251 ( 96%)]  Loss: 3.018 (2.92)  Time: 0.653s, 1568.42/s  (0.660s, 1552.10/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 301 [1250/1251 (100%)]  Loss: 3.099 (2.92)  Time: 0.650s, 1574.32/s  (0.660s, 1552.48/s)  LR: 5.000e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.030 (3.030)  Loss:  0.3691 (0.3691)  Acc@1: 93.1641 (93.1641)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.162 (0.323)  Loss:  0.4937 (0.7930)  Acc@1: 87.5000 (81.5880)  Acc@5: 98.5849 (95.7360)
Train: 302 [   0/1251 (  0%)]  Loss: 3.212 (3.21)  Time: 3.702s,  276.63/s  (3.702s,  276.63/s)  LR: 5.000e-06  Data: 1.669 (1.669)
Train: 302 [  50/1251 (  4%)]  Loss: 2.798 (3.01)  Time: 0.643s, 1591.61/s  (0.680s, 1505.96/s)  LR: 5.000e-06  Data: 0.013 (0.047)
Train: 302 [ 100/1251 (  8%)]  Loss: 2.921 (2.98)  Time: 0.664s, 1542.72/s  (0.665s, 1540.01/s)  LR: 5.000e-06  Data: 0.013 (0.031)
Train: 302 [ 150/1251 ( 12%)]  Loss: 2.760 (2.92)  Time: 0.654s, 1566.64/s  (0.661s, 1548.98/s)  LR: 5.000e-06  Data: 0.013 (0.025)
Train: 302 [ 200/1251 ( 16%)]  Loss: 2.842 (2.91)  Time: 0.659s, 1555.04/s  (0.659s, 1553.28/s)  LR: 5.000e-06  Data: 0.014 (0.022)
Train: 302 [ 250/1251 ( 20%)]  Loss: 2.862 (2.90)  Time: 0.666s, 1538.55/s  (0.658s, 1556.08/s)  LR: 5.000e-06  Data: 0.013 (0.021)
Train: 302 [ 300/1251 ( 24%)]  Loss: 2.926 (2.90)  Time: 0.659s, 1553.89/s  (0.658s, 1557.34/s)  LR: 5.000e-06  Data: 0.014 (0.019)
Train: 302 [ 350/1251 ( 28%)]  Loss: 2.693 (2.88)  Time: 0.653s, 1567.21/s  (0.658s, 1557.24/s)  LR: 5.000e-06  Data: 0.013 (0.019)
Train: 302 [ 400/1251 ( 32%)]  Loss: 2.689 (2.86)  Time: 0.658s, 1555.94/s  (0.658s, 1557.22/s)  LR: 5.000e-06  Data: 0.018 (0.018)
Train: 302 [ 450/1251 ( 36%)]  Loss: 3.044 (2.87)  Time: 0.665s, 1539.24/s  (0.657s, 1557.45/s)  LR: 5.000e-06  Data: 0.014 (0.018)
Train: 302 [ 500/1251 ( 40%)]  Loss: 2.921 (2.88)  Time: 0.666s, 1536.71/s  (0.658s, 1557.20/s)  LR: 5.000e-06  Data: 0.015 (0.017)
Train: 302 [ 550/1251 ( 44%)]  Loss: 2.943 (2.88)  Time: 0.661s, 1549.58/s  (0.658s, 1557.16/s)  LR: 5.000e-06  Data: 0.014 (0.017)
Train: 302 [ 600/1251 ( 48%)]  Loss: 3.072 (2.90)  Time: 0.657s, 1558.82/s  (0.658s, 1557.08/s)  LR: 5.000e-06  Data: 0.015 (0.017)
Train: 302 [ 650/1251 ( 52%)]  Loss: 2.945 (2.90)  Time: 0.663s, 1544.49/s  (0.658s, 1556.61/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 302 [ 700/1251 ( 56%)]  Loss: 3.180 (2.92)  Time: 0.661s, 1549.53/s  (0.658s, 1556.04/s)  LR: 5.000e-06  Data: 0.017 (0.016)
Train: 302 [ 750/1251 ( 60%)]  Loss: 3.078 (2.93)  Time: 0.668s, 1532.77/s  (0.658s, 1555.97/s)  LR: 5.000e-06  Data: 0.014 (0.016)
Train: 302 [ 800/1251 ( 64%)]  Loss: 2.836 (2.92)  Time: 0.668s, 1532.62/s  (0.658s, 1556.08/s)  LR: 5.000e-06  Data: 0.015 (0.016)
Train: 302 [ 850/1251 ( 68%)]  Loss: 3.074 (2.93)  Time: 0.656s, 1561.66/s  (0.658s, 1556.16/s)  LR: 5.000e-06  Data: 0.014 (0.016)
Train: 302 [ 900/1251 ( 72%)]  Loss: 2.663 (2.92)  Time: 0.649s, 1577.77/s  (0.658s, 1556.09/s)  LR: 5.000e-06  Data: 0.016 (0.016)
Train: 302 [ 950/1251 ( 76%)]  Loss: 2.750 (2.91)  Time: 0.650s, 1576.26/s  (0.658s, 1556.03/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 302 [1000/1251 ( 80%)]  Loss: 2.737 (2.90)  Time: 0.648s, 1579.27/s  (0.658s, 1556.06/s)  LR: 5.000e-06  Data: 0.014 (0.015)
Train: 302 [1050/1251 ( 84%)]  Loss: 2.863 (2.90)  Time: 0.662s, 1546.76/s  (0.658s, 1556.18/s)  LR: 5.000e-06  Data: 0.012 (0.015)
Train: 302 [1100/1251 ( 88%)]  Loss: 2.772 (2.89)  Time: 0.660s, 1552.55/s  (0.658s, 1555.85/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 302 [1150/1251 ( 92%)]  Loss: 2.796 (2.89)  Time: 0.651s, 1572.50/s  (0.658s, 1555.57/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 302 [1200/1251 ( 96%)]  Loss: 2.996 (2.89)  Time: 0.665s, 1540.52/s  (0.658s, 1555.32/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 302 [1250/1251 (100%)]  Loss: 2.737 (2.89)  Time: 0.647s, 1581.52/s  (0.658s, 1555.20/s)  LR: 5.000e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.763 (2.763)  Loss:  0.3706 (0.3706)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.161 (0.316)  Loss:  0.4893 (0.7921)  Acc@1: 87.7358 (81.6100)  Acc@5: 98.7028 (95.7300)
Train: 303 [   0/1251 (  0%)]  Loss: 3.074 (3.07)  Time: 3.332s,  307.31/s  (3.332s,  307.31/s)  LR: 5.000e-06  Data: 1.699 (1.699)
Train: 303 [  50/1251 (  4%)]  Loss: 2.801 (2.94)  Time: 0.647s, 1583.13/s  (0.682s, 1500.56/s)  LR: 5.000e-06  Data: 0.015 (0.048)
Train: 303 [ 100/1251 (  8%)]  Loss: 2.516 (2.80)  Time: 0.660s, 1551.98/s  (0.668s, 1533.55/s)  LR: 5.000e-06  Data: 0.016 (0.031)
Train: 303 [ 150/1251 ( 12%)]  Loss: 2.777 (2.79)  Time: 0.648s, 1580.13/s  (0.665s, 1539.59/s)  LR: 5.000e-06  Data: 0.014 (0.025)
Train: 303 [ 200/1251 ( 16%)]  Loss: 3.195 (2.87)  Time: 0.666s, 1537.47/s  (0.664s, 1541.95/s)  LR: 5.000e-06  Data: 0.014 (0.022)
Train: 303 [ 250/1251 ( 20%)]  Loss: 2.668 (2.84)  Time: 0.653s, 1567.04/s  (0.663s, 1543.75/s)  LR: 5.000e-06  Data: 0.015 (0.021)
Train: 303 [ 300/1251 ( 24%)]  Loss: 2.701 (2.82)  Time: 0.660s, 1552.28/s  (0.663s, 1543.95/s)  LR: 5.000e-06  Data: 0.013 (0.020)
Train: 303 [ 350/1251 ( 28%)]  Loss: 2.896 (2.83)  Time: 0.666s, 1536.67/s  (0.663s, 1544.05/s)  LR: 5.000e-06  Data: 0.016 (0.019)
Train: 303 [ 400/1251 ( 32%)]  Loss: 2.931 (2.84)  Time: 0.663s, 1544.26/s  (0.663s, 1545.03/s)  LR: 5.000e-06  Data: 0.014 (0.018)
Train: 303 [ 450/1251 ( 36%)]  Loss: 2.717 (2.83)  Time: 0.658s, 1555.72/s  (0.662s, 1545.73/s)  LR: 5.000e-06  Data: 0.013 (0.018)
Train: 303 [ 500/1251 ( 40%)]  Loss: 2.876 (2.83)  Time: 0.661s, 1549.53/s  (0.662s, 1546.15/s)  LR: 5.000e-06  Data: 0.016 (0.017)
Train: 303 [ 550/1251 ( 44%)]  Loss: 2.672 (2.82)  Time: 0.663s, 1544.76/s  (0.662s, 1546.14/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 303 [ 600/1251 ( 48%)]  Loss: 3.145 (2.84)  Time: 0.672s, 1523.77/s  (0.662s, 1546.21/s)  LR: 5.000e-06  Data: 0.015 (0.017)
Train: 303 [ 650/1251 ( 52%)]  Loss: 2.957 (2.85)  Time: 0.664s, 1541.84/s  (0.662s, 1546.54/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 303 [ 700/1251 ( 56%)]  Loss: 2.951 (2.86)  Time: 0.653s, 1567.12/s  (0.662s, 1546.81/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 303 [ 750/1251 ( 60%)]  Loss: 2.722 (2.85)  Time: 0.657s, 1558.92/s  (0.662s, 1547.14/s)  LR: 5.000e-06  Data: 0.014 (0.016)
Train: 303 [ 800/1251 ( 64%)]  Loss: 2.953 (2.86)  Time: 0.659s, 1554.69/s  (0.662s, 1547.11/s)  LR: 5.000e-06  Data: 0.015 (0.016)
Train: 303 [ 850/1251 ( 68%)]  Loss: 3.126 (2.87)  Time: 0.659s, 1553.67/s  (0.662s, 1546.89/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 303 [ 900/1251 ( 72%)]  Loss: 3.261 (2.89)  Time: 0.663s, 1544.57/s  (0.662s, 1547.02/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 303 [ 950/1251 ( 76%)]  Loss: 2.937 (2.89)  Time: 0.665s, 1540.24/s  (0.662s, 1546.87/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 303 [1000/1251 ( 80%)]  Loss: 2.830 (2.89)  Time: 0.655s, 1563.62/s  (0.662s, 1547.07/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 303 [1050/1251 ( 84%)]  Loss: 2.704 (2.88)  Time: 0.664s, 1541.08/s  (0.662s, 1547.17/s)  LR: 5.000e-06  Data: 0.014 (0.015)
Train: 303 [1100/1251 ( 88%)]  Loss: 2.945 (2.88)  Time: 0.659s, 1554.11/s  (0.662s, 1547.36/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 303 [1150/1251 ( 92%)]  Loss: 2.839 (2.88)  Time: 0.660s, 1552.12/s  (0.662s, 1547.48/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 303 [1200/1251 ( 96%)]  Loss: 2.961 (2.89)  Time: 0.653s, 1569.08/s  (0.662s, 1547.61/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 303 [1250/1251 (100%)]  Loss: 2.890 (2.89)  Time: 0.655s, 1564.15/s  (0.662s, 1547.62/s)  LR: 5.000e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.864 (2.864)  Loss:  0.3696 (0.3696)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.162 (0.321)  Loss:  0.4893 (0.7913)  Acc@1: 87.7359 (81.6380)  Acc@5: 98.7028 (95.7220)
Train: 304 [   0/1251 (  0%)]  Loss: 3.097 (3.10)  Time: 3.513s,  291.46/s  (3.513s,  291.46/s)  LR: 5.000e-06  Data: 1.754 (1.754)
Train: 304 [  50/1251 (  4%)]  Loss: 3.020 (3.06)  Time: 0.640s, 1599.82/s  (0.678s, 1510.65/s)  LR: 5.000e-06  Data: 0.018 (0.049)
Train: 304 [ 100/1251 (  8%)]  Loss: 2.609 (2.91)  Time: 0.665s, 1540.15/s  (0.666s, 1537.40/s)  LR: 5.000e-06  Data: 0.012 (0.032)
Train: 304 [ 150/1251 ( 12%)]  Loss: 2.807 (2.88)  Time: 0.664s, 1541.96/s  (0.663s, 1543.60/s)  LR: 5.000e-06  Data: 0.012 (0.026)
Train: 304 [ 200/1251 ( 16%)]  Loss: 3.140 (2.93)  Time: 0.660s, 1551.31/s  (0.663s, 1544.91/s)  LR: 5.000e-06  Data: 0.013 (0.023)
Train: 304 [ 250/1251 ( 20%)]  Loss: 3.060 (2.96)  Time: 0.647s, 1581.69/s  (0.663s, 1544.87/s)  LR: 5.000e-06  Data: 0.013 (0.021)
Train: 304 [ 300/1251 ( 24%)]  Loss: 2.861 (2.94)  Time: 0.672s, 1522.81/s  (0.663s, 1545.13/s)  LR: 5.000e-06  Data: 0.013 (0.020)
Train: 304 [ 350/1251 ( 28%)]  Loss: 2.916 (2.94)  Time: 0.672s, 1523.98/s  (0.663s, 1544.71/s)  LR: 5.000e-06  Data: 0.014 (0.019)
Train: 304 [ 400/1251 ( 32%)]  Loss: 2.714 (2.91)  Time: 0.665s, 1539.20/s  (0.663s, 1544.43/s)  LR: 5.000e-06  Data: 0.013 (0.018)
Train: 304 [ 450/1251 ( 36%)]  Loss: 2.994 (2.92)  Time: 0.667s, 1535.05/s  (0.663s, 1544.25/s)  LR: 5.000e-06  Data: 0.011 (0.018)
Train: 304 [ 500/1251 ( 40%)]  Loss: 2.921 (2.92)  Time: 0.676s, 1515.70/s  (0.663s, 1543.69/s)  LR: 5.000e-06  Data: 0.014 (0.017)
Train: 304 [ 550/1251 ( 44%)]  Loss: 2.822 (2.91)  Time: 0.661s, 1548.60/s  (0.663s, 1543.88/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 304 [ 600/1251 ( 48%)]  Loss: 3.048 (2.92)  Time: 0.661s, 1550.11/s  (0.663s, 1543.78/s)  LR: 5.000e-06  Data: 0.014 (0.017)
Train: 304 [ 650/1251 ( 52%)]  Loss: 2.885 (2.92)  Time: 0.666s, 1536.55/s  (0.663s, 1544.01/s)  LR: 5.000e-06  Data: 0.019 (0.016)
Train: 304 [ 700/1251 ( 56%)]  Loss: 2.907 (2.92)  Time: 0.659s, 1553.01/s  (0.663s, 1544.06/s)  LR: 5.000e-06  Data: 0.014 (0.016)
Train: 304 [ 750/1251 ( 60%)]  Loss: 2.701 (2.91)  Time: 0.666s, 1537.57/s  (0.663s, 1543.99/s)  LR: 5.000e-06  Data: 0.015 (0.016)
Train: 304 [ 800/1251 ( 64%)]  Loss: 3.051 (2.91)  Time: 0.657s, 1558.01/s  (0.663s, 1544.20/s)  LR: 5.000e-06  Data: 0.017 (0.016)
Train: 304 [ 850/1251 ( 68%)]  Loss: 2.909 (2.91)  Time: 0.662s, 1547.47/s  (0.663s, 1544.36/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 304 [ 900/1251 ( 72%)]  Loss: 2.945 (2.92)  Time: 0.664s, 1542.20/s  (0.663s, 1544.02/s)  LR: 5.000e-06  Data: 0.016 (0.016)
Train: 304 [ 950/1251 ( 76%)]  Loss: 2.701 (2.91)  Time: 0.665s, 1538.85/s  (0.663s, 1544.00/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 304 [1000/1251 ( 80%)]  Loss: 3.151 (2.92)  Time: 0.666s, 1537.91/s  (0.663s, 1543.65/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 304 [1050/1251 ( 84%)]  Loss: 2.863 (2.91)  Time: 0.671s, 1526.17/s  (0.663s, 1543.44/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 304 [1100/1251 ( 88%)]  Loss: 2.675 (2.90)  Time: 0.655s, 1564.17/s  (0.663s, 1543.48/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 304 [1150/1251 ( 92%)]  Loss: 2.968 (2.91)  Time: 0.669s, 1529.81/s  (0.663s, 1543.42/s)  LR: 5.000e-06  Data: 0.012 (0.015)
Train: 304 [1200/1251 ( 96%)]  Loss: 2.937 (2.91)  Time: 0.651s, 1572.44/s  (0.664s, 1543.31/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 304 [1250/1251 (100%)]  Loss: 2.984 (2.91)  Time: 0.646s, 1584.98/s  (0.663s, 1543.41/s)  LR: 5.000e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.827 (2.827)  Loss:  0.3674 (0.3674)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.160 (0.318)  Loss:  0.4875 (0.7913)  Acc@1: 87.6179 (81.6160)  Acc@5: 98.8207 (95.6980)
Train: 305 [   0/1251 (  0%)]  Loss: 3.138 (3.14)  Time: 3.436s,  298.06/s  (3.436s,  298.06/s)  LR: 5.000e-06  Data: 1.824 (1.824)
Train: 305 [  50/1251 (  4%)]  Loss: 2.912 (3.03)  Time: 0.656s, 1560.29/s  (0.685s, 1494.23/s)  LR: 5.000e-06  Data: 0.013 (0.049)
Train: 305 [ 100/1251 (  8%)]  Loss: 2.890 (2.98)  Time: 0.660s, 1551.60/s  (0.672s, 1524.18/s)  LR: 5.000e-06  Data: 0.014 (0.032)
Train: 305 [ 150/1251 ( 12%)]  Loss: 2.465 (2.85)  Time: 0.658s, 1557.16/s  (0.668s, 1533.56/s)  LR: 5.000e-06  Data: 0.013 (0.026)
Train: 305 [ 200/1251 ( 16%)]  Loss: 2.819 (2.84)  Time: 0.664s, 1542.31/s  (0.666s, 1536.90/s)  LR: 5.000e-06  Data: 0.014 (0.023)
Train: 305 [ 250/1251 ( 20%)]  Loss: 2.900 (2.85)  Time: 0.669s, 1531.70/s  (0.665s, 1539.81/s)  LR: 5.000e-06  Data: 0.013 (0.021)
Train: 305 [ 300/1251 ( 24%)]  Loss: 2.816 (2.85)  Time: 0.661s, 1549.25/s  (0.665s, 1540.65/s)  LR: 5.000e-06  Data: 0.014 (0.020)
Train: 305 [ 350/1251 ( 28%)]  Loss: 2.803 (2.84)  Time: 0.669s, 1530.87/s  (0.665s, 1540.68/s)  LR: 5.000e-06  Data: 0.014 (0.019)
Train: 305 [ 400/1251 ( 32%)]  Loss: 2.902 (2.85)  Time: 0.674s, 1519.57/s  (0.665s, 1540.92/s)  LR: 5.000e-06  Data: 0.014 (0.018)
Train: 305 [ 450/1251 ( 36%)]  Loss: 3.067 (2.87)  Time: 0.664s, 1541.60/s  (0.664s, 1541.02/s)  LR: 5.000e-06  Data: 0.013 (0.018)
Train: 305 [ 500/1251 ( 40%)]  Loss: 2.574 (2.84)  Time: 0.669s, 1530.83/s  (0.665s, 1540.64/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 305 [ 550/1251 ( 44%)]  Loss: 2.846 (2.84)  Time: 0.664s, 1541.55/s  (0.665s, 1540.93/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 305 [ 600/1251 ( 48%)]  Loss: 3.148 (2.87)  Time: 0.661s, 1548.21/s  (0.664s, 1541.60/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 305 [ 650/1251 ( 52%)]  Loss: 2.808 (2.86)  Time: 0.665s, 1538.71/s  (0.664s, 1542.30/s)  LR: 5.000e-06  Data: 0.012 (0.016)
Train: 305 [ 700/1251 ( 56%)]  Loss: 2.890 (2.87)  Time: 0.649s, 1576.96/s  (0.664s, 1542.84/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 305 [ 750/1251 ( 60%)]  Loss: 2.888 (2.87)  Time: 0.656s, 1559.79/s  (0.664s, 1543.17/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 305 [ 800/1251 ( 64%)]  Loss: 2.873 (2.87)  Time: 0.676s, 1515.27/s  (0.663s, 1543.38/s)  LR: 5.000e-06  Data: 0.014 (0.016)
Train: 305 [ 850/1251 ( 68%)]  Loss: 2.877 (2.87)  Time: 0.653s, 1568.58/s  (0.663s, 1543.67/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 305 [ 900/1251 ( 72%)]  Loss: 2.988 (2.87)  Time: 0.664s, 1543.30/s  (0.663s, 1543.97/s)  LR: 5.000e-06  Data: 0.014 (0.016)
Train: 305 [ 950/1251 ( 76%)]  Loss: 2.897 (2.88)  Time: 0.661s, 1550.04/s  (0.663s, 1544.10/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 305 [1000/1251 ( 80%)]  Loss: 2.872 (2.87)  Time: 0.661s, 1549.19/s  (0.663s, 1544.12/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 305 [1050/1251 ( 84%)]  Loss: 2.894 (2.88)  Time: 0.665s, 1539.94/s  (0.663s, 1544.21/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 305 [1100/1251 ( 88%)]  Loss: 2.988 (2.88)  Time: 0.661s, 1548.87/s  (0.663s, 1544.20/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 305 [1150/1251 ( 92%)]  Loss: 3.033 (2.89)  Time: 0.661s, 1548.43/s  (0.663s, 1544.24/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 305 [1200/1251 ( 96%)]  Loss: 2.858 (2.89)  Time: 0.657s, 1558.05/s  (0.663s, 1544.50/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 305 [1250/1251 (100%)]  Loss: 3.042 (2.89)  Time: 0.660s, 1551.15/s  (0.663s, 1544.50/s)  LR: 5.000e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.906 (2.906)  Loss:  0.3645 (0.3645)  Acc@1: 93.2617 (93.2617)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.167 (0.314)  Loss:  0.4817 (0.7879)  Acc@1: 88.2076 (81.6620)  Acc@5: 98.5849 (95.7480)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-299.pth.tar', 81.72800002441406)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-293.pth.tar', 81.71199997314453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-298.pth.tar', 81.67600002441407)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-305.pth.tar', 81.66200004882812)

Train: 306 [   0/1251 (  0%)]  Loss: 2.894 (2.89)  Time: 3.245s,  315.57/s  (3.245s,  315.57/s)  LR: 5.000e-06  Data: 1.622 (1.622)
Train: 306 [  50/1251 (  4%)]  Loss: 2.768 (2.83)  Time: 0.643s, 1591.87/s  (0.680s, 1506.71/s)  LR: 5.000e-06  Data: 0.014 (0.045)
Train: 306 [ 100/1251 (  8%)]  Loss: 3.084 (2.92)  Time: 0.657s, 1557.99/s  (0.667s, 1535.46/s)  LR: 5.000e-06  Data: 0.012 (0.030)
Train: 306 [ 150/1251 ( 12%)]  Loss: 2.692 (2.86)  Time: 0.663s, 1545.07/s  (0.664s, 1541.91/s)  LR: 5.000e-06  Data: 0.013 (0.025)
Train: 306 [ 200/1251 ( 16%)]  Loss: 2.742 (2.84)  Time: 0.655s, 1564.50/s  (0.664s, 1542.54/s)  LR: 5.000e-06  Data: 0.016 (0.022)
Train: 306 [ 250/1251 ( 20%)]  Loss: 3.036 (2.87)  Time: 0.670s, 1529.10/s  (0.664s, 1541.57/s)  LR: 5.000e-06  Data: 0.013 (0.020)
Train: 306 [ 300/1251 ( 24%)]  Loss: 2.907 (2.87)  Time: 0.668s, 1532.61/s  (0.665s, 1540.25/s)  LR: 5.000e-06  Data: 0.013 (0.019)
Train: 306 [ 350/1251 ( 28%)]  Loss: 2.814 (2.87)  Time: 0.657s, 1558.12/s  (0.665s, 1540.23/s)  LR: 5.000e-06  Data: 0.013 (0.018)
Train: 306 [ 400/1251 ( 32%)]  Loss: 2.919 (2.87)  Time: 0.658s, 1555.23/s  (0.665s, 1540.17/s)  LR: 5.000e-06  Data: 0.014 (0.018)
Train: 306 [ 450/1251 ( 36%)]  Loss: 3.066 (2.89)  Time: 0.669s, 1531.49/s  (0.665s, 1540.76/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 306 [ 500/1251 ( 40%)]  Loss: 3.119 (2.91)  Time: 0.660s, 1552.59/s  (0.665s, 1540.79/s)  LR: 5.000e-06  Data: 0.015 (0.017)
Train: 306 [ 550/1251 ( 44%)]  Loss: 3.246 (2.94)  Time: 0.664s, 1541.79/s  (0.665s, 1540.87/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 306 [ 600/1251 ( 48%)]  Loss: 2.519 (2.91)  Time: 0.669s, 1530.98/s  (0.665s, 1540.37/s)  LR: 5.000e-06  Data: 0.016 (0.016)
Train: 306 [ 650/1251 ( 52%)]  Loss: 2.853 (2.90)  Time: 0.669s, 1530.51/s  (0.665s, 1539.61/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 306 [ 700/1251 ( 56%)]  Loss: 2.844 (2.90)  Time: 0.677s, 1513.43/s  (0.665s, 1539.21/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 306 [ 750/1251 ( 60%)]  Loss: 2.733 (2.89)  Time: 0.660s, 1552.25/s  (0.665s, 1538.89/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 306 [ 800/1251 ( 64%)]  Loss: 2.715 (2.88)  Time: 0.664s, 1541.89/s  (0.666s, 1538.54/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 306 [ 850/1251 ( 68%)]  Loss: 2.691 (2.87)  Time: 0.663s, 1544.70/s  (0.665s, 1539.07/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 306 [ 900/1251 ( 72%)]  Loss: 2.703 (2.86)  Time: 0.661s, 1549.58/s  (0.665s, 1539.14/s)  LR: 5.000e-06  Data: 0.016 (0.016)
Train: 306 [ 950/1251 ( 76%)]  Loss: 2.785 (2.86)  Time: 0.665s, 1539.09/s  (0.665s, 1539.13/s)  LR: 5.000e-06  Data: 0.014 (0.015)
Train: 306 [1000/1251 ( 80%)]  Loss: 3.057 (2.87)  Time: 0.665s, 1539.88/s  (0.665s, 1539.46/s)  LR: 5.000e-06  Data: 0.015 (0.015)
Train: 306 [1050/1251 ( 84%)]  Loss: 2.997 (2.87)  Time: 0.665s, 1539.16/s  (0.665s, 1539.90/s)  LR: 5.000e-06  Data: 0.014 (0.015)
Train: 306 [1100/1251 ( 88%)]  Loss: 2.835 (2.87)  Time: 0.664s, 1543.25/s  (0.665s, 1540.47/s)  LR: 5.000e-06  Data: 0.015 (0.015)
Train: 306 [1150/1251 ( 92%)]  Loss: 2.938 (2.87)  Time: 0.663s, 1544.88/s  (0.665s, 1540.85/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 306 [1200/1251 ( 96%)]  Loss: 3.116 (2.88)  Time: 0.659s, 1553.94/s  (0.664s, 1541.09/s)  LR: 5.000e-06  Data: 0.017 (0.015)
Train: 306 [1250/1251 (100%)]  Loss: 2.886 (2.88)  Time: 0.655s, 1563.25/s  (0.664s, 1541.28/s)  LR: 5.000e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.899 (2.899)  Loss:  0.3679 (0.3679)  Acc@1: 93.0664 (93.0664)  Acc@5: 98.4375 (98.4375)
Test: [  48/48]  Time: 0.162 (0.322)  Loss:  0.4829 (0.7887)  Acc@1: 87.6179 (81.6420)  Acc@5: 98.7028 (95.7480)
Train: 307 [   0/1251 (  0%)]  Loss: 2.658 (2.66)  Time: 3.413s,  299.99/s  (3.413s,  299.99/s)  LR: 5.000e-06  Data: 1.890 (1.890)
Train: 307 [  50/1251 (  4%)]  Loss: 3.048 (2.85)  Time: 0.648s, 1580.71/s  (0.681s, 1504.25/s)  LR: 5.000e-06  Data: 0.013 (0.051)
Train: 307 [ 100/1251 (  8%)]  Loss: 3.052 (2.92)  Time: 0.655s, 1563.12/s  (0.667s, 1535.96/s)  LR: 5.000e-06  Data: 0.013 (0.032)
Train: 307 [ 150/1251 ( 12%)]  Loss: 2.754 (2.88)  Time: 0.649s, 1576.84/s  (0.663s, 1544.17/s)  LR: 5.000e-06  Data: 0.012 (0.026)
Train: 307 [ 200/1251 ( 16%)]  Loss: 2.779 (2.86)  Time: 0.660s, 1550.97/s  (0.662s, 1546.57/s)  LR: 5.000e-06  Data: 0.013 (0.023)
Train: 307 [ 250/1251 ( 20%)]  Loss: 2.548 (2.81)  Time: 0.661s, 1548.52/s  (0.662s, 1547.14/s)  LR: 5.000e-06  Data: 0.013 (0.021)
Train: 307 [ 300/1251 ( 24%)]  Loss: 2.869 (2.82)  Time: 0.672s, 1523.64/s  (0.662s, 1546.30/s)  LR: 5.000e-06  Data: 0.016 (0.020)
Train: 307 [ 350/1251 ( 28%)]  Loss: 2.719 (2.80)  Time: 0.663s, 1544.66/s  (0.662s, 1546.14/s)  LR: 5.000e-06  Data: 0.013 (0.019)
Train: 307 [ 400/1251 ( 32%)]  Loss: 2.756 (2.80)  Time: 0.655s, 1563.22/s  (0.662s, 1545.76/s)  LR: 5.000e-06  Data: 0.013 (0.018)
Train: 307 [ 450/1251 ( 36%)]  Loss: 2.975 (2.82)  Time: 0.656s, 1560.16/s  (0.663s, 1545.01/s)  LR: 5.000e-06  Data: 0.014 (0.018)
Train: 307 [ 500/1251 ( 40%)]  Loss: 3.064 (2.84)  Time: 0.677s, 1512.97/s  (0.663s, 1544.48/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 307 [ 550/1251 ( 44%)]  Loss: 2.879 (2.84)  Time: 0.667s, 1535.95/s  (0.663s, 1544.31/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 307 [ 600/1251 ( 48%)]  Loss: 2.648 (2.83)  Time: 0.668s, 1533.90/s  (0.663s, 1543.48/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 307 [ 650/1251 ( 52%)]  Loss: 2.795 (2.82)  Time: 0.661s, 1549.78/s  (0.664s, 1543.03/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 307 [ 700/1251 ( 56%)]  Loss: 2.891 (2.83)  Time: 0.660s, 1551.61/s  (0.664s, 1542.65/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 307 [ 750/1251 ( 60%)]  Loss: 2.851 (2.83)  Time: 0.668s, 1532.35/s  (0.664s, 1542.68/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 307 [ 800/1251 ( 64%)]  Loss: 2.943 (2.84)  Time: 0.680s, 1506.30/s  (0.664s, 1542.33/s)  LR: 5.000e-06  Data: 0.012 (0.016)
Train: 307 [ 850/1251 ( 68%)]  Loss: 2.682 (2.83)  Time: 0.663s, 1543.69/s  (0.664s, 1542.15/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 307 [ 900/1251 ( 72%)]  Loss: 2.933 (2.83)  Time: 0.669s, 1530.73/s  (0.664s, 1541.91/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 307 [ 950/1251 ( 76%)]  Loss: 3.001 (2.84)  Time: 0.670s, 1528.88/s  (0.664s, 1541.35/s)  LR: 5.000e-06  Data: 0.014 (0.016)
Train: 307 [1000/1251 ( 80%)]  Loss: 2.846 (2.84)  Time: 0.657s, 1558.85/s  (0.664s, 1541.07/s)  LR: 5.000e-06  Data: 0.015 (0.016)
Train: 307 [1050/1251 ( 84%)]  Loss: 2.938 (2.85)  Time: 0.666s, 1536.89/s  (0.665s, 1540.75/s)  LR: 5.000e-06  Data: 0.014 (0.015)
Train: 307 [1100/1251 ( 88%)]  Loss: 2.638 (2.84)  Time: 0.673s, 1521.90/s  (0.665s, 1540.35/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 307 [1150/1251 ( 92%)]  Loss: 2.718 (2.83)  Time: 0.677s, 1511.55/s  (0.665s, 1540.13/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 307 [1200/1251 ( 96%)]  Loss: 2.923 (2.84)  Time: 0.673s, 1522.41/s  (0.665s, 1539.99/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 307 [1250/1251 (100%)]  Loss: 2.825 (2.84)  Time: 0.650s, 1574.53/s  (0.665s, 1540.11/s)  LR: 5.000e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.893 (2.893)  Loss:  0.3728 (0.3728)  Acc@1: 92.7734 (92.7734)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.316)  Loss:  0.4880 (0.7915)  Acc@1: 87.5000 (81.6880)  Acc@5: 98.7028 (95.7220)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-299.pth.tar', 81.72800002441406)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-293.pth.tar', 81.71199997314453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-307.pth.tar', 81.688)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-298.pth.tar', 81.67600002441407)

Train: 308 [   0/1251 (  0%)]  Loss: 2.573 (2.57)  Time: 3.046s,  336.14/s  (3.046s,  336.14/s)  LR: 5.000e-06  Data: 1.681 (1.681)
Train: 308 [  50/1251 (  4%)]  Loss: 2.682 (2.63)  Time: 0.653s, 1566.95/s  (0.675s, 1517.27/s)  LR: 5.000e-06  Data: 0.013 (0.047)
Train: 308 [ 100/1251 (  8%)]  Loss: 2.902 (2.72)  Time: 0.660s, 1550.87/s  (0.666s, 1538.57/s)  LR: 5.000e-06  Data: 0.014 (0.031)
Train: 308 [ 150/1251 ( 12%)]  Loss: 2.539 (2.67)  Time: 0.663s, 1543.54/s  (0.664s, 1541.67/s)  LR: 5.000e-06  Data: 0.013 (0.025)
Train: 308 [ 200/1251 ( 16%)]  Loss: 2.900 (2.72)  Time: 0.657s, 1558.62/s  (0.664s, 1542.07/s)  LR: 5.000e-06  Data: 0.013 (0.022)
Train: 308 [ 250/1251 ( 20%)]  Loss: 2.772 (2.73)  Time: 0.668s, 1532.74/s  (0.664s, 1542.01/s)  LR: 5.000e-06  Data: 0.014 (0.020)
Train: 308 [ 300/1251 ( 24%)]  Loss: 2.953 (2.76)  Time: 0.671s, 1525.74/s  (0.664s, 1541.55/s)  LR: 5.000e-06  Data: 0.015 (0.019)
Train: 308 [ 350/1251 ( 28%)]  Loss: 2.897 (2.78)  Time: 0.679s, 1508.88/s  (0.665s, 1540.65/s)  LR: 5.000e-06  Data: 0.012 (0.019)
Train: 308 [ 400/1251 ( 32%)]  Loss: 2.602 (2.76)  Time: 0.673s, 1521.07/s  (0.665s, 1540.15/s)  LR: 5.000e-06  Data: 0.013 (0.018)
Train: 308 [ 450/1251 ( 36%)]  Loss: 2.812 (2.76)  Time: 0.670s, 1528.51/s  (0.665s, 1539.92/s)  LR: 5.000e-06  Data: 0.014 (0.017)
Train: 308 [ 500/1251 ( 40%)]  Loss: 3.059 (2.79)  Time: 0.667s, 1535.50/s  (0.665s, 1539.84/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 308 [ 550/1251 ( 44%)]  Loss: 3.079 (2.81)  Time: 0.666s, 1536.77/s  (0.665s, 1539.92/s)  LR: 5.000e-06  Data: 0.014 (0.017)
Train: 308 [ 600/1251 ( 48%)]  Loss: 2.769 (2.81)  Time: 0.674s, 1520.37/s  (0.665s, 1539.72/s)  LR: 5.000e-06  Data: 0.016 (0.016)
Train: 308 [ 650/1251 ( 52%)]  Loss: 2.936 (2.82)  Time: 0.667s, 1534.49/s  (0.665s, 1539.41/s)  LR: 5.000e-06  Data: 0.019 (0.016)
Train: 308 [ 700/1251 ( 56%)]  Loss: 2.884 (2.82)  Time: 0.669s, 1531.26/s  (0.665s, 1538.82/s)  LR: 5.000e-06  Data: 0.016 (0.016)
Train: 308 [ 750/1251 ( 60%)]  Loss: 2.976 (2.83)  Time: 0.664s, 1542.90/s  (0.666s, 1538.41/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 308 [ 800/1251 ( 64%)]  Loss: 2.869 (2.84)  Time: 0.667s, 1535.25/s  (0.666s, 1538.15/s)  LR: 5.000e-06  Data: 0.016 (0.016)
Train: 308 [ 850/1251 ( 68%)]  Loss: 2.949 (2.84)  Time: 0.670s, 1527.62/s  (0.666s, 1538.24/s)  LR: 5.000e-06  Data: 0.014 (0.016)
Train: 308 [ 900/1251 ( 72%)]  Loss: 3.114 (2.86)  Time: 0.661s, 1549.10/s  (0.666s, 1538.45/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 308 [ 950/1251 ( 76%)]  Loss: 2.826 (2.85)  Time: 0.665s, 1539.50/s  (0.665s, 1538.74/s)  LR: 5.000e-06  Data: 0.014 (0.015)
Train: 308 [1000/1251 ( 80%)]  Loss: 2.946 (2.86)  Time: 0.657s, 1559.62/s  (0.665s, 1538.97/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 308 [1050/1251 ( 84%)]  Loss: 2.950 (2.86)  Time: 0.659s, 1554.21/s  (0.665s, 1539.07/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 308 [1100/1251 ( 88%)]  Loss: 2.869 (2.86)  Time: 0.661s, 1548.40/s  (0.665s, 1539.32/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 308 [1150/1251 ( 92%)]  Loss: 3.086 (2.87)  Time: 0.668s, 1533.76/s  (0.665s, 1539.48/s)  LR: 5.000e-06  Data: 0.016 (0.015)
Train: 308 [1200/1251 ( 96%)]  Loss: 2.947 (2.88)  Time: 0.665s, 1539.14/s  (0.665s, 1539.50/s)  LR: 5.000e-06  Data: 0.015 (0.015)
Train: 308 [1250/1251 (100%)]  Loss: 2.707 (2.87)  Time: 0.652s, 1570.11/s  (0.665s, 1539.56/s)  LR: 5.000e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 3.063 (3.063)  Loss:  0.3755 (0.3755)  Acc@1: 92.9688 (92.9688)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.162 (0.321)  Loss:  0.4902 (0.7910)  Acc@1: 87.9717 (81.7260)  Acc@5: 98.5849 (95.7600)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-299.pth.tar', 81.72800002441406)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-308.pth.tar', 81.72599994628906)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-293.pth.tar', 81.71199997314453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-307.pth.tar', 81.688)

Train: 309 [   0/1251 (  0%)]  Loss: 3.031 (3.03)  Time: 3.262s,  313.89/s  (3.262s,  313.89/s)  LR: 5.000e-06  Data: 2.108 (2.108)
Train: 309 [  50/1251 (  4%)]  Loss: 2.713 (2.87)  Time: 0.644s, 1589.50/s  (0.669s, 1530.07/s)  LR: 5.000e-06  Data: 0.014 (0.055)
Train: 309 [ 100/1251 (  8%)]  Loss: 2.920 (2.89)  Time: 0.655s, 1563.71/s  (0.660s, 1550.34/s)  LR: 5.000e-06  Data: 0.017 (0.034)
Train: 309 [ 150/1251 ( 12%)]  Loss: 2.718 (2.85)  Time: 0.661s, 1548.65/s  (0.660s, 1551.08/s)  LR: 5.000e-06  Data: 0.014 (0.028)
Train: 309 [ 200/1251 ( 16%)]  Loss: 2.902 (2.86)  Time: 0.673s, 1521.72/s  (0.661s, 1549.13/s)  LR: 5.000e-06  Data: 0.014 (0.024)
Train: 309 [ 250/1251 ( 20%)]  Loss: 2.989 (2.88)  Time: 0.668s, 1531.90/s  (0.662s, 1547.29/s)  LR: 5.000e-06  Data: 0.016 (0.022)
Train: 309 [ 300/1251 ( 24%)]  Loss: 3.024 (2.90)  Time: 0.664s, 1541.82/s  (0.662s, 1546.39/s)  LR: 5.000e-06  Data: 0.013 (0.021)
Train: 309 [ 350/1251 ( 28%)]  Loss: 3.076 (2.92)  Time: 0.660s, 1552.67/s  (0.662s, 1545.78/s)  LR: 5.000e-06  Data: 0.013 (0.020)
Train: 309 [ 400/1251 ( 32%)]  Loss: 2.735 (2.90)  Time: 0.668s, 1531.89/s  (0.663s, 1544.61/s)  LR: 5.000e-06  Data: 0.013 (0.019)
Train: 309 [ 450/1251 ( 36%)]  Loss: 2.886 (2.90)  Time: 0.669s, 1531.73/s  (0.663s, 1544.31/s)  LR: 5.000e-06  Data: 0.013 (0.018)
Train: 309 [ 500/1251 ( 40%)]  Loss: 2.855 (2.90)  Time: 0.660s, 1552.28/s  (0.663s, 1543.87/s)  LR: 5.000e-06  Data: 0.013 (0.018)
Train: 309 [ 550/1251 ( 44%)]  Loss: 3.074 (2.91)  Time: 0.663s, 1543.46/s  (0.663s, 1543.47/s)  LR: 5.000e-06  Data: 0.014 (0.017)
Train: 309 [ 600/1251 ( 48%)]  Loss: 3.038 (2.92)  Time: 0.666s, 1538.34/s  (0.664s, 1543.19/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 309 [ 650/1251 ( 52%)]  Loss: 3.014 (2.93)  Time: 0.659s, 1553.49/s  (0.664s, 1542.93/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 309 [ 700/1251 ( 56%)]  Loss: 2.785 (2.92)  Time: 0.662s, 1547.24/s  (0.664s, 1542.94/s)  LR: 5.000e-06  Data: 0.013 (0.017)
Train: 309 [ 750/1251 ( 60%)]  Loss: 2.868 (2.91)  Time: 0.665s, 1540.30/s  (0.664s, 1543.04/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 309 [ 800/1251 ( 64%)]  Loss: 2.909 (2.91)  Time: 0.657s, 1559.14/s  (0.664s, 1543.28/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 309 [ 850/1251 ( 68%)]  Loss: 2.951 (2.92)  Time: 0.673s, 1520.75/s  (0.663s, 1543.37/s)  LR: 5.000e-06  Data: 0.012 (0.016)
Train: 309 [ 900/1251 ( 72%)]  Loss: 3.045 (2.92)  Time: 0.662s, 1547.53/s  (0.663s, 1543.52/s)  LR: 5.000e-06  Data: 0.018 (0.016)
Train: 309 [ 950/1251 ( 76%)]  Loss: 3.209 (2.94)  Time: 0.664s, 1541.66/s  (0.663s, 1543.55/s)  LR: 5.000e-06  Data: 0.014 (0.016)
Train: 309 [1000/1251 ( 80%)]  Loss: 2.970 (2.94)  Time: 0.660s, 1551.99/s  (0.663s, 1543.51/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 309 [1050/1251 ( 84%)]  Loss: 2.918 (2.94)  Time: 0.666s, 1538.18/s  (0.663s, 1543.52/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 309 [1100/1251 ( 88%)]  Loss: 3.106 (2.95)  Time: 0.655s, 1562.56/s  (0.663s, 1543.74/s)  LR: 5.000e-06  Data: 0.013 (0.016)
Train: 309 [1150/1251 ( 92%)]  Loss: 3.020 (2.95)  Time: 0.657s, 1559.16/s  (0.663s, 1543.95/s)  LR: 5.000e-06  Data: 0.013 (0.015)
Train: 309 [1200/1251 ( 96%)]  Loss: 3.011 (2.95)  Time: 0.664s, 1542.62/s  (0.663s, 1544.02/s)  LR: 5.000e-06  Data: 0.014 (0.015)
Train: 309 [1250/1251 (100%)]  Loss: 2.934 (2.95)  Time: 0.657s, 1557.82/s  (0.663s, 1544.29/s)  LR: 5.000e-06  Data: 0.000 (0.015)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.764 (2.764)  Loss:  0.3726 (0.3726)  Acc@1: 92.8711 (92.8711)  Acc@5: 98.5352 (98.5352)
Test: [  48/48]  Time: 0.161 (0.321)  Loss:  0.4878 (0.7906)  Acc@1: 87.7359 (81.6880)  Acc@5: 98.4670 (95.7280)
Current checkpoints:
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-299.pth.tar', 81.72800002441406)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-308.pth.tar', 81.72599994628906)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-293.pth.tar', 81.71199997314453)
 ('/workspace/akane/runs/20231009-134638-nat_s_tiny-224/checkpoint-309.pth.tar', 81.68800010253906)

*** Best metric: 81.72800002441406 (epoch 299)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: best_metric ‚ñÅ
wandb:       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:   eval_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   eval_top1 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:   eval_top5 ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:  train_loss ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: best_metric 81.728
wandb:       epoch 309
wandb:   eval_loss 0.79062
wandb:   eval_top1 81.688
wandb:   eval_top5 95.728
wandb:  train_loss 2.95006
wandb: 
wandb: üöÄ View run nat_s_tiny at: https://wandb.ai/compyle/wintome-dinat-s-IN1k/runs/20231009-134638-nat_s_tiny-224
wandb: Ô∏è‚ö° View job at https://wandb.ai/compyle/wintome-dinat-s-IN1k/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNTMyNzI4Ng==/version_details/v1
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20231009_134641-20231009-134638-nat_s_tiny-224/logs
